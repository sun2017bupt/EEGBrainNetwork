/home/sjf/eegall/main.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,0],dtype=int)
/home/sjf/eegall/main.py:480: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:],dtype=int)
/home/sjf/eegall/main.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
/home/sjf/eegall/main.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
--------- FACED DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([123, 312, 32, 7]), harm_x: torch.Size([123, 312, 32, 7]) all_labels: torch.Size([123, 312]) 
 base_graph: torch.Size([123, 312, 32, 32]) harm_graph: torch.Size([123, 312, 32, 32])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
******** mix subject_0 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 0.11656580865383148
step: 100, Loss: 0.12341722846031189
step: 200, Loss: 0.11670227348804474
step: 300, Loss: 0.11661119759082794
step: 400, Loss: 0.11532986909151077
step: 500, Loss: 0.11646652221679688
step: 600, Loss: 0.11533939838409424
step: 700, Loss: 0.11600762605667114
step: 800, Loss: 0.11564227938652039
step: 900, Loss: 0.11537440121173859
step: 1000, Loss: 0.11562545597553253
step: 1100, Loss: 0.11475434899330139
step: 1200, Loss: 0.11556224524974823
step: 1300, Loss: 0.11700368672609329
step: 1400, Loss: 0.1187339723110199
step: 1500, Loss: 0.11461605876684189
step: 1600, Loss: 0.11548617482185364
step: 1700, Loss: 0.11613388359546661
step: 1800, Loss: 0.11561667919158936
step: 1900, Loss: 0.11503061652183533
step: 2000, Loss: 0.11464395374059677
step: 2100, Loss: 0.11645257472991943
step: 2200, Loss: 0.11478715389966965
step: 2300, Loss: 0.11596880853176117
step: 2400, Loss: 0.1185062974691391
step: 2500, Loss: 0.11703523993492126
step: 2600, Loss: 0.11500483006238937
step: 2700, Loss: 0.11736295372247696
step: 2800, Loss: 0.11426515132188797
step: 2900, Loss: 0.11730770766735077
step: 3000, Loss: 0.38226309418678284
step: 3100, Loss: 0.14844247698783875
step: 3200, Loss: 0.15267367660999298
step: 3300, Loss: 0.14065812528133392
step: 3400, Loss: 0.15033414959907532
step: 3500, Loss: 0.13113030791282654
step: 3600, Loss: 0.13752613961696625
step: 3700, Loss: 0.12797626852989197
step: 3800, Loss: 0.130417600274086
step: 3900, Loss: 0.12078259885311127
step: 4000, Loss: 0.12836353480815887
step: 4100, Loss: 0.11972008645534515
step: 4200, Loss: 0.12407183647155762
step: 4300, Loss: 0.11697334051132202
step: 4400, Loss: 0.12180881947278976
step: 4500, Loss: 0.1159207671880722
step: 4600, Loss: 0.12317563593387604
step: 4700, Loss: 0.11667550355195999
step: 4800, Loss: 0.11902114003896713
step: 4900, Loss: 0.11390658468008041
step: 5000, Loss: 0.11941342055797577
step: 5100, Loss: 0.11415750533342361
step: 5200, Loss: 0.11841708421707153
step: 5300, Loss: 0.11547748744487762
step: 5400, Loss: 0.11703905463218689
step: 5500, Loss: 0.11406709253787994
step: 5600, Loss: 0.11554187536239624
step: 5700, Loss: 0.11425239592790604
step: 5800, Loss: 0.11542792618274689
step: 5900, Loss: 0.1149686872959137
step: 6000, Loss: 0.11484948545694351
step: 6100, Loss: 0.11428910493850708
step: 6200, Loss: 0.1139383390545845
step: 6300, Loss: 0.11396781355142593
step: 6400, Loss: 0.1139468252658844
step: 6500, Loss: 0.11314932256937027
step: 6600, Loss: 0.11348595470190048
step: 6700, Loss: 0.11505717784166336
step: 6800, Loss: 0.11412917077541351
step: 6900, Loss: 0.11428643763065338
step: 7000, Loss: 0.11305525153875351
step: 7100, Loss: 0.11418819427490234
step: 7200, Loss: 0.11321195960044861
step: 7300, Loss: 0.11355487257242203
step: 7400, Loss: 0.11322943866252899
step: 7500, Loss: 0.11260301619768143
step: 7600, Loss: 0.1140051931142807
step: 7700, Loss: 0.1140734925866127
step: 7800, Loss: 0.11374949663877487
step: 7900, Loss: 0.11360932886600494
step: 8000, Loss: 0.11510307341814041
step: 8100, Loss: 0.11389927566051483
step: 8200, Loss: 0.11387807875871658
step: 8300, Loss: 0.11359386891126633
step: 8400, Loss: 0.11333435773849487
step: 8500, Loss: 0.11349008977413177
step: 8600, Loss: 0.11414498090744019
step: 8700, Loss: 0.11521995812654495
step: 8800, Loss: 0.11320820450782776
step: 8900, Loss: 0.11415506899356842
step: 9000, Loss: 0.11295093595981598
step: 9100, Loss: 0.11316663026809692
step: 9200, Loss: 0.11355967819690704
step: 9300, Loss: 0.11586107313632965
step: 9400, Loss: 0.11322327703237534
step: 9500, Loss: 0.11406542360782623
step: 9600, Loss: 0.11382158100605011
step: 9700, Loss: 0.11556785553693771
step: 9800, Loss: 0.11455871164798737
step: 9900, Loss: 0.11351381987333298
training successfully ended.
validating...
validate data length:32
acc: 0.5625
precision: 0.4375
recall: 0.5833333333333334
F_score: 0.5
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.6810911893844604
step: 100, Loss: 0.12338672578334808
step: 200, Loss: 0.12014862149953842
step: 300, Loss: 0.1163623109459877
step: 400, Loss: 0.11579232662916183
step: 500, Loss: 0.11470481753349304
step: 600, Loss: 0.11493251472711563
step: 700, Loss: 0.11339890956878662
step: 800, Loss: 0.11420989781618118
step: 900, Loss: 0.11405797302722931
step: 1000, Loss: 0.11334347724914551
step: 1100, Loss: 0.1127329170703888
step: 1200, Loss: 0.11439841985702515
step: 1300, Loss: 0.11290617287158966
step: 1400, Loss: 0.1128319501876831
step: 1500, Loss: 0.11332574486732483
step: 1600, Loss: 0.11392930150032043
step: 1700, Loss: 0.11328889429569244
step: 1800, Loss: 0.11394938081502914
step: 1900, Loss: 0.11299820244312286
step: 2000, Loss: 0.11433874070644379
step: 2100, Loss: 0.11242717504501343
step: 2200, Loss: 0.11299946159124374
step: 2300, Loss: 0.11339374631643295
step: 2400, Loss: 0.11356299370527267
step: 2500, Loss: 0.11323367804288864
step: 2600, Loss: 0.11344002187252045
step: 2700, Loss: 0.11443136632442474
step: 2800, Loss: 0.1146831288933754
step: 2900, Loss: 0.115141861140728
step: 3000, Loss: 0.113562673330307
step: 3100, Loss: 0.11322851479053497
step: 3200, Loss: 0.11360255628824234
step: 3300, Loss: 0.11608360707759857
step: 3400, Loss: 0.11436022073030472
step: 3500, Loss: 0.11304128915071487
step: 3600, Loss: 0.11474879831075668
step: 3700, Loss: 0.11400454491376877
step: 3800, Loss: 0.11407816410064697
step: 3900, Loss: 0.11408311128616333
step: 4000, Loss: 0.11377175897359848
step: 4100, Loss: 0.11441890150308609
step: 4200, Loss: 0.11420515179634094
step: 4300, Loss: 0.11514639854431152
step: 4400, Loss: 0.11530542373657227
step: 4500, Loss: 0.11574458330869675
step: 4600, Loss: 0.115170419216156
step: 4700, Loss: 0.11526040732860565
step: 4800, Loss: 0.11431849002838135
step: 4900, Loss: 0.11781475692987442
step: 5000, Loss: 0.11434179544448853
step: 5100, Loss: 0.11459322273731232
step: 5200, Loss: 0.11441203951835632
step: 5300, Loss: 0.11402715742588043
step: 5400, Loss: 0.11652126908302307
step: 5500, Loss: 0.11414226144552231
step: 5600, Loss: 0.11620891094207764
step: 5700, Loss: 1.2705376148223877
step: 5800, Loss: 0.20955891907215118
step: 5900, Loss: 0.14909224212169647
step: 6000, Loss: 0.1379057765007019
step: 6100, Loss: 0.13320207595825195
step: 6200, Loss: 0.12499512732028961
step: 6300, Loss: 0.12640480697155
step: 6400, Loss: 0.12586575746536255
step: 6500, Loss: 0.12385018169879913
step: 6600, Loss: 0.12001602351665497
step: 6700, Loss: 0.12286648154258728
step: 6800, Loss: 0.11758853495121002
step: 6900, Loss: 0.11943421512842178
step: 7000, Loss: 0.11758376657962799
step: 7100, Loss: 0.11588304489850998
step: 7200, Loss: 0.11654474586248398
step: 7300, Loss: 0.11618654429912567
step: 7400, Loss: 0.11559474468231201
step: 7500, Loss: 0.11661592870950699
step: 7600, Loss: 0.11483387649059296
step: 7700, Loss: 0.11509259790182114
step: 7800, Loss: 0.11502046883106232
step: 7900, Loss: 0.11437182873487473
step: 8000, Loss: 0.11596668511629105
step: 8100, Loss: 0.11421222239732742
step: 8200, Loss: 0.11473172158002853
step: 8300, Loss: 0.11482549458742142
step: 8400, Loss: 0.11307642608880997
step: 8500, Loss: 0.11426321417093277
step: 8600, Loss: 0.11471066623926163
step: 8700, Loss: 0.11329372972249985
step: 8800, Loss: 0.11464281380176544
step: 8900, Loss: 0.1133384108543396
step: 9000, Loss: 0.11572745442390442
step: 9100, Loss: 0.11367909610271454
step: 9200, Loss: 0.11415084451436996
step: 9300, Loss: 0.11467298865318298
step: 9400, Loss: 0.11437732726335526
step: 9500, Loss: 0.11578309535980225
step: 9600, Loss: 0.11364156752824783
step: 9700, Loss: 0.11389043182134628
step: 9800, Loss: 0.11552434414625168
--------- DEAP DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([32, 760, 40, 7]), harm_x: torch.Size([32, 760, 40, 7]) all_labels: torch.Size([32, 760, 4]) 
 base_graph: torch.Size([32, 760, 40, 40]) harm_graph: torch.Size([32, 760, 40, 40])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
******** mix subject_0 ********

[380, 380]
******fold 1******

*******Initializing new model*******
Training... train_data length:684
step: 0, Loss: 12.9747314453125
step: 100, Loss: 1.4354908466339111
step: 200, Loss: 0.35227006673812866
step: 300, Loss: 0.39059925079345703
step: 400, Loss: 0.1792203038930893
step: 500, Loss: 0.19107915461063385
step: 600, Loss: 0.15844008326530457
step: 700, Loss: 0.15320438146591187
step: 800, Loss: 0.1454312950372696
step: 900, Loss: 0.13745443522930145
step: 1000, Loss: 0.129627525806427
step: 1100, Loss: 0.13863228261470795
step: 1200, Loss: 0.13534173369407654
step: 1300, Loss: 0.13572444021701813
step: 1400, Loss: 0.13382557034492493
step: 1500, Loss: 0.21979506313800812
step: 1600, Loss: 0.13287730515003204
step: 1700, Loss: 0.129598930478096
step: 1800, Loss: 0.12684816122055054
step: 1900, Loss: 0.12228208780288696
step: 2000, Loss: 0.12184897065162659
step: 2100, Loss: 0.12268573045730591
step: 2200, Loss: 0.13030976057052612
step: 2300, Loss: 0.121649369597435
step: 2400, Loss: 0.12834903597831726
step: 2500, Loss: 0.12624971568584442
step: 2600, Loss: 0.12299810349941254
step: 2700, Loss: 0.12209566682577133
step: 2800, Loss: 0.12052807211875916
step: 2900, Loss: 0.1188836470246315
step: 3000, Loss: 0.12094762176275253
step: 3100, Loss: 0.12207217514514923
step: 3200, Loss: 0.12622791528701782
step: 3300, Loss: 0.1213793084025383
step: 3400, Loss: 0.1940385401248932
step: 3500, Loss: 0.11865349858999252
step: 3600, Loss: 0.11725889146327972
step: 3700, Loss: 0.11533816158771515
step: 3800, Loss: 0.11702664196491241
step: 3900, Loss: 0.1194177195429802
step: 4000, Loss: 0.12009582668542862
step: 4100, Loss: 0.12059164047241211
step: 4200, Loss: 0.14379717409610748
step: 4300, Loss: 1.1629441976547241
step: 4400, Loss: 2.328511953353882
step: 4500, Loss: 0.22977252304553986
step: 4600, Loss: 0.1494094878435135
step: 4700, Loss: 0.1515854001045227
step: 4800, Loss: 0.1479288637638092
step: 4900, Loss: 0.15306977927684784
step: 5000, Loss: 0.15372490882873535
step: 5100, Loss: 0.14167249202728271
step: 5200, Loss: 0.13921061158180237
step: 5300, Loss: 0.23753541707992554
step: 5400, Loss: 0.14501848816871643
step: 5500, Loss: 0.13132129609584808
step: 5600, Loss: 0.13699427247047424
step: 5700, Loss: 0.12553833425045013
step: 5800, Loss: 0.1283709704875946
step: 5900, Loss: 0.12625499069690704
step: 6000, Loss: 0.12783250212669373
step: 6100, Loss: 0.122398242354393
step: 6200, Loss: 0.12252435833215714
step: 6300, Loss: 0.12143436074256897
step: 6400, Loss: 0.12445022165775299
step: 6500, Loss: 0.12177542597055435
step: 6600, Loss: 0.12675325572490692
step: 6700, Loss: 0.11988396942615509
step: 6800, Loss: 0.1209942102432251
step: 6900, Loss: 0.13100454211235046
step: 7000, Loss: 0.12134087085723877
step: 7100, Loss: 0.11661380529403687
step: 7200, Loss: 0.20164094865322113
step: 7300, Loss: 0.1252254694700241
step: 7400, Loss: 0.1237504705786705
step: 7500, Loss: 0.11702986061573029
step: 7600, Loss: 0.12075111269950867
step: 7700, Loss: 0.11695194989442825
step: 7800, Loss: 0.12267808616161346
step: 7900, Loss: 0.11737017333507538
step: 8000, Loss: 0.12036645412445068
step: 8100, Loss: 0.11928721517324448
step: 8200, Loss: 0.11705499142408371
step: 8300, Loss: 0.12164198607206345
step: 8400, Loss: 0.11688873171806335
step: 8500, Loss: 0.1150643602013588
step: 8600, Loss: 0.11475671827793121
step: 8700, Loss: 0.11797120422124863
step: 8800, Loss: 0.11928943544626236
step: 8900, Loss: 0.11640964448451996
step: 9000, Loss: 0.11476302891969681
step: 9100, Loss: 0.1984146535396576
step: 9200, Loss: 0.11520202457904816
step: 9300, Loss: 0.12649022042751312
step: 9400, Loss: 0.11849738657474518
step: 9500, Loss: 0.11939828842878342
step: 9600, Loss: 0.11554235965013504
step: 9700, Loss: 0.11892654001712799
step: 9800, Loss: 0.13076472282409668
step: 9900, Loss: 0.1443224400281906
training successfully ended.
validating...
validate data length:76
acc: 0.8194444444444444
precision: 0.8484848484848485
recall: 0.7777777777777778
F_score: 0.8115942028985507
******fold 2******

Training... train_data length:684
step: 0, Loss: 2.5542757511138916
step: 100, Loss: 0.1361747831106186
step: 200, Loss: 0.13836778700351715
step: 300, Loss: 0.11936528235673904
step: 400, Loss: 0.12016430497169495
step: 500, Loss: 0.11886057257652283
step: 600, Loss: 0.11821022629737854
step: 700, Loss: 0.11785455048084259
step: 800, Loss: 0.11772986501455307
step: 900, Loss: 0.1157546192407608
step: 1000, Loss: 0.11522336304187775
step: 1100, Loss: 0.11433365195989609
step: 1200, Loss: 0.11558151245117188
step: 1300, Loss: 0.11660264432430267
step: 1400, Loss: 0.11375760287046432
step: 1500, Loss: 0.19597046077251434
step: 1600, Loss: 0.11670156568288803
step: 1700, Loss: 0.11399626731872559
step: 1800, Loss: 0.11391571909189224
step: 1900, Loss: 0.11657542735338211
step: 2000, Loss: 0.14152979850769043
step: 2100, Loss: 0.20324665307998657
step: 2200, Loss: 0.416637122631073
step: 2300, Loss: 0.1694764494895935
step: 2400, Loss: 0.1613215208053589
step: 2500, Loss: 0.14840282499790192
step: 2600, Loss: 0.14713844656944275
step: 2700, Loss: 0.13459330797195435
step: 2800, Loss: 0.13177213072776794
step: 2900, Loss: 0.12876513600349426
step: 3000, Loss: 0.12105531990528107
step: 3100, Loss: 0.12284557521343231
step: 3200, Loss: 0.12535840272903442
step: 3300, Loss: 0.12558218836784363
step: 3400, Loss: 0.2177535742521286
step: 3500, Loss: 0.11904637515544891
step: 3600, Loss: 0.12402994930744171
step: 3700, Loss: 0.12122474610805511
step: 3800, Loss: 0.12370534241199493
step: 3900, Loss: 0.12331120669841766
step: 4000, Loss: 0.11845951527357101
step: 4100, Loss: 0.12101438641548157
step: 4200, Loss: 0.11833613365888596
step: 4300, Loss: 0.1213286817073822
step: 4400, Loss: 0.12005986273288727
step: 4500, Loss: 0.11992219090461731
step: 4600, Loss: 0.11868107318878174
step: 4700, Loss: 0.11947479099035263
step: 4800, Loss: 0.11989668756723404
step: 4900, Loss: 0.11626232415437698
step: 5000, Loss: 0.11853517591953278
step: 5100, Loss: 0.12033908814191818
step: 5200, Loss: 0.11563671380281448
step: 5300, Loss: 0.19505402445793152
step: 5400, Loss: 0.11518289148807526
step: 5500, Loss: 0.1177976205945015
step: 5600, Loss: 0.11722684651613235
step: 5700, Loss: 0.11511851847171783
step: 5800, Loss: 0.11858217418193817
step: 5900, Loss: 0.11524761468172073
step: 6000, Loss: 0.11781075596809387
step: 6100, Loss: 0.11859893053770065
step: 6200, Loss: 0.11500412970781326
step: 6300, Loss: 0.1149999275803566
step: 6400, Loss: 0.11598175764083862
step: 6500, Loss: 0.11497265100479126
step: 6600, Loss: 0.11652817577123642
step: 6700, Loss: 0.11647042632102966
step: 6800, Loss: 0.12491142749786377
step: 6900, Loss: 0.11632154881954193
step: 7000, Loss: 0.12316659837961197
step: 7100, Loss: 0.11805517971515656
step: 7200, Loss: 0.19304485619068146
step: 7300, Loss: 0.1167084276676178
step: 7400, Loss: 0.11793593317270279
step: 7500, Loss: 0.11601262539625168
step: 7600, Loss: 0.1187148317694664
step: 7700, Loss: 0.12459980696439743
step: 7800, Loss: 0.115333691239357
step: 7900, Loss: 0.13229215145111084
step: 8000, Loss: 0.11785843223333359
step: 8100, Loss: 0.1318494826555252
step: 8200, Loss: 0.11567626893520355
step: 8300, Loss: 0.12449768930673599
step: 8400, Loss: 0.11803462356328964
step: 8500, Loss: 0.12072810530662537
step: 8600, Loss: 0.11801615357398987
step: 8700, Loss: 0.12256985157728195
step: 8800, Loss: 0.12765172123908997
step: 8900, Loss: 0.12219592928886414
step: 9000, Loss: 2.1178691387176514
step: 9100, Loss: 0.3528457283973694
step: 9200, Loss: 0.14746245741844177
step: 9300, Loss: 0.13628776371479034
step: 9400, Loss: 0.13959702849388123
step: 9500, Loss: 0.13362792134284973
step: 9600, Loss: 0.13268445432186127
step: 9900, Loss: 0.11334235221147537
training successfully ended.
validating...
validate data length:32
acc: 0.65625
precision: 0.6086956521739131
recall: 0.875
F_score: 0.717948717948718
******fold 3******

Training... train_data length:281
step: 0, Loss: 10.611198425292969
step: 100, Loss: 0.12282232195138931
step: 200, Loss: 0.1187862902879715
step: 300, Loss: 0.11618994176387787
step: 400, Loss: 0.11398838460445404
step: 500, Loss: 0.11501553654670715
step: 600, Loss: 0.11548227071762085
step: 700, Loss: 0.11333715170621872
step: 800, Loss: 0.11445450037717819
step: 900, Loss: 0.11313097923994064
step: 1000, Loss: 0.11516617238521576
step: 1100, Loss: 0.11324066668748856
step: 1200, Loss: 0.11453419178724289
step: 1300, Loss: 0.11547709256410599
step: 1400, Loss: 0.11447583138942719
step: 1500, Loss: 0.11464785039424896
step: 1600, Loss: 0.1164301335811615
step: 1700, Loss: 0.11354506015777588
step: 1800, Loss: 0.11375874280929565
step: 1900, Loss: 0.11275007575750351
step: 2000, Loss: 0.11388915032148361
step: 2100, Loss: 0.11563058197498322
step: 2200, Loss: 0.11429695785045624
step: 2300, Loss: 0.1136479303240776
step: 2400, Loss: 0.11318126320838928
step: 2500, Loss: 0.11366676539182663
step: 2600, Loss: 0.11407124996185303
step: 2700, Loss: 0.1148538887500763
step: 2800, Loss: 0.11423832923173904
step: 2900, Loss: 0.11424805223941803
step: 3000, Loss: 0.11370617896318436
step: 3100, Loss: 0.1133938580751419
step: 3200, Loss: 0.11536981165409088
step: 3300, Loss: 0.11560982465744019
step: 3400, Loss: 0.11324083805084229
step: 3500, Loss: 0.1144850105047226
step: 3600, Loss: 0.11315707862377167
step: 3700, Loss: 0.11468016356229782
step: 3800, Loss: 0.1145300567150116
step: 3900, Loss: 0.11582580953836441
step: 4000, Loss: 0.11373132467269897
step: 4100, Loss: 0.11452025175094604
step: 4200, Loss: 0.11350779235363007
step: 4300, Loss: 0.11455953121185303
step: 4400, Loss: 0.11619247496128082
step: 4500, Loss: 0.11397837847471237
step: 4600, Loss: 0.1139983981847763
step: 4700, Loss: 0.11393487453460693
step: 4800, Loss: 0.11384144425392151
step: 4900, Loss: 0.11523178964853287
step: 5000, Loss: 0.11378657072782516
step: 5100, Loss: 0.11368995904922485
step: 5200, Loss: 0.11384311318397522
step: 5300, Loss: 0.11474296450614929
step: 5400, Loss: 0.11602771282196045
step: 5500, Loss: 0.11599726974964142
step: 5600, Loss: 0.11497434973716736
step: 5700, Loss: 0.114158034324646
step: 5800, Loss: 0.11500157415866852
step: 5900, Loss: 0.11581405252218246
step: 6000, Loss: 0.1149052307009697
step: 6100, Loss: 0.11368656903505325
step: 6200, Loss: 0.1166665330529213
step: 6300, Loss: 0.11563676595687866
step: 6400, Loss: 0.11557772755622864
step: 6500, Loss: 0.11689922958612442
step: 6600, Loss: 0.11402680724859238
step: 6700, Loss: 0.11481888592243195
step: 6800, Loss: 0.11676725000143051
step: 6900, Loss: 0.11643259227275848
step: 7000, Loss: 0.11589794605970383
step: 7100, Loss: 0.11457215249538422
step: 7200, Loss: 0.11503668129444122
step: 7300, Loss: 0.11486846953630447
step: 7400, Loss: 0.11351972073316574
step: 7500, Loss: 0.11443284153938293
step: 7600, Loss: 0.11465488374233246
step: 7700, Loss: 0.11526905745267868
step: 7800, Loss: 0.11459743976593018
step: 7900, Loss: 0.11416320502758026
step: 8000, Loss: 0.11571706831455231
step: 8100, Loss: 0.11426486819982529
step: 8200, Loss: 0.11379031836986542
step: 8300, Loss: 0.11590984463691711
step: 8400, Loss: 7.786813259124756
step: 8500, Loss: 0.17477381229400635
step: 8600, Loss: 0.14415068924427032
step: 8700, Loss: 0.13770633935928345
step: 8800, Loss: 0.13777321577072144
step: 8900, Loss: 0.12964271008968353
step: 9000, Loss: 0.12373794615268707
step: 9100, Loss: 0.12566834688186646
step: 9200, Loss: 0.11986123025417328
step: 9300, Loss: 0.12329841405153275
step: 9400, Loss: 0.12013054639101028
step: 9500, Loss: 0.11717615276575089
step: 9600, Loss: 0.11979661881923676
step: 9700, Loss: 0.11880102753639221
step: 9800, Loss: 0.11633243411779404
step: 9900, Loss: 0.1163872480392456
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7777777777777778
recall: 0.9333333333333333
F_score: 0.8484848484848485
******fold 4******

Training... train_data length:281
step: 0, Loss: 1.688550591468811
step: 100, Loss: 0.1207456886768341
step: 200, Loss: 0.11732436716556549
step: 300, Loss: 0.11420170962810516
step: 400, Loss: 0.11604328453540802
step: 500, Loss: 0.11501919478178024
step: 600, Loss: 0.11435161530971527
step: 700, Loss: 0.11362810432910919
step: 800, Loss: 0.11355510354042053
step: 900, Loss: 0.11565622687339783
step: 1000, Loss: 0.11428992450237274
step: 1100, Loss: 0.11384595930576324
step: 1200, Loss: 0.11337833106517792
step: 1300, Loss: 0.11305233836174011
step: 1400, Loss: 0.11470843106508255
step: 1500, Loss: 0.11311103403568268
step: 1600, Loss: 0.11364329606294632
step: 1700, Loss: 0.11485744267702103
step: 1800, Loss: 0.11469721794128418
step: 1900, Loss: 0.11475878208875656
step: 2000, Loss: 0.11491773277521133
step: 2100, Loss: 0.11448346078395844
step: 2200, Loss: 0.11358753591775894
step: 2300, Loss: 0.11524613946676254
step: 2400, Loss: 0.11402750015258789
step: 2500, Loss: 0.11516772955656052
step: 2600, Loss: 0.1144673153758049
step: 2700, Loss: 0.11329607665538788
step: 2800, Loss: 0.1141224056482315
step: 2900, Loss: 0.11437509208917618
step: 3000, Loss: 0.11422796547412872
step: 3100, Loss: 0.11367800086736679
step: 3200, Loss: 0.11419258266687393
step: 3300, Loss: 0.11334357410669327
step: 3400, Loss: 0.11421923339366913
step: 3500, Loss: 0.11393839120864868
step: 3600, Loss: 0.1150493323802948
step: 3700, Loss: 0.1153765618801117
step: 3800, Loss: 0.11391488462686539
step: 3900, Loss: 0.11419804394245148
step: 4000, Loss: 0.11461209505796432
step: 4100, Loss: 0.11444301903247833
step: 4200, Loss: 0.11518114060163498
step: 4300, Loss: 0.11430838704109192
step: 4400, Loss: 0.11326362192630768
step: 4500, Loss: 0.11511864513158798
step: 4600, Loss: 0.11393449455499649
step: 4700, Loss: 0.11594271659851074
step: 4800, Loss: 0.11386773735284805
step: 4900, Loss: 0.11354220658540726
step: 5000, Loss: 0.1142425388097763
step: 5100, Loss: 0.11637778580188751
step: 5200, Loss: 0.11452076584100723
step: 5300, Loss: 0.11381356418132782
step: 5400, Loss: 0.11483410000801086
step: 5500, Loss: 0.11506115645170212
step: 5600, Loss: 0.11533099412918091
step: 5700, Loss: 0.11567465960979462
step: 5800, Loss: 0.11466644704341888
step: 5900, Loss: 0.11349394917488098
step: 6000, Loss: 0.26493966579437256
step: 6100, Loss: 0.1600843071937561
step: 6200, Loss: 0.13784585893154144
step: 6300, Loss: 0.12307631969451904
step: 6400, Loss: 0.12007199972867966
step: 6500, Loss: 0.12591031193733215
step: 6600, Loss: 0.11921846866607666
step: 6700, Loss: 0.11963438987731934
step: 6800, Loss: 0.12047648429870605
step: 6900, Loss: 0.11530499905347824
step: 7000, Loss: 0.11556410044431686
step: 7100, Loss: 0.11834818124771118
step: 7200, Loss: 0.11608277261257172
step: 7300, Loss: 0.115411177277565
step: 7400, Loss: 0.1165371909737587
step: 7500, Loss: 0.11502477526664734
step: 7600, Loss: 0.11548562347888947
step: 7700, Loss: 0.11332152783870697
step: 7800, Loss: 0.11575941741466522
step: 7900, Loss: 0.11592531204223633
step: 8000, Loss: 0.1137324646115303
step: 8100, Loss: 0.11384110152721405
step: 8200, Loss: 0.11424631625413895
step: 8300, Loss: 0.11422964930534363
step: 8400, Loss: 0.11422638595104218
step: 8500, Loss: 0.11455923318862915
step: 8600, Loss: 0.11492262035608292
step: 8700, Loss: 0.113686203956604
step: 8800, Loss: 0.11447888612747192
step: 8900, Loss: 0.11330455541610718
step: 9000, Loss: 0.11499880254268646
step: 9100, Loss: 0.11460892111063004
step: 9200, Loss: 0.11542851477861404
step: 9300, Loss: 0.11469179391860962
step: 9400, Loss: 0.11385945230722427
step: 9500, Loss: 0.11328443884849548
step: 9600, Loss: 0.11322000622749329
step: 9700, Loss: 0.112818643450737
step: 9800, Loss: 0.11386515200138092
step: 9900, Loss: 0.11271985620260239
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7647058823529411
recall: 0.8666666666666667
F_score: 0.8125
******fold 5******

step: 9700, Loss: 0.13502836227416992
step: 9800, Loss: 0.13313362002372742
step: 9900, Loss: 0.12436601519584656
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 1.0
recall: 0.9
F_score: 0.9473684210526316
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.7534813284873962
step: 100, Loss: 0.14113479852676392
step: 200, Loss: 0.12499359250068665
step: 300, Loss: 0.12988486886024475
step: 400, Loss: 0.12549160420894623
step: 500, Loss: 0.12323112785816193
step: 600, Loss: 0.12373952567577362
step: 700, Loss: 0.12243451923131943
step: 800, Loss: 0.1296466886997223
step: 900, Loss: 0.1189703494310379
step: 1000, Loss: 0.1175210028886795
step: 1100, Loss: 0.11769437789916992
step: 1200, Loss: 0.11544880270957947
step: 1300, Loss: 0.12142191082239151
step: 1400, Loss: 0.11580215394496918
step: 1500, Loss: 0.1946159303188324
step: 1600, Loss: 0.11647011339664459
step: 1700, Loss: 0.11721488833427429
step: 1800, Loss: 0.11737125366926193
step: 1900, Loss: 0.12011529505252838
step: 2000, Loss: 0.1177951991558075
step: 2100, Loss: 0.11523306369781494
step: 2200, Loss: 0.11621617525815964
step: 2300, Loss: 0.11508157104253769
step: 2400, Loss: 0.11559026688337326
step: 2500, Loss: 0.1202712431550026
step: 2600, Loss: 0.1220194548368454
step: 2700, Loss: 0.12219028919935226
step: 2800, Loss: 0.11663290858268738
step: 2900, Loss: 0.11729560792446136
step: 3000, Loss: 2.4997360706329346
step: 3100, Loss: 0.15688510239124298
step: 3200, Loss: 0.1485723853111267
step: 3300, Loss: 0.15165428817272186
step: 3400, Loss: 0.23209738731384277
step: 3500, Loss: 0.13710811734199524
step: 3600, Loss: 0.13303489983081818
step: 3700, Loss: 0.13029974699020386
step: 3800, Loss: 0.12820719182491302
step: 3900, Loss: 0.12169307470321655
step: 4000, Loss: 0.1273835450410843
step: 4100, Loss: 0.12929841876029968
step: 4200, Loss: 0.12539437413215637
step: 4300, Loss: 0.1291913092136383
step: 4400, Loss: 0.121920645236969
step: 4500, Loss: 0.11837095767259598
step: 4600, Loss: 0.12257766723632812
step: 4700, Loss: 0.12303363531827927
step: 4800, Loss: 0.11936420947313309
step: 4900, Loss: 0.11838638037443161
step: 5000, Loss: 0.11881545186042786
step: 5100, Loss: 0.11986620724201202
step: 5200, Loss: 0.12727230787277222
step: 5300, Loss: 0.19637109339237213
step: 5400, Loss: 0.12187360227108002
step: 5500, Loss: 0.12229299545288086
step: 5600, Loss: 0.11807847768068314
step: 5700, Loss: 0.11906453967094421
step: 5800, Loss: 0.11685104668140411
step: 5900, Loss: 0.11788827180862427
step: 6000, Loss: 0.11958976089954376
step: 6100, Loss: 0.11501000821590424
step: 6200, Loss: 0.11555954068899155
step: 6300, Loss: 0.11519695073366165
step: 6400, Loss: 0.1150529608130455
step: 6500, Loss: 0.11473993957042694
step: 6600, Loss: 0.11601810902357101
step: 6700, Loss: 0.11460675299167633
step: 6800, Loss: 0.11719753593206406
step: 6900, Loss: 0.1157105341553688
step: 7000, Loss: 0.11639250069856644
step: 7100, Loss: 0.1175357848405838
step: 7200, Loss: 0.1896032840013504
step: 7300, Loss: 0.1151309683918953
step: 7400, Loss: 0.11622482538223267
step: 7500, Loss: 0.11428327113389969
step: 7600, Loss: 0.11309456825256348
step: 7700, Loss: 0.1143469586968422
step: 7800, Loss: 0.11386886984109879
step: 7900, Loss: 0.11520210653543472
step: 8000, Loss: 0.11670593917369843
step: 8100, Loss: 0.11697759479284286
step: 8200, Loss: 0.11412690579891205
step: 8300, Loss: 0.11435079574584961
step: 8400, Loss: 0.11834830045700073
step: 8500, Loss: 0.11498762667179108
step: 8600, Loss: 0.11500457674264908
step: 8700, Loss: 0.11771117150783539
step: 8800, Loss: 0.11995558440685272
step: 8900, Loss: 0.11430881917476654
step: 9000, Loss: 0.11998701095581055
step: 9100, Loss: 0.1963941752910614
step: 9200, Loss: 0.12166990339756012
step: 9300, Loss: 0.1232273280620575
step: 9400, Loss: 5.995826244354248
step: 9500, Loss: 0.24158860743045807
step: 9600, Loss: 0.16488897800445557
step: 9700, Loss: 0.1466190367937088
step: 9800, Loss: 0.1409439891576767
step: 9900, Loss: 0.14252346754074097
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.925
recall: 0.925
F_score: 0.925
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.5482737421989441
step: 100, Loss: 0.14847004413604736
step: 200, Loss: 0.12900236248970032
step: 300, Loss: 0.13748201727867126
step: 400, Loss: 0.12716922163963318
step: 500, Loss: 0.12335003912448883
step: 600, Loss: 0.12593090534210205
step: 700, Loss: 0.12307245284318924
step: 800, Loss: 0.12008005380630493
step: 900, Loss: 0.11962898075580597
step: 1000, Loss: 0.1265643835067749
step: 1100, Loss: 0.12155092507600784
step: 1200, Loss: 0.11859795451164246
step: 1300, Loss: 0.1255541443824768
step: 1400, Loss: 0.12428249418735504
step: 1500, Loss: 0.19042035937309265
step: 1600, Loss: 0.1153625026345253
step: 1700, Loss: 0.1175851821899414
step: 1800, Loss: 0.11660507321357727
step: 1900, Loss: 0.1154782697558403
step: 2000, Loss: 0.11600197106599808
step: 2100, Loss: 0.11549334973096848
step: 2200, Loss: 0.11784610152244568
step: 2300, Loss: 0.11629356443881989
step: 2400, Loss: 0.11548642069101334
step: 2500, Loss: 0.11673466116189957
step: 2600, Loss: 0.11719848215579987
step: 2700, Loss: 0.1157064437866211
step: 2800, Loss: 0.11429780721664429
step: 2900, Loss: 0.1144150048494339
step: 3000, Loss: 0.11368540674448013
step: 3100, Loss: 0.12360882759094238
step: 3200, Loss: 0.12344244122505188
step: 3300, Loss: 0.1356453001499176
step: 3400, Loss: 0.209661066532135
step: 3500, Loss: 0.12297803163528442
step: 3600, Loss: 4.2488932609558105
step: 3700, Loss: 0.1424376517534256
step: 3800, Loss: 0.13847897946834564
step: 3900, Loss: 0.1379455178976059
step: 4000, Loss: 0.13077549636363983
step: 4100, Loss: 0.12420220673084259
step: 4200, Loss: 0.13125652074813843
step: 4300, Loss: 0.12689894437789917
step: 4400, Loss: 0.12971271574497223
step: 4500, Loss: 0.12831206619739532
step: 4600, Loss: 0.12071362882852554
step: 4700, Loss: 0.13098111748695374
step: 4800, Loss: 0.11958804726600647
step: 4900, Loss: 0.1213749349117279
step: 5000, Loss: 0.12498714029788971
step: 5100, Loss: 0.12132148444652557
step: 5200, Loss: 0.11819078773260117
step: 5300, Loss: 0.20515476167201996
step: 5400, Loss: 0.1180553287267685
step: 5500, Loss: 0.11956904828548431
step: 5600, Loss: 0.11887763440608978
step: 5700, Loss: 0.11892058700323105
step: 5800, Loss: 0.11874766647815704
step: 5900, Loss: 0.11771265417337418
step: 6000, Loss: 0.11713162809610367
step: 6100, Loss: 0.1181730255484581
step: 6200, Loss: 0.11681490391492844
step: 6300, Loss: 0.12224072217941284
step: 6400, Loss: 0.11482275277376175
step: 6500, Loss: 0.11649876832962036
step: 6600, Loss: 0.11667971312999725
step: 6700, Loss: 0.11605913937091827
step: 6800, Loss: 0.11849792301654816
step: 6900, Loss: 0.11615879833698273
step: 7000, Loss: 0.11702810227870941
step: 7100, Loss: 0.11369902640581131
step: 7200, Loss: 0.19122210144996643
step: 7300, Loss: 0.11494918912649155
step: 7400, Loss: 0.11426332592964172
step: 7500, Loss: 0.11530622839927673
step: 7600, Loss: 0.11581071466207504
step: 7700, Loss: 0.11375976353883743
step: 7800, Loss: 0.1141132339835167
step: 7900, Loss: 0.11454133689403534
step: 8000, Loss: 0.11498941481113434
step: 8100, Loss: 0.11586733162403107
step: 8200, Loss: 0.11411037296056747
step: 8300, Loss: 0.1147298738360405
step: 8400, Loss: 0.11423025280237198
step: 8500, Loss: 0.11772315204143524
step: 8600, Loss: 0.1159389466047287
step: 8700, Loss: 0.11633333563804626
step: 8800, Loss: 0.11726552993059158
step: 8900, Loss: 0.11994600296020508
step: 9000, Loss: 0.11567829549312592
step: 9100, Loss: 0.19502906501293182
step: 9200, Loss: 0.11450345814228058
step: 9300, Loss: 0.1146094873547554
step: 9400, Loss: 0.12028610706329346
step: 9500, Loss: 0.12242759764194489
step: 9600, Loss: 0.12074317038059235
step: 9700, Loss: 0.11955958604812622
step: 9800, Loss: 0.11637981981039047
step: 9900, Loss: 0.12155640870332718
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.17386306822299957
step: 100, Loss: 0.11660727858543396
step: 200, Loss: 0.116244837641716
step: 300, Loss: 0.11503330618143082
step: 400, Loss: 0.11357760429382324
step: 500, Loss: 0.11541980504989624
step: 600, Loss: 0.11483177542686462
step: 700, Loss: 0.11575261503458023
step: 800, Loss: 0.11635550111532211
step: 900, Loss: 0.12003745883703232
step: 1000, Loss: 0.11958129703998566
step: 1100, Loss: 2.019658327102661
step: 1200, Loss: 1.709665298461914
step: 1300, Loss: 0.14921724796295166
step: 1400, Loss: 0.14080415666103363
step: 1500, Loss: 0.2362566590309143
step: 1600, Loss: 0.14264102280139923
step: 1700, Loss: 0.1240696981549263
step: 1800, Loss: 0.12586739659309387
step: 1900, Loss: 0.12204248458147049
step: 2000, Loss: 0.12565983831882477
step: 2100, Loss: 0.12279082089662552
step: 2200, Loss: 0.12042583525180817
step: 2300, Loss: 0.12209322303533554
step: 2400, Loss: 0.1224512904882431
step: 2500, Loss: 0.11982151865959167
step: 2600, Loss: 0.11930686235427856
step: 2700, Loss: 0.12034951895475388
step: 2800, Loss: 0.11955661326646805
step: 2900, Loss: 0.11887560039758682
step: 3000, Loss: 0.11668287962675095
step: 3100, Loss: 0.1170545294880867
step: 3200, Loss: 0.11954538524150848
step: 3300, Loss: 0.11936208605766296
step: 3400, Loss: 0.19424927234649658
step: 3500, Loss: 0.11636722832918167
step: 3600, Loss: 0.11859282106161118
step: 3700, Loss: 0.11845551431179047
step: 3800, Loss: 0.11512535810470581
step: 3900, Loss: 0.11610676348209381
step: 4000, Loss: 0.11581555753946304
step: 4100, Loss: 0.11687611043453217
step: 4200, Loss: 0.11533766239881516
step: 4300, Loss: 0.1150863990187645
step: 4400, Loss: 0.11437243968248367
step: 4500, Loss: 0.11461842805147171
step: 4600, Loss: 0.1136094480752945
step: 4700, Loss: 0.11514139920473099
step: 4800, Loss: 0.11363883316516876
step: 4900, Loss: 0.1138368770480156
step: 5000, Loss: 0.11269234120845795
step: 5100, Loss: 0.11578348278999329
step: 5200, Loss: 0.11453964561223984
step: 5300, Loss: 0.19026347994804382
step: 5400, Loss: 0.11277790367603302
step: 5500, Loss: 0.1136755719780922
step: 5600, Loss: 0.11393280327320099
step: 5700, Loss: 0.11348612606525421
step: 5800, Loss: 0.11385924369096756
step: 5900, Loss: 0.11361165344715118
step: 6000, Loss: 0.11352788656949997
step: 6100, Loss: 0.11405771225690842
step: 6200, Loss: 0.11416736245155334
step: 6300, Loss: 0.11430011689662933
step: 6400, Loss: 0.11432742327451706
step: 6500, Loss: 0.11323137581348419
step: 6600, Loss: 0.11692149937152863
step: 6700, Loss: 0.1165514588356018
step: 6800, Loss: 0.12519077956676483
step: 6900, Loss: 0.11408711224794388
step: 7000, Loss: 0.11554165929555893
step: 7100, Loss: 0.11700791120529175
step: 7200, Loss: 0.19326812028884888
step: 7300, Loss: 0.11927104741334915
step: 7400, Loss: 0.1244765892624855
step: 7500, Loss: 0.12431781738996506
step: 7600, Loss: 0.12025322765111923
step: 7700, Loss: 0.13297565281391144
step: 7800, Loss: 2.986868143081665
step: 7900, Loss: 0.34699955582618713
step: 8000, Loss: 0.15103712677955627
step: 8100, Loss: 0.14144602417945862
step: 8200, Loss: 0.13255859911441803
step: 8300, Loss: 0.13424567878246307
step: 8400, Loss: 0.13102920353412628
step: 8500, Loss: 0.13263152539730072
step: 8600, Loss: 0.13038194179534912
step: 8700, Loss: 0.12881548702716827
step: 8800, Loss: 0.12660087645053864
step: 8900, Loss: 0.13046731054782867
step: 9000, Loss: 0.12503117322921753
step: 9100, Loss: 0.20690500736236572
step: 9200, Loss: 0.12121910601854324
step: 9300, Loss: 0.1218567043542862
step: 9400, Loss: 0.12192961573600769
step: 9500, Loss: 0.12198741734027863
step: 9600, Loss: 0.11809394508600235
step: 9700, Loss: 0.11849220842123032
step: 9800, Loss: 0.11835988610982895
step: 9900, Loss: 0.1172529086470604
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9714285714285714
recall: 0.9714285714285714
F_score: 0.9714285714285714
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.15469840168952942
step: 100, Loss: 0.12211934477090836
step: 200, Loss: 0.11439739912748337
step: 300, Loss: 0.11670928448438644
step: 400, Loss: 0.11389376223087311
step: 500, Loss: 0.11484097689390182
step: 600, Loss: 0.11453395336866379
step: 700, Loss: 0.11382494121789932
step: 800, Loss: 0.11340032517910004
step: 900, Loss: 0.11442737281322479
step: 1000, Loss: 0.11802142858505249
step: 1100, Loss: 0.11985789239406586
step: 1200, Loss: 0.12025273591279984
step: 1300, Loss: 0.12739580869674683
step: 1400, Loss: 0.4346280097961426
step: 1500, Loss: 0.29307636618614197
step: 1600, Loss: 0.14129072427749634
step: 1700, Loss: 0.14219969511032104
step: 1800, Loss: 0.14407362043857574
step: 1900, Loss: 0.13428649306297302
step: 2000, Loss: 0.13142099976539612
step: 2100, Loss: 0.12912453711032867
step: 2200, Loss: 0.12548157572746277
step: 2300, Loss: 0.12359888851642609
step: 2400, Loss: 0.12187844514846802
step: 2500, Loss: 0.11952988803386688
step: 2600, Loss: 0.1222480982542038
step: 2700, Loss: 0.12179666757583618
step: 2800, Loss: 0.119636170566082
step: 2900, Loss: 0.12018246948719025
step: 3000, Loss: 0.1214357540011406
step: 3100, Loss: 0.1209120973944664
step: 3200, Loss: 0.12067534029483795
step: 3300, Loss: 0.12186262011528015
step: 3400, Loss: 0.19408594071865082
step: 3500, Loss: 0.11923432350158691
step: 3600, Loss: 0.11675524711608887
step: 3700, Loss: 0.11853212863206863
step: 3800, Loss: 0.118325375020504
step: 3900, Loss: 0.1200045794248581
step: 4000, Loss: 0.11466801911592484
step: 4100, Loss: 0.11480293422937393
step: 4200, Loss: 0.1160045638680458
step: 4300, Loss: 0.11598643660545349
step: 4400, Loss: 0.11515584588050842
step: 4500, Loss: 0.1149899959564209
step: 4600, Loss: 0.11587853729724884
step: 4700, Loss: 0.11519700288772583
step: 4800, Loss: 0.1146807000041008
step: 4900, Loss: 0.11633655428886414
step: 5000, Loss: 0.11490822583436966
step: 5100, Loss: 0.11435407400131226
step: 5200, Loss: 0.11386381089687347
step: 5300, Loss: 0.19323110580444336
step: 5400, Loss: 0.11531439423561096
step: 5500, Loss: 0.11799363791942596
step: 5600, Loss: 0.11503999680280685
step: 5700, Loss: 0.11565551906824112
step: 5800, Loss: 0.11381672322750092
step: 5900, Loss: 0.11527851223945618
step: 6000, Loss: 0.11431670188903809
step: 6100, Loss: 0.11377596110105515
step: 6200, Loss: 0.1141977310180664
step: 6300, Loss: 0.11431816965341568
step: 6400, Loss: 0.11428146809339523
step: 6500, Loss: 0.11341559886932373
step: 6600, Loss: 0.11491258442401886
step: 6700, Loss: 0.11404493451118469
step: 6800, Loss: 0.11390526592731476
step: 6900, Loss: 0.11533024162054062
step: 7000, Loss: 0.11356227099895477
step: 7100, Loss: 0.1144833192229271
step: 7200, Loss: 0.1990831196308136
step: 7300, Loss: 0.12156596779823303
step: 7400, Loss: 0.11371032893657684
step: 7500, Loss: 0.11467058956623077
step: 7600, Loss: 0.11594779789447784
step: 7700, Loss: 0.12321365624666214
step: 7800, Loss: 0.12091667205095291
step: 7900, Loss: 5.643026828765869
step: 8000, Loss: 3.3908770084381104
step: 8100, Loss: 0.15636789798736572
step: 8200, Loss: 0.1390233337879181
step: 8300, Loss: 0.14060303568840027
step: 8400, Loss: 0.1348932385444641
step: 8500, Loss: 0.13667920231819153
step: 8600, Loss: 0.13254527747631073
step: 8700, Loss: 0.1277410387992859
step: 8800, Loss: 0.1271510124206543
step: 8900, Loss: 0.12353865057229996
step: 9000, Loss: 0.1264144331216812
step: 9100, Loss: 0.21038566529750824
step: 9200, Loss: 0.12021048367023468
step: 9300, Loss: 0.12182442098855972
step: 9400, Loss: 0.1206747516989708
step: 9500, Loss: 0.12385919690132141
step: 9600, Loss: 0.11917075514793396
step: 9700, Loss: 0.11722180247306824
step: 9800, Loss: 0.121953584253788
step: 9900, Loss: 0.12061279267072678
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.918918918918919
recall: 0.9714285714285714
F_score: 0.9444444444444445
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.15956321358680725
step: 100, Loss: 0.11894211918115616
step: 200, Loss: 0.11505100876092911
step: 300, Loss: 0.11649803817272186
Training... train_data length:281
step: 0, Loss: 1.7702815532684326
step: 100, Loss: 0.12105357646942139
step: 200, Loss: 0.11704734712839127
step: 300, Loss: 0.11448606848716736
step: 400, Loss: 0.11532414704561234
step: 500, Loss: 0.11525624245405197
step: 600, Loss: 0.1135803610086441
step: 700, Loss: 0.11442554742097855
step: 800, Loss: 0.11577518284320831
step: 900, Loss: 0.11628985404968262
step: 1000, Loss: 0.1153755933046341
step: 1100, Loss: 0.1142197996377945
step: 1200, Loss: 0.11498481780290604
step: 1300, Loss: 0.11384278535842896
step: 1400, Loss: 0.11450794339179993
step: 1500, Loss: 0.11476664245128632
step: 1600, Loss: 0.11486655473709106
step: 1700, Loss: 0.11399895697832108
step: 1800, Loss: 0.11648866534233093
step: 1900, Loss: 0.11469210684299469
step: 2000, Loss: 0.11510954052209854
step: 2100, Loss: 0.11487787216901779
step: 2200, Loss: 0.1141316294670105
step: 2300, Loss: 0.11599116027355194
step: 2400, Loss: 0.11381204426288605
step: 2500, Loss: 0.11368484795093536
step: 2600, Loss: 0.1141553595662117
step: 2700, Loss: 0.11372531950473785
step: 2800, Loss: 0.11599589139223099
step: 2900, Loss: 0.1154223084449768
step: 3000, Loss: 0.11463795602321625
step: 3100, Loss: 0.11396795511245728
step: 3200, Loss: 0.11547864228487015
step: 3300, Loss: 0.11426745355129242
step: 3400, Loss: 0.11428484320640564
step: 3500, Loss: 0.11332719027996063
step: 3600, Loss: 0.1150415763258934
step: 3700, Loss: 0.11394413560628891
step: 3800, Loss: 0.11332059651613235
step: 3900, Loss: 0.11419074982404709
step: 4000, Loss: 0.11429999768733978
step: 4100, Loss: 0.11471772193908691
step: 4200, Loss: 0.11467456072568893
step: 4300, Loss: 0.11435022205114365
step: 4400, Loss: 0.11330849677324295
step: 4500, Loss: 0.11526606976985931
step: 4600, Loss: 0.11417102813720703
step: 4700, Loss: 0.11445611715316772
step: 4800, Loss: 0.11415483802556992
step: 4900, Loss: 0.11332596093416214
step: 5000, Loss: 0.11414960026741028
step: 5100, Loss: 0.11459244042634964
step: 5200, Loss: 0.11345215141773224
step: 5300, Loss: 0.11289949715137482
step: 5400, Loss: 0.11493969708681107
step: 5500, Loss: 0.11338368058204651
step: 5600, Loss: 0.11341166496276855
step: 5700, Loss: 0.11430800706148148
step: 5800, Loss: 0.11409156769514084
step: 5900, Loss: 0.11562003195285797
step: 6000, Loss: 0.11469410359859467
step: 6100, Loss: 0.11402755230665207
step: 6200, Loss: 0.1147158294916153
step: 6300, Loss: 0.11452953517436981
step: 6400, Loss: 1.440419316291809
step: 6500, Loss: 0.1483810544013977
step: 6600, Loss: 0.14386624097824097
step: 6700, Loss: 0.12955854833126068
step: 6800, Loss: 0.12220832705497742
step: 6900, Loss: 0.11972711980342865
step: 7000, Loss: 0.11863212287425995
step: 7100, Loss: 0.1182233989238739
step: 7200, Loss: 0.11432898044586182
step: 7300, Loss: 0.11944426596164703
step: 7400, Loss: 0.11802688241004944
step: 7500, Loss: 0.11633851379156113
step: 7600, Loss: 0.11401453614234924
step: 7700, Loss: 0.11615172773599625
step: 7800, Loss: 0.11793657392263412
step: 7900, Loss: 0.11664537340402603
step: 8000, Loss: 0.1150142177939415
step: 8100, Loss: 0.11714959889650345
step: 8200, Loss: 0.11379337310791016
step: 8300, Loss: 0.11483451724052429
step: 8400, Loss: 0.1143336147069931
step: 8500, Loss: 0.11505229771137238
step: 8600, Loss: 0.11372971534729004
step: 8700, Loss: 0.11553062498569489
step: 8800, Loss: 0.11396357417106628
step: 8900, Loss: 0.11449913680553436
step: 9000, Loss: 0.11380390077829361
step: 9100, Loss: 0.1137244924902916
step: 9200, Loss: 0.11373120546340942
step: 9300, Loss: 0.11302818357944489
step: 9400, Loss: 0.11418244242668152
step: 9500, Loss: 0.1145535483956337
step: 9600, Loss: 0.11367049813270569
step: 9700, Loss: 0.11399585753679276
step: 9800, Loss: 0.11411254107952118
step: 9900, Loss: 0.113791324198246
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8333333333333334
recall: 0.8333333333333334
F_score: 0.8333333333333334
******fold 6******

Training... train_data length:281
step: 0, Loss: 1.616773247718811
step: 100, Loss: 0.11987708508968353
step: 200, Loss: 0.11591200530529022
step: 300, Loss: 0.11568648368120193
step: 400, Loss: 0.11509943008422852
step: 500, Loss: 0.11472515761852264
step: 600, Loss: 0.11392008513212204
step: 700, Loss: 0.1149890348315239
step: 800, Loss: 0.11553873866796494
step: 900, Loss: 0.11441119015216827
step: 1000, Loss: 0.11377713084220886
step: 1100, Loss: 0.11538214981555939
step: 1200, Loss: 0.1138702929019928
step: 1300, Loss: 0.1147027388215065
step: 1400, Loss: 0.11572453379631042
step: 1500, Loss: 0.11540346592664719
step: 1600, Loss: 0.11508984863758087
step: 1700, Loss: 0.11453726142644882
step: 1800, Loss: 0.11417095363140106
step: 1900, Loss: 0.11374624073505402
step: 2000, Loss: 0.11425579339265823
step: 2100, Loss: 0.1131535992026329
step: 2200, Loss: 0.11393199861049652
step: 2300, Loss: 0.11296071112155914
step: 2400, Loss: 0.11440324783325195
step: 2500, Loss: 0.11451032757759094
step: 2600, Loss: 0.11363135278224945
step: 2700, Loss: 0.11435116827487946
step: 2800, Loss: 0.1136639341711998
step: 2900, Loss: 0.11362232267856598
step: 3000, Loss: 0.11425702273845673
step: 3100, Loss: 0.11348366737365723
step: 3200, Loss: 0.11436435580253601
step: 3300, Loss: 0.11582943052053452
step: 3400, Loss: 0.113589808344841
step: 3500, Loss: 0.11282970011234283
step: 3600, Loss: 0.11364788562059402
step: 3700, Loss: 0.1133972704410553
step: 3800, Loss: 0.11458981782197952
step: 3900, Loss: 0.11300642788410187
step: 4000, Loss: 0.11304880678653717
step: 4100, Loss: 0.11461131274700165
step: 4200, Loss: 0.11700422316789627
step: 4300, Loss: 0.1145472303032875
step: 4400, Loss: 0.11515895277261734
step: 4500, Loss: 0.11351456493139267
step: 4600, Loss: 0.11568893492221832
step: 4700, Loss: 0.11547704041004181
step: 4800, Loss: 0.1159958615899086
step: 4900, Loss: 0.11458756029605865
step: 5000, Loss: 0.11378642171621323
step: 5100, Loss: 0.11348288506269455
step: 5200, Loss: 0.11577795445919037
step: 5300, Loss: 0.11602064967155457
step: 5400, Loss: 0.11507132649421692
step: 5500, Loss: 0.1143929585814476
step: 5600, Loss: 0.11478636413812637
step: 5700, Loss: 4.682090759277344
step: 5800, Loss: 0.1844778060913086
step: 5900, Loss: 0.15228357911109924
step: 6000, Loss: 0.14289557933807373
step: 6100, Loss: 0.13427972793579102
step: 6200, Loss: 0.13696062564849854
step: 6300, Loss: 0.12732166051864624
step: 6400, Loss: 0.12507343292236328
step: 6500, Loss: 0.12473351508378983
step: 6600, Loss: 0.1214917004108429
step: 6700, Loss: 0.12192537635564804
step: 6800, Loss: 0.11902152001857758
step: 6900, Loss: 0.11819282919168472
step: 7000, Loss: 0.11853165924549103
step: 7100, Loss: 0.11943672597408295
step: 7200, Loss: 0.11761720478534698
step: 7300, Loss: 0.1189640611410141
step: 7400, Loss: 0.11691736429929733
step: 7500, Loss: 0.11606127768754959
step: 7600, Loss: 0.11688490957021713
step: 7700, Loss: 0.11670896410942078
step: 7800, Loss: 0.11624845862388611
step: 7900, Loss: 0.11555016040802002
step: 8000, Loss: 0.11574627459049225
step: 8100, Loss: 0.11522737145423889
step: 8200, Loss: 0.11371134221553802
step: 8300, Loss: 0.11460653692483902
step: 8400, Loss: 0.11388493329286575
step: 8500, Loss: 0.11443041265010834
step: 8600, Loss: 0.11439502239227295
step: 8700, Loss: 0.11412343382835388
step: 8800, Loss: 0.11421924829483032
step: 8900, Loss: 0.11434200406074524
step: 9000, Loss: 0.11376795172691345
step: 9100, Loss: 0.11413998901844025
step: 9200, Loss: 0.11369335651397705
step: 9300, Loss: 0.1146269291639328
step: 9400, Loss: 0.11365503072738647
step: 9500, Loss: 0.1139143630862236
step: 9600, Loss: 0.1157665103673935
step: 9700, Loss: 0.11335121095180511
step: 9800, Loss: 0.1135571151971817
step: 9900, Loss: 0.11392942070960999
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.9411764705882353
recall: 0.8888888888888888
F_score: 0.9142857142857143
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.13331978023052216
step: 100, Loss: 0.11837586760520935
step: 200, Loss: 0.11682863533496857
step: 300, Loss: 0.11517675220966339
step: 400, Loss: 0.11435329914093018
step: 400, Loss: 0.11378862708806992
step: 500, Loss: 0.1162930577993393
step: 600, Loss: 0.11462804675102234
step: 700, Loss: 0.11628954857587814
step: 800, Loss: 0.11468811333179474
step: 900, Loss: 0.11791419982910156
step: 1000, Loss: 0.1329108029603958
step: 1100, Loss: 0.12223924696445465
step: 1200, Loss: 0.13624322414398193
step: 1300, Loss: 0.5786495804786682
step: 1400, Loss: 0.17620490491390228
step: 1500, Loss: 0.2663832902908325
step: 1600, Loss: 0.13682352006435394
step: 1700, Loss: 0.1444791853427887
step: 1800, Loss: 0.1506616175174713
step: 1900, Loss: 0.1427905112504959
step: 2000, Loss: 0.13420052826404572
step: 2100, Loss: 0.1308738887310028
step: 2200, Loss: 0.12771651148796082
step: 2300, Loss: 0.12160468101501465
step: 2400, Loss: 0.12818941473960876
step: 2500, Loss: 0.12493135035037994
step: 2600, Loss: 0.12149467319250107
step: 2700, Loss: 0.12633821368217468
step: 2800, Loss: 0.12274017930030823
step: 2900, Loss: 0.12153740227222443
step: 3000, Loss: 0.11834506690502167
step: 3100, Loss: 0.1183944046497345
step: 3200, Loss: 0.12218238413333893
step: 3300, Loss: 0.11857689172029495
step: 3400, Loss: 0.20203110575675964
step: 3500, Loss: 0.11637517809867859
step: 3600, Loss: 0.11840319633483887
step: 3700, Loss: 0.12038245797157288
step: 3800, Loss: 0.12032602727413177
step: 3900, Loss: 0.11703836172819138
step: 4000, Loss: 0.11654306203126907
step: 4100, Loss: 0.11612605303525925
step: 4200, Loss: 0.11544595658779144
step: 4300, Loss: 0.11426403373479843
step: 4400, Loss: 0.11695776879787445
step: 4500, Loss: 0.11706368625164032
step: 4600, Loss: 0.11604218930006027
step: 4700, Loss: 0.11418727785348892
step: 4800, Loss: 0.11431463062763214
step: 4900, Loss: 0.11526133865118027
step: 5000, Loss: 0.11416751891374588
step: 5100, Loss: 0.11418221890926361
step: 5200, Loss: 0.1172342300415039
step: 5300, Loss: 0.19430741667747498
step: 5400, Loss: 0.11404410004615784
step: 5500, Loss: 0.11381184309720993
step: 5600, Loss: 0.11611694097518921
step: 5700, Loss: 0.11570941656827927
step: 5800, Loss: 0.1139795333147049
step: 5900, Loss: 0.11456384509801865
step: 6000, Loss: 0.11507342755794525
step: 6100, Loss: 0.11461301147937775
step: 6200, Loss: 0.11480768769979477
step: 6300, Loss: 0.11638270318508148
step: 6400, Loss: 0.11267787963151932
step: 6500, Loss: 0.11608272790908813
step: 6600, Loss: 0.11571384966373444
step: 6700, Loss: 0.1148926168680191
step: 6800, Loss: 0.11727537959814072
step: 6900, Loss: 0.11525657027959824
step: 7000, Loss: 0.11550559103488922
step: 7100, Loss: 0.11511080712080002
step: 7200, Loss: 0.19267304241657257
step: 7300, Loss: 0.11418388038873672
step: 7400, Loss: 0.11422112584114075
step: 7500, Loss: 0.11692442744970322
step: 7600, Loss: 0.11585865914821625
step: 7700, Loss: 0.11592521518468857
step: 7800, Loss: 1.8590019941329956
step: 7900, Loss: 0.1557915210723877
step: 8000, Loss: 0.14832057058811188
step: 8100, Loss: 0.15298596024513245
step: 8200, Loss: 0.13526742160320282
step: 8300, Loss: 0.1357121765613556
step: 8400, Loss: 0.1296921968460083
step: 8500, Loss: 0.1293850541114807
step: 8600, Loss: 0.1283646523952484
step: 8700, Loss: 0.12346568703651428
step: 8800, Loss: 0.12369734048843384
step: 8900, Loss: 0.13171528279781342
step: 9000, Loss: 0.12194593995809555
step: 9100, Loss: 0.20304428040981293
step: 9200, Loss: 0.1269523799419403
step: 9300, Loss: 0.1233135238289833
step: 9400, Loss: 0.1197991818189621
step: 9500, Loss: 0.12113460898399353
step: 9600, Loss: 0.1216171383857727
step: 9700, Loss: 0.11582501232624054
step: 9800, Loss: 0.11809919774532318
step: 9900, Loss: 0.11699065566062927
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.8888888888888888
recall: 1.0
F_score: 0.9411764705882353
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.1707301288843155
step: 100, Loss: 0.11800708621740341
step: 200, Loss: 0.1187121719121933
step: 300, Loss: 0.11505784839391708
step: 400, Loss: 0.11629664897918701
step: 500, Loss: 0.1149490475654602
step: 600, Loss: 0.11546666920185089
step: 700, Loss: 0.1205548346042633
step: 800, Loss: 0.1305381953716278
step: 900, Loss: 0.11772556602954865
step: 1000, Loss: 0.12813185155391693
step: 1100, Loss: 0.1174284964799881
step: 1200, Loss: 0.12285121530294418
step: 1300, Loss: 0.12090589106082916
step: 1400, Loss: 0.1268840730190277
step: 1500, Loss: 8.442520141601562
step: 1600, Loss: 0.15153789520263672
step: 1700, Loss: 0.14150747656822205
step: 1800, Loss: 0.12771405279636383
step: 1900, Loss: 0.1328006386756897
step: 2000, Loss: 0.1289718747138977
step: 2100, Loss: 0.12751123309135437
step: 2200, Loss: 0.12848584353923798
step: 2300, Loss: 0.12540757656097412
step: 2400, Loss: 0.12381262332201004
step: 2500, Loss: 0.1221427395939827
step: 2600, Loss: 0.12410101294517517
step: 2700, Loss: 0.12103427946567535
step: 2800, Loss: 0.12024552375078201
step: 2900, Loss: 0.11871960014104843
step: 3000, Loss: 0.12000621855258942
step: 3100, Loss: 0.11970348656177521
step: 3200, Loss: 0.1198839321732521
step: 3300, Loss: 0.11733369529247284
step: 3400, Loss: 0.19554145634174347
step: 3500, Loss: 0.12027488648891449
step: 3600, Loss: 0.11493131518363953
step: 3700, Loss: 0.11909586936235428
step: 3800, Loss: 0.12041190266609192
step: 3900, Loss: 0.11832942068576813
step: 4000, Loss: 0.11821886897087097
step: 4100, Loss: 0.11871911585330963
step: 4200, Loss: 0.11774048209190369
step: 4300, Loss: 0.11571823805570602
step: 4400, Loss: 0.11676811426877975
step: 4500, Loss: 0.1173873171210289
step: 4600, Loss: 0.1145172193646431
step: 4700, Loss: 0.11479087918996811
step: 4800, Loss: 0.11643223464488983
step: 4900, Loss: 0.11667391657829285
step: 5000, Loss: 0.11600296199321747
step: 5100, Loss: 0.11808888614177704
step: 5200, Loss: 0.11526775360107422
step: 5300, Loss: 0.19219322502613068
step: 5400, Loss: 0.11392505466938019
step: 5500, Loss: 0.11392995715141296
step: 5600, Loss: 0.11287950724363327
step: 5700, Loss: 0.11427248269319534
step: 5800, Loss: 0.11588897556066513
step: 5900, Loss: 0.11522165685892105
step: 6000, Loss: 0.11514579504728317
step: 6100, Loss: 0.11317244917154312
step: 6200, Loss: 0.11520220339298248
step: 6300, Loss: 0.11306250095367432
step: 6400, Loss: 0.11359705775976181
step: 6500, Loss: 0.11552903056144714
step: 6600, Loss: 0.11486874520778656
step: 6700, Loss: 0.11427681148052216
step: 6800, Loss: 0.11364689469337463
step: 6900, Loss: 0.11382400989532471
step: 7000, Loss: 0.11592717468738556
step: 7100, Loss: 0.1144789308309555
step: 7200, Loss: 0.1916068196296692
step: 7300, Loss: 0.11569283902645111
step: 7400, Loss: 0.11378689110279083
step: 7500, Loss: 0.1156863272190094
step: 7600, Loss: 0.1138199120759964
step: 7700, Loss: 0.12158416211605072
step: 7800, Loss: 0.11406829953193665
step: 7900, Loss: 0.1137021854519844
step: 8000, Loss: 0.11478076130151749
step: 8100, Loss: 0.11643318831920624
step: 8200, Loss: 0.12398359924554825
step: 8300, Loss: 1.1519876718521118
step: 8400, Loss: 0.22225910425186157
step: 8500, Loss: 0.14869807660579681
step: 8600, Loss: 0.13859950006008148
step: 8700, Loss: 0.1321571171283722
step: 8800, Loss: 0.127333402633667
step: 8900, Loss: 0.13717792928218842
step: 9000, Loss: 0.1277201622724533
step: 9100, Loss: 0.21336349844932556
step: 9200, Loss: 0.1286754161119461
step: 9300, Loss: 0.12205413728952408
step: 9400, Loss: 0.12308978289365768
step: 9500, Loss: 0.12338057160377502
step: 9600, Loss: 0.12336723506450653
step: 9700, Loss: 0.12092524766921997
step: 9800, Loss: 0.11923930048942566
step: 9900, Loss: 0.12224352359771729
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9722222222222222
recall: 1.0
F_score: 0.9859154929577464
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.17990681529045105
step: 100, Loss: 0.11910432577133179
step: 200, Loss: 0.11677912622690201
step: 300, Loss: 0.11657115817070007
step: 400, Loss: 0.11407583951950073
step: 500, Loss: 0.11465683579444885
step: 600, Loss: 0.11450247466564178
step: 700, Loss: 0.11454527080059052
step: 800, Loss: 0.1218639686703682
step: 900, Loss: 0.11812490224838257
step: 500, Loss: 0.11631380021572113
step: 600, Loss: 0.11302077770233154
step: 700, Loss: 0.11433573812246323
step: 800, Loss: 0.11517822742462158
step: 900, Loss: 0.11416120827198029
step: 1000, Loss: 0.11406184732913971
step: 1100, Loss: 0.11326994746923447
step: 1200, Loss: 0.11373021453619003
step: 1300, Loss: 0.1145620197057724
step: 1400, Loss: 0.11310269683599472
step: 1500, Loss: 0.11340111494064331
step: 1600, Loss: 0.11347696185112
step: 1700, Loss: 0.11480740457773209
step: 1800, Loss: 0.11392754316329956
step: 1900, Loss: 0.1137150377035141
step: 2000, Loss: 0.11439160257577896
step: 2100, Loss: 0.1146901547908783
step: 2200, Loss: 0.11350814253091812
step: 2300, Loss: 0.11351752281188965
step: 2400, Loss: 0.11327103525400162
step: 2500, Loss: 0.11461061984300613
step: 2600, Loss: 0.11425422877073288
step: 2700, Loss: 0.114214226603508
step: 2800, Loss: 0.11344794929027557
step: 2900, Loss: 0.11361546814441681
step: 3000, Loss: 0.1141071766614914
step: 3100, Loss: 0.11300728470087051
step: 3200, Loss: 0.11491943150758743
step: 3300, Loss: 0.11628562211990356
step: 3400, Loss: 0.11332310736179352
step: 3500, Loss: 0.11344845592975616
step: 3600, Loss: 0.11460606008768082
step: 3700, Loss: 0.11405695974826813
step: 3800, Loss: 0.1141510084271431
step: 3900, Loss: 0.11476600915193558
step: 4000, Loss: 0.11315809190273285
step: 4100, Loss: 0.11428859829902649
step: 4200, Loss: 0.11398542672395706
step: 4300, Loss: 0.11406631767749786
step: 4400, Loss: 0.11424729973077774
step: 4500, Loss: 0.11412682384252548
step: 4600, Loss: 0.11472766101360321
step: 4700, Loss: 0.11365311592817307
step: 4800, Loss: 0.11412341147661209
step: 4900, Loss: 0.11397536098957062
step: 5000, Loss: 0.11516056209802628
step: 5100, Loss: 0.11515241861343384
step: 5200, Loss: 0.1147892028093338
step: 5300, Loss: 0.1140836626291275
step: 5400, Loss: 0.11318327486515045
step: 5500, Loss: 0.11375410109758377
step: 5600, Loss: 0.11516471207141876
step: 5700, Loss: 0.11344300955533981
step: 5800, Loss: 0.11485733836889267
step: 5900, Loss: 0.1143614798784256
step: 6000, Loss: 0.11483129858970642
step: 6100, Loss: 0.11377675831317902
step: 6200, Loss: 0.11414572596549988
step: 6300, Loss: 0.11479596793651581
step: 6400, Loss: 0.11403429508209229
step: 6500, Loss: 0.1141127347946167
step: 6600, Loss: 0.11376327276229858
step: 6700, Loss: 0.11442774534225464
step: 6800, Loss: 0.11285790801048279
step: 6900, Loss: 0.1138656958937645
step: 7000, Loss: 0.11476004123687744
step: 7100, Loss: 0.11445818096399307
step: 7200, Loss: 0.11389213800430298
step: 7300, Loss: 0.11342138797044754
step: 7400, Loss: 0.11419020593166351
step: 7500, Loss: 5.0945329666137695
step: 7600, Loss: 0.14829395711421967
step: 7700, Loss: 0.13402467966079712
step: 7800, Loss: 0.13248279690742493
step: 7900, Loss: 0.1268279254436493
step: 8000, Loss: 0.12728174030780792
step: 8100, Loss: 0.12535887956619263
step: 8200, Loss: 0.12292095273733139
step: 8300, Loss: 0.1221766397356987
step: 8400, Loss: 0.11948481947183609
step: 8500, Loss: 0.11729823797941208
step: 8600, Loss: 0.11781816184520721
step: 8700, Loss: 0.11588210612535477
step: 8800, Loss: 0.11722613871097565
step: 8900, Loss: 0.11527220904827118
step: 9000, Loss: 0.11797484755516052
step: 9100, Loss: 0.11459824442863464
step: 9200, Loss: 0.11781343817710876
step: 9300, Loss: 0.11524810642004013
step: 9400, Loss: 0.11416490375995636
step: 9500, Loss: 0.11564987897872925
step: 9600, Loss: 0.11664731055498123
step: 9700, Loss: 0.11713595688343048
step: 9800, Loss: 0.11514680087566376
step: 9900, Loss: 0.11516718566417694
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.8125
recall: 0.7647058823529411
F_score: 0.787878787878788
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.13807927072048187
step: 100, Loss: 0.12104160338640213
step: 200, Loss: 0.11702828109264374
step: 300, Loss: 0.11361557990312576
step: 400, Loss: 0.11606096476316452
step: 500, Loss: 0.11452692747116089
step: 600, Loss: 0.11409347504377365
step: 700, Loss: 0.11495421826839447
step: 800, Loss: 0.11364859342575073
step: 900, Loss: 0.11553247272968292
step: 1000, Loss: 0.11455290019512177
step: 1100, Loss: 0.11386477202177048
step: 1200, Loss: 0.11591920256614685
step: 1300, Loss: 0.11524751037359238
step: 1400, Loss: 0.11363587528467178
step: 1500, Loss: 0.11351253092288971
step: 1600, Loss: 0.11487683653831482
step: 1700, Loss: 0.1133451759815216
step: 1800, Loss: 0.11470760405063629
step: 1900, Loss: 0.11512769013643265
step: 2000, Loss: 0.1152908056974411
step: 2100, Loss: 0.11409413814544678
step: 2200, Loss: 0.1133570447564125
step: 2300, Loss: 0.11376947164535522
step: 2400, Loss: 0.11507944762706757
step: 2500, Loss: 0.11348573118448257
step: 2600, Loss: 0.11454907059669495
step: 2700, Loss: 0.11359316110610962
step: 2800, Loss: 0.11567787081003189
step: 2900, Loss: 0.11401030421257019
step: 3000, Loss: 0.11421281099319458
step: 3100, Loss: 0.11431074142456055
step: 3200, Loss: 0.1138109415769577
step: 3300, Loss: 0.11435193568468094
step: 3400, Loss: 0.11392074823379517
step: 3500, Loss: 0.11426607519388199
step: 3600, Loss: 0.1131705641746521
step: 3700, Loss: 0.11371815204620361
step: 3800, Loss: 0.11392362415790558
step: 3900, Loss: 0.11307942867279053
step: 4000, Loss: 0.11477302014827728
step: 4100, Loss: 0.11400569975376129
step: 4200, Loss: 0.11402881145477295
step: 4300, Loss: 0.1142863780260086
step: 4400, Loss: 0.11439673602581024
step: 4500, Loss: 0.11339213699102402
step: 4600, Loss: 0.11379824578762054
step: 4700, Loss: 0.11323767900466919
step: 4800, Loss: 0.11548759043216705
step: 4900, Loss: 0.11439959704875946
step: 5000, Loss: 0.11381388455629349
step: 5100, Loss: 0.11329232156276703
step: 5200, Loss: 0.11484412848949432
step: 5300, Loss: 0.11342364549636841
step: 5400, Loss: 0.11394046992063522
step: 5500, Loss: 0.11572962999343872
step: 5600, Loss: 0.11459353566169739
step: 5700, Loss: 0.11332479119300842
step: 5800, Loss: 0.1157517284154892
step: 5900, Loss: 0.11548183858394623
step: 6000, Loss: 0.11299699544906616
step: 6100, Loss: 0.11280867457389832
step: 6200, Loss: 0.1135624349117279
step: 6300, Loss: 0.11622252315282822
step: 6400, Loss: 0.1135118156671524
step: 6500, Loss: 0.11624559015035629
step: 6600, Loss: 0.1145375445485115
step: 6700, Loss: 0.11396877467632294
step: 6800, Loss: 0.11507454514503479
step: 6900, Loss: 0.11578110605478287
step: 7000, Loss: 0.11348019540309906
step: 7100, Loss: 0.11284438520669937
step: 7200, Loss: 0.11456555128097534
step: 7300, Loss: 0.11446588486433029
step: 7400, Loss: 0.17793042957782745
step: 7500, Loss: 0.13180166482925415
step: 7600, Loss: 0.137266606092453
step: 7700, Loss: 0.12696516513824463
step: 7800, Loss: 0.12752041220664978
step: 7900, Loss: 0.1204543262720108
step: 8000, Loss: 0.12222275137901306
step: 8100, Loss: 0.1201552003622055
step: 8200, Loss: 0.1209864616394043
step: 8300, Loss: 0.118320032954216
step: 8400, Loss: 0.1188339963555336
step: 8500, Loss: 0.11788982152938843
step: 8600, Loss: 0.11616728454828262
step: 8700, Loss: 0.11538373678922653
step: 8800, Loss: 0.1161571592092514
step: 8900, Loss: 0.11771689355373383
step: 9000, Loss: 0.11493072658777237
step: 9100, Loss: 0.11941513419151306
step: 9200, Loss: 0.11498649418354034
step: 9300, Loss: 0.11564836651086807
step: 9400, Loss: 0.11408248543739319
step: 9500, Loss: 0.11408892273902893
step: 9600, Loss: 0.11494103074073792
step: 9700, Loss: 0.11332058906555176
step: 9800, Loss: 0.11423133313655853
step: 9900, Loss: 0.11433325707912445
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9230769230769231
recall: 0.8
F_score: 0.8571428571428571
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.13067463040351868
step: 100, Loss: 0.1167394295334816
step: 200, Loss: 0.1167665421962738
step: 300, Loss: 0.11344044655561447
step: 400, Loss: 0.11722695827484131
step: 500, Loss: 0.11339148879051208
step: 600, Loss: 0.11394064873456955
step: 700, Loss: 0.11410152167081833
step: 800, Loss: 0.11317815631628036
step: 900, Loss: 0.11593746393918991
step: 1000, Loss: 0.1139373630285263
step: 1000, Loss: 0.1184234470129013
step: 1100, Loss: 0.12271103262901306
step: 1200, Loss: 0.11641567945480347
step: 1300, Loss: 2.5558972358703613
step: 1400, Loss: 0.5861927270889282
step: 1500, Loss: 0.2719815969467163
step: 1600, Loss: 0.13912397623062134
step: 1700, Loss: 0.1278948187828064
step: 1800, Loss: 0.13416123390197754
step: 1900, Loss: 0.13500958681106567
step: 2000, Loss: 0.12185736745595932
step: 2100, Loss: 0.12382390350103378
step: 2200, Loss: 0.12797453999519348
step: 2300, Loss: 0.13169506192207336
step: 2400, Loss: 0.12631474435329437
step: 2500, Loss: 0.12395022064447403
step: 2600, Loss: 0.12462155520915985
step: 2700, Loss: 0.12375639379024506
step: 2800, Loss: 0.11916281282901764
step: 2900, Loss: 0.11982467770576477
step: 3000, Loss: 0.12137242406606674
step: 3100, Loss: 0.11667805910110474
step: 3200, Loss: 0.11797778308391571
step: 3300, Loss: 0.11795993149280548
step: 3400, Loss: 0.1964506208896637
step: 3500, Loss: 0.11883664131164551
step: 3600, Loss: 0.11655013263225555
step: 3700, Loss: 0.12129337340593338
step: 3800, Loss: 0.11720557510852814
step: 3900, Loss: 0.11487148702144623
step: 4000, Loss: 0.11525528132915497
step: 4100, Loss: 0.11795680969953537
step: 4200, Loss: 0.11873184889554977
step: 4300, Loss: 0.11414386332035065
step: 4400, Loss: 0.11929292976856232
step: 4500, Loss: 0.11479347944259644
step: 4600, Loss: 0.11641503125429153
step: 4700, Loss: 0.1144457757472992
step: 4800, Loss: 0.11536155641078949
step: 4900, Loss: 0.11398155987262726
step: 5000, Loss: 0.11478118598461151
step: 5100, Loss: 0.11408456414937973
step: 5200, Loss: 0.11369717866182327
step: 5300, Loss: 0.1927943229675293
step: 5400, Loss: 0.11422916501760483
step: 5500, Loss: 0.11373723298311234
step: 5600, Loss: 0.11472561955451965
step: 5700, Loss: 0.1142168864607811
step: 5800, Loss: 0.1137627512216568
step: 5900, Loss: 0.11340608447790146
step: 6000, Loss: 0.11556614935398102
step: 6100, Loss: 0.1131371408700943
step: 6200, Loss: 0.11466162651777267
step: 6300, Loss: 0.11428254097700119
step: 6400, Loss: 0.114934042096138
step: 6500, Loss: 0.11401904374361038
step: 6600, Loss: 0.11464685201644897
step: 6700, Loss: 0.11581514775753021
step: 6800, Loss: 0.1149725541472435
step: 6900, Loss: 0.11508876085281372
step: 7000, Loss: 0.11587855219841003
step: 7100, Loss: 0.11704697459936142
step: 7200, Loss: 0.19368955492973328
step: 7300, Loss: 0.11421534419059753
step: 7400, Loss: 0.1147599071264267
step: 7500, Loss: 0.12149308621883392
step: 7600, Loss: 0.11567836999893188
step: 7700, Loss: 0.11703807860612869
step: 7800, Loss: 0.115646593272686
step: 7900, Loss: 0.11571402102708817
step: 8000, Loss: 0.11824287474155426
step: 8100, Loss: 0.1369837075471878
step: 8200, Loss: 1.769020915031433
step: 8300, Loss: 0.15473374724388123
step: 8400, Loss: 0.15522752702236176
step: 8500, Loss: 0.14249053597450256
step: 8600, Loss: 0.1390993595123291
step: 8700, Loss: 0.13266168534755707
step: 8800, Loss: 0.12947484850883484
step: 8900, Loss: 0.13016903400421143
step: 9000, Loss: 0.13587959110736847
step: 9100, Loss: 0.21846765279769897
step: 9200, Loss: 0.13013802468776703
step: 9300, Loss: 0.13080014288425446
step: 9400, Loss: 0.1203312799334526
step: 9500, Loss: 0.12154112011194229
step: 9600, Loss: 0.1299997717142105
step: 9700, Loss: 0.12180778384208679
step: 9800, Loss: 0.12130177021026611
step: 9900, Loss: 0.12364627420902252
training successfully ended.
validating...
validate data length:76
acc: 0.8888888888888888
precision: 0.8333333333333334
recall: 0.9722222222222222
F_score: 0.8974358974358975
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.15924429893493652
step: 100, Loss: 0.11468382924795151
step: 200, Loss: 0.11456760764122009
step: 300, Loss: 0.11473207175731659
step: 400, Loss: 0.11427781730890274
step: 500, Loss: 0.11452777683734894
step: 600, Loss: 0.11482848972082138
step: 700, Loss: 0.113688625395298
step: 800, Loss: 0.11410871148109436
step: 900, Loss: 0.11677190661430359
step: 1000, Loss: 0.11450766026973724
step: 1100, Loss: 0.11511708796024323
step: 1200, Loss: 0.12253940850496292
step: 1300, Loss: 0.12522195279598236
step: 1400, Loss: 0.13495494425296783
step: 1500, Loss: 0.2339678257703781
step: 1600, Loss: 0.27941709756851196
step: 1700, Loss: 0.16499081254005432
step: 1800, Loss: 0.13404953479766846
step: 1900, Loss: 0.12884125113487244
step: 2000, Loss: 0.13479289412498474
step: 2100, Loss: 0.13506324589252472
step: 2200, Loss: 0.13862070441246033
step: 2300, Loss: 0.12623149156570435
step: 2400, Loss: 0.12633076310157776
step: 2500, Loss: 0.12311388552188873
step: 2600, Loss: 0.12461711466312408
step: 2700, Loss: 0.12486415356397629
step: 2800, Loss: 0.121725894510746
step: 2900, Loss: 0.12153470516204834
step: 3000, Loss: 0.12280087172985077
step: 3100, Loss: 0.12012454867362976
step: 3200, Loss: 0.12570498883724213
step: 3300, Loss: 0.11903993040323257
step: 3400, Loss: 0.1970198005437851
step: 3500, Loss: 0.11929184198379517
step: 3600, Loss: 0.11735514551401138
step: 3700, Loss: 0.11719875782728195
step: 3800, Loss: 0.11639361828565598
step: 3900, Loss: 0.11783178150653839
step: 4000, Loss: 0.11943662166595459
step: 4100, Loss: 0.11495797336101532
step: 4200, Loss: 0.11559710651636124
step: 4300, Loss: 0.11548161506652832
step: 4400, Loss: 0.11484573781490326
step: 4500, Loss: 0.11589401960372925
step: 4600, Loss: 0.11502687633037567
step: 4700, Loss: 0.11493167281150818
step: 4800, Loss: 0.11317499727010727
step: 4900, Loss: 0.1151590570807457
step: 5000, Loss: 0.11473903805017471
step: 5100, Loss: 0.11592889577150345
step: 5200, Loss: 0.11514802277088165
step: 5300, Loss: 0.1911648064851761
step: 5400, Loss: 0.11623731255531311
step: 5500, Loss: 0.11462189257144928
step: 5600, Loss: 0.11367364227771759
step: 5700, Loss: 0.11335869878530502
step: 5800, Loss: 0.11359015107154846
step: 5900, Loss: 0.11558505892753601
step: 6000, Loss: 0.1150117889046669
step: 6100, Loss: 0.11371497064828873
step: 6200, Loss: 0.11564327776432037
step: 6300, Loss: 0.11331683397293091
step: 6400, Loss: 0.11494438350200653
step: 6500, Loss: 0.11521449685096741
step: 6600, Loss: 0.11688520014286041
step: 6700, Loss: 0.11511456966400146
step: 6800, Loss: 0.12278401106595993
step: 6900, Loss: 0.11922943592071533
step: 7000, Loss: 0.116487056016922
step: 7100, Loss: 0.11446741968393326
step: 7200, Loss: 0.19419986009597778
step: 7300, Loss: 0.11662460118532181
step: 7400, Loss: 0.11432598531246185
step: 7500, Loss: 0.11909034848213196
step: 7600, Loss: 0.11674292385578156
step: 7700, Loss: 0.11535567045211792
step: 7800, Loss: 0.1140832006931305
step: 7900, Loss: 0.11505482345819473
step: 8000, Loss: 0.11892913281917572
step: 8100, Loss: 0.117861807346344
step: 8200, Loss: 0.12462309002876282
step: 8300, Loss: 0.12088076770305634
step: 8400, Loss: 1.7859137058258057
step: 8500, Loss: 0.8990933299064636
step: 8600, Loss: 0.14571258425712585
step: 8700, Loss: 0.15213215351104736
step: 8800, Loss: 0.12967656552791595
step: 8900, Loss: 0.13749998807907104
step: 9000, Loss: 0.1321156919002533
step: 9100, Loss: 0.22132733464241028
step: 9200, Loss: 0.14280523359775543
step: 9300, Loss: 0.13098542392253876
step: 9400, Loss: 0.13187871873378754
step: 9500, Loss: 0.12347637116909027
step: 9600, Loss: 0.11862267553806305
step: 9700, Loss: 0.12202301621437073
step: 9800, Loss: 0.12105655670166016
step: 9900, Loss: 0.1215309351682663
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.868421052631579
recall: 1.0
F_score: 0.9295774647887324
subject 0 Avgacc: 0.9347222222222221 Avgfscore: 0.9353940965594809 
 Max acc:1.0, Max f score:1.0
******** mix subject_1 ********

[247, 513]
******fold 1******

Training... train_data length:923
step: 0, Loss: 25.62920379638672
step: 100, Loss: 0.1968335509300232
step: 200, Loss: 0.16722159087657928
step: 300, Loss: 0.15694507956504822
step: 400, Loss: 0.13831524550914764
step: 500, Loss: 0.1331036239862442
step: 600, Loss: 0.12694740295410156
step: 700, Loss: 0.12724696099758148
step: 800, Loss: 0.125843346118927
step: 900, Loss: 0.12135154753923416
step: 1000, Loss: 0.12287508696317673
step: 1100, Loss: 0.12201055884361267
step: 1100, Loss: 0.11401523649692535
step: 1200, Loss: 0.11284567415714264
step: 1300, Loss: 0.1140344887971878
step: 1400, Loss: 0.11308487504720688
step: 1500, Loss: 0.1129915788769722
step: 1600, Loss: 0.1145629808306694
step: 1700, Loss: 0.11470796912908554
step: 1800, Loss: 0.11561018228530884
step: 1900, Loss: 0.11378024518489838
step: 2000, Loss: 0.11432197690010071
step: 2100, Loss: 0.11396289616823196
step: 2200, Loss: 0.11444827914237976
step: 2300, Loss: 0.11405789852142334
step: 2400, Loss: 0.11336328089237213
step: 2500, Loss: 0.11438433080911636
step: 2600, Loss: 0.11415806412696838
step: 2700, Loss: 0.11427552998065948
step: 2800, Loss: 0.11337822675704956
step: 2900, Loss: 0.11389222741127014
step: 3000, Loss: 0.11387820541858673
step: 3100, Loss: 0.11297039687633514
step: 3200, Loss: 0.11357799917459488
step: 3300, Loss: 0.11347126960754395
step: 3400, Loss: 0.11506670713424683
step: 3500, Loss: 0.1138579398393631
step: 3600, Loss: 0.11396797001361847
step: 3700, Loss: 0.1135634183883667
step: 3800, Loss: 0.11438947170972824
step: 3900, Loss: 0.11388254165649414
step: 4000, Loss: 0.11464071273803711
step: 4100, Loss: 0.1131175085902214
step: 4200, Loss: 0.1149960458278656
step: 4300, Loss: 0.1152692660689354
step: 4400, Loss: 0.1147366389632225
step: 4500, Loss: 0.11426766216754913
step: 4600, Loss: 0.11415787041187286
step: 4700, Loss: 0.11536451429128647
step: 4800, Loss: 0.11477773636579514
step: 4900, Loss: 0.1132797822356224
step: 5000, Loss: 0.11451811343431473
step: 5100, Loss: 0.11395876109600067
step: 5200, Loss: 0.11464177072048187
step: 5300, Loss: 0.11301982402801514
step: 5400, Loss: 0.11444889008998871
step: 5500, Loss: 0.11820108443498611
step: 5600, Loss: 0.11517474800348282
step: 5700, Loss: 0.11385499686002731
step: 5800, Loss: 0.11436425149440765
step: 5900, Loss: 0.11424703896045685
step: 6000, Loss: 0.11273649334907532
step: 6100, Loss: 0.11430712044239044
step: 6200, Loss: 0.11325220763683319
step: 6300, Loss: 0.11397181451320648
step: 6400, Loss: 0.11634561419487
step: 6500, Loss: 0.11686965823173523
step: 6600, Loss: 0.1150248721241951
step: 6700, Loss: 0.11427999287843704
step: 6800, Loss: 0.11502698063850403
step: 6900, Loss: 2.631575584411621
step: 7000, Loss: 0.16367647051811218
step: 7100, Loss: 0.13389240205287933
step: 7200, Loss: 0.13144180178642273
step: 7300, Loss: 0.12994253635406494
step: 7400, Loss: 0.12054865807294846
step: 7500, Loss: 0.12284192442893982
step: 7600, Loss: 0.11860555410385132
step: 7700, Loss: 0.11879808455705643
step: 7800, Loss: 0.11759766191244125
step: 7900, Loss: 0.12261992692947388
step: 8000, Loss: 0.11553019285202026
step: 8100, Loss: 0.1176094189286232
step: 8200, Loss: 0.1156257838010788
step: 8300, Loss: 0.1196417436003685
step: 8400, Loss: 0.11473281681537628
step: 8500, Loss: 0.11845508217811584
step: 8600, Loss: 0.11467433720827103
step: 8700, Loss: 0.11693941056728363
step: 8800, Loss: 0.11729993671178818
step: 8900, Loss: 0.11570103466510773
step: 9000, Loss: 0.11503540724515915
step: 9100, Loss: 0.11518500000238419
step: 9200, Loss: 0.1133657693862915
step: 9300, Loss: 0.11520879715681076
step: 9400, Loss: 0.11315968632698059
step: 9500, Loss: 0.11519362032413483
step: 9600, Loss: 0.1134711503982544
step: 9700, Loss: 0.11592891067266464
step: 9800, Loss: 0.11380800604820251
step: 9900, Loss: 0.11516249924898148
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8333333333333334
recall: 0.8823529411764706
F_score: 0.8571428571428571
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.13724622130393982
step: 100, Loss: 0.11742712557315826
step: 200, Loss: 0.11634685099124908
step: 300, Loss: 0.11496986448764801
step: 400, Loss: 0.11500398814678192
step: 500, Loss: 0.11675070226192474
step: 600, Loss: 0.11331702023744583
step: 700, Loss: 0.11359626054763794
step: 800, Loss: 0.11383909732103348
step: 900, Loss: 0.11459997296333313
step: 1000, Loss: 0.11335564404726028
step: 1100, Loss: 0.11356222629547119
step: 1200, Loss: 0.11421270668506622
step: 1300, Loss: 0.11421389877796173
step: 1400, Loss: 0.11426610499620438
step: 1500, Loss: 0.11461669951677322
step: 1600, Loss: 0.11321520060300827
step: 1700, Loss: 0.11377739906311035
step: 1800, Loss: 0.11386645585298538
step: 1900, Loss: 0.11423304677009583
step: 2000, Loss: 0.11324025690555573
step: 2100, Loss: 0.11352041363716125
step: 2200, Loss: 0.1134495660662651
step: 2300, Loss: 0.11307857930660248
step: 2400, Loss: 0.11225386708974838
step: 2500, Loss: 0.1160065084695816
step: 2600, Loss: 0.1132180392742157
step: 2700, Loss: 0.11600777506828308
step: 2800, Loss: 0.1155046820640564
step: 2900, Loss: 0.11375701427459717
step: 3000, Loss: 0.11459658294916153
step: 3100, Loss: 0.11297314614057541
step: 3200, Loss: 0.11482717096805573
step: 3300, Loss: 0.11395139247179031
step: 3400, Loss: 0.11316341906785965
step: 3500, Loss: 0.11440292000770569
step: 3600, Loss: 0.1132798120379448
step: 3700, Loss: 0.11387438327074051
step: 3800, Loss: 0.11456288397312164
step: 3900, Loss: 0.11377550661563873
step: 4000, Loss: 0.11501877009868622
step: 4100, Loss: 0.11425872147083282
step: 4200, Loss: 0.11550139635801315
step: 4300, Loss: 0.12056972831487656
step: 4400, Loss: 0.11359833925962448
step: 4500, Loss: 0.11436924338340759
step: 4600, Loss: 0.11392273753881454
step: 4700, Loss: 0.1158398762345314
step: 4800, Loss: 0.11444924771785736
step: 4900, Loss: 0.11456465721130371
step: 5000, Loss: 0.11349952965974808
step: 5100, Loss: 0.11420301347970963
step: 5200, Loss: 0.11533957719802856
step: 5300, Loss: 0.11392775177955627
step: 5400, Loss: 0.11662209033966064
step: 5500, Loss: 0.11391876637935638
step: 5600, Loss: 0.114827461540699
step: 5700, Loss: 0.11340049654245377
step: 5800, Loss: 0.11362061649560928
step: 5900, Loss: 0.11516513675451279
step: 6000, Loss: 0.11565552651882172
step: 6100, Loss: 0.11381969600915909
step: 6200, Loss: 0.11531906574964523
step: 6300, Loss: 0.11544261872768402
step: 6400, Loss: 0.11380349099636078
step: 6500, Loss: 0.11469315737485886
step: 6600, Loss: 0.11362911760807037
step: 6700, Loss: 0.1145891398191452
step: 6800, Loss: 0.11479637026786804
step: 6900, Loss: 0.11371277272701263
step: 7000, Loss: 0.1140449121594429
step: 7100, Loss: 0.11384928226470947
step: 7200, Loss: 0.11350207030773163
step: 7300, Loss: 0.11380817741155624
step: 7400, Loss: 0.11613202840089798
step: 7500, Loss: 0.11597585678100586
step: 7600, Loss: 0.11536525934934616
step: 7700, Loss: 0.3247005343437195
step: 7800, Loss: 0.14057403802871704
step: 7900, Loss: 0.12739895284175873
step: 8000, Loss: 0.1298026591539383
step: 8100, Loss: 0.1270212084054947
step: 8200, Loss: 0.12476209551095963
step: 8300, Loss: 0.11874675750732422
step: 8400, Loss: 0.12418312579393387
step: 8500, Loss: 0.11691293865442276
step: 8600, Loss: 0.11923607438802719
step: 8700, Loss: 0.1171080470085144
step: 8800, Loss: 0.1160157173871994
step: 8900, Loss: 0.11617500334978104
step: 9000, Loss: 0.11716278642416
step: 9100, Loss: 0.11668069660663605
step: 9200, Loss: 0.11628683656454086
step: 9300, Loss: 0.11475390940904617
step: 9400, Loss: 0.11487625539302826
step: 9500, Loss: 0.11508992314338684
step: 9600, Loss: 0.11488611251115799
step: 9700, Loss: 0.11609584093093872
step: 9800, Loss: 0.11521592736244202
step: 9900, Loss: 0.11319220066070557
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7333333333333333
recall: 0.9166666666666666
F_score: 0.8148148148148148
subject 0 Avgacc: 0.7852083333333333 Avgfscore: 0.7943531931031931 
 Max acc:0.9, Max f score:0.9142857142857143
******** mix subject_1 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 47.469337463378906
step: 100, Loss: 0.4254417419433594
step: 200, Loss: 0.21865589916706085
step: 300, Loss: 0.13760130107402802
step: 400, Loss: 0.1377934366464615
step: 500, Loss: 0.12156994640827179
step: 600, Loss: 0.12478955835103989
step: 700, Loss: 0.12367397546768188
step: 800, Loss: 0.12211489677429199
step: 900, Loss: 0.11511734873056412
step: 1000, Loss: 0.11871571093797684
step: 1100, Loss: 0.12005376070737839
step: 1200, Loss: 0.11998401582241058
step: 1300, Loss: 0.11938559263944626
step: 1400, Loss: 0.11914017796516418
step: 1500, Loss: 0.12009139358997345
step: 1600, Loss: 0.1172916442155838
step: 1700, Loss: 0.11821530014276505
step: 1800, Loss: 0.1188565194606781
step: 1900, Loss: 0.11707086861133575
step: 2000, Loss: 0.11818468570709229
step: 2100, Loss: 0.1160275936126709
step: 2200, Loss: 0.1151176318526268
step: 2300, Loss: 0.11677199602127075
step: 2400, Loss: 0.11334031820297241
step: 2500, Loss: 0.11506500095129013
step: 2600, Loss: 0.11584498733282089
step: 2700, Loss: 0.11465825140476227
step: 2800, Loss: 0.11905898153781891
step: 2900, Loss: 0.1151990219950676
step: 3000, Loss: 0.1144050806760788
step: 3100, Loss: 0.11403993517160416
step: 3200, Loss: 0.1172058954834938
step: 3300, Loss: 0.11518050730228424
step: 3400, Loss: 0.1165519505739212
step: 3500, Loss: 0.11476601660251617
step: 3600, Loss: 0.11412347853183746
step: 3700, Loss: 0.11577795445919037
step: 3800, Loss: 0.11626796424388885
step: 3900, Loss: 0.11423731595277786
step: 4000, Loss: 0.11527746915817261
step: 4100, Loss: 0.11716698110103607
step: 4200, Loss: 0.11678425222635269
step: 4300, Loss: 0.11533394455909729
step: 4400, Loss: 0.11744587123394012
step: 4500, Loss: 0.1178959459066391
step: 4600, Loss: 0.11506102234125137
step: 4700, Loss: 0.11711886525154114
step: 4800, Loss: 0.11830723285675049
step: 4900, Loss: 0.12413787841796875
step: 5000, Loss: 0.11923089623451233
step: 5100, Loss: 0.1170966774225235
step: 5200, Loss: 0.116716668009758
step: 5300, Loss: 0.11381769925355911
step: 5400, Loss: 0.11374542117118835
step: 5500, Loss: 0.11410319060087204
step: 5600, Loss: 0.11529459804296494
step: 5700, Loss: 0.11816632002592087
step: 5800, Loss: 0.11570502817630768
step: 5900, Loss: 0.11329536139965057
step: 6000, Loss: 0.11667494475841522
step: 6100, Loss: 0.11735784262418747
step: 6200, Loss: 0.11477689445018768
step: 6300, Loss: 0.11726858466863632
step: 6400, Loss: 0.11759679764509201
step: 6500, Loss: 1.9480738639831543
step: 6600, Loss: 0.20129460096359253
step: 6700, Loss: 0.16591981053352356
step: 6800, Loss: 0.15512652695178986
step: 6900, Loss: 0.14141963422298431
step: 7000, Loss: 0.140286386013031
step: 7100, Loss: 0.13748931884765625
step: 7200, Loss: 0.13873718678951263
step: 7300, Loss: 0.1373685598373413
step: 7400, Loss: 0.1267905980348587
step: 7500, Loss: 0.13166022300720215
step: 7600, Loss: 0.12912753224372864
step: 7700, Loss: 0.12817205488681793
step: 7800, Loss: 0.12554161250591278
step: 7900, Loss: 0.124900683760643
step: 8000, Loss: 0.12890781462192535
step: 8100, Loss: 0.12166301161050797
step: 8200, Loss: 0.12699943780899048
step: 8300, Loss: 0.12510763108730316
step: 8400, Loss: 0.12297137826681137
step: 8500, Loss: 0.12187625467777252
step: 8600, Loss: 0.1223343163728714
step: 8700, Loss: 0.1213800236582756
step: 8800, Loss: 0.119489386677742
step: 8900, Loss: 0.12036611139774323
step: 9000, Loss: 0.12268677353858948
step: 9100, Loss: 0.11906646192073822
step: 9200, Loss: 0.12064895033836365
step: 9300, Loss: 0.12003607302904129
step: 9400, Loss: 0.11933011561632156
step: 9500, Loss: 0.11890567094087601
step: 9600, Loss: 0.11641488969326019
step: 9700, Loss: 0.11832892149686813
step: 9800, Loss: 0.11913636326789856
step: 9900, Loss: 0.11655780673027039
training successfully ended.
validating...
validate data length:103
acc: 0.9791666666666666
precision: 0.9555555555555556
recall: 1.0
F_score: 0.9772727272727273
******fold 2******

Training... train_data length:923
step: 0, Loss: 0.2254684567451477
step: 100, Loss: 0.12049194425344467
step: 200, Loss: 0.11689754575490952
step: 300, Loss: 0.11682887375354767
step: 400, Loss: 0.11469902098178864
step: 500, Loss: 0.11550192534923553
step: 600, Loss: 0.11631195992231369
step: 700, Loss: 0.1192953884601593
step: 800, Loss: 0.11552897840738297
step: 900, Loss: 0.11748331040143967
step: 1000, Loss: 0.1144866943359375
step: 1100, Loss: 0.11521477997303009
step: 1200, Loss: 0.11476268619298935
step: 1300, Loss: 0.11574037373065948
step: 1400, Loss: 0.1168263629078865
step: 1500, Loss: 0.1138526126742363
step: 1600, Loss: 0.11382871866226196
step: 1700, Loss: 0.11453494429588318
step: 1800, Loss: 0.1162719652056694
step: 1900, Loss: 0.11427416652441025
step: 2000, Loss: 0.11325376480817795
step: 2100, Loss: 0.11880200356245041
step: 2200, Loss: 0.12935000658035278
step: 2300, Loss: 0.22468289732933044
step: 2400, Loss: 0.1558624505996704
step: 2500, Loss: 0.14818257093429565
step: 2600, Loss: 0.14942210912704468
step: 2700, Loss: 0.13451553881168365
step: 2800, Loss: 0.13716861605644226
step: 2900, Loss: 0.13121986389160156
step: 3000, Loss: 0.13213282823562622
step: 3100, Loss: 0.12864753603935242
step: 3200, Loss: 0.12553109228610992
step: 3300, Loss: 0.1253076195716858
step: 3400, Loss: 0.12284059077501297
step: 3500, Loss: 0.12268462032079697
step: 3600, Loss: 0.12058040499687195
step: 3700, Loss: 0.1225527748465538
step: 3800, Loss: 0.1256120353937149
step: 3900, Loss: 0.12248557806015015
step: 4000, Loss: 0.11984506249427795
step: 4100, Loss: 0.12042538821697235
step: 4200, Loss: 0.11685788631439209
step: 4300, Loss: 0.11782278120517731
step: 4400, Loss: 0.11845500022172928
step: 4500, Loss: 0.1186056137084961
step: 4600, Loss: 0.11795379221439362
step: 4700, Loss: 0.1183524876832962
step: 4800, Loss: 0.11684117466211319
step: 4900, Loss: 0.11593817919492722
step: 5000, Loss: 0.11569783091545105
step: 5100, Loss: 0.1155439242720604
step: 5200, Loss: 0.11732575297355652
step: 5300, Loss: 0.11475911736488342
step: 5400, Loss: 0.11443685740232468
step: 5500, Loss: 0.11586783081293106
step: 5600, Loss: 0.11549603193998337
step: 5700, Loss: 0.11630990356206894
step: 5800, Loss: 0.1135198101401329
step: 5900, Loss: 0.11399310827255249
step: 6000, Loss: 0.1135694682598114
step: 6100, Loss: 0.11422547698020935
step: 6200, Loss: 0.11363063752651215
step: 6300, Loss: 0.11603138595819473
step: 6400, Loss: 0.11319361627101898
step: 6500, Loss: 0.11391631513834
step: 6600, Loss: 0.11467550694942474
step: 6700, Loss: 0.11423458904027939
step: 6800, Loss: 0.1146405041217804
step: 6900, Loss: 0.11386605352163315
step: 7000, Loss: 0.11348296701908112
step: 7100, Loss: 0.11327403783798218
step: 7200, Loss: 0.11440067738294601
step: 7300, Loss: 0.11327624320983887
step: 7400, Loss: 0.11399725824594498
step: 7500, Loss: 0.11406334489583969
step: 7600, Loss: 0.11370491236448288
step: 7700, Loss: 0.11364393681287766
step: 7800, Loss: 0.11334492266178131
step: 7900, Loss: 0.11531338095664978
step: 8000, Loss: 0.11341790854930878
step: 8100, Loss: 0.11367097496986389
step: 8200, Loss: 0.11488549411296844
step: 8300, Loss: 0.11337713152170181
step: 8400, Loss: 0.11400756239891052
step: 8500, Loss: 0.11367633193731308
step: 8600, Loss: 0.1136779934167862
step: 8700, Loss: 0.11590174585580826
step: 8800, Loss: 0.11448070406913757
step: 8900, Loss: 0.11426675319671631
step: 9000, Loss: 0.11481014639139175
step: 9100, Loss: 0.11528107523918152
step: 9200, Loss: 0.11429284512996674
step: 9300, Loss: 0.11508704721927643
step: 9400, Loss: 0.12279503792524338
step: 9500, Loss: 0.12174498289823532
step: 9600, Loss: 0.4291360676288605
step: 9700, Loss: 0.16043217480182648
step: 9800, Loss: 0.1410772204399109
step: 9900, Loss: 0.1379653662443161
training successfully ended.
validating...
validate data length:103
acc: 0.9791666666666666
precision: 0.9574468085106383
recall: 1.0
F_score: 0.9782608695652174
******fold 3******

Training... train_data length:923
step: 0, Loss: 0.18385592103004456
step: 100, Loss: 0.12179955840110779
step: 200, Loss: 0.11604585498571396
step: 300, Loss: 0.11813156306743622
step: 400, Loss: 0.11754627525806427
step: 500, Loss: 0.11716456711292267
step: 600, Loss: 0.11748778820037842
step: 700, Loss: 0.1161535456776619
step: 800, Loss: 0.11676710098981857
step: 900, Loss: 0.11456558108329773
step: 1000, Loss: 0.11859014630317688
step: 1100, Loss: 0.11610660701990128
step: 1200, Loss: 0.11589132249355316
step: 1300, Loss: 0.11602171510457993
step: 1400, Loss: 0.11382368206977844
step: 1500, Loss: 0.11465151607990265
step: 1600, Loss: 0.11919596046209335
step: 1700, Loss: 0.115273617208004
step: 1200, Loss: 0.11570653319358826
step: 1300, Loss: 0.11469928175210953
step: 1400, Loss: 0.11848495155572891
step: 1500, Loss: 0.11521482467651367
step: 1600, Loss: 0.1175934225320816
step: 1700, Loss: 0.11603716015815735
step: 1800, Loss: 0.11730849742889404
step: 1900, Loss: 0.11705222725868225
step: 2000, Loss: 0.11828368902206421
step: 2100, Loss: 0.11969926953315735
step: 2200, Loss: 0.11425470560789108
step: 2300, Loss: 0.11834025382995605
step: 2400, Loss: 0.1156756728887558
step: 2500, Loss: 0.11484109610319138
step: 2600, Loss: 0.11603663861751556
step: 2700, Loss: 0.1155838742852211
step: 2800, Loss: 0.11497172713279724
step: 2900, Loss: 0.11486084759235382
step: 3000, Loss: 0.11616411805152893
step: 3100, Loss: 0.11529179662466049
step: 3200, Loss: 0.11675199121236801
step: 3300, Loss: 0.11737046390771866
step: 3400, Loss: 0.1168598160147667
step: 3500, Loss: 0.11796565353870392
step: 3600, Loss: 0.11958380043506622
step: 3700, Loss: 0.11463553458452225
step: 3800, Loss: 0.12045786529779434
step: 3900, Loss: 0.11692222952842712
step: 4000, Loss: 0.11555876582860947
step: 4100, Loss: 0.11705231666564941
step: 4200, Loss: 0.11535992473363876
step: 4300, Loss: 0.11724139750003815
step: 4400, Loss: 0.11657680571079254
step: 4500, Loss: 0.1159750446677208
step: 4600, Loss: 0.1172444075345993
step: 4700, Loss: 0.11680593341588974
step: 4800, Loss: 0.11772722750902176
step: 4900, Loss: 0.11831299215555191
step: 5000, Loss: 0.11883305013179779
step: 5100, Loss: 0.11503003537654877
step: 5200, Loss: 0.11460383236408234
step: 5300, Loss: 0.1163596510887146
step: 5400, Loss: 0.11972340941429138
step: 5500, Loss: 0.11629664152860641
step: 5600, Loss: 0.11647952347993851
step: 5700, Loss: 0.16494905948638916
step: 5800, Loss: 0.5571061372756958
step: 5900, Loss: 0.15232227742671967
step: 6000, Loss: 0.14250081777572632
step: 6100, Loss: 0.13402770459651947
step: 6200, Loss: 0.12609659135341644
step: 6300, Loss: 0.12645560503005981
step: 6400, Loss: 0.1256323605775833
step: 6500, Loss: 0.12395914644002914
step: 6600, Loss: 0.12206197530031204
step: 6700, Loss: 0.12590822577476501
step: 6800, Loss: 0.11752407252788544
step: 6900, Loss: 0.12080642580986023
step: 7000, Loss: 0.11936906725168228
step: 7100, Loss: 0.12186077982187271
step: 7200, Loss: 0.11774171888828278
step: 7300, Loss: 0.12065629661083221
step: 7400, Loss: 0.11503353714942932
step: 7500, Loss: 0.11619172245264053
step: 7600, Loss: 0.11717851459980011
step: 7700, Loss: 0.11587008833885193
step: 7800, Loss: 0.11693736910820007
step: 7900, Loss: 0.11623382568359375
step: 8000, Loss: 0.11632248759269714
step: 8100, Loss: 0.11579063534736633
step: 8200, Loss: 0.11520703136920929
step: 8300, Loss: 0.11514711380004883
step: 8400, Loss: 0.11508124321699142
step: 8500, Loss: 0.11421700567007065
step: 8600, Loss: 0.11582794785499573
step: 8700, Loss: 0.11683492362499237
step: 8800, Loss: 0.12116102874279022
step: 8900, Loss: 0.11568030714988708
step: 9000, Loss: 0.11531659215688705
step: 9100, Loss: 0.11503895372152328
step: 9200, Loss: 0.11624328792095184
step: 9300, Loss: 0.11486426740884781
step: 9400, Loss: 0.11749366670846939
step: 9500, Loss: 0.11510147154331207
step: 9600, Loss: 0.11455156654119492
step: 9700, Loss: 0.11507788300514221
step: 9800, Loss: 0.11426988989114761
step: 9900, Loss: 0.11522280424833298
training successfully ended.
validating...
validate data length:32
acc: 0.46875
precision: 0.38095238095238093
recall: 0.6666666666666666
F_score: 0.4848484848484849
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.6878219246864319
step: 100, Loss: 0.13305199146270752
step: 200, Loss: 0.1257108449935913
step: 300, Loss: 0.12174225598573685
step: 400, Loss: 0.12654733657836914
step: 500, Loss: 0.11757589876651764
step: 600, Loss: 0.11734172701835632
step: 700, Loss: 0.117233045399189
step: 800, Loss: 0.11783366650342941
step: 900, Loss: 0.11534354090690613
step: 1000, Loss: 0.11478528380393982
step: 1100, Loss: 0.11453451961278915
step: 1200, Loss: 0.11402514576911926
step: 1300, Loss: 0.11589037626981735
step: 1400, Loss: 0.11353828012943268
step: 1500, Loss: 0.11404313147068024
step: 1600, Loss: 0.11591967940330505
step: 1700, Loss: 0.11475799977779388
step: 1800, Loss: 0.11485959589481354
step: 1900, Loss: 0.11470082402229309
step: 2000, Loss: 0.11507126688957214
step: 2100, Loss: 0.11575669795274734
step: 2200, Loss: 0.11382803320884705
step: 2300, Loss: 0.11496132612228394
step: 2400, Loss: 0.11479503661394119
step: 2500, Loss: 0.11559134721755981
step: 2600, Loss: 0.1153814047574997
step: 2700, Loss: 0.11555391550064087
step: 2800, Loss: 0.11376555263996124
step: 2900, Loss: 0.11537928879261017
step: 3000, Loss: 0.11486990749835968
step: 3100, Loss: 0.11379104852676392
step: 3200, Loss: 0.11874715238809586
step: 3300, Loss: 0.11714590340852737
step: 3400, Loss: 0.11668242514133453
step: 3500, Loss: 0.11408818513154984
step: 3600, Loss: 0.11438897252082825
step: 3700, Loss: 0.11421424150466919
step: 3800, Loss: 0.11740431934595108
step: 3900, Loss: 0.11418677866458893
step: 4000, Loss: 0.1160869374871254
step: 4100, Loss: 0.11525420844554901
step: 4200, Loss: 0.11445797979831696
step: 4300, Loss: 0.11518250405788422
step: 4400, Loss: 0.11461633443832397
step: 4500, Loss: 0.11420351266860962
step: 4600, Loss: 0.11408008635044098
step: 4700, Loss: 0.11719824373722076
step: 4800, Loss: 0.28354179859161377
step: 4900, Loss: 0.14118294417858124
step: 5000, Loss: 0.12171129882335663
step: 5100, Loss: 0.12076995521783829
step: 5200, Loss: 0.11858576536178589
step: 5300, Loss: 0.11794020235538483
step: 5400, Loss: 0.12223084270954132
step: 5500, Loss: 0.11691749095916748
step: 5600, Loss: 0.11781318485736847
step: 5700, Loss: 0.11683273315429688
step: 5800, Loss: 0.11679636687040329
step: 5900, Loss: 0.11469857394695282
step: 6000, Loss: 0.11568129807710648
step: 6100, Loss: 0.11412559449672699
step: 6200, Loss: 0.11565768718719482
step: 6300, Loss: 0.11804800480604172
step: 6400, Loss: 0.11608966439962387
step: 6500, Loss: 0.11371223628520966
step: 6600, Loss: 0.11443212628364563
step: 6700, Loss: 0.12028589844703674
step: 6800, Loss: 0.11484817415475845
step: 6900, Loss: 0.11361008137464523
step: 7000, Loss: 0.11437669396400452
step: 7100, Loss: 0.11471027135848999
step: 7200, Loss: 0.11412915587425232
step: 7300, Loss: 0.11575297266244888
step: 7400, Loss: 0.11316900700330734
step: 7500, Loss: 0.115294449031353
step: 7600, Loss: 0.11522911489009857
step: 7700, Loss: 0.1139470711350441
step: 7800, Loss: 0.11694779247045517
step: 7900, Loss: 0.1151045560836792
step: 8000, Loss: 0.11327233165502548
step: 8100, Loss: 0.11651763319969177
step: 8200, Loss: 0.11436717957258224
step: 8300, Loss: 0.11448660492897034
step: 8400, Loss: 0.11541718244552612
step: 8500, Loss: 0.11463745683431625
step: 8600, Loss: 0.11680775135755539
step: 8700, Loss: 0.11523638665676117
step: 8800, Loss: 0.11404833942651749
step: 8900, Loss: 0.11586511135101318
step: 9000, Loss: 0.11421895772218704
step: 9100, Loss: 0.11424189060926437
step: 9200, Loss: 0.11382319033145905
step: 9300, Loss: 0.11431865394115448
step: 9400, Loss: 0.11435016989707947
step: 9500, Loss: 0.1145973801612854
step: 9600, Loss: 0.11520060151815414
step: 9700, Loss: 0.11559484153985977
step: 9800, Loss: 0.1144898384809494
step: 9900, Loss: 0.11424804478883743
training successfully ended.
validating...
validate data length:32
acc: 0.875
precision: 0.8333333333333334
recall: 0.9375
F_score: 0.8823529411764706
******fold 3******

Training... train_data length:281
step: 0, Loss: 2.4013304710388184
step: 100, Loss: 0.12194252014160156
step: 200, Loss: 0.11748068779706955
step: 300, Loss: 0.12020067125558853
step: 400, Loss: 0.11907905340194702
step: 500, Loss: 0.11652937531471252
step: 600, Loss: 0.11382031440734863
step: 700, Loss: 0.11703779548406601
step: 800, Loss: 0.11494415998458862
step: 900, Loss: 0.1143014207482338
step: 1000, Loss: 0.11369910836219788
step: 1100, Loss: 0.1141849160194397
step: 1200, Loss: 0.11419324576854706
step: 1300, Loss: 0.11563517153263092
step: 1400, Loss: 0.11407898366451263
step: 1500, Loss: 0.11380063742399216
step: 1600, Loss: 0.11559233069419861
step: 1800, Loss: 0.11700717359781265
step: 1900, Loss: 0.12212221324443817
step: 2000, Loss: 0.12023404240608215
step: 2100, Loss: 0.12276433408260345
step: 2200, Loss: 0.1223982647061348
step: 2300, Loss: 0.12154139578342438
step: 2400, Loss: 0.12436483055353165
step: 2500, Loss: 0.4590897858142853
step: 2600, Loss: 0.19346192479133606
step: 2700, Loss: 0.1511606127023697
step: 2800, Loss: 0.13948611915111542
step: 2900, Loss: 0.1372755616903305
step: 3000, Loss: 0.12706305086612701
step: 3100, Loss: 0.13961976766586304
step: 3200, Loss: 0.12635791301727295
step: 3300, Loss: 0.13102328777313232
step: 3400, Loss: 0.13326852023601532
step: 3500, Loss: 0.12965118885040283
step: 3600, Loss: 0.13291886448860168
step: 3700, Loss: 0.12696725130081177
step: 3800, Loss: 0.12526851892471313
step: 3900, Loss: 0.12559258937835693
step: 4000, Loss: 0.12060590088367462
step: 4100, Loss: 0.12299247086048126
step: 4200, Loss: 0.12072911858558655
step: 4300, Loss: 0.11781048774719238
step: 4400, Loss: 0.11891412734985352
step: 4500, Loss: 0.11830385029315948
step: 4600, Loss: 0.12274041771888733
step: 4700, Loss: 0.12032023072242737
step: 4800, Loss: 0.11697155237197876
step: 4900, Loss: 0.11806359887123108
step: 5000, Loss: 0.11965637654066086
step: 5100, Loss: 0.11732671409845352
step: 5200, Loss: 0.11557258665561676
step: 5300, Loss: 0.117359459400177
step: 5400, Loss: 0.1189100369811058
step: 5500, Loss: 0.11402399092912674
step: 5600, Loss: 0.11878010630607605
step: 5700, Loss: 0.11750046908855438
step: 5800, Loss: 0.11687595397233963
step: 5900, Loss: 0.11608196794986725
step: 6000, Loss: 0.11402443796396255
step: 6100, Loss: 0.11498847603797913
step: 6200, Loss: 0.11715393513441086
step: 6300, Loss: 0.11288318783044815
step: 6400, Loss: 0.11483630537986755
step: 6500, Loss: 0.11505496501922607
step: 6600, Loss: 0.11342660337686539
step: 6700, Loss: 0.11426074802875519
step: 6800, Loss: 0.11477170884609222
step: 6900, Loss: 0.11501270532608032
step: 7000, Loss: 0.11357452720403671
step: 7100, Loss: 0.11383134871721268
step: 7200, Loss: 0.11327508091926575
step: 7300, Loss: 0.11441747099161148
step: 7400, Loss: 0.11418256908655167
step: 7500, Loss: 0.11430737376213074
step: 7600, Loss: 0.11419031023979187
step: 7700, Loss: 0.11422687023878098
step: 7800, Loss: 0.114146389067173
step: 7900, Loss: 0.1135837584733963
step: 8000, Loss: 0.1156252771615982
step: 8100, Loss: 0.1139955148100853
step: 8200, Loss: 0.11253318935632706
step: 8300, Loss: 0.11324945837259293
step: 8400, Loss: 0.11370285600423813
step: 8500, Loss: 0.11354061216115952
step: 8600, Loss: 0.11386354267597198
step: 8700, Loss: 0.11365890502929688
step: 8800, Loss: 0.114583820104599
step: 8900, Loss: 0.11410995572805405
step: 9000, Loss: 0.11427200585603714
step: 9100, Loss: 0.11391515284776688
step: 9200, Loss: 0.11582131683826447
step: 9300, Loss: 0.11418019235134125
step: 9400, Loss: 0.11563540995121002
step: 9500, Loss: 0.11593914031982422
step: 9600, Loss: 0.11418038606643677
step: 9700, Loss: 0.11394508183002472
step: 9800, Loss: 0.11503911018371582
step: 9900, Loss: 0.11585837602615356
training successfully ended.
validating...
validate data length:103
acc: 0.9895833333333334
precision: 0.9818181818181818
recall: 1.0
F_score: 0.9908256880733944
******fold 4******

Training... train_data length:923
step: 0, Loss: 0.12295553833246231
step: 100, Loss: 0.11669731140136719
step: 200, Loss: 0.11542890965938568
step: 300, Loss: 0.11492055654525757
step: 400, Loss: 0.1146441325545311
step: 500, Loss: 0.1135932058095932
step: 600, Loss: 0.11702122539281845
step: 700, Loss: 0.11997263133525848
step: 800, Loss: 0.11797849833965302
step: 900, Loss: 0.11854621767997742
step: 1000, Loss: 0.13491344451904297
step: 1100, Loss: 0.5630861520767212
step: 1200, Loss: 0.16660936176776886
step: 1300, Loss: 0.1497119963169098
step: 1400, Loss: 0.13319236040115356
step: 1500, Loss: 0.1304044872522354
step: 1600, Loss: 0.12830667197704315
step: 1700, Loss: 0.12907317280769348
step: 1800, Loss: 0.12763550877571106
step: 1900, Loss: 0.12591250240802765
step: 2000, Loss: 0.12445737421512604
step: 2100, Loss: 0.1261039525270462
step: 2200, Loss: 0.11966609954833984
step: 2300, Loss: 0.12345977127552032
step: 2400, Loss: 0.11948959529399872
step: 2500, Loss: 0.11828786134719849
step: 2600, Loss: 0.11899018287658691
step: 2700, Loss: 0.1197650134563446
step: 2800, Loss: 0.11963864415884018
step: 2900, Loss: 0.11891689151525497
step: 3000, Loss: 0.1188150942325592
step: 3100, Loss: 0.11702743917703629
step: 3200, Loss: 0.11573076248168945
step: 3300, Loss: 0.11667831242084503
step: 3400, Loss: 0.11838474869728088
step: 3500, Loss: 0.11523455381393433
step: 3600, Loss: 0.11534292250871658
step: 3700, Loss: 0.11848117411136627
step: 3800, Loss: 0.1150568500161171
step: 3900, Loss: 0.11351598054170609
step: 4000, Loss: 0.11617622524499893
step: 4100, Loss: 0.11608360707759857
step: 4200, Loss: 0.114814892411232
step: 4300, Loss: 0.11560137569904327
step: 4400, Loss: 0.11386902630329132
step: 4500, Loss: 0.11590240895748138
step: 4600, Loss: 0.11388666927814484
step: 4700, Loss: 0.11534222960472107
step: 4800, Loss: 0.11460021138191223
step: 4900, Loss: 0.11332619935274124
step: 5000, Loss: 0.11367002129554749
step: 5100, Loss: 0.11520078778266907
step: 5200, Loss: 0.1146196722984314
step: 5300, Loss: 0.11336244642734528
step: 5400, Loss: 0.11335715651512146
step: 5500, Loss: 0.11309534311294556
step: 5600, Loss: 0.1155228465795517
step: 5700, Loss: 0.1132233589887619
step: 5800, Loss: 0.11286745965480804
step: 5900, Loss: 0.1142791211605072
step: 6000, Loss: 0.11359438300132751
step: 6100, Loss: 0.11493444442749023
step: 6200, Loss: 0.11461538821458817
step: 6300, Loss: 0.1152566596865654
step: 6400, Loss: 0.11569462716579437
step: 6500, Loss: 0.11427940428256989
step: 6600, Loss: 0.11317255347967148
step: 6700, Loss: 0.11776542663574219
step: 6800, Loss: 0.11665129661560059
step: 6900, Loss: 0.11700277030467987
step: 7000, Loss: 0.11800409853458405
step: 7100, Loss: 0.11553892493247986
step: 7200, Loss: 0.11975807696580887
step: 7300, Loss: 0.11435108631849289
step: 7400, Loss: 0.11529887467622757
step: 7500, Loss: 0.11446869373321533
step: 7600, Loss: 0.11413425207138062
step: 7700, Loss: 0.11481175571680069
step: 7800, Loss: 0.11376500874757767
step: 7900, Loss: 0.11568783223628998
step: 8000, Loss: 0.11439689993858337
step: 8100, Loss: 0.11596041917800903
step: 8200, Loss: 0.11487961560487747
step: 8300, Loss: 0.11717565357685089
step: 8400, Loss: 0.11380155384540558
step: 8500, Loss: 0.11593905091285706
step: 8600, Loss: 0.11517803370952606
step: 8700, Loss: 0.11666740477085114
step: 8800, Loss: 0.11536343395709991
step: 8900, Loss: 0.11681714653968811
step: 9000, Loss: 0.11570426821708679
step: 9100, Loss: 0.11490132659673691
step: 9200, Loss: 0.11416339874267578
step: 9300, Loss: 0.1152651458978653
step: 9400, Loss: 0.11666335165500641
step: 9500, Loss: 0.11423482745885849
step: 9600, Loss: 1.8489080667495728
step: 9700, Loss: 0.16394677758216858
step: 9800, Loss: 0.13038590550422668
step: 9900, Loss: 0.13314326107501984
training successfully ended.
validating...
validate data length:103
acc: 0.9895833333333334
precision: 0.9830508474576272
recall: 1.0
F_score: 0.9914529914529915
******fold 5******

Training... train_data length:923
step: 0, Loss: 0.12078092247247696
step: 100, Loss: 0.11717362701892853
step: 200, Loss: 0.11716172099113464
step: 300, Loss: 0.11496347188949585
step: 400, Loss: 0.11351253092288971
step: 500, Loss: 0.115554079413414
step: 600, Loss: 0.11609962582588196
step: 700, Loss: 0.12098275125026703
step: 800, Loss: 0.1157553493976593
step: 900, Loss: 0.12157128006219864
step: 1000, Loss: 0.11987597495317459
step: 1100, Loss: 0.14454959332942963
step: 1200, Loss: 0.18945929408073425
step: 1300, Loss: 0.17134898900985718
step: 1400, Loss: 0.1556262969970703
step: 1500, Loss: 0.14824309945106506
step: 1600, Loss: 0.13268056511878967
step: 1700, Loss: 0.12905946373939514
step: 1800, Loss: 0.12944698333740234
step: 1900, Loss: 0.12532168626785278
step: 2000, Loss: 0.12123610079288483
step: 2100, Loss: 0.12556898593902588
step: 2200, Loss: 0.12157765030860901
step: 1700, Loss: 0.11459248512983322
step: 1800, Loss: 0.11434166133403778
step: 1900, Loss: 0.11574621498584747
step: 2000, Loss: 0.11447015404701233
step: 2100, Loss: 0.11496100574731827
step: 2200, Loss: 0.11403314769268036
step: 2300, Loss: 0.1152293011546135
step: 2400, Loss: 0.11442678421735764
step: 2500, Loss: 0.11637254804372787
step: 2600, Loss: 0.13673773407936096
step: 2700, Loss: 0.13827615976333618
step: 2800, Loss: 0.12427697330713272
step: 2900, Loss: 0.12569370865821838
step: 3000, Loss: 0.12252592295408249
step: 3100, Loss: 0.12399687618017197
step: 3200, Loss: 0.12199930101633072
step: 3300, Loss: 0.11840952932834625
step: 3400, Loss: 0.11622624844312668
step: 3500, Loss: 0.11746276915073395
step: 3600, Loss: 0.1175597682595253
step: 3700, Loss: 0.11676515638828278
step: 3800, Loss: 0.11687347292900085
step: 3900, Loss: 0.11582089215517044
step: 4000, Loss: 0.11703581362962723
step: 4100, Loss: 0.11633561551570892
step: 4200, Loss: 0.11466711014509201
step: 4300, Loss: 0.11468826234340668
step: 4400, Loss: 0.11425192654132843
step: 4500, Loss: 0.1171608418226242
step: 4600, Loss: 0.11449380218982697
step: 4700, Loss: 0.11508330702781677
step: 4800, Loss: 0.11489740014076233
step: 4900, Loss: 0.11391699314117432
step: 5000, Loss: 0.1161453053355217
step: 5100, Loss: 0.11315342783927917
step: 5200, Loss: 0.11657851189374924
step: 5300, Loss: 0.11607496440410614
step: 5400, Loss: 0.11615471541881561
step: 5500, Loss: 0.11534016579389572
step: 5600, Loss: 0.1153777614235878
step: 5700, Loss: 0.1145610511302948
step: 5800, Loss: 0.11515242606401443
step: 5900, Loss: 0.11395083367824554
step: 6000, Loss: 0.11403633654117584
step: 6100, Loss: 0.11545176804065704
step: 6200, Loss: 0.11470124870538712
step: 6300, Loss: 0.11297128349542618
step: 6400, Loss: 0.11358489841222763
step: 6500, Loss: 0.11392953246831894
step: 6600, Loss: 0.1146153137087822
step: 6700, Loss: 0.11369293928146362
step: 6800, Loss: 0.11517849564552307
step: 6900, Loss: 0.11472093313932419
step: 7000, Loss: 0.11345907300710678
step: 7100, Loss: 0.11447759717702866
step: 7200, Loss: 0.11367644369602203
step: 7300, Loss: 0.11340123414993286
step: 7400, Loss: 0.11472679674625397
step: 7500, Loss: 0.11597025394439697
step: 7600, Loss: 0.11518736183643341
step: 7700, Loss: 0.11406733095645905
step: 7800, Loss: 0.11494414508342743
step: 7900, Loss: 0.11503933370113373
step: 8000, Loss: 0.1155310869216919
step: 8100, Loss: 0.11272196471691132
step: 8200, Loss: 0.11413142830133438
step: 8300, Loss: 0.11375865340232849
step: 8400, Loss: 0.11636568605899811
step: 8500, Loss: 0.11257782578468323
step: 8600, Loss: 0.11506737023591995
step: 8700, Loss: 0.1178063228726387
step: 8800, Loss: 0.11444111168384552
step: 8900, Loss: 0.23901233077049255
step: 9000, Loss: 0.14280450344085693
step: 9100, Loss: 0.12349546700716019
step: 9200, Loss: 0.13847188651561737
step: 9300, Loss: 0.12177873402833939
step: 9400, Loss: 0.12130540609359741
step: 9500, Loss: 0.11899884790182114
step: 9600, Loss: 0.12114521861076355
step: 9700, Loss: 0.11663797497749329
step: 9800, Loss: 0.1216076985001564
step: 9900, Loss: 0.135574609041214
training successfully ended.
validating...
validate data length:31
acc: 0.6666666666666666
precision: 0.6666666666666666
recall: 0.6666666666666666
F_score: 0.6666666666666666
******fold 4******

Training... train_data length:281
step: 0, Loss: 2.07415771484375
step: 100, Loss: 0.11641737073659897
step: 200, Loss: 0.12159549444913864
step: 300, Loss: 0.1145002618432045
step: 400, Loss: 0.11684571206569672
step: 500, Loss: 0.11772550642490387
step: 600, Loss: 0.11814790964126587
step: 700, Loss: 0.11344854533672333
step: 800, Loss: 0.11431675404310226
step: 900, Loss: 0.1142902672290802
step: 1000, Loss: 0.11527357995510101
step: 1100, Loss: 0.1145501360297203
step: 1200, Loss: 0.11480297893285751
step: 1300, Loss: 0.11582928895950317
step: 1400, Loss: 0.11528563499450684
step: 1500, Loss: 0.11369435489177704
step: 1600, Loss: 0.11333928257226944
step: 1700, Loss: 0.11606022715568542
step: 1800, Loss: 0.11423972249031067
step: 1900, Loss: 0.11489877104759216
step: 2000, Loss: 0.11315514147281647
step: 2100, Loss: 0.11482656747102737
step: 2200, Loss: 0.11392955482006073
step: 2300, Loss: 0.1163465678691864
step: 2400, Loss: 0.1137516051530838
step: 2500, Loss: 0.11480280011892319
step: 2600, Loss: 0.11445006728172302
step: 2700, Loss: 0.11594058573246002
step: 2800, Loss: 0.11554010957479477
step: 2900, Loss: 0.11441069841384888
step: 3000, Loss: 0.11454842239618301
step: 3100, Loss: 0.11548499017953873
step: 3200, Loss: 0.11595220118761063
step: 3300, Loss: 0.11431677639484406
step: 3400, Loss: 0.11579354107379913
step: 3500, Loss: 0.11647827923297882
step: 3600, Loss: 0.11429508030414581
step: 3700, Loss: 0.11447888612747192
step: 3800, Loss: 0.11613938957452774
step: 3900, Loss: 0.11793579161167145
step: 4000, Loss: 0.11356037110090256
step: 4100, Loss: 0.11561684310436249
step: 4200, Loss: 0.11832140386104584
step: 4300, Loss: 0.11391522735357285
step: 4400, Loss: 0.11402653902769089
step: 4500, Loss: 0.11467817425727844
step: 4600, Loss: 0.11449384689331055
step: 4700, Loss: 0.15658347308635712
step: 4800, Loss: 0.12467782199382782
step: 4900, Loss: 0.12926128506660461
step: 5000, Loss: 0.11577020585536957
step: 5100, Loss: 0.12291362881660461
step: 5200, Loss: 0.11825473606586456
step: 5300, Loss: 0.12190525233745575
step: 5400, Loss: 0.11848242580890656
step: 5500, Loss: 0.11981414258480072
step: 5600, Loss: 0.12106864154338837
step: 5700, Loss: 0.11769808083772659
step: 5800, Loss: 0.11727797985076904
step: 5900, Loss: 0.11607228964567184
step: 6000, Loss: 0.11309141665697098
step: 6100, Loss: 0.11660895496606827
step: 6200, Loss: 0.11491957306861877
step: 6300, Loss: 0.11651425808668137
step: 6400, Loss: 0.11644193530082703
step: 6500, Loss: 0.11612435430288315
step: 6600, Loss: 0.11316125094890594
step: 6700, Loss: 0.11628080904483795
step: 6800, Loss: 0.11487969756126404
step: 6900, Loss: 0.11572752892971039
step: 7000, Loss: 0.1149129867553711
step: 7100, Loss: 0.11410096287727356
step: 7200, Loss: 0.11634647846221924
step: 7300, Loss: 0.1139407828450203
step: 7400, Loss: 0.11463528126478195
step: 7500, Loss: 0.11397051811218262
step: 7600, Loss: 0.11290991306304932
step: 7700, Loss: 0.11473254859447479
step: 7800, Loss: 0.11470308899879456
step: 7900, Loss: 0.11489061266183853
step: 8000, Loss: 0.11419700086116791
step: 8100, Loss: 0.1140609160065651
step: 8200, Loss: 0.11365662515163422
step: 8300, Loss: 0.11393308639526367
step: 8400, Loss: 0.1137455552816391
step: 8500, Loss: 0.11463330686092377
step: 8600, Loss: 0.1140323281288147
step: 8700, Loss: 0.11272599548101425
step: 8800, Loss: 0.11503300815820694
step: 8900, Loss: 0.1146303340792656
step: 9000, Loss: 0.11423271149396896
step: 9100, Loss: 0.1141442283987999
step: 9200, Loss: 0.1147821694612503
step: 9300, Loss: 0.11469194293022156
step: 9400, Loss: 0.11434520781040192
step: 9500, Loss: 0.11392159014940262
step: 9600, Loss: 0.11682889610528946
step: 9700, Loss: 0.11478382349014282
step: 9800, Loss: 0.11399617791175842
step: 9900, Loss: 0.11652571707963943
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.9285714285714286
recall: 0.8666666666666667
F_score: 0.896551724137931
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.1654423177242279
step: 100, Loss: 0.11845224350690842
step: 200, Loss: 0.1157776489853859
step: 300, Loss: 0.11584837734699249
step: 400, Loss: 0.11440994590520859
step: 500, Loss: 0.11487553268671036
step: 600, Loss: 0.11332674324512482
step: 700, Loss: 0.11372527480125427
step: 800, Loss: 0.11545617133378983
step: 900, Loss: 0.11363928020000458
step: 1000, Loss: 0.11414114385843277
step: 1100, Loss: 0.11639171838760376
step: 1200, Loss: 0.11380346864461899
step: 1300, Loss: 0.11392046511173248
step: 1400, Loss: 0.11360287666320801
step: 1500, Loss: 0.11387783288955688
step: 1600, Loss: 0.11373938620090485
step: 1700, Loss: 0.1161116361618042
step: 1800, Loss: 0.11340384185314178
step: 1900, Loss: 0.11408060044050217
step: 2000, Loss: 0.11415131390094757
step: 2100, Loss: 0.1145310327410698
step: 2300, Loss: 0.11905854940414429
step: 2400, Loss: 0.11773786693811417
step: 2500, Loss: 0.11732952296733856
step: 2600, Loss: 0.11718742549419403
step: 2700, Loss: 0.11586315184831619
step: 2800, Loss: 0.11553902179002762
step: 2900, Loss: 0.11515861004590988
step: 3000, Loss: 0.11518233269453049
step: 3100, Loss: 0.1170426532626152
step: 3200, Loss: 0.11554655432701111
step: 3300, Loss: 0.11567756533622742
step: 3400, Loss: 0.11526817083358765
step: 3500, Loss: 0.11489024013280869
step: 3600, Loss: 0.11491604894399643
step: 3700, Loss: 0.11325082182884216
step: 3800, Loss: 0.11495502293109894
step: 3900, Loss: 0.11404050886631012
step: 4000, Loss: 0.11555086076259613
step: 4100, Loss: 0.11620543897151947
step: 4200, Loss: 0.1141805574297905
step: 4300, Loss: 0.11550402641296387
step: 4400, Loss: 0.11428961157798767
step: 4500, Loss: 0.11309511959552765
step: 4600, Loss: 0.1146356463432312
step: 4700, Loss: 0.11329066753387451
step: 4800, Loss: 0.11370991170406342
step: 4900, Loss: 0.11469876021146774
step: 5000, Loss: 0.1151406466960907
step: 5100, Loss: 0.11399003863334656
step: 5200, Loss: 0.11442753672599792
step: 5300, Loss: 0.11424507200717926
step: 5400, Loss: 0.11550652235746384
step: 5500, Loss: 0.11384937167167664
step: 5600, Loss: 0.11391538381576538
step: 5700, Loss: 0.1126476302742958
step: 5800, Loss: 0.11405983567237854
step: 5900, Loss: 0.11409110575914383
step: 6000, Loss: 0.11245370656251907
step: 6100, Loss: 0.1133793368935585
step: 6200, Loss: 0.11332739144563675
step: 6300, Loss: 0.11373242735862732
step: 6400, Loss: 0.11368248611688614
step: 6500, Loss: 0.11444390565156937
step: 6600, Loss: 0.11504188179969788
step: 6700, Loss: 0.11437945812940598
step: 6800, Loss: 0.11440138518810272
step: 6900, Loss: 0.11428216844797134
step: 7000, Loss: 0.11444833874702454
step: 7100, Loss: 0.11365014314651489
step: 7200, Loss: 0.11453380435705185
step: 7300, Loss: 0.1143922284245491
step: 7400, Loss: 0.11435767263174057
step: 7500, Loss: 0.11320380866527557
step: 7600, Loss: 0.11374739557504654
step: 7700, Loss: 0.11457870155572891
step: 7800, Loss: 0.11508645117282867
step: 7900, Loss: 0.1150769591331482
step: 8000, Loss: 0.11591238528490067
step: 8100, Loss: 0.11349158734083176
step: 8200, Loss: 0.11415370553731918
step: 8300, Loss: 0.11501281708478928
step: 8400, Loss: 0.11620847880840302
step: 8500, Loss: 0.11415217816829681
step: 8600, Loss: 0.11663053929805756
step: 8700, Loss: 0.11498190462589264
step: 8800, Loss: 0.11437392979860306
step: 8900, Loss: 0.11401095241308212
step: 9000, Loss: 0.11440834403038025
step: 9100, Loss: 0.11530411243438721
step: 9200, Loss: 0.11508028954267502
step: 9300, Loss: 0.11464054882526398
step: 9400, Loss: 0.11374358832836151
step: 9500, Loss: 0.11420983076095581
step: 9600, Loss: 1.75648832321167
step: 9700, Loss: 0.1874309927225113
step: 9800, Loss: 0.15628376603126526
step: 9900, Loss: 0.14858272671699524
training successfully ended.
validating...
validate data length:103
acc: 0.9791666666666666
precision: 0.9607843137254902
recall: 1.0
F_score: 0.98
******fold 6******

Training... train_data length:923
step: 0, Loss: 0.12044118344783783
step: 100, Loss: 0.11849641799926758
step: 200, Loss: 0.11439716815948486
step: 300, Loss: 0.11732657998800278
step: 400, Loss: 0.1154521033167839
step: 500, Loss: 0.11531148105859756
step: 600, Loss: 0.1146516352891922
step: 700, Loss: 0.11602389812469482
step: 800, Loss: 0.11516175419092178
step: 900, Loss: 0.1164800375699997
step: 1000, Loss: 0.42821139097213745
step: 1100, Loss: 0.1627890169620514
step: 1200, Loss: 0.16312353312969208
step: 1300, Loss: 0.14428667724132538
step: 1400, Loss: 0.13464006781578064
step: 1500, Loss: 0.13300509750843048
step: 1600, Loss: 0.13243508338928223
step: 1700, Loss: 0.12519840896129608
step: 1800, Loss: 0.12690481543540955
step: 1900, Loss: 0.12152555584907532
step: 2000, Loss: 0.12222132831811905
step: 2100, Loss: 0.1254366934299469
step: 2200, Loss: 0.11800870299339294
step: 2300, Loss: 0.11835967004299164
step: 2400, Loss: 0.11760404706001282
step: 2500, Loss: 0.11719674617052078
step: 2600, Loss: 0.11711016297340393
step: 2700, Loss: 0.11830383539199829
step: 2800, Loss: 0.1173347681760788
step: 2900, Loss: 0.11568211019039154
step: 3000, Loss: 0.11728048324584961
step: 3100, Loss: 0.1147720143198967
step: 3200, Loss: 0.11692097783088684
step: 3300, Loss: 0.11702573299407959
step: 3400, Loss: 0.11601552367210388
step: 3500, Loss: 0.11599928140640259
step: 3600, Loss: 0.11580075323581696
step: 3700, Loss: 0.11407896876335144
step: 3800, Loss: 0.11509188264608383
step: 3900, Loss: 0.11684641242027283
step: 4000, Loss: 0.11508163064718246
step: 4100, Loss: 0.11289294064044952
step: 4200, Loss: 0.11392071843147278
step: 4300, Loss: 0.11335980892181396
step: 4400, Loss: 0.11397456377744675
step: 4500, Loss: 0.11372192203998566
step: 4600, Loss: 0.11388854682445526
step: 4700, Loss: 0.11415667831897736
step: 4800, Loss: 0.11432591080665588
step: 4900, Loss: 0.1142829954624176
step: 5000, Loss: 0.11353804171085358
step: 5100, Loss: 0.11402397602796555
step: 5200, Loss: 0.11374658346176147
step: 5300, Loss: 0.11445003002882004
step: 5400, Loss: 0.1139017716050148
step: 5500, Loss: 0.11439348757266998
step: 5600, Loss: 0.11355124413967133
step: 5700, Loss: 0.1137041300535202
step: 5800, Loss: 0.1138964518904686
step: 5900, Loss: 0.11535464227199554
step: 6000, Loss: 0.11279993504285812
step: 6100, Loss: 0.11451685428619385
step: 6200, Loss: 0.11259932816028595
step: 6300, Loss: 0.11465466022491455
step: 6400, Loss: 0.11468025296926498
step: 6500, Loss: 0.11554116755723953
step: 6600, Loss: 0.11443451046943665
step: 6700, Loss: 0.11542598903179169
step: 6800, Loss: 0.1150612160563469
step: 6900, Loss: 0.11389666795730591
step: 7000, Loss: 0.1159801185131073
step: 7100, Loss: 0.11470737308263779
step: 7200, Loss: 0.11456550657749176
step: 7300, Loss: 0.11534721404314041
step: 7400, Loss: 0.11543296277523041
step: 7500, Loss: 0.12085847556591034
step: 7600, Loss: 0.12133996188640594
step: 7700, Loss: 0.11797318607568741
step: 7800, Loss: 0.49611565470695496
step: 7900, Loss: 0.19075199961662292
step: 8000, Loss: 0.14709477126598358
step: 8100, Loss: 0.14481186866760254
step: 8200, Loss: 0.1328621655702591
step: 8300, Loss: 0.13180311024188995
step: 8400, Loss: 0.13693419098854065
step: 8500, Loss: 0.1272141933441162
step: 8600, Loss: 0.1276075392961502
step: 8700, Loss: 0.12532712519168854
step: 8800, Loss: 0.118543840944767
step: 8900, Loss: 0.12052348256111145
step: 9000, Loss: 0.12160362303256989
step: 9100, Loss: 0.11978444457054138
step: 9200, Loss: 0.12181852757930756
step: 9300, Loss: 0.1192987710237503
step: 9400, Loss: 0.11575038731098175
step: 9500, Loss: 0.1181783378124237
step: 9600, Loss: 0.1168537586927414
step: 9700, Loss: 0.11610544472932816
step: 9800, Loss: 0.11872455477714539
step: 9900, Loss: 0.11689721047878265
training successfully ended.
validating...
validate data length:103
acc: 0.96875
precision: 0.94
recall: 1.0
F_score: 0.9690721649484536
******fold 7******

Training... train_data length:924
step: 0, Loss: 0.12009289115667343
step: 100, Loss: 0.11715494096279144
step: 200, Loss: 0.11747890710830688
step: 300, Loss: 0.11524737626314163
step: 400, Loss: 0.11789267510175705
step: 500, Loss: 0.11468468606472015
step: 600, Loss: 0.12324173748493195
step: 700, Loss: 0.20259985327720642
step: 800, Loss: 0.18946020305156708
step: 900, Loss: 0.1560814380645752
step: 1000, Loss: 0.1336851865053177
step: 1100, Loss: 0.14873676002025604
step: 1200, Loss: 0.13235493004322052
step: 1300, Loss: 0.13153743743896484
step: 1400, Loss: 0.1255437731742859
step: 1500, Loss: 0.12533962726593018
step: 1600, Loss: 0.12180410325527191
step: 1700, Loss: 0.1240084245800972
step: 1800, Loss: 0.12109123915433884
step: 1900, Loss: 0.12152975052595139
step: 2000, Loss: 0.11841239780187607
step: 2100, Loss: 0.11876882612705231
step: 2200, Loss: 0.11927582323551178
step: 2300, Loss: 0.11934815347194672
step: 2400, Loss: 0.11743982136249542
step: 2500, Loss: 0.11811138689517975
step: 2600, Loss: 0.11608096957206726
step: 2700, Loss: 0.11840703338384628
step: 2800, Loss: 0.11593320220708847
step: 2200, Loss: 0.1146727055311203
step: 2300, Loss: 0.115129254758358
step: 2400, Loss: 0.11385337263345718
step: 2500, Loss: 0.11478397995233536
step: 2600, Loss: 0.1154625415802002
step: 2700, Loss: 0.11529235541820526
step: 2800, Loss: 0.1151251494884491
step: 2900, Loss: 0.11419759690761566
step: 3000, Loss: 0.11427107453346252
step: 3100, Loss: 0.11425218731164932
step: 3200, Loss: 0.11533981561660767
step: 3300, Loss: 0.11296695470809937
step: 3400, Loss: 0.11506698280572891
step: 3500, Loss: 0.11517993360757828
step: 3600, Loss: 0.11502180248498917
step: 3700, Loss: 0.24963468313217163
step: 3800, Loss: 0.14001144468784332
step: 3900, Loss: 0.12766721844673157
step: 4000, Loss: 0.1297556608915329
step: 4100, Loss: 0.12688152492046356
step: 4200, Loss: 0.12336820363998413
step: 4300, Loss: 0.12082815915346146
step: 4400, Loss: 0.12549476325511932
step: 4500, Loss: 0.11943067610263824
step: 4600, Loss: 0.1218661367893219
step: 4700, Loss: 0.11810701340436935
step: 4800, Loss: 0.11999095976352692
step: 4900, Loss: 0.11948862671852112
step: 5000, Loss: 0.1177172064781189
step: 5100, Loss: 0.11560778319835663
step: 5200, Loss: 0.11693017929792404
step: 5300, Loss: 0.11567691713571548
step: 5400, Loss: 0.11659780144691467
step: 5500, Loss: 0.11630447953939438
step: 5600, Loss: 0.11619065701961517
step: 5700, Loss: 0.11655881255865097
step: 5800, Loss: 0.11468841135501862
step: 5900, Loss: 0.11510665714740753
step: 6000, Loss: 0.1143297553062439
step: 6100, Loss: 0.11705586314201355
step: 6200, Loss: 0.11395008862018585
step: 6300, Loss: 0.11692026257514954
step: 6400, Loss: 0.1154196709394455
step: 6500, Loss: 0.11508208513259888
step: 6600, Loss: 0.11545585095882416
step: 6700, Loss: 0.1184844821691513
step: 6800, Loss: 0.1167067438364029
step: 6900, Loss: 0.1133405938744545
step: 7000, Loss: 0.11453596502542496
step: 7100, Loss: 0.11449961364269257
step: 7200, Loss: 0.11441290378570557
step: 7300, Loss: 0.1153189018368721
step: 7400, Loss: 0.11498671770095825
step: 7500, Loss: 0.11548462510108948
step: 7600, Loss: 0.11513837426900864
step: 7700, Loss: 0.11449675261974335
step: 7800, Loss: 0.11459005624055862
step: 7900, Loss: 0.11287246644496918
step: 8000, Loss: 0.11528287827968597
step: 8100, Loss: 0.1141357496380806
step: 8200, Loss: 0.11459098756313324
step: 8300, Loss: 0.11533790826797485
step: 8400, Loss: 0.115566685795784
step: 8500, Loss: 0.11613042652606964
step: 8600, Loss: 0.11453261971473694
step: 8700, Loss: 0.11357300728559494
step: 8800, Loss: 0.11545386910438538
step: 8900, Loss: 0.11685839295387268
step: 9000, Loss: 0.11566099524497986
step: 9100, Loss: 0.11409218609333038
step: 9200, Loss: 0.1138167679309845
step: 9300, Loss: 0.11414537578821182
step: 9400, Loss: 0.11316271871328354
step: 9500, Loss: 0.11439608037471771
step: 9600, Loss: 0.115459144115448
step: 9700, Loss: 0.11343371123075485
step: 9800, Loss: 0.11406750977039337
step: 9900, Loss: 0.11503763496875763
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.9333333333333333
recall: 0.7777777777777778
F_score: 0.8484848484848485
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.16065138578414917
step: 100, Loss: 0.11854563653469086
step: 200, Loss: 0.1179938018321991
step: 300, Loss: 0.11847423762083054
step: 400, Loss: 0.11495359241962433
step: 500, Loss: 0.11547296494245529
step: 600, Loss: 0.1150541603565216
step: 700, Loss: 0.11541695147752762
step: 800, Loss: 0.11579370498657227
step: 900, Loss: 0.11600333452224731
step: 1000, Loss: 0.11620040982961655
step: 1100, Loss: 0.11382505297660828
step: 1200, Loss: 0.1157531589269638
step: 1300, Loss: 0.11376823484897614
step: 1400, Loss: 0.11325377970933914
step: 1500, Loss: 0.11479750275611877
step: 1600, Loss: 0.11669690907001495
step: 1700, Loss: 0.11514462530612946
step: 1800, Loss: 0.11533145606517792
step: 1900, Loss: 0.11444468051195145
step: 2000, Loss: 0.1145763024687767
step: 2100, Loss: 0.11438675969839096
step: 2200, Loss: 0.11513769626617432
step: 2300, Loss: 0.11402666568756104
step: 2400, Loss: 0.11495110392570496
step: 2500, Loss: 0.11425533145666122
step: 2600, Loss: 2.411560535430908
step: 2700, Loss: 0.14247334003448486
step: 2800, Loss: 0.1354169249534607
step: 2900, Loss: 0.12410248816013336
step: 3000, Loss: 0.126046821475029
step: 3100, Loss: 0.1272946447134018
step: 3200, Loss: 0.12015901505947113
step: 3300, Loss: 0.12150224298238754
step: 3400, Loss: 0.11818809807300568
step: 3500, Loss: 0.11757305264472961
step: 3600, Loss: 0.11774741113185883
step: 3700, Loss: 0.11695586889982224
step: 3800, Loss: 0.11605813354253769
step: 3900, Loss: 0.11996865272521973
step: 4000, Loss: 0.11976809799671173
step: 4100, Loss: 0.11677208542823792
step: 4200, Loss: 0.1157686784863472
step: 4300, Loss: 0.11981359869241714
step: 4400, Loss: 0.11533129215240479
step: 4500, Loss: 0.11518261581659317
step: 4600, Loss: 0.11435988545417786
step: 4700, Loss: 0.1151215210556984
step: 4800, Loss: 0.11417635530233383
step: 4900, Loss: 0.11432601511478424
step: 5000, Loss: 0.115911565721035
step: 5100, Loss: 0.11531282961368561
step: 5200, Loss: 0.11352189630270004
step: 5300, Loss: 0.11632061004638672
step: 5400, Loss: 0.11357444524765015
step: 5500, Loss: 0.11491058766841888
step: 5600, Loss: 0.11693143844604492
step: 5700, Loss: 0.11430828273296356
step: 5800, Loss: 0.11521389335393906
step: 5900, Loss: 0.11337321996688843
step: 6000, Loss: 0.11490337550640106
step: 6100, Loss: 0.11473910510540009
step: 6200, Loss: 0.11409464478492737
step: 6300, Loss: 0.1153939962387085
step: 6400, Loss: 0.11432047188282013
step: 6500, Loss: 0.1158352792263031
step: 6600, Loss: 0.1145443320274353
step: 6700, Loss: 0.11377696692943573
step: 6800, Loss: 0.1151091530919075
step: 6900, Loss: 0.11292984336614609
step: 7000, Loss: 0.11389665305614471
step: 7100, Loss: 0.11390604823827744
step: 7200, Loss: 0.11442768573760986
step: 7300, Loss: 0.11435575038194656
step: 7400, Loss: 0.11470004171133041
step: 7500, Loss: 0.11354278028011322
step: 7600, Loss: 0.1143239438533783
step: 7700, Loss: 0.11381521075963974
step: 7800, Loss: 0.11661116778850555
step: 7900, Loss: 0.11532807350158691
step: 8000, Loss: 0.11308638751506805
step: 8100, Loss: 0.11463980376720428
step: 8200, Loss: 0.11327922344207764
step: 8300, Loss: 0.11638219654560089
step: 8400, Loss: 0.11552222073078156
step: 8500, Loss: 0.11349760740995407
step: 8600, Loss: 0.1134619191288948
step: 8700, Loss: 0.11384037882089615
step: 8800, Loss: 0.11833274364471436
step: 8900, Loss: 0.11414854228496552
step: 9000, Loss: 0.11462043225765228
step: 9100, Loss: 0.11475694924592972
step: 9200, Loss: 0.11517009884119034
step: 9300, Loss: 0.1150256097316742
step: 9400, Loss: 0.11742991209030151
step: 9500, Loss: 0.11625780165195465
step: 9600, Loss: 0.11336027830839157
step: 9700, Loss: 0.11383156478404999
step: 9800, Loss: 0.11604980379343033
step: 9900, Loss: 0.1140296682715416
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.9444444444444444
recall: 0.9444444444444444
F_score: 0.9444444444444444
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.11663953959941864
step: 100, Loss: 0.11651596426963806
step: 200, Loss: 0.11490121483802795
step: 300, Loss: 0.11603637784719467
step: 400, Loss: 0.11709959805011749
step: 500, Loss: 0.1185113936662674
step: 600, Loss: 0.1144787073135376
step: 700, Loss: 0.1147402748465538
step: 800, Loss: 0.11406353116035461
step: 900, Loss: 0.1144586130976677
step: 1000, Loss: 0.11418987810611725
step: 1100, Loss: 0.11460451781749725
step: 1200, Loss: 0.11324040591716766
step: 1300, Loss: 0.11288212239742279
step: 1400, Loss: 0.11384609341621399
step: 1500, Loss: 0.11443989723920822
step: 1600, Loss: 0.11629112809896469
step: 1700, Loss: 0.12518110871315002
step: 1800, Loss: 0.12089377641677856
step: 1900, Loss: 0.12008403241634369
step: 2000, Loss: 0.12174807488918304
step: 2100, Loss: 0.12392036616802216
step: 2200, Loss: 0.11687760055065155
step: 2300, Loss: 0.11913225799798965
step: 2400, Loss: 0.11697723716497421
step: 2500, Loss: 0.11605285853147507
step: 2600, Loss: 0.11480435729026794
step: 2900, Loss: 0.11684059351682663
step: 3000, Loss: 0.11709500104188919
step: 3100, Loss: 0.11351977288722992
step: 3200, Loss: 0.11593787372112274
step: 3300, Loss: 0.11530502140522003
step: 3400, Loss: 0.11329977214336395
step: 3500, Loss: 0.11387887597084045
step: 3600, Loss: 0.11509464681148529
step: 3700, Loss: 0.11576743423938751
step: 3800, Loss: 0.11360818147659302
step: 3900, Loss: 0.11545084416866302
step: 4000, Loss: 0.11458985507488251
step: 4100, Loss: 0.1133921816945076
step: 4200, Loss: 0.11373625695705414
step: 4300, Loss: 0.1148497462272644
step: 4400, Loss: 0.11346176266670227
step: 4500, Loss: 0.11420910060405731
step: 4600, Loss: 0.11442260444164276
step: 4700, Loss: 0.1146935224533081
step: 4800, Loss: 0.11442354321479797
step: 4900, Loss: 0.1159467101097107
step: 5000, Loss: 0.11436083167791367
step: 5100, Loss: 0.11614110320806503
step: 5200, Loss: 0.1131623387336731
step: 5300, Loss: 0.11415189504623413
step: 5400, Loss: 0.113770991563797
step: 5500, Loss: 0.11340747773647308
step: 5600, Loss: 0.11483792960643768
step: 5700, Loss: 0.1138211116194725
step: 5800, Loss: 0.11277773976325989
step: 5900, Loss: 0.1129273846745491
step: 6000, Loss: 0.11429174244403839
step: 6100, Loss: 0.11449206620454788
step: 6200, Loss: 0.11464755237102509
step: 6300, Loss: 0.1124739870429039
step: 6400, Loss: 0.1139797791838646
step: 6500, Loss: 0.11441829800605774
step: 6600, Loss: 0.11478114873170853
step: 6700, Loss: 0.1161368116736412
step: 6800, Loss: 0.11656320840120316
step: 6900, Loss: 0.1142401322722435
step: 7000, Loss: 3.822554111480713
step: 7100, Loss: 0.20298156142234802
step: 7200, Loss: 0.1610335260629654
step: 7300, Loss: 0.1522011160850525
step: 7400, Loss: 0.137224942445755
step: 7500, Loss: 0.13625043630599976
step: 7600, Loss: 0.13313722610473633
step: 7700, Loss: 0.12607549130916595
step: 7800, Loss: 0.1258518099784851
step: 7900, Loss: 0.12624970078468323
step: 8000, Loss: 0.12252344936132431
step: 8100, Loss: 0.12476305663585663
step: 8200, Loss: 0.12295906245708466
step: 8300, Loss: 0.12149905413389206
step: 8400, Loss: 0.12104582786560059
step: 8500, Loss: 0.12118542194366455
step: 8600, Loss: 0.12135545164346695
step: 8700, Loss: 0.12378112971782684
step: 8800, Loss: 0.1187257245182991
step: 8900, Loss: 0.12005670368671417
step: 9000, Loss: 0.11734700202941895
step: 9100, Loss: 0.11852608621120453
step: 9200, Loss: 0.11933610588312149
step: 9300, Loss: 0.11984196305274963
step: 9400, Loss: 0.1167706549167633
step: 9500, Loss: 0.1189763993024826
step: 9600, Loss: 0.11555797606706619
step: 9700, Loss: 0.1169663518667221
step: 9800, Loss: 0.11607006192207336
step: 9900, Loss: 0.11576854437589645
training successfully ended.
validating...
validate data length:102
acc: 0.9895833333333334
precision: 0.9761904761904762
recall: 1.0
F_score: 0.9879518072289156
******fold 8******

Training... train_data length:924
step: 0, Loss: 0.12346571683883667
step: 100, Loss: 0.11710575222969055
step: 200, Loss: 0.11612934619188309
step: 300, Loss: 0.11505794525146484
step: 400, Loss: 0.11674351245164871
step: 500, Loss: 0.1140560731291771
step: 600, Loss: 0.117794930934906
step: 700, Loss: 0.11534931510686874
step: 800, Loss: 0.11696191132068634
step: 900, Loss: 0.12148217856884003
step: 1000, Loss: 0.615503191947937
step: 1100, Loss: 0.15593159198760986
step: 1200, Loss: 0.13770811259746552
step: 1300, Loss: 0.13500036299228668
step: 1400, Loss: 0.12783430516719818
step: 1500, Loss: 0.12464698404073715
step: 1600, Loss: 0.12451246380805969
step: 1700, Loss: 0.12576505541801453
step: 1800, Loss: 0.1241784393787384
step: 1900, Loss: 0.12066437304019928
step: 2000, Loss: 0.11805054545402527
step: 2100, Loss: 0.11867369711399078
step: 2200, Loss: 0.11779908835887909
step: 2300, Loss: 0.11982470750808716
step: 2400, Loss: 0.11873577535152435
step: 2500, Loss: 0.11959291994571686
step: 2600, Loss: 0.1168377548456192
step: 2700, Loss: 0.11658468097448349
step: 2800, Loss: 0.11524640023708344
step: 2900, Loss: 0.11651679873466492
step: 3000, Loss: 0.11503954976797104
step: 3100, Loss: 0.11482124030590057
step: 3200, Loss: 0.11478717625141144
step: 3300, Loss: 0.11404699087142944
step: 3400, Loss: 0.11393348127603531
step: 3500, Loss: 0.11407474428415298
step: 3600, Loss: 0.11582082509994507
step: 3700, Loss: 0.11439945548772812
step: 3800, Loss: 0.11372928321361542
step: 3900, Loss: 0.11386428773403168
step: 4000, Loss: 0.11381728947162628
step: 4100, Loss: 0.11382271349430084
step: 4200, Loss: 0.11427509784698486
step: 4300, Loss: 0.11338349431753159
step: 4400, Loss: 0.11456193029880524
step: 4500, Loss: 0.11429265141487122
step: 4600, Loss: 0.11317296326160431
step: 4700, Loss: 0.11385547369718552
step: 4800, Loss: 0.11434687674045563
step: 4900, Loss: 0.1137041449546814
step: 5000, Loss: 0.11390967667102814
step: 5100, Loss: 0.113863505423069
step: 5200, Loss: 0.1138923242688179
step: 5300, Loss: 0.11418052762746811
step: 5400, Loss: 0.11640657484531403
step: 5500, Loss: 0.11482837051153183
step: 5600, Loss: 0.11362886428833008
step: 5700, Loss: 0.11436064541339874
step: 5800, Loss: 0.11464324593544006
step: 5900, Loss: 0.11380510777235031
step: 6000, Loss: 0.11399157345294952
step: 6100, Loss: 0.11443962156772614
step: 6200, Loss: 0.11372759938240051
step: 6300, Loss: 0.11337542533874512
step: 6400, Loss: 0.11458951234817505
step: 6500, Loss: 0.11539601534605026
step: 6600, Loss: 0.11387219280004501
step: 6700, Loss: 0.11396598815917969
step: 6800, Loss: 0.11468659341335297
step: 6900, Loss: 0.1140531376004219
step: 7000, Loss: 0.11330905556678772
step: 7100, Loss: 0.11383376270532608
step: 7200, Loss: 0.11386822164058685
step: 7300, Loss: 0.11425665766000748
step: 7400, Loss: 0.11811798810958862
step: 7500, Loss: 0.11539195477962494
step: 7600, Loss: 0.11679042875766754
step: 7700, Loss: 0.11564966291189194
step: 7800, Loss: 0.11898235976696014
step: 7900, Loss: 0.12700529396533966
step: 8000, Loss: 4.061516761779785
step: 8100, Loss: 0.1470383107662201
step: 8200, Loss: 0.13856983184814453
step: 8300, Loss: 0.13007843494415283
step: 8400, Loss: 0.1335878074169159
step: 8500, Loss: 0.12854993343353271
step: 8600, Loss: 0.123145692050457
step: 8700, Loss: 0.12215428799390793
step: 8800, Loss: 0.11986494809389114
step: 8900, Loss: 0.12007629871368408
step: 9000, Loss: 0.11951636523008347
step: 9100, Loss: 0.12407422065734863
step: 9200, Loss: 0.11727438867092133
step: 9300, Loss: 0.11783669143915176
step: 9400, Loss: 0.11688517034053802
step: 9500, Loss: 0.11720670759677887
step: 9600, Loss: 0.11715047061443329
step: 9700, Loss: 0.1151759922504425
step: 9800, Loss: 0.11598207801580429
step: 9900, Loss: 0.11930400133132935
training successfully ended.
validating...
validate data length:102
acc: 0.9895833333333334
precision: 0.9787234042553191
recall: 1.0
F_score: 0.989247311827957
******fold 9******

Training... train_data length:924
step: 0, Loss: 0.12111581116914749
step: 100, Loss: 0.11636323481798172
step: 200, Loss: 0.11516180634498596
step: 300, Loss: 0.1138572245836258
step: 400, Loss: 0.11366929858922958
step: 500, Loss: 0.11406482756137848
step: 600, Loss: 0.11516273766756058
step: 700, Loss: 0.11374650150537491
step: 800, Loss: 0.11415902525186539
step: 900, Loss: 0.11499212682247162
step: 1000, Loss: 0.11680015921592712
step: 1100, Loss: 0.11491855978965759
step: 1200, Loss: 0.1146516278386116
step: 1300, Loss: 0.1315131038427353
step: 1400, Loss: 0.20812642574310303
step: 1500, Loss: 0.15475410223007202
step: 1600, Loss: 0.13793224096298218
step: 1700, Loss: 0.1347113847732544
step: 1800, Loss: 0.12761420011520386
step: 1900, Loss: 0.127232164144516
step: 2000, Loss: 0.12580789625644684
step: 2100, Loss: 0.12879541516304016
step: 2200, Loss: 0.12167898565530777
step: 2300, Loss: 0.12636122107505798
step: 2400, Loss: 0.1263653039932251
step: 2500, Loss: 0.12440246343612671
step: 2600, Loss: 0.1247442290186882
step: 2700, Loss: 0.12464416772127151
step: 2800, Loss: 0.12191453576087952
step: 2900, Loss: 0.12018976360559464
step: 3000, Loss: 0.12352285534143448
step: 3100, Loss: 0.12455036491155624
step: 3200, Loss: 0.11783473193645477
step: 3300, Loss: 0.11921890079975128
step: 3400, Loss: 0.1173948422074318
step: 2700, Loss: 0.11701752990484238
step: 2800, Loss: 0.11562348157167435
step: 2900, Loss: 0.11522496491670609
step: 3000, Loss: 0.11664365231990814
step: 3100, Loss: 0.11767219007015228
step: 3200, Loss: 0.11548461019992828
step: 3300, Loss: 0.11768648028373718
step: 3400, Loss: 0.11579637229442596
step: 3500, Loss: 0.1173795685172081
step: 3600, Loss: 0.11381492018699646
step: 3700, Loss: 0.11451881378889084
step: 3800, Loss: 0.11576732248067856
step: 3900, Loss: 0.11422615498304367
step: 4000, Loss: 0.11429084837436676
step: 4100, Loss: 0.11441853642463684
step: 4200, Loss: 0.11514335870742798
step: 4300, Loss: 0.11401351541280746
step: 4400, Loss: 0.11378754675388336
step: 4500, Loss: 0.11476638913154602
step: 4600, Loss: 0.11396480351686478
step: 4700, Loss: 0.11540688574314117
step: 4800, Loss: 0.11416969448328018
step: 4900, Loss: 0.11389147490262985
step: 5000, Loss: 0.11460552364587784
step: 5100, Loss: 0.11484410613775253
step: 5200, Loss: 0.11411859840154648
step: 5300, Loss: 0.11279657483100891
step: 5400, Loss: 0.11386269330978394
step: 5500, Loss: 0.11357106268405914
step: 5600, Loss: 0.11469672620296478
step: 5700, Loss: 0.11465983837842941
step: 5800, Loss: 0.11373760551214218
step: 5900, Loss: 0.11342106759548187
step: 6000, Loss: 0.11463641375303268
step: 6100, Loss: 0.11367370933294296
step: 6200, Loss: 0.11424484103918076
step: 6300, Loss: 0.11282910406589508
step: 6400, Loss: 0.11317836493253708
step: 6500, Loss: 0.11331900209188461
step: 6600, Loss: 0.11331883072853088
step: 6700, Loss: 0.11320926994085312
step: 6800, Loss: 0.11462381482124329
step: 6900, Loss: 0.11496944725513458
step: 7000, Loss: 0.11518736928701401
step: 7100, Loss: 0.11513441801071167
step: 7200, Loss: 0.113503098487854
step: 7300, Loss: 0.11412401497364044
step: 7400, Loss: 0.11411858350038528
step: 7500, Loss: 0.11361175030469894
step: 7600, Loss: 0.11382986605167389
step: 7700, Loss: 0.11328934133052826
step: 7800, Loss: 0.1138274073600769
step: 7900, Loss: 0.11387920379638672
step: 8000, Loss: 0.11423647403717041
step: 8100, Loss: 0.114532969892025
step: 8200, Loss: 0.11396709084510803
step: 8300, Loss: 0.11573147773742676
step: 8400, Loss: 0.11588581651449203
step: 8500, Loss: 0.1131025180220604
step: 8600, Loss: 0.11513002216815948
step: 8700, Loss: 0.11465747654438019
step: 8800, Loss: 0.11412806808948517
step: 8900, Loss: 0.11344564706087112
step: 9000, Loss: 0.11674399673938751
step: 9100, Loss: 0.844072699546814
step: 9200, Loss: 0.1295476257801056
step: 9300, Loss: 0.12334651499986649
step: 9400, Loss: 0.11697450280189514
step: 9500, Loss: 0.11942867934703827
step: 9600, Loss: 0.12161705642938614
step: 9700, Loss: 0.12225708365440369
step: 9800, Loss: 0.1191321313381195
step: 9900, Loss: 0.11788572371006012
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8235294117647058
recall: 0.8235294117647058
F_score: 0.8235294117647058
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.11640722304582596
step: 100, Loss: 0.11824238300323486
step: 200, Loss: 0.11749187111854553
step: 300, Loss: 0.114886075258255
step: 400, Loss: 0.11384017020463943
step: 500, Loss: 0.11460442841053009
step: 600, Loss: 0.11574070155620575
step: 700, Loss: 0.11482522636651993
step: 800, Loss: 0.11395497620105743
step: 900, Loss: 0.1147231012582779
step: 1000, Loss: 0.11298880726099014
step: 1100, Loss: 0.11478898674249649
step: 1200, Loss: 0.11423933506011963
step: 1300, Loss: 0.11600601673126221
step: 1400, Loss: 0.11324414610862732
step: 1500, Loss: 0.11459487676620483
step: 1600, Loss: 0.11569109559059143
step: 1700, Loss: 0.11344124376773834
step: 1800, Loss: 0.11558344960212708
step: 1900, Loss: 0.11489949375391006
step: 2000, Loss: 0.11482197046279907
step: 2100, Loss: 0.11439590901136398
step: 2200, Loss: 0.1138605922460556
step: 2300, Loss: 0.11433470249176025
step: 2400, Loss: 7.295942783355713
step: 2500, Loss: 0.12571316957473755
step: 2600, Loss: 0.12512847781181335
step: 2700, Loss: 0.12140703946352005
step: 2800, Loss: 0.1204778328537941
step: 2900, Loss: 0.11848226189613342
step: 3000, Loss: 0.11760269850492477
step: 3100, Loss: 0.11711737513542175
step: 3200, Loss: 0.11808565258979797
step: 3300, Loss: 0.11580465734004974
step: 3400, Loss: 0.1170748770236969
step: 3500, Loss: 0.11802966892719269
step: 3600, Loss: 0.11598765105009079
step: 3700, Loss: 0.11662904918193817
step: 3800, Loss: 0.11607887595891953
step: 3900, Loss: 0.11594311892986298
step: 4000, Loss: 0.11536340415477753
step: 4100, Loss: 0.1145651787519455
step: 4200, Loss: 0.11480311304330826
step: 4300, Loss: 0.11643096059560776
step: 4400, Loss: 0.11503136903047562
step: 4500, Loss: 0.11429671198129654
step: 4600, Loss: 0.11538589000701904
step: 4700, Loss: 0.11398235708475113
step: 4800, Loss: 0.11636333912611008
step: 4900, Loss: 0.11407947540283203
step: 5000, Loss: 0.11426082998514175
step: 5100, Loss: 0.11387456208467484
step: 5200, Loss: 0.1134374737739563
step: 5300, Loss: 0.11478549242019653
step: 5400, Loss: 0.1158374771475792
step: 5500, Loss: 0.11466296017169952
step: 5600, Loss: 0.11659026145935059
step: 5700, Loss: 0.11449821293354034
step: 5800, Loss: 0.1143711656332016
step: 5900, Loss: 0.11574476212263107
step: 6000, Loss: 0.11416729539632797
step: 6100, Loss: 0.1138458102941513
step: 6200, Loss: 0.11464224755764008
step: 6300, Loss: 0.1137331873178482
step: 6400, Loss: 0.1142953410744667
step: 6500, Loss: 0.11431198567152023
step: 6600, Loss: 0.11428775638341904
step: 6700, Loss: 0.11451753228902817
step: 6800, Loss: 0.11415054649114609
step: 6900, Loss: 0.1162877306342125
step: 7000, Loss: 0.11353549361228943
step: 7100, Loss: 0.11438620090484619
step: 7200, Loss: 0.1139921173453331
step: 7300, Loss: 0.11918261647224426
step: 7400, Loss: 0.11544474959373474
step: 7500, Loss: 0.11433569341897964
step: 7600, Loss: 0.11386556178331375
step: 7700, Loss: 0.11525722593069077
step: 7800, Loss: 0.11595044285058975
step: 7900, Loss: 0.11373785138130188
step: 8000, Loss: 0.11447511613368988
step: 8100, Loss: 0.11676927655935287
step: 8200, Loss: 0.11713983863592148
step: 8300, Loss: 0.1142100840806961
step: 8400, Loss: 0.11508090794086456
step: 8500, Loss: 0.1158062219619751
step: 8600, Loss: 0.11403508484363556
step: 8700, Loss: 0.11609300971031189
step: 8800, Loss: 0.11375696957111359
step: 8900, Loss: 0.11416220664978027
step: 9000, Loss: 0.11427155137062073
step: 9100, Loss: 0.1143706887960434
step: 9200, Loss: 0.11638785898685455
step: 9300, Loss: 0.11728081852197647
step: 9400, Loss: 0.11411546915769577
step: 9500, Loss: 0.11598320305347443
step: 9600, Loss: 0.11572781950235367
step: 9700, Loss: 0.11415129154920578
step: 9800, Loss: 0.11688360571861267
step: 9900, Loss: 0.1135299801826477
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.8823529411764706
recall: 1.0
F_score: 0.9375
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.11679402738809586
step: 100, Loss: 0.11657929420471191
step: 200, Loss: 0.11509279161691666
step: 300, Loss: 0.11437173932790756
step: 400, Loss: 0.1135980412364006
step: 500, Loss: 0.11266104876995087
step: 600, Loss: 0.11501027643680573
step: 700, Loss: 0.11524903774261475
step: 800, Loss: 0.11396203935146332
step: 900, Loss: 0.11488106846809387
step: 1000, Loss: 0.11364272236824036
step: 1100, Loss: 0.11314237117767334
step: 1200, Loss: 0.11724227666854858
step: 1300, Loss: 0.11463024467229843
step: 1400, Loss: 0.11429539322853088
step: 1500, Loss: 0.11615303158760071
step: 1600, Loss: 0.11456859856843948
step: 1700, Loss: 0.11376877874135971
step: 1800, Loss: 0.11522675305604935
step: 1900, Loss: 0.11439163982868195
step: 2000, Loss: 0.11458929628133774
step: 2100, Loss: 0.11356548964977264
step: 2200, Loss: 0.11451489478349686
step: 2300, Loss: 0.11382835358381271
step: 2400, Loss: 0.11407793313264847
step: 2500, Loss: 0.11384857445955276
step: 2600, Loss: 0.11654835194349289
step: 2700, Loss: 0.11460661143064499
step: 2800, Loss: 0.11313924938440323
step: 2900, Loss: 0.11389833688735962
step: 3000, Loss: 0.11579310894012451
step: 3100, Loss: 0.11433816701173782
step: 3200, Loss: 0.1153450608253479
step: 3500, Loss: 0.11933491379022598
step: 3600, Loss: 0.11890333145856857
step: 3700, Loss: 0.11810483783483505
step: 3800, Loss: 0.1161109060049057
step: 3900, Loss: 0.11659218370914459
step: 4000, Loss: 0.11916224658489227
step: 4100, Loss: 0.1161840558052063
step: 4200, Loss: 0.11660522222518921
step: 4300, Loss: 0.11562693864107132
step: 4400, Loss: 0.11551570892333984
step: 4500, Loss: 0.11577960103750229
step: 4600, Loss: 0.1158437505364418
step: 4700, Loss: 0.11587812751531601
step: 4800, Loss: 0.11541527509689331
step: 4900, Loss: 0.11475422233343124
step: 5000, Loss: 0.1159680187702179
step: 5100, Loss: 0.11493881791830063
step: 5200, Loss: 0.11571000516414642
step: 5300, Loss: 0.1139136478304863
step: 5400, Loss: 0.11475160717964172
step: 5500, Loss: 0.11426016688346863
step: 5600, Loss: 0.1151127889752388
step: 5700, Loss: 0.11517329514026642
step: 5800, Loss: 0.11477798223495483
step: 5900, Loss: 0.11350564658641815
step: 6000, Loss: 0.11567266285419464
step: 6100, Loss: 0.11485591530799866
step: 6200, Loss: 0.11402565985918045
step: 6300, Loss: 0.11391767859458923
step: 6400, Loss: 0.11584794521331787
step: 6500, Loss: 0.11506256461143494
step: 6600, Loss: 0.11549222469329834
step: 6700, Loss: 0.11449027806520462
step: 6800, Loss: 0.11547468602657318
step: 6900, Loss: 0.11514347046613693
step: 7000, Loss: 0.11467389762401581
step: 7100, Loss: 0.11669619381427765
step: 7200, Loss: 0.114039346575737
step: 7300, Loss: 0.11548630893230438
step: 7400, Loss: 0.11513853073120117
step: 7500, Loss: 0.11393047869205475
step: 7600, Loss: 0.11370057612657547
step: 7700, Loss: 0.11460155993700027
step: 7800, Loss: 0.11370821297168732
step: 7900, Loss: 0.11527119576931
step: 8000, Loss: 0.11500023305416107
step: 8100, Loss: 0.11508478224277496
step: 8200, Loss: 0.11445606499910355
step: 8300, Loss: 0.114874929189682
step: 8400, Loss: 0.11412602663040161
step: 8500, Loss: 0.11336198449134827
step: 8600, Loss: 0.11462272703647614
step: 8700, Loss: 0.11543095111846924
step: 8800, Loss: 0.11485512554645538
step: 8900, Loss: 0.11475051194429398
step: 9000, Loss: 0.11625942587852478
step: 9100, Loss: 0.11397278308868408
step: 9200, Loss: 0.11438143253326416
step: 9300, Loss: 0.11345541477203369
step: 9400, Loss: 0.11415715515613556
step: 9500, Loss: 0.1174752414226532
step: 9600, Loss: 0.11459711194038391
step: 9700, Loss: 0.11454183608293533
step: 9800, Loss: 0.11357486248016357
step: 9900, Loss: 0.11472360044717789
training successfully ended.
validating...
validate data length:102
acc: 0.9791666666666666
precision: 0.9636363636363636
recall: 1.0
F_score: 0.9814814814814815
******fold 10******

Training... train_data length:924
step: 0, Loss: 0.12033668160438538
step: 100, Loss: 0.11730073392391205
step: 200, Loss: 0.11568540334701538
step: 300, Loss: 0.11588867008686066
step: 400, Loss: 0.11572922766208649
step: 500, Loss: 0.11643120646476746
step: 600, Loss: 0.11529363691806793
step: 700, Loss: 0.11434128880500793
step: 800, Loss: 0.1156945526599884
step: 900, Loss: 0.18408004939556122
step: 1000, Loss: 0.25763192772865295
step: 1100, Loss: 0.1671774834394455
step: 1200, Loss: 0.16128894686698914
step: 1300, Loss: 0.14970721304416656
step: 1400, Loss: 0.14214056730270386
step: 1500, Loss: 0.13624341785907745
step: 1600, Loss: 0.1367529183626175
step: 1700, Loss: 0.1321302056312561
step: 1800, Loss: 0.130865216255188
step: 1900, Loss: 0.1283624768257141
step: 2000, Loss: 0.1286155879497528
step: 2100, Loss: 0.12586987018585205
step: 2200, Loss: 0.12735676765441895
step: 2300, Loss: 0.12517715990543365
step: 2400, Loss: 0.12157996743917465
step: 2500, Loss: 0.125290185213089
step: 2600, Loss: 0.11925207078456879
step: 2700, Loss: 0.12360531091690063
step: 2800, Loss: 0.1235060840845108
step: 2900, Loss: 0.12177298218011856
step: 3000, Loss: 0.11899560689926147
step: 3100, Loss: 0.120895616710186
step: 3200, Loss: 0.1186862513422966
step: 3300, Loss: 0.11794381588697433
step: 3400, Loss: 0.11713452637195587
step: 3500, Loss: 0.11832301318645477
step: 3600, Loss: 0.11680896580219269
step: 3700, Loss: 0.1153227910399437
step: 3800, Loss: 0.1172969788312912
step: 3900, Loss: 0.11570288240909576
step: 4000, Loss: 0.1160227581858635
step: 4100, Loss: 0.11580534279346466
step: 4200, Loss: 0.1140533834695816
step: 4300, Loss: 0.11459299921989441
step: 4400, Loss: 0.11481864750385284
step: 4500, Loss: 0.11373952776193619
step: 4600, Loss: 0.11394459754228592
step: 4700, Loss: 0.1144205778837204
step: 4800, Loss: 0.11480153352022171
step: 4900, Loss: 0.1146782785654068
step: 5000, Loss: 0.11446812748908997
step: 5100, Loss: 0.11627961695194244
step: 5200, Loss: 0.11474120616912842
step: 5300, Loss: 0.11445997655391693
step: 5400, Loss: 0.11500727385282516
step: 5500, Loss: 0.1132236197590828
step: 5600, Loss: 0.11377698183059692
step: 5700, Loss: 0.11394686996936798
step: 5800, Loss: 0.11499089002609253
step: 5900, Loss: 0.11506909877061844
step: 6000, Loss: 0.11293552070856094
step: 6100, Loss: 0.1138206496834755
step: 6200, Loss: 0.1145162358880043
step: 6300, Loss: 0.1138777956366539
step: 6400, Loss: 0.11547581106424332
step: 6500, Loss: 0.11433248221874237
step: 6600, Loss: 0.11473899334669113
step: 6700, Loss: 0.11480862647294998
step: 6800, Loss: 0.11395135521888733
step: 6900, Loss: 0.11579696089029312
step: 7000, Loss: 0.11468524485826492
step: 7100, Loss: 0.11467014998197556
step: 7200, Loss: 0.11357846856117249
step: 7300, Loss: 0.11451895534992218
step: 7400, Loss: 0.11339065432548523
step: 7500, Loss: 0.11748497188091278
step: 7600, Loss: 0.11600008606910706
step: 7700, Loss: 0.11679662019014359
step: 7800, Loss: 0.12218447029590607
step: 7900, Loss: 0.1215786561369896
step: 8000, Loss: 0.11882929503917694
step: 8100, Loss: 0.11581248790025711
step: 8200, Loss: 0.1153867170214653
step: 8300, Loss: 0.11442635208368301
step: 8400, Loss: 0.11462845653295517
step: 8500, Loss: 0.11379501968622208
step: 8600, Loss: 0.1142226979136467
step: 8700, Loss: 0.11638432741165161
step: 8800, Loss: 0.1146167516708374
step: 8900, Loss: 0.11370095610618591
step: 9000, Loss: 0.11533989012241364
step: 9100, Loss: 0.11486700177192688
step: 9200, Loss: 0.11418164521455765
step: 9300, Loss: 0.11577382683753967
step: 9400, Loss: 0.11369764059782028
step: 9500, Loss: 0.11406306177377701
step: 9600, Loss: 0.11344224214553833
step: 9700, Loss: 0.11550645530223846
step: 9800, Loss: 0.11475224792957306
step: 9900, Loss: 0.11584246158599854
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 1 Avgacc: 0.984375 Avgfscore: 0.9845565041851139 
 Max acc:1.0, Max f score:1.0
******** mix subject_2 ********

[304, 456]
******fold 1******

Training... train_data length:820
step: 0, Loss: 35.68293380737305
step: 100, Loss: 3.812469959259033
step: 200, Loss: 0.7339679598808289
step: 300, Loss: 2.7383792400360107
step: 400, Loss: 0.5554077625274658
step: 500, Loss: 0.21158942580223083
step: 600, Loss: 0.17639979720115662
step: 700, Loss: 0.15337462723255157
step: 800, Loss: 0.15951627492904663
step: 900, Loss: 0.13704872131347656
step: 1000, Loss: 0.14150922000408173
step: 1100, Loss: 0.13667042553424835
step: 1200, Loss: 0.13383762538433075
step: 1300, Loss: 0.13285914063453674
step: 1400, Loss: 0.12562397122383118
step: 1500, Loss: 0.13479217886924744
step: 1600, Loss: 0.1353234499692917
step: 1700, Loss: 0.1307603269815445
step: 1800, Loss: 0.125062957406044
step: 1900, Loss: 0.12789097428321838
step: 2000, Loss: 0.42233970761299133
step: 2100, Loss: 0.1333540678024292
step: 2200, Loss: 0.12598195672035217
step: 2300, Loss: 0.1288553774356842
step: 2400, Loss: 0.12151044607162476
step: 2500, Loss: 0.12790030241012573
step: 2600, Loss: 0.12060309946537018
step: 2700, Loss: 0.12650775909423828
step: 2800, Loss: 0.1194077730178833
step: 2900, Loss: 0.12530794739723206
step: 3000, Loss: 0.11884216219186783
step: 3100, Loss: 0.11754810810089111
step: 3200, Loss: 0.12164941430091858
step: 3300, Loss: 0.12688995897769928
step: 3400, Loss: 0.12063875794410706
step: 3500, Loss: 0.12020516395568848
step: 3600, Loss: 0.11963464319705963
step: 3700, Loss: 0.11796186864376068
step: 3800, Loss: 0.12202287465333939
step: 3300, Loss: 0.11577752977609634
step: 3400, Loss: 0.11389234662055969
step: 3500, Loss: 0.11482080072164536
step: 3600, Loss: 1.6571732759475708
step: 3700, Loss: 0.16970644891262054
step: 3800, Loss: 0.12995260953903198
step: 3900, Loss: 0.12686967849731445
step: 4000, Loss: 0.12458246946334839
step: 4100, Loss: 0.12322219461202621
step: 4200, Loss: 0.13398021459579468
step: 4300, Loss: 0.11891472339630127
step: 4400, Loss: 0.12344735860824585
step: 4500, Loss: 0.11834798008203506
step: 4600, Loss: 0.11823249608278275
step: 4700, Loss: 0.11859874427318573
step: 4800, Loss: 0.11759732663631439
step: 4900, Loss: 0.11814819276332855
step: 5000, Loss: 0.11826753616333008
step: 5100, Loss: 0.11945110559463501
step: 5200, Loss: 0.11770392954349518
step: 5300, Loss: 0.12045402824878693
step: 5400, Loss: 0.11830942332744598
step: 5500, Loss: 0.12666767835617065
step: 5600, Loss: 0.11799177527427673
step: 5700, Loss: 0.11546751111745834
step: 5800, Loss: 0.11582117527723312
step: 5900, Loss: 0.11640849709510803
step: 6000, Loss: 0.11898549646139145
step: 6100, Loss: 0.11398353427648544
step: 6200, Loss: 0.1167878806591034
step: 6300, Loss: 0.11443262547254562
step: 6400, Loss: 0.11685936152935028
step: 6500, Loss: 0.1139412373304367
step: 6600, Loss: 0.11416710168123245
step: 6700, Loss: 0.11526468396186829
step: 6800, Loss: 0.11740521341562271
step: 6900, Loss: 0.11361313611268997
step: 7000, Loss: 0.11718396097421646
step: 7100, Loss: 0.11569593846797943
step: 7200, Loss: 0.11563500761985779
step: 7300, Loss: 0.11602258682250977
step: 7400, Loss: 0.11571379750967026
step: 7500, Loss: 0.11521829664707184
step: 7600, Loss: 0.11569476127624512
step: 7700, Loss: 0.11482079327106476
step: 7800, Loss: 0.11469283699989319
step: 7900, Loss: 0.11820520460605621
step: 8000, Loss: 0.11558264493942261
step: 8100, Loss: 0.12121061980724335
step: 8200, Loss: 0.11309430748224258
step: 8300, Loss: 0.1170000210404396
step: 8400, Loss: 0.11477628350257874
step: 8500, Loss: 0.11395122855901718
step: 8600, Loss: 0.1149698868393898
step: 8700, Loss: 0.11423009634017944
step: 8800, Loss: 0.1153961718082428
step: 8900, Loss: 0.12133459746837616
step: 9000, Loss: 0.11454957723617554
step: 9100, Loss: 0.11396974325180054
step: 9200, Loss: 0.1160687580704689
step: 9300, Loss: 0.13850432634353638
step: 9400, Loss: 0.11381667107343674
step: 9500, Loss: 0.1160714253783226
step: 9600, Loss: 0.11576642841100693
step: 9700, Loss: 0.117325559258461
step: 9800, Loss: 0.11497002094984055
step: 9900, Loss: 0.115271657705307
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.9375
recall: 0.8823529411764706
F_score: 0.9090909090909091
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.11695842444896698
step: 100, Loss: 0.1164621114730835
step: 200, Loss: 0.1154753714799881
step: 300, Loss: 0.11588755249977112
step: 400, Loss: 0.11524412035942078
step: 500, Loss: 0.11406237632036209
step: 600, Loss: 0.1137964278459549
step: 700, Loss: 0.11590539664030075
step: 800, Loss: 0.11574318259954453
step: 900, Loss: 0.11337056010961533
step: 1000, Loss: 0.11414410173892975
step: 1100, Loss: 0.11326474696397781
step: 1200, Loss: 0.11328691244125366
step: 1300, Loss: 0.11445914953947067
step: 1400, Loss: 0.115205317735672
step: 1500, Loss: 4.900908470153809
step: 1600, Loss: 0.14537663757801056
step: 1700, Loss: 0.13190029561519623
step: 1800, Loss: 0.1294357180595398
step: 1900, Loss: 0.12568680942058563
step: 2000, Loss: 0.12292608618736267
step: 2100, Loss: 0.1223258376121521
step: 2200, Loss: 0.12198404967784882
step: 2300, Loss: 0.1178356185555458
step: 2400, Loss: 0.11819639801979065
step: 2500, Loss: 0.11653514951467514
step: 2600, Loss: 0.11908169090747833
step: 2700, Loss: 0.11982008814811707
step: 2800, Loss: 0.12259764224290848
step: 2900, Loss: 0.11827456951141357
step: 3000, Loss: 0.12291368842124939
step: 3100, Loss: 0.11590208113193512
step: 3200, Loss: 0.11699870228767395
step: 3300, Loss: 0.11548981070518494
step: 3400, Loss: 0.1152087152004242
step: 3500, Loss: 0.11577153950929642
step: 3600, Loss: 0.1152571514248848
step: 3700, Loss: 0.11592082679271698
step: 3800, Loss: 0.1155022606253624
step: 3900, Loss: 0.11679846048355103
step: 4000, Loss: 0.11420010775327682
step: 4100, Loss: 0.11518855392932892
step: 4200, Loss: 0.11477997899055481
step: 4300, Loss: 0.11514660716056824
step: 4400, Loss: 0.1164969727396965
step: 4500, Loss: 0.11406819522380829
step: 4600, Loss: 0.11746320128440857
step: 4700, Loss: 0.11430640518665314
step: 4800, Loss: 0.11383871734142303
step: 4900, Loss: 0.11471886932849884
step: 5000, Loss: 0.11815974116325378
step: 5100, Loss: 0.1168314516544342
step: 5200, Loss: 0.11524394899606705
step: 5300, Loss: 0.11402322351932526
step: 5400, Loss: 0.11452959477901459
step: 5500, Loss: 0.1144043505191803
step: 5600, Loss: 0.11307967454195023
step: 5700, Loss: 0.11452874541282654
step: 5800, Loss: 0.114284947514534
step: 5900, Loss: 0.11499708890914917
step: 6000, Loss: 0.11449446529150009
step: 6100, Loss: 0.11356646567583084
step: 6200, Loss: 0.11393769085407257
step: 6300, Loss: 0.11456263065338135
step: 6400, Loss: 0.11642319709062576
step: 6500, Loss: 0.11351058632135391
step: 6600, Loss: 0.11829203367233276
step: 6700, Loss: 0.11618299037218094
step: 6800, Loss: 0.1134742945432663
step: 6900, Loss: 0.11536278575658798
step: 7000, Loss: 0.11522610485553741
step: 7100, Loss: 0.11361314356327057
step: 7200, Loss: 0.11614090949296951
step: 7300, Loss: 0.1153019592165947
step: 7400, Loss: 0.11426595598459244
step: 7500, Loss: 0.11447037011384964
step: 7600, Loss: 0.11407075822353363
step: 7700, Loss: 0.11451368033885956
step: 7800, Loss: 0.1144164428114891
step: 7900, Loss: 0.11875762045383453
step: 8000, Loss: 0.1156592145562172
step: 8100, Loss: 0.11621592938899994
step: 8200, Loss: 0.11544530093669891
step: 8300, Loss: 0.11495620012283325
step: 8400, Loss: 0.1129482239484787
step: 8500, Loss: 0.1138785257935524
step: 8600, Loss: 0.11486136168241501
step: 8700, Loss: 0.11332052946090698
step: 8800, Loss: 0.11387646943330765
step: 8900, Loss: 0.1149747222661972
step: 9000, Loss: 0.11435715109109879
step: 9100, Loss: 0.11398205161094666
step: 9200, Loss: 0.11504775285720825
step: 9300, Loss: 0.11302962154150009
step: 9400, Loss: 0.11441400647163391
step: 9500, Loss: 0.11466693133115768
step: 9600, Loss: 0.11628890037536621
step: 9700, Loss: 0.11402947455644608
step: 9800, Loss: 0.11412602663040161
step: 9900, Loss: 0.11558273434638977
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 1 Avgacc: 0.8310416666666667 Avgfscore: 0.8393469430614461 
 Max acc:1.0, Max f score:1.0
******** mix subject_2 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 40.85306930541992
step: 100, Loss: 1.7634443044662476
step: 200, Loss: 0.14007849991321564
step: 300, Loss: 0.12581494450569153
step: 400, Loss: 0.12011928111314774
step: 500, Loss: 0.11857185512781143
step: 600, Loss: 0.1207074522972107
step: 700, Loss: 0.11779945343732834
step: 800, Loss: 0.11943107843399048
step: 900, Loss: 0.11592451483011246
step: 1000, Loss: 0.11820903420448303
step: 1100, Loss: 0.1149381771683693
step: 1200, Loss: 0.1154957041144371
step: 1300, Loss: 0.11505652219057083
step: 1400, Loss: 0.11391273885965347
step: 1500, Loss: 0.11428513377904892
step: 1600, Loss: 0.11510227620601654
step: 1700, Loss: 0.11554843932390213
step: 1800, Loss: 0.11463907361030579
step: 1900, Loss: 0.11488529294729233
step: 2000, Loss: 0.11420217156410217
step: 2100, Loss: 0.1136295348405838
step: 2200, Loss: 0.11954164505004883
step: 2300, Loss: 0.11532782018184662
step: 2400, Loss: 0.1142050251364708
step: 2500, Loss: 0.1141638532280922
step: 2600, Loss: 0.11475244164466858
step: 2700, Loss: 0.11429142206907272
step: 2800, Loss: 0.11337106674909592
step: 2900, Loss: 0.12339209020137787
step: 3000, Loss: 0.5915749073028564
step: 3100, Loss: 0.12748479843139648
step: 3200, Loss: 0.12994173169136047
step: 3300, Loss: 0.12005909532308578
step: 3400, Loss: 0.1271267980337143
step: 3500, Loss: 0.12560158967971802
step: 3900, Loss: 0.13121697306632996
step: 4000, Loss: 6.664112567901611
step: 4100, Loss: 0.4686448276042938
step: 4200, Loss: 1.0374386310577393
step: 4300, Loss: 0.7817849516868591
step: 4400, Loss: 0.15936778485774994
step: 4500, Loss: 0.1541934311389923
step: 4600, Loss: 0.13410615921020508
step: 4700, Loss: 0.13301457464694977
step: 4800, Loss: 0.14600428938865662
step: 4900, Loss: 0.12943081557750702
step: 5000, Loss: 0.12931261956691742
step: 5100, Loss: 0.12454207986593246
step: 5200, Loss: 0.1331230103969574
step: 5300, Loss: 0.12238483130931854
step: 5400, Loss: 0.12578630447387695
step: 5500, Loss: 0.12177079916000366
step: 5600, Loss: 0.1262221783399582
step: 5700, Loss: 0.1329595297574997
step: 5800, Loss: 0.12283556163311005
step: 5900, Loss: 0.11934167146682739
step: 6000, Loss: 0.1203753724694252
step: 6100, Loss: 0.12036506086587906
step: 6200, Loss: 0.11859592795372009
step: 6300, Loss: 0.11951834708452225
step: 6400, Loss: 0.11940722167491913
step: 6500, Loss: 0.120914027094841
step: 6600, Loss: 0.3882857859134674
step: 6700, Loss: 0.12117385864257812
step: 6800, Loss: 0.1221802681684494
step: 6900, Loss: 0.11997231841087341
step: 7000, Loss: 0.11839216947555542
step: 7100, Loss: 0.12083498388528824
step: 7200, Loss: 0.1154744029045105
step: 7300, Loss: 0.11892236024141312
step: 7400, Loss: 0.11670747399330139
step: 7500, Loss: 0.11553354561328888
step: 7600, Loss: 0.11795036494731903
step: 7700, Loss: 0.11686079949140549
step: 7800, Loss: 0.11727971583604813
step: 7900, Loss: 0.11689216643571854
step: 8000, Loss: 0.11599603295326233
step: 8100, Loss: 0.11569458246231079
step: 8200, Loss: 0.11798185855150223
step: 8300, Loss: 0.11775130778551102
step: 8400, Loss: 0.11471499502658844
step: 8500, Loss: 0.11915865540504456
step: 8600, Loss: 0.1150011345744133
step: 8700, Loss: 0.11753690242767334
step: 8800, Loss: 0.11628518998622894
step: 8900, Loss: 0.4184645414352417
step: 9000, Loss: 0.11442135274410248
step: 9100, Loss: 0.11709141731262207
step: 9200, Loss: 0.11706831306219101
step: 9300, Loss: 0.11412513256072998
step: 9400, Loss: 0.11562082171440125
step: 9500, Loss: 0.11337261646986008
step: 9600, Loss: 0.11621815711259842
step: 9700, Loss: 0.11377817392349243
step: 9800, Loss: 0.11723241209983826
step: 9900, Loss: 0.11569518595933914
training successfully ended.
validating...
validate data length:92
acc: 0.8522727272727273
precision: 0.8490566037735849
recall: 0.9
F_score: 0.8737864077669903
******fold 2******

Training... train_data length:820
step: 0, Loss: 3.4175314903259277
step: 100, Loss: 0.13126125931739807
step: 200, Loss: 0.13250912725925446
step: 300, Loss: 0.12095083296298981
step: 400, Loss: 0.11504928022623062
step: 500, Loss: 0.11878534406423569
step: 600, Loss: 0.11550095677375793
step: 700, Loss: 0.12225955724716187
step: 800, Loss: 0.11656837165355682
step: 900, Loss: 0.12158214300870895
step: 1000, Loss: 0.12038203328847885
step: 1100, Loss: 0.11795461922883987
step: 1200, Loss: 0.11667250096797943
step: 1300, Loss: 0.11627320945262909
step: 1400, Loss: 0.11593441665172577
step: 1500, Loss: 0.11557960510253906
step: 1600, Loss: 0.11542680859565735
step: 1700, Loss: 0.1142316460609436
step: 1800, Loss: 0.11356982588768005
step: 1900, Loss: 0.11467759311199188
step: 2000, Loss: 0.40101224184036255
step: 2100, Loss: 0.11662250012159348
step: 2200, Loss: 0.11614202708005905
step: 2300, Loss: 0.11467194557189941
step: 2400, Loss: 0.11579727381467819
step: 2500, Loss: 0.1135663241147995
step: 2600, Loss: 0.11307213455438614
step: 2700, Loss: 0.11407885700464249
step: 2800, Loss: 0.11444110423326492
step: 2900, Loss: 0.11327317357063293
step: 3000, Loss: 0.11630323529243469
step: 3100, Loss: 0.11443884670734406
step: 3200, Loss: 0.11453105509281158
step: 3300, Loss: 0.11581047624349594
step: 3400, Loss: 0.11440136283636093
step: 3500, Loss: 0.11402377486228943
step: 3600, Loss: 0.11485268175601959
step: 3700, Loss: 0.11632814258337021
step: 3800, Loss: 0.116077721118927
step: 3900, Loss: 0.11656290292739868
step: 4000, Loss: 0.1148742064833641
step: 4100, Loss: 2.64890193939209
step: 4200, Loss: 0.206090047955513
step: 4300, Loss: 0.643337070941925
step: 4400, Loss: 0.1293415129184723
step: 4500, Loss: 0.13267049193382263
step: 4600, Loss: 0.12434852123260498
step: 4700, Loss: 0.12657621502876282
step: 4800, Loss: 0.12912994623184204
step: 4900, Loss: 0.12370087206363678
step: 5000, Loss: 0.12238115817308426
step: 5100, Loss: 0.12374427914619446
step: 5200, Loss: 0.1242779940366745
step: 5300, Loss: 0.12203642725944519
step: 5400, Loss: 0.11821240186691284
step: 5500, Loss: 0.12120452523231506
step: 5600, Loss: 0.12001362442970276
step: 5700, Loss: 0.12221519649028778
step: 5800, Loss: 0.11718107759952545
step: 5900, Loss: 0.12246512621641159
step: 6000, Loss: 0.11499278247356415
step: 6100, Loss: 0.11810094863176346
step: 6200, Loss: 0.12039108574390411
step: 6300, Loss: 0.11422006040811539
step: 6400, Loss: 0.11818883568048477
step: 6500, Loss: 0.11651250720024109
step: 6600, Loss: 0.3917659521102905
step: 6700, Loss: 0.1166965588927269
step: 6800, Loss: 0.11759351193904877
step: 6900, Loss: 0.1142672598361969
step: 7000, Loss: 0.1144527867436409
step: 7100, Loss: 0.11807873845100403
step: 7200, Loss: 0.11390021443367004
step: 7300, Loss: 0.11684644967317581
step: 7400, Loss: 0.11330968141555786
step: 7500, Loss: 0.11658020317554474
step: 7600, Loss: 0.11557228118181229
step: 7700, Loss: 0.11393377929925919
step: 7800, Loss: 0.11699897050857544
step: 7900, Loss: 0.1146431565284729
step: 8000, Loss: 0.11387410759925842
step: 8100, Loss: 0.11565757542848587
step: 8200, Loss: 0.11462118476629257
step: 8300, Loss: 0.11623479425907135
step: 8400, Loss: 0.11534976959228516
step: 8500, Loss: 0.11697729676961899
step: 8600, Loss: 0.11587028205394745
step: 8700, Loss: 0.12124180793762207
step: 8800, Loss: 0.11384077370166779
step: 8900, Loss: 0.4289015531539917
step: 9000, Loss: 0.11594913899898529
step: 9100, Loss: 0.11482769250869751
step: 9200, Loss: 0.11412981897592545
step: 9300, Loss: 0.11443955451250076
step: 9400, Loss: 0.1144910454750061
step: 9500, Loss: 0.11360731720924377
step: 9600, Loss: 0.11462069302797318
step: 9700, Loss: 0.11405427008867264
step: 9800, Loss: 0.11476782709360123
step: 9900, Loss: 0.11698747426271439
training successfully ended.
validating...
validate data length:92
acc: 0.9318181818181818
precision: 0.8947368421052632
recall: 0.9444444444444444
F_score: 0.918918918918919
******fold 3******

Training... train_data length:821
step: 0, Loss: 0.11568646878004074
step: 100, Loss: 0.24279901385307312
step: 200, Loss: 0.1174439787864685
step: 300, Loss: 0.11591112613677979
step: 400, Loss: 0.11581427603960037
step: 500, Loss: 0.11606910824775696
step: 600, Loss: 0.11891812086105347
step: 700, Loss: 0.11913664638996124
step: 800, Loss: 0.11562327295541763
step: 900, Loss: 0.11606962233781815
step: 1000, Loss: 0.11487774550914764
step: 1100, Loss: 0.11467151343822479
step: 1200, Loss: 0.11451254785060883
step: 1300, Loss: 0.11526011675596237
step: 1400, Loss: 0.11457154899835587
step: 1500, Loss: 0.11549495160579681
step: 1600, Loss: 0.11433695256710052
step: 1700, Loss: 0.11561695486307144
step: 1800, Loss: 0.11502726376056671
step: 1900, Loss: 0.11375178396701813
step: 2000, Loss: 0.3676702678203583
step: 2100, Loss: 0.11452677845954895
step: 2200, Loss: 0.11641967296600342
step: 2300, Loss: 0.11468221992254257
step: 2400, Loss: 0.11406386643648148
step: 2500, Loss: 0.11370893567800522
step: 2600, Loss: 0.11473870277404785
step: 2700, Loss: 0.11435004323720932
step: 2800, Loss: 0.1139935553073883
step: 2900, Loss: 0.11372110247612
step: 3000, Loss: 0.1133720874786377
step: 3100, Loss: 1.5115313529968262
step: 3200, Loss: 0.6217284202575684
step: 3300, Loss: 0.16888588666915894
step: 3400, Loss: 0.13824301958084106
step: 3500, Loss: 0.1319691240787506
step: 3600, Loss: 0.12862147390842438
step: 3700, Loss: 0.1255793273448944
step: 3800, Loss: 0.1289052516222
step: 3900, Loss: 0.12659581005573273
step: 4000, Loss: 0.12014883011579514
step: 4100, Loss: 0.12232853472232819
step: 4200, Loss: 0.1202513575553894
step: 4300, Loss: 0.37325018644332886
step: 4400, Loss: 0.11923636496067047step: 3600, Loss: 0.12860354781150818
step: 3700, Loss: 0.1271992325782776
step: 3800, Loss: 0.12043360620737076
step: 3900, Loss: 0.11725053936243057
step: 4000, Loss: 0.12011964619159698
step: 4100, Loss: 0.11905829608440399
step: 4200, Loss: 0.11902718245983124
step: 4300, Loss: 0.11971031129360199
step: 4400, Loss: 0.12238816916942596
step: 4500, Loss: 0.11740797758102417
step: 4600, Loss: 0.11816460639238358
step: 4700, Loss: 0.11839786171913147
step: 4800, Loss: 0.11701449751853943
step: 4900, Loss: 0.1142207682132721
step: 5000, Loss: 0.1162482500076294
step: 5100, Loss: 0.11448401212692261
step: 5200, Loss: 0.11760696768760681
step: 5300, Loss: 0.1143408939242363
step: 5400, Loss: 0.1177046000957489
step: 5500, Loss: 0.11449971795082092
step: 5600, Loss: 0.11398470401763916
step: 5700, Loss: 0.1153668612241745
step: 5800, Loss: 0.11524296551942825
step: 5900, Loss: 0.11422739923000336
step: 6000, Loss: 0.11595624685287476
step: 6100, Loss: 0.11596138030290604
step: 6200, Loss: 0.11658062040805817
step: 6300, Loss: 0.11435828357934952
step: 6400, Loss: 0.11383382976055145
step: 6500, Loss: 0.1147121787071228
step: 6600, Loss: 0.1141558587551117
step: 6700, Loss: 0.11406596004962921
step: 6800, Loss: 0.11658446490764618
step: 6900, Loss: 0.11580401659011841
step: 7000, Loss: 0.11770421266555786
step: 7100, Loss: 0.11486081033945084
step: 7200, Loss: 0.11499819159507751
step: 7300, Loss: 0.11414631456136703
step: 7400, Loss: 0.11929993331432343
step: 7500, Loss: 0.11514583230018616
step: 7600, Loss: 0.11629564315080643
step: 7700, Loss: 0.11566922068595886
step: 7800, Loss: 0.11498618125915527
step: 7900, Loss: 0.11461814492940903
step: 8000, Loss: 0.11409361660480499
step: 8100, Loss: 0.11377467215061188
step: 8200, Loss: 0.11572249233722687
step: 8300, Loss: 0.11477385461330414
step: 8400, Loss: 0.11365464329719543
step: 8500, Loss: 0.11378609389066696
step: 8600, Loss: 0.11715826392173767
step: 8700, Loss: 0.11339728534221649
step: 8800, Loss: 0.11619912832975388
step: 8900, Loss: 0.1155988872051239
step: 9000, Loss: 0.11541658639907837
step: 9100, Loss: 0.11753647029399872
step: 9200, Loss: 0.11576273292303085
step: 9300, Loss: 0.11386435478925705
step: 9400, Loss: 0.11629516631364822
step: 9500, Loss: 0.11651873588562012
step: 9600, Loss: 0.11457916349172592
step: 9700, Loss: 0.11638855934143066
step: 9800, Loss: 7.904524803161621
step: 9900, Loss: 0.13197773694992065
training successfully ended.
validating...
validate data length:32
acc: 0.5
precision: 0.4090909090909091
recall: 0.75
F_score: 0.5294117647058824
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.5704354643821716
step: 100, Loss: 0.13146299123764038
step: 200, Loss: 0.12409625947475433
step: 300, Loss: 0.12278816103935242
step: 400, Loss: 0.12526530027389526
step: 500, Loss: 0.12233836948871613
step: 600, Loss: 0.11780351400375366
step: 700, Loss: 0.11821901053190231
step: 800, Loss: 0.11659321188926697
step: 900, Loss: 0.11604070663452148
step: 1000, Loss: 0.11606532335281372
step: 1100, Loss: 0.11752120405435562
step: 1200, Loss: 0.11442683637142181
step: 1300, Loss: 0.11384719610214233
step: 1400, Loss: 0.11891795694828033
step: 1500, Loss: 0.11448270082473755
step: 1600, Loss: 0.11544622480869293
step: 1700, Loss: 0.11589320003986359
step: 1800, Loss: 0.11481129378080368
step: 1900, Loss: 0.11577149480581284
step: 2000, Loss: 0.11511224508285522
step: 2100, Loss: 0.11682393401861191
step: 2200, Loss: 0.11513343453407288
step: 2300, Loss: 0.11471571028232574
step: 2400, Loss: 0.11451420187950134
step: 2500, Loss: 0.11520262062549591
step: 2600, Loss: 0.11675450205802917
step: 2700, Loss: 0.12058095633983612
step: 2800, Loss: 0.1253875195980072
step: 2900, Loss: 0.11510305106639862
step: 3000, Loss: 0.11421594023704529
step: 3100, Loss: 0.1153835579752922
step: 3200, Loss: 0.1159009262919426
step: 3300, Loss: 0.11749652028083801
step: 3400, Loss: 0.11510254442691803
step: 3500, Loss: 0.15884758532047272
step: 3600, Loss: 0.13030870258808136
step: 3700, Loss: 0.12559711933135986
step: 3800, Loss: 0.12423759698867798
step: 3900, Loss: 0.12324342876672745
step: 4000, Loss: 0.12209556996822357
step: 4100, Loss: 0.12272511422634125
step: 4200, Loss: 0.1210535541176796
step: 4300, Loss: 0.11828712373971939
step: 4400, Loss: 0.11848190426826477
step: 4500, Loss: 0.12028571963310242
step: 4600, Loss: 0.11817736178636551
step: 4700, Loss: 0.11785265803337097
step: 4800, Loss: 0.11598297953605652
step: 4900, Loss: 0.11484163999557495
step: 5000, Loss: 0.1161612793803215
step: 5100, Loss: 0.11693139374256134
step: 5200, Loss: 0.1158689334988594
step: 5300, Loss: 0.11697065830230713
step: 5400, Loss: 0.11523167043924332
step: 5500, Loss: 0.11532353609800339
step: 5600, Loss: 0.11673109978437424
step: 5700, Loss: 0.1155225932598114
step: 5800, Loss: 0.11462234705686569
step: 5900, Loss: 0.1164521723985672
step: 6000, Loss: 0.11504966020584106
step: 6100, Loss: 0.11607716977596283
step: 6200, Loss: 0.11490802466869354
step: 6300, Loss: 0.11575093865394592
step: 6400, Loss: 0.11525582522153854
step: 6500, Loss: 0.11430899053812027
step: 6600, Loss: 0.11531461775302887
step: 6700, Loss: 0.11552935838699341
step: 6800, Loss: 0.11817089468240738
step: 6900, Loss: 0.11506639420986176
step: 7000, Loss: 0.11374936997890472
step: 7100, Loss: 0.1147889494895935
step: 7200, Loss: 0.11642063409090042
step: 7300, Loss: 0.1146877110004425
step: 7400, Loss: 0.11296077072620392
step: 7500, Loss: 0.11758333444595337
step: 7600, Loss: 0.11474166065454483
step: 7700, Loss: 0.11475954949855804
step: 7800, Loss: 0.11614852398633957
step: 7900, Loss: 0.11773444712162018
step: 8000, Loss: 0.11353032290935516
step: 8100, Loss: 0.11685137450695038
step: 8200, Loss: 0.11408057808876038
step: 8300, Loss: 0.11489398032426834
step: 8400, Loss: 0.11461548507213593
step: 8500, Loss: 0.1142510399222374
step: 8600, Loss: 0.11464688926935196
step: 8700, Loss: 0.1139557883143425
step: 8800, Loss: 0.11548273265361786
step: 8900, Loss: 0.11529957503080368
step: 9000, Loss: 0.11527310311794281
step: 9100, Loss: 0.11925755441188812
step: 9200, Loss: 0.11470967531204224
step: 9300, Loss: 0.11450475454330444
step: 9400, Loss: 0.11617021262645721
step: 9500, Loss: 0.11479184031486511
step: 9600, Loss: 0.11343889683485031
step: 9700, Loss: 0.11540636420249939
step: 9800, Loss: 0.11442920565605164
step: 9900, Loss: 0.11361657083034515
training successfully ended.
validating...
validate data length:32
acc: 0.84375
precision: 0.7894736842105263
recall: 0.9375
F_score: 0.8571428571428572
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.8009254932403564
step: 100, Loss: 0.11708030104637146
step: 200, Loss: 0.11790274828672409
step: 300, Loss: 0.11802144348621368
step: 400, Loss: 0.11542107164859772
step: 500, Loss: 0.11514933407306671
step: 600, Loss: 0.11587096750736237
step: 700, Loss: 0.11645621806383133
step: 800, Loss: 0.11624900996685028
step: 900, Loss: 0.1218797042965889
step: 1000, Loss: 0.11421211063861847
step: 1100, Loss: 0.11531132459640503
step: 1200, Loss: 0.11431065946817398
step: 1300, Loss: 0.11565236002206802
step: 1400, Loss: 0.11343689262866974
step: 1500, Loss: 0.11579903960227966
step: 1600, Loss: 0.11364782601594925
step: 1700, Loss: 0.11527369171380997
step: 1800, Loss: 0.11516688764095306
step: 1900, Loss: 0.11513382941484451
step: 2000, Loss: 0.11351169645786285
step: 2100, Loss: 0.11485074460506439
step: 2200, Loss: 0.11545327305793762
step: 2300, Loss: 0.11671276390552521
step: 2400, Loss: 0.11414816975593567
step: 2500, Loss: 0.1151377409696579
step: 2600, Loss: 0.11376636475324631
step: 2700, Loss: 0.11486566811800003
step: 2800, Loss: 0.11451944708824158
step: 2900, Loss: 0.11452293395996094
step: 3000, Loss: 0.1150132343173027
step: 3100, Loss: 0.1140550971031189
step: 3200, Loss: 0.11388975381851196
step: 3300, Loss: 0.11424902081489563
step: 3400, Loss: 0.11564265191555023
step: 3500, Loss: 0.11519264429807663
step: 3600, Loss: 0.11478153616189957
step: 3700, Loss: 0.11435277760028839
step: 3800, Loss: 0.11469847708940506
step: 3900, Loss: 0.11514939367771149
step: 4000, Loss: 0.11439917236566544
step: 4100, Loss: 0.11769651621580124

step: 4500, Loss: 0.11544710397720337
step: 4600, Loss: 0.11898930370807648
step: 4700, Loss: 0.11847865581512451
step: 4800, Loss: 0.11675872653722763
step: 4900, Loss: 0.11874039471149445
step: 5000, Loss: 0.11637236177921295
step: 5100, Loss: 0.11708368360996246
step: 5200, Loss: 0.11493698507547379
step: 5300, Loss: 0.11721466481685638
step: 5400, Loss: 0.11598416417837143
step: 5500, Loss: 0.11723196506500244
step: 5600, Loss: 0.11751879751682281
step: 5700, Loss: 0.11539296060800552
step: 5800, Loss: 0.11552604287862778
step: 5900, Loss: 0.11624724417924881
step: 6000, Loss: 0.11895837634801865
step: 6100, Loss: 0.11593177914619446
step: 6200, Loss: 0.11660580337047577
step: 6300, Loss: 0.11473202705383301
step: 6400, Loss: 0.11491409689188004
step: 6500, Loss: 0.11537939310073853
step: 6600, Loss: 0.3583470582962036
step: 6700, Loss: 0.11549665778875351
step: 6800, Loss: 0.11620025336742401
step: 6900, Loss: 0.11450055241584778
step: 7000, Loss: 0.11377812176942825
step: 7100, Loss: 0.11430644243955612
step: 7200, Loss: 0.11532296240329742
step: 7300, Loss: 0.11710323393344879
step: 7400, Loss: 0.11406778544187546
step: 7500, Loss: 0.1155722439289093
step: 7600, Loss: 0.1137050986289978
step: 7700, Loss: 0.11969950050115585
step: 7800, Loss: 0.11439695209264755
step: 7900, Loss: 0.11369756609201431
step: 8000, Loss: 0.1129160150885582
step: 8100, Loss: 0.11495424807071686
step: 8200, Loss: 0.11461812257766724
step: 8300, Loss: 0.11550551652908325
step: 8400, Loss: 0.11384137719869614
step: 8500, Loss: 0.11432306468486786
step: 8600, Loss: 0.11419142782688141
step: 8700, Loss: 0.11451246589422226
step: 8800, Loss: 0.1138312891125679
step: 8900, Loss: 0.3584306836128235
step: 9000, Loss: 0.11484618484973907
step: 9100, Loss: 0.11593768000602722
step: 9200, Loss: 0.11441044509410858
step: 9300, Loss: 0.11461034417152405
step: 9400, Loss: 0.11384207010269165
step: 9500, Loss: 0.1133929044008255
step: 9600, Loss: 0.11536343395709991
step: 9700, Loss: 0.11655612289905548
step: 9800, Loss: 0.12082120776176453
step: 9900, Loss: 0.21129004657268524
training successfully ended.
validating...
validate data length:91
acc: 0.9545454545454546
precision: 0.9361702127659575
recall: 0.9777777777777777
F_score: 0.9565217391304347
******fold 4******

Training... train_data length:821
step: 0, Loss: 0.16824288666248322
step: 100, Loss: 0.14372417330741882
step: 200, Loss: 0.13041067123413086
step: 300, Loss: 0.12579719722270966
step: 400, Loss: 0.12025941908359528
step: 500, Loss: 0.1176157146692276
step: 600, Loss: 0.11783988028764725
step: 700, Loss: 0.11592395603656769
step: 800, Loss: 0.1145850121974945
step: 900, Loss: 0.11427381634712219
step: 1000, Loss: 0.11640185117721558
step: 1100, Loss: 0.11474042385816574
step: 1200, Loss: 0.11674801260232925
step: 1300, Loss: 0.11627798527479172
step: 1400, Loss: 0.11784224212169647
step: 1500, Loss: 0.11511676013469696
step: 1600, Loss: 0.11446969211101532
step: 1700, Loss: 0.114403635263443
step: 1800, Loss: 0.11641460657119751
step: 1900, Loss: 0.12043823301792145
step: 2000, Loss: 0.40151339769363403
step: 2100, Loss: 0.12191125750541687
step: 2200, Loss: 0.1139458417892456
step: 2300, Loss: 0.1140783354640007
step: 2400, Loss: 0.1146014928817749
step: 2500, Loss: 0.11390423029661179
step: 2600, Loss: 0.11385393142700195
step: 2700, Loss: 0.11420489102602005
step: 2800, Loss: 0.11433342844247818
step: 2900, Loss: 0.11549904942512512
step: 3000, Loss: 0.1162545308470726
step: 3100, Loss: 0.11375301331281662
step: 3200, Loss: 0.11267004162073135
step: 3300, Loss: 0.4618365466594696
step: 3400, Loss: 0.22126364707946777
step: 3500, Loss: 0.13131409883499146
step: 3600, Loss: 0.12069493532180786
step: 3700, Loss: 0.12616600096225739
step: 3800, Loss: 0.12604853510856628
step: 3900, Loss: 0.1224590539932251
step: 4000, Loss: 0.12166699767112732
step: 4100, Loss: 0.12375444173812866
step: 4200, Loss: 0.12090442329645157
step: 4300, Loss: 0.3921552300453186
step: 4400, Loss: 0.12245810031890869
step: 4500, Loss: 0.12034782767295837
step: 4600, Loss: 0.11769738793373108
step: 4700, Loss: 0.11943016201257706
step: 4800, Loss: 0.11730338633060455
step: 4900, Loss: 0.11669883131980896
step: 5000, Loss: 0.11566871404647827
step: 5100, Loss: 0.11751420795917511
step: 5200, Loss: 0.11951448023319244
step: 5300, Loss: 0.11517462879419327
step: 5400, Loss: 0.11441423743963242
step: 5500, Loss: 0.11835051327943802
step: 5600, Loss: 0.11629388481378555
step: 5700, Loss: 0.11648139357566833
step: 5800, Loss: 0.11476094275712967
step: 5900, Loss: 0.11705778539180756
step: 6000, Loss: 0.11636395752429962
step: 6100, Loss: 0.11374892294406891
step: 6200, Loss: 0.1162630096077919
step: 6300, Loss: 0.11478453129529953
step: 6400, Loss: 0.11512923240661621
step: 6500, Loss: 0.1164839044213295
step: 6600, Loss: 0.3656735420227051
step: 6700, Loss: 0.11581750959157944
step: 6800, Loss: 0.114293172955513
step: 6900, Loss: 0.11408986151218414
step: 7000, Loss: 0.11302021145820618
step: 7100, Loss: 0.11496172100305557
step: 7200, Loss: 0.11604803055524826
step: 7300, Loss: 0.1164058968424797
step: 7400, Loss: 0.1144721731543541
step: 7500, Loss: 0.11407322436571121
step: 7600, Loss: 0.1152549535036087
step: 7700, Loss: 0.11387425661087036
step: 7800, Loss: 0.11424116045236588
step: 7900, Loss: 0.11428407579660416
step: 8000, Loss: 0.1141691654920578
step: 8100, Loss: 0.11444249749183655
step: 8200, Loss: 0.11399859189987183
step: 8300, Loss: 0.11399396508932114
step: 8400, Loss: 0.11404786258935928
step: 8500, Loss: 0.11561001092195511
step: 8600, Loss: 0.114163339138031
step: 8700, Loss: 0.11452022194862366
step: 8800, Loss: 0.11487135291099548
step: 8900, Loss: 0.35882991552352905
step: 9000, Loss: 0.11552035063505173
step: 9100, Loss: 0.11485742777585983
step: 9200, Loss: 0.11338142305612564
step: 9300, Loss: 0.11449863761663437
step: 9400, Loss: 0.11679399013519287
step: 9500, Loss: 0.11532655358314514
step: 9600, Loss: 0.1132672056555748
step: 9700, Loss: 0.11305229365825653
step: 9800, Loss: 0.11455560475587845
step: 9900, Loss: 0.11382309347391129
training successfully ended.
validating...
validate data length:91
acc: 0.9886363636363636
precision: 0.9803921568627451
recall: 1.0
F_score: 0.99009900990099
******fold 5******

Training... train_data length:821
step: 0, Loss: 0.11253751069307327
step: 100, Loss: 0.11714984476566315
step: 200, Loss: 0.11683129519224167
step: 300, Loss: 0.11440042406320572
step: 400, Loss: 0.11556226015090942
step: 500, Loss: 0.11364007741212845
step: 600, Loss: 0.11415128409862518
step: 700, Loss: 0.11398648470640182
step: 800, Loss: 0.11479545384645462
step: 900, Loss: 0.11309228092432022
step: 1000, Loss: 0.1146824061870575
step: 1100, Loss: 0.1138523668050766
step: 1200, Loss: 0.11292999982833862
step: 1300, Loss: 0.11344883590936661
step: 1400, Loss: 0.11432261765003204
step: 1500, Loss: 0.11373847723007202
step: 1600, Loss: 0.11426430940628052
step: 1700, Loss: 0.11462454497814178
step: 1800, Loss: 0.11457905173301697
step: 1900, Loss: 0.11578469723463058
step: 2000, Loss: 0.3849341869354248
step: 2100, Loss: 0.11329328268766403
step: 2200, Loss: 0.11409253627061844
step: 2300, Loss: 0.11350730806589127
step: 2400, Loss: 0.11348440498113632
step: 2500, Loss: 0.11400163918733597
step: 2600, Loss: 0.11441728472709656
step: 2700, Loss: 0.11398211121559143
step: 2800, Loss: 0.11341254413127899
step: 2900, Loss: 0.11383181065320969
step: 3000, Loss: 0.11452734470367432
step: 3100, Loss: 0.11398956179618835
step: 3200, Loss: 0.11490973085165024
step: 3300, Loss: 0.14440764486789703
step: 3400, Loss: 0.14022982120513916
step: 3500, Loss: 0.12657669186592102
step: 3600, Loss: 0.12033925950527191
step: 3700, Loss: 0.12156625092029572
step: 3800, Loss: 0.12066985666751862
step: 3900, Loss: 0.12186530232429504
step: 4000, Loss: 0.11852529644966125
step: 4100, Loss: 0.11677329242229462
step: 4200, Loss: 0.11839604377746582
step: 4300, Loss: 0.4206640124320984
step: 4400, Loss: 0.11741091310977936
step: 4500, Loss: 0.11717792600393295
step: 4600, Loss: 0.11969231814146042
step: 4700, Loss: 0.11515282094478607
step: 4800, Loss: 0.11819068342447281
step: 4900, Loss: 0.11667636036872864
step: 4200, Loss: 0.11620235443115234
step: 4300, Loss: 0.3024088740348816
step: 4400, Loss: 0.1371973752975464
step: 4500, Loss: 0.13911044597625732
step: 4600, Loss: 0.12643495202064514
step: 4700, Loss: 0.1371580958366394
step: 4800, Loss: 0.12845522165298462
step: 4900, Loss: 0.13132160902023315
step: 5000, Loss: 0.12025249749422073
step: 5100, Loss: 0.1291990578174591
step: 5200, Loss: 0.11889341473579407
step: 5300, Loss: 0.12421974539756775
step: 5400, Loss: 0.11947880685329437
step: 5500, Loss: 0.12228018790483475
step: 5600, Loss: 0.11908844858407974
step: 5700, Loss: 0.12292557954788208
step: 5800, Loss: 0.11802712827920914
step: 5900, Loss: 0.11983014643192291
step: 6000, Loss: 0.12031225115060806
step: 6100, Loss: 0.11959861218929291
step: 6200, Loss: 0.12093466520309448
step: 6300, Loss: 0.12342429161071777
step: 6400, Loss: 0.11556734889745712
step: 6500, Loss: 0.12258421629667282
step: 6600, Loss: 0.11629787087440491
step: 6700, Loss: 0.11896055936813354
step: 6800, Loss: 0.11716268211603165
step: 6900, Loss: 0.11574169993400574
step: 7000, Loss: 0.1172180026769638
step: 7100, Loss: 0.12129706144332886
step: 7200, Loss: 0.11957865208387375
step: 7300, Loss: 0.12011133879423141
step: 7400, Loss: 0.11661000549793243
step: 7500, Loss: 0.11854925751686096
step: 7600, Loss: 0.11561144888401031
step: 7700, Loss: 0.11996698379516602
step: 7800, Loss: 0.11449247598648071
step: 7900, Loss: 0.11965771019458771
step: 8000, Loss: 0.11569014191627502
step: 8100, Loss: 0.1207459419965744
step: 8200, Loss: 0.11456302553415298
step: 8300, Loss: 0.1139650046825409
step: 8400, Loss: 0.11488315463066101
step: 8500, Loss: 0.11439387500286102
step: 8600, Loss: 0.11294018477201462
step: 8700, Loss: 0.11600054800510406
step: 8800, Loss: 0.11452753841876984
step: 8900, Loss: 0.11471204459667206
step: 9000, Loss: 0.11506955325603485
step: 9100, Loss: 0.11527731269598007
step: 9200, Loss: 0.11610381305217743
step: 9300, Loss: 0.12100623548030853
step: 9400, Loss: 0.11388449370861053
step: 9500, Loss: 0.11574948579072952
step: 9600, Loss: 0.11494232714176178
step: 9700, Loss: 0.1153775006532669
step: 9800, Loss: 0.11426877230405807
step: 9900, Loss: 0.11462114751338959
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 1.0
recall: 0.9333333333333333
F_score: 0.9655172413793104
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.4651932418346405
step: 100, Loss: 0.11604028940200806
step: 200, Loss: 0.11641906201839447
step: 300, Loss: 0.11424434185028076
step: 400, Loss: 0.11390779912471771
step: 500, Loss: 0.11667343229055405
step: 600, Loss: 0.1164780855178833
step: 700, Loss: 0.11414451152086258
step: 800, Loss: 0.11344991624355316
step: 900, Loss: 0.1184229850769043
step: 1000, Loss: 0.11465711146593094
step: 1100, Loss: 0.11556868255138397
step: 1200, Loss: 0.11387332528829575
step: 1300, Loss: 0.11627732962369919
step: 1400, Loss: 0.11428914219141006
step: 1500, Loss: 0.11609138548374176
step: 1600, Loss: 0.11403118818998337
step: 1700, Loss: 0.11662246286869049
step: 1800, Loss: 0.11403854936361313
step: 1900, Loss: 0.11401860415935516
step: 2000, Loss: 0.1139330118894577
step: 2100, Loss: 0.1147834062576294
step: 2200, Loss: 0.11522861570119858
step: 2300, Loss: 0.11401450634002686
step: 2400, Loss: 0.11458231508731842
step: 2500, Loss: 0.11484681069850922
step: 2600, Loss: 0.11416522413492203
step: 2700, Loss: 0.11466298997402191
step: 2800, Loss: 0.11450177431106567
step: 2900, Loss: 0.11294747143983841
step: 3000, Loss: 0.11411620676517487
step: 3100, Loss: 0.11822175979614258
step: 3200, Loss: 0.11389833688735962
step: 3300, Loss: 0.11473146080970764
step: 3400, Loss: 0.1135258600115776
step: 3500, Loss: 0.11422291398048401
step: 3600, Loss: 0.1139967069029808
step: 3700, Loss: 0.11419262737035751
step: 3800, Loss: 0.11639495193958282
step: 3900, Loss: 0.11386667937040329
step: 4000, Loss: 0.11664050817489624
step: 4100, Loss: 0.11668410897254944
step: 4200, Loss: 0.11761164665222168
step: 4300, Loss: 0.24398565292358398
step: 4400, Loss: 0.16906991600990295
step: 4500, Loss: 0.14500127732753754
step: 4600, Loss: 0.1403457075357437
step: 4700, Loss: 0.14149287343025208
step: 4800, Loss: 0.13418830931186676
step: 4900, Loss: 0.1472589075565338
step: 5000, Loss: 0.12551341950893402
step: 5100, Loss: 0.12080870568752289
step: 5200, Loss: 0.1329590380191803
step: 5300, Loss: 0.13222530484199524
step: 5400, Loss: 0.12184235453605652
step: 5500, Loss: 0.11800343543291092
step: 5600, Loss: 0.12232542037963867
step: 5700, Loss: 0.11881765723228455
step: 5800, Loss: 0.1182420551776886
step: 5900, Loss: 0.11985678970813751
step: 6000, Loss: 0.12392295897006989
step: 6100, Loss: 0.13464245200157166
step: 6200, Loss: 0.1190822497010231
step: 6300, Loss: 0.11773192882537842
step: 6400, Loss: 0.11898726969957352
step: 6500, Loss: 0.11535240709781647
step: 6600, Loss: 0.11792495101690292
step: 6700, Loss: 0.1160859540104866
step: 6800, Loss: 0.1164688840508461
step: 6900, Loss: 0.11608751118183136
step: 7000, Loss: 0.11828020215034485
step: 7100, Loss: 0.11425288766622543
step: 7200, Loss: 0.11421395093202591
step: 7300, Loss: 0.1158914715051651
step: 7400, Loss: 0.11480855941772461
step: 7500, Loss: 0.11627469956874847
step: 7600, Loss: 0.11506499350070953
step: 7700, Loss: 0.11434587836265564
step: 7800, Loss: 0.11657793819904327
step: 7900, Loss: 0.11514367908239365
step: 8000, Loss: 0.11594091355800629
step: 8100, Loss: 0.11549355834722519
step: 8200, Loss: 0.11976457387208939
step: 8300, Loss: 0.11415117979049683
step: 8400, Loss: 0.11330651491880417
step: 8500, Loss: 0.11778336018323898
step: 8600, Loss: 0.11427828669548035
step: 8700, Loss: 0.11551810801029205
step: 8800, Loss: 0.11389763653278351
step: 8900, Loss: 0.11451476067304611
step: 9000, Loss: 0.11367352306842804
step: 9100, Loss: 0.11574742943048477
step: 9200, Loss: 0.11335594952106476
step: 9300, Loss: 0.11629121005535126
step: 9400, Loss: 0.11568684130907059
step: 9500, Loss: 0.11586438864469528
step: 9600, Loss: 0.11601472645998001
step: 9700, Loss: 0.11352169513702393
step: 9800, Loss: 0.11596144735813141
step: 9900, Loss: 0.11503373831510544
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8333333333333334
recall: 1.0
F_score: 0.9090909090909091
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.4263240098953247
step: 100, Loss: 0.11767491698265076
step: 200, Loss: 0.11507841944694519
step: 300, Loss: 0.11444379389286041
step: 400, Loss: 0.11400092393159866
step: 500, Loss: 0.11401683837175369
step: 600, Loss: 0.11346361041069031
step: 700, Loss: 0.11331063508987427
step: 800, Loss: 0.11565474420785904
step: 900, Loss: 0.11483343690633774
step: 1000, Loss: 0.11443769931793213
step: 1100, Loss: 0.11344513297080994
step: 1200, Loss: 0.11359402537345886
step: 1300, Loss: 0.11413595825433731
step: 1400, Loss: 0.11350838094949722
step: 1500, Loss: 0.11525749415159225
step: 1600, Loss: 0.1152821034193039
step: 1700, Loss: 0.11389822512865067
step: 1800, Loss: 0.11448110640048981
step: 1900, Loss: 0.11306514590978622
step: 2000, Loss: 0.11441663652658463
step: 2100, Loss: 0.11412647366523743
step: 2200, Loss: 0.113895483314991
step: 2300, Loss: 0.11390867084264755
step: 2400, Loss: 0.113709457218647
step: 2500, Loss: 0.11642628908157349
step: 2600, Loss: 0.11243614554405212
step: 2700, Loss: 0.11405787616968155
step: 2800, Loss: 0.11445392668247223
step: 2900, Loss: 0.11356986314058304
step: 3000, Loss: 0.11398636549711227
step: 3100, Loss: 0.11380825936794281
step: 3200, Loss: 0.11373188346624374
step: 3300, Loss: 0.11500507593154907
step: 3400, Loss: 0.113408163189888
step: 3500, Loss: 0.1130148321390152
step: 3600, Loss: 0.11504857987165451
step: 3700, Loss: 0.11515919864177704
step: 3800, Loss: 0.1140940710902214
step: 3900, Loss: 0.11428321897983551
step: 4000, Loss: 0.11393651366233826
step: 4100, Loss: 1.0925252437591553
step: 4200, Loss: 0.13604497909545898
step: 4300, Loss: 0.13342362642288208
step: 4400, Loss: 0.1504327952861786
step: 4500, Loss: 0.11937840282917023
step: 4600, Loss: 0.1227099671959877
step: 4700, Loss: 0.119667649269104
step: 5000, Loss: 0.11535037308931351
step: 5100, Loss: 0.11575902253389359
step: 5200, Loss: 0.1192033588886261
step: 5300, Loss: 0.11600285023450851
step: 5400, Loss: 0.11722217500209808
step: 5500, Loss: 0.13266025483608246
step: 5600, Loss: 0.11679403483867645
step: 5700, Loss: 0.11459112167358398
step: 5800, Loss: 0.11556237190961838
step: 5900, Loss: 0.1141122579574585
step: 6000, Loss: 0.11661756038665771
step: 6100, Loss: 0.11400294303894043
step: 6200, Loss: 0.11405305564403534
step: 6300, Loss: 0.11616045236587524
step: 6400, Loss: 0.11398474872112274
step: 6500, Loss: 0.1154928058385849
step: 6600, Loss: 0.3765299320220947
step: 6700, Loss: 0.11616338789463043
step: 6800, Loss: 0.11533407121896744
step: 6900, Loss: 0.1139596477150917
step: 7000, Loss: 0.1135254055261612
step: 7100, Loss: 0.1151210367679596
step: 7200, Loss: 0.1142551600933075
step: 7300, Loss: 0.11466129869222641
step: 7400, Loss: 0.11923005431890488
step: 7500, Loss: 0.11724316328763962
step: 7600, Loss: 0.11510372906923294
step: 7700, Loss: 0.11408642679452896
step: 7800, Loss: 0.11559928208589554
step: 7900, Loss: 0.1152108907699585
step: 8000, Loss: 0.11274368315935135
step: 8100, Loss: 0.11314952373504639
step: 8200, Loss: 0.11391566693782806
step: 8300, Loss: 0.11332187056541443
step: 8400, Loss: 0.11321626603603363
step: 8500, Loss: 0.11434456706047058
step: 8600, Loss: 2.209534168243408
step: 8700, Loss: 0.966864824295044
step: 8800, Loss: 0.1328345537185669
step: 8900, Loss: 0.4384850859642029
step: 9000, Loss: 0.12930075824260712
step: 9100, Loss: 0.12013038247823715
step: 9200, Loss: 0.12344491481781006
step: 9300, Loss: 0.12363599240779877
step: 9400, Loss: 0.11884691566228867
step: 9500, Loss: 0.11815866082906723
step: 9600, Loss: 0.11903597414493561
step: 9700, Loss: 0.118869848549366
step: 9800, Loss: 0.11597748100757599
step: 9900, Loss: 0.11770109832286835
training successfully ended.
validating...
validate data length:91
acc: 0.9431818181818182
precision: 0.9302325581395349
recall: 0.9523809523809523
F_score: 0.9411764705882352
******fold 6******

Training... train_data length:821
step: 0, Loss: 0.11444611102342606
step: 100, Loss: 0.11820356547832489
step: 200, Loss: 0.11612335592508316
step: 300, Loss: 0.11427357792854309
step: 400, Loss: 0.11597416549921036
step: 500, Loss: 0.11420376598834991
step: 600, Loss: 0.11381169408559799
step: 700, Loss: 0.11384814977645874
step: 800, Loss: 0.11354154348373413
step: 900, Loss: 0.11467275023460388
step: 1000, Loss: 0.1157461553812027
step: 1100, Loss: 0.11371590197086334
step: 1200, Loss: 0.1139615848660469
step: 1300, Loss: 0.11417439579963684
step: 1400, Loss: 0.11391256749629974
step: 1500, Loss: 0.11335428059101105
step: 1600, Loss: 0.1157330870628357
step: 1700, Loss: 0.11565369367599487
step: 1800, Loss: 0.11467547714710236
step: 1900, Loss: 0.11516369134187698
step: 2000, Loss: 0.35957396030426025
step: 2100, Loss: 0.11478732526302338
step: 2200, Loss: 0.11432056128978729
step: 2300, Loss: 0.11425943672657013
step: 2400, Loss: 0.11540961265563965
step: 2500, Loss: 0.11307744681835175
step: 2600, Loss: 0.11360165476799011
step: 2700, Loss: 0.11509504914283752
step: 2800, Loss: 0.11352743208408356
step: 2900, Loss: 0.11552780866622925
step: 3000, Loss: 0.11830341070890427
step: 3100, Loss: 0.873406171798706
step: 3200, Loss: 0.17732080817222595
step: 3300, Loss: 0.13385385274887085
step: 3400, Loss: 0.12684550881385803
step: 3500, Loss: 0.12276454269886017
step: 3600, Loss: 0.12023244798183441
step: 3700, Loss: 0.12668460607528687
step: 3800, Loss: 0.12135554850101471
step: 3900, Loss: 0.1172361746430397
step: 4000, Loss: 0.11742784082889557
step: 4100, Loss: 0.11596786230802536
step: 4200, Loss: 0.1227504089474678
step: 4300, Loss: 0.38548222184181213
step: 4400, Loss: 0.11702261120080948
step: 4500, Loss: 0.11821481585502625
step: 4600, Loss: 0.11476397514343262
step: 4700, Loss: 0.11660072952508926
step: 4800, Loss: 0.11502515524625778
step: 4900, Loss: 0.11539597809314728
step: 5000, Loss: 0.11726617068052292
step: 5100, Loss: 0.11492833495140076
step: 5200, Loss: 0.11586509644985199
step: 5300, Loss: 0.11658754199743271
step: 5400, Loss: 0.11426880955696106
step: 5500, Loss: 0.1153256818652153
step: 5600, Loss: 0.11483258008956909
step: 5700, Loss: 0.11394003033638
step: 5800, Loss: 0.11570951342582703
step: 5900, Loss: 0.1145620346069336
step: 6000, Loss: 0.11448141932487488
step: 6100, Loss: 0.11505010724067688
step: 6200, Loss: 0.11601096391677856
step: 6300, Loss: 0.11727342754602432
step: 6400, Loss: 0.11363426595926285
step: 6500, Loss: 0.11368470638990402
step: 6600, Loss: 0.36443179845809937
step: 6700, Loss: 0.11342877894639969
step: 6800, Loss: 0.11469709128141403
step: 6900, Loss: 0.11324295401573181
step: 7000, Loss: 0.1142987459897995
step: 7100, Loss: 0.113350510597229
step: 7200, Loss: 0.11636058986186981
step: 7300, Loss: 0.11459974944591522
step: 7400, Loss: 0.11486256122589111
step: 7500, Loss: 0.11334197223186493
step: 7600, Loss: 0.11449611932039261
step: 7700, Loss: 0.11454106867313385
step: 7800, Loss: 0.11398020386695862
step: 7900, Loss: 0.11557372659444809
step: 8000, Loss: 0.11388570815324783
step: 8100, Loss: 0.11368481814861298
step: 8200, Loss: 0.115297831594944
step: 8300, Loss: 0.11457251012325287
step: 8400, Loss: 0.11416327953338623
step: 8500, Loss: 0.11384248733520508
step: 8600, Loss: 0.11461161822080612
step: 8700, Loss: 0.11309663951396942
step: 8800, Loss: 0.1133337989449501
step: 8900, Loss: 0.3683832287788391
step: 9000, Loss: 0.11471462994813919
step: 9100, Loss: 0.1142931655049324
step: 9200, Loss: 0.1136336624622345
step: 9300, Loss: 5.584404945373535
step: 9400, Loss: 0.15604284405708313
step: 9500, Loss: 0.14531733095645905
step: 9600, Loss: 0.12425854802131653
step: 9700, Loss: 0.12602293491363525
step: 9800, Loss: 0.1238044947385788
step: 9900, Loss: 0.12330245226621628
training successfully ended.
validating...
validate data length:91
acc: 0.9659090909090909
precision: 0.9534883720930233
recall: 0.9761904761904762
F_score: 0.9647058823529412
******fold 7******

Training... train_data length:821
step: 0, Loss: 0.11526235193014145
step: 100, Loss: 0.11588466167449951
step: 200, Loss: 0.11465191841125488
step: 300, Loss: 0.1140923798084259
step: 400, Loss: 0.11539071798324585
step: 500, Loss: 0.11325889825820923
step: 600, Loss: 0.11319282650947571
step: 700, Loss: 0.11372752487659454
step: 800, Loss: 0.11662657558917999
step: 900, Loss: 0.11311863362789154
step: 1000, Loss: 0.1147129088640213
step: 1100, Loss: 0.11603747308254242
step: 1200, Loss: 0.1133991926908493
step: 1300, Loss: 0.11450383812189102
step: 1400, Loss: 0.11481302231550217
step: 1500, Loss: 0.11453310400247574
step: 1600, Loss: 0.11371839046478271
step: 1700, Loss: 0.11319480836391449
step: 1800, Loss: 0.11840806156396866
step: 1900, Loss: 0.113670714199543
step: 2000, Loss: 0.35984495282173157
step: 2100, Loss: 0.1202794685959816
step: 2200, Loss: 0.11388185620307922
step: 2300, Loss: 0.11517404019832611
step: 2400, Loss: 0.11592614650726318
step: 2500, Loss: 0.11586271226406097
step: 2600, Loss: 0.16549895703792572
step: 2700, Loss: 0.15410010516643524
step: 2800, Loss: 0.13426801562309265
step: 2900, Loss: 0.1272868812084198
step: 3000, Loss: 0.1277308613061905
step: 3100, Loss: 0.12926800549030304
step: 3200, Loss: 0.11838564276695251
step: 3300, Loss: 0.1180833950638771
step: 3400, Loss: 0.11783585697412491
step: 3500, Loss: 0.11795704066753387
step: 3600, Loss: 0.11641264706850052
step: 3700, Loss: 0.11893667280673981
step: 3800, Loss: 0.11860811710357666
step: 3900, Loss: 0.117555633187294
step: 4000, Loss: 0.11680108308792114
step: 4100, Loss: 0.12103944271802902
step: 4200, Loss: 0.11778496950864792
step: 4300, Loss: 0.4127957820892334
step: 4400, Loss: 0.11864607781171799
step: 4500, Loss: 0.11668359488248825
step: 4600, Loss: 0.11459139734506607
step: 4700, Loss: 0.11458566039800644
step: 4800, Loss: 0.11461297422647476
step: 4900, Loss: 0.11672571301460266
step: 5000, Loss: 0.11469109356403351
step: 5100, Loss: 0.1166757270693779
step: 5200, Loss: 0.11390936374664307
step: 5300, Loss: 0.11449860036373138
step: 5400, Loss: 0.11561897397041321
step: 4800, Loss: 0.12265440821647644
step: 4900, Loss: 0.12003085017204285
step: 5000, Loss: 0.11768097430467606
step: 5100, Loss: 0.12050364911556244
step: 5200, Loss: 0.11633332073688507
step: 5300, Loss: 0.11628293991088867
step: 5400, Loss: 0.11712779104709625
step: 5500, Loss: 0.11682644486427307
step: 5600, Loss: 0.11586305499076843
step: 5700, Loss: 0.11742833256721497
step: 5800, Loss: 0.11574750393629074
step: 5900, Loss: 0.11882734298706055
step: 6000, Loss: 0.11525989323854446
step: 6100, Loss: 0.11726823449134827
step: 6200, Loss: 0.11761361360549927
step: 6300, Loss: 0.11546836793422699
step: 6400, Loss: 0.11550041288137436
step: 6500, Loss: 0.11491820216178894
step: 6600, Loss: 0.11637771129608154
step: 6700, Loss: 0.11532524973154068
step: 6800, Loss: 0.11557014286518097
step: 6900, Loss: 0.11565889418125153
step: 7000, Loss: 0.11495194584131241
step: 7100, Loss: 0.11618669331073761
step: 7200, Loss: 0.11354506015777588
step: 7300, Loss: 0.11578954756259918
step: 7400, Loss: 0.11404094099998474
step: 7500, Loss: 0.11485885083675385
step: 7600, Loss: 0.1171175092458725
step: 7700, Loss: 0.11404763162136078
step: 7800, Loss: 0.11362022161483765
step: 7900, Loss: 0.11530289053916931
step: 8000, Loss: 0.11607062816619873
step: 8100, Loss: 0.11624451726675034
step: 8200, Loss: 0.11588987708091736
step: 8300, Loss: 0.11451219022274017
step: 8400, Loss: 0.11587304621934891
step: 8500, Loss: 0.11321941763162613
step: 8600, Loss: 0.11356252431869507
step: 8700, Loss: 0.11386768519878387
step: 8800, Loss: 0.11373619735240936
step: 8900, Loss: 0.11389025300741196
step: 9000, Loss: 0.11410568654537201
step: 9100, Loss: 0.11480595916509628
step: 9200, Loss: 0.1156446561217308
step: 9300, Loss: 0.1139744445681572
step: 9400, Loss: 0.1142558678984642
step: 9500, Loss: 0.11445197463035583
step: 9600, Loss: 0.11393605172634125
step: 9700, Loss: 0.11350332945585251
step: 9800, Loss: 0.1136663556098938
step: 9900, Loss: 0.11347581446170807
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8421052631578947
recall: 0.8888888888888888
F_score: 0.8648648648648649
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.3805525004863739
step: 100, Loss: 0.11628814041614532
step: 200, Loss: 0.11515075713396072
step: 300, Loss: 0.11508561670780182
step: 400, Loss: 0.11476153880357742
step: 500, Loss: 0.11521751433610916
step: 600, Loss: 0.11446008086204529
step: 700, Loss: 0.11415167897939682
step: 800, Loss: 0.11275491118431091
step: 900, Loss: 0.11850724369287491
step: 1000, Loss: 0.1129240095615387
step: 1100, Loss: 0.11489424854516983
step: 1200, Loss: 0.11337456107139587
step: 1300, Loss: 0.11385747045278549
step: 1400, Loss: 0.1137893944978714
step: 1500, Loss: 0.11310022324323654
step: 1600, Loss: 0.11580396443605423
step: 1700, Loss: 0.11486777663230896
step: 1800, Loss: 0.11511191725730896
step: 1900, Loss: 0.11309358477592468
step: 2000, Loss: 0.11475759744644165
step: 2100, Loss: 0.11451350897550583
step: 2200, Loss: 0.11314177513122559
step: 2300, Loss: 0.11390592902898788
step: 2400, Loss: 0.11407296359539032
step: 2500, Loss: 0.11472419649362564
step: 2600, Loss: 0.11336714029312134
step: 2700, Loss: 0.11519190669059753
step: 2800, Loss: 0.1144033670425415
step: 2900, Loss: 0.1146949976682663
step: 3000, Loss: 0.1158548966050148
step: 3100, Loss: 0.1151074469089508
step: 3200, Loss: 0.1143910214304924
step: 3300, Loss: 0.11616059392690659
step: 3400, Loss: 0.11472248286008835
step: 3500, Loss: 0.11622709780931473
step: 3600, Loss: 0.11405983567237854
step: 3700, Loss: 0.11678379774093628
step: 3800, Loss: 0.11381932348012924
step: 3900, Loss: 0.11645840108394623
step: 4000, Loss: 0.11503615975379944
step: 4100, Loss: 0.11451731622219086
step: 4200, Loss: 0.11508353054523468
step: 4300, Loss: 0.11383780837059021
step: 4400, Loss: 0.11495840549468994
step: 4500, Loss: 0.11501434445381165
step: 4600, Loss: 0.11414578557014465
step: 4700, Loss: 4.705965518951416
step: 4800, Loss: 0.14776653051376343
step: 4900, Loss: 0.1447046548128128
step: 5000, Loss: 0.13368691504001617
step: 5100, Loss: 0.12559904158115387
step: 5200, Loss: 0.12341739237308502
step: 5300, Loss: 0.12489904463291168
step: 5400, Loss: 0.12395388633012772
step: 5500, Loss: 0.12104775011539459
step: 5600, Loss: 0.11758145689964294
step: 5700, Loss: 0.12066315114498138
step: 5800, Loss: 0.11666624993085861
step: 5900, Loss: 0.15418890118598938
step: 6000, Loss: 0.11917001754045486
step: 6100, Loss: 0.11776015162467957
step: 6200, Loss: 0.11639505624771118
step: 6300, Loss: 0.11966104805469513
step: 6400, Loss: 0.11786490678787231
step: 6500, Loss: 0.11632134020328522
step: 6600, Loss: 0.11493290960788727
step: 6700, Loss: 0.1170077696442604
step: 6800, Loss: 0.1145544946193695
step: 6900, Loss: 0.11734823882579803
step: 7000, Loss: 0.11623784154653549
step: 7100, Loss: 0.11841541528701782
step: 7200, Loss: 0.11442898213863373
step: 7300, Loss: 0.11637537181377411
step: 7400, Loss: 0.1145123690366745
step: 7500, Loss: 0.11460824310779572
step: 7600, Loss: 0.11590556800365448
step: 7700, Loss: 0.114091657102108
step: 7800, Loss: 0.11552530527114868
step: 7900, Loss: 0.11500835418701172
step: 8000, Loss: 0.11499078571796417
step: 8100, Loss: 0.11403244733810425
step: 8200, Loss: 0.11532286554574966
step: 8300, Loss: 0.11495247483253479
step: 8400, Loss: 0.11511054635047913
step: 8500, Loss: 0.11527057737112045
step: 8600, Loss: 0.1137142926454544
step: 8700, Loss: 0.11440043896436691
step: 8800, Loss: 0.11558881402015686
step: 8900, Loss: 0.11713594198226929
step: 9000, Loss: 0.11449380964040756
step: 9100, Loss: 0.11464546620845795
step: 9200, Loss: 0.1145944893360138
step: 9300, Loss: 0.1180310994386673
step: 9400, Loss: 0.11440566182136536
step: 9500, Loss: 0.11516307294368744
step: 9600, Loss: 0.11471740901470184
step: 9700, Loss: 0.11433697491884232
step: 9800, Loss: 0.11408347636461258
step: 9900, Loss: 0.11404690146446228
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8947368421052632
recall: 0.9444444444444444
F_score: 0.918918918918919
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.4874427616596222
step: 100, Loss: 0.11635742336511612
step: 200, Loss: 0.11678757518529892
step: 300, Loss: 0.11934497952461243
step: 400, Loss: 0.11486169695854187
step: 500, Loss: 0.11488775908946991
step: 600, Loss: 0.11560527980327606
step: 700, Loss: 0.11489035189151764
step: 800, Loss: 0.1135435625910759
step: 900, Loss: 0.11386565864086151
step: 1000, Loss: 0.11547493934631348
step: 1100, Loss: 0.11439644545316696
step: 1200, Loss: 0.1143338680267334
step: 1300, Loss: 0.11314074695110321
step: 1400, Loss: 0.11365960538387299
step: 1500, Loss: 0.11393691599369049
step: 1600, Loss: 0.11572298407554626
step: 1700, Loss: 0.1155933365225792
step: 1800, Loss: 0.1142539232969284
step: 1900, Loss: 0.11362795531749725
step: 2000, Loss: 0.11557773500680923
step: 2100, Loss: 0.11545412242412567
step: 2200, Loss: 0.11346669495105743
step: 2300, Loss: 0.1142570972442627
step: 2400, Loss: 0.11367738246917725
step: 2500, Loss: 0.11415693908929825
step: 2600, Loss: 0.11384027451276779
step: 2700, Loss: 0.11413954198360443
step: 2800, Loss: 0.11475052684545517
step: 2900, Loss: 0.11424151808023453
step: 3000, Loss: 0.11571281403303146
step: 3100, Loss: 0.11436626315116882
step: 3200, Loss: 0.11391442269086838
step: 3300, Loss: 0.11342044174671173
step: 3400, Loss: 0.11346842348575592
step: 3500, Loss: 0.11439267545938492
step: 3600, Loss: 0.11398330330848694
step: 3700, Loss: 0.11585526913404465
step: 3800, Loss: 0.1145155280828476
step: 3900, Loss: 0.11484237760305405
step: 4000, Loss: 0.11469795554876328
step: 4100, Loss: 0.11412385106086731
step: 4200, Loss: 0.11429057270288467
step: 4300, Loss: 0.9415476322174072
step: 4400, Loss: 0.16135653853416443
step: 4500, Loss: 0.1369149535894394
step: 4600, Loss: 0.1302686333656311
step: 4700, Loss: 0.12426214665174484
step: 4800, Loss: 0.12701284885406494
step: 4900, Loss: 0.11827584356069565
step: 5000, Loss: 0.11807899922132492
step: 5100, Loss: 0.12424870580434799
step: 5200, Loss: 0.11859665811061859
step: 5500, Loss: 0.11423318088054657
step: 5600, Loss: 0.11413107812404633
step: 5700, Loss: 0.11589351296424866
step: 5800, Loss: 0.11414140462875366
step: 5900, Loss: 0.11459269374608994
step: 6000, Loss: 0.11534251272678375
step: 6100, Loss: 0.11614277958869934
step: 6200, Loss: 0.11540402472019196
step: 6300, Loss: 0.11431697756052017
step: 6400, Loss: 0.11477296054363251
step: 6500, Loss: 0.11526156961917877
step: 6600, Loss: 0.3545494079589844
step: 6700, Loss: 0.1199561357498169
step: 6800, Loss: 0.11303436756134033
step: 6900, Loss: 0.11365073174238205
step: 7000, Loss: 0.1137414425611496
step: 7100, Loss: 0.11298667639493942
step: 7200, Loss: 0.11384187638759613
step: 7300, Loss: 0.11369342356920242
step: 7400, Loss: 0.11372823268175125
step: 7500, Loss: 0.11496298015117645
step: 7600, Loss: 0.11278511583805084
step: 7700, Loss: 0.11392775177955627
step: 7800, Loss: 0.11534926295280457
step: 7900, Loss: 0.11369934678077698
step: 8000, Loss: 0.11516990512609482
step: 8100, Loss: 0.11361567676067352
step: 8200, Loss: 0.11514566838741302
step: 8300, Loss: 0.405890554189682
step: 8400, Loss: 1.4094207286834717
step: 8500, Loss: 0.12751524150371552
step: 8600, Loss: 0.1281360238790512
step: 8700, Loss: 0.12263114005327225
step: 8800, Loss: 0.12242589890956879
step: 8900, Loss: 0.4395914673805237
step: 9000, Loss: 0.12225931137800217
step: 9100, Loss: 0.11642763018608093
step: 9200, Loss: 0.12030298262834549
step: 9300, Loss: 0.1206338033080101
step: 9400, Loss: 0.12351245433092117
step: 9500, Loss: 0.11794780939817429
step: 9600, Loss: 0.11654037237167358
step: 9700, Loss: 0.11748231947422028
step: 9800, Loss: 0.12040182203054428
step: 9900, Loss: 0.11630876362323761
training successfully ended.
validating...
validate data length:91
acc: 0.9545454545454546
precision: 0.9473684210526315
recall: 0.9473684210526315
F_score: 0.9473684210526315
******fold 8******

Training... train_data length:821
step: 0, Loss: 0.11324802041053772
step: 100, Loss: 0.11664728820323944
step: 200, Loss: 0.11487868428230286
step: 300, Loss: 0.11617686599493027
step: 400, Loss: 0.11700324714183807
step: 500, Loss: 0.1166180819272995
step: 600, Loss: 0.11363250762224197
step: 700, Loss: 0.11443350464105606
step: 800, Loss: 0.11351668834686279
step: 900, Loss: 0.11381666362285614
step: 1000, Loss: 0.11398085951805115
step: 1100, Loss: 0.11523804068565369
step: 1200, Loss: 0.11283911764621735
step: 1300, Loss: 0.11272747814655304
step: 1400, Loss: 0.11406150460243225
step: 1500, Loss: 0.11406460404396057
step: 1600, Loss: 0.11280721426010132
step: 1700, Loss: 0.11442747712135315
step: 1800, Loss: 0.11596958339214325
step: 1900, Loss: 0.11346075683832169
step: 2000, Loss: 0.3816525638103485
step: 2100, Loss: 0.1144939661026001
step: 2200, Loss: 0.11397160589694977
step: 2300, Loss: 0.11298318207263947
step: 2400, Loss: 0.11334044486284256
step: 2500, Loss: 0.11453867703676224
step: 2600, Loss: 0.11408789455890656
step: 2700, Loss: 0.11441696435213089
step: 2800, Loss: 0.11293511837720871
step: 2900, Loss: 0.11410802602767944
step: 3000, Loss: 0.11250539124011993
step: 3100, Loss: 0.1138443723320961
step: 3200, Loss: 0.11387664079666138
step: 3300, Loss: 0.11287043988704681
step: 3400, Loss: 0.11445357650518417
step: 3500, Loss: 0.11514163762331009
step: 3600, Loss: 0.11487457156181335
step: 3700, Loss: 1.317525863647461
step: 3800, Loss: 0.2446441650390625
step: 3900, Loss: 0.1397046148777008
step: 4000, Loss: 0.13816869258880615
step: 4100, Loss: 0.1231299340724945
step: 4200, Loss: 0.12244509160518646
step: 4300, Loss: 0.41320452094078064
step: 4400, Loss: 0.12012161314487457
step: 4500, Loss: 0.11987745761871338
step: 4600, Loss: 0.11817269772291183
step: 4700, Loss: 0.1244276612997055
step: 4800, Loss: 0.12070836126804352
step: 4900, Loss: 0.11722849309444427
step: 5000, Loss: 0.11537858843803406
step: 5100, Loss: 0.11655811220407486
step: 5200, Loss: 0.11690347641706467
step: 5300, Loss: 0.11456550657749176
step: 5400, Loss: 0.11586097627878189
step: 5500, Loss: 0.12338592857122421
step: 5600, Loss: 0.11456907540559769
step: 5700, Loss: 0.11594860255718231
step: 5800, Loss: 0.11737743020057678
step: 5900, Loss: 0.11602696031332016
step: 6000, Loss: 0.11301220953464508
step: 6100, Loss: 0.11580818146467209
step: 6200, Loss: 0.11570553481578827
step: 6300, Loss: 0.11549516767263412
step: 6400, Loss: 0.11555055528879166
step: 6500, Loss: 0.11571359634399414
step: 6600, Loss: 0.37368977069854736
step: 6700, Loss: 0.11666770279407501
step: 6800, Loss: 0.11536037176847458
step: 6900, Loss: 0.11402475088834763
step: 7000, Loss: 0.11598874628543854
step: 7100, Loss: 0.11512989550828934
step: 7200, Loss: 0.11467602849006653
step: 7300, Loss: 0.11464396864175797
step: 7400, Loss: 0.11571548134088516
step: 7500, Loss: 0.11416905373334885
step: 7600, Loss: 0.11595135927200317
step: 7700, Loss: 0.11317093670368195
step: 7800, Loss: 0.11497373133897781
step: 7900, Loss: 0.11601447314023972
step: 8000, Loss: 0.11620207130908966
step: 8100, Loss: 0.11256980150938034
step: 8200, Loss: 0.11527637392282486
step: 8300, Loss: 0.11510534584522247
step: 8400, Loss: 0.11509475111961365
step: 8500, Loss: 0.11445855349302292
step: 8600, Loss: 0.11342784762382507
step: 8700, Loss: 0.11521201580762863
step: 8800, Loss: 0.11569911241531372
step: 8900, Loss: 0.37705308198928833
step: 9000, Loss: 0.11617494374513626
step: 9100, Loss: 0.11325067281723022
step: 9200, Loss: 0.11313888430595398
step: 9300, Loss: 0.11398662626743317
step: 9400, Loss: 0.11286461353302002
step: 9500, Loss: 0.11434673517942429
step: 9600, Loss: 0.11388503760099411
step: 9700, Loss: 0.11351186037063599
step: 9800, Loss: 0.11557228118181229
step: 9900, Loss: 0.11257568001747131
training successfully ended.
validating...
validate data length:91
acc: 0.9772727272727273
precision: 0.9795918367346939
recall: 0.9795918367346939
F_score: 0.9795918367346939
******fold 9******

Training... train_data length:821
step: 0, Loss: 0.1151416152715683
step: 100, Loss: 0.12243925034999847
step: 200, Loss: 0.11483314633369446
step: 300, Loss: 0.11397071927785873
step: 400, Loss: 0.114472895860672
step: 500, Loss: 0.11607491970062256
step: 600, Loss: 0.11408927291631699
step: 700, Loss: 0.11412063241004944
step: 800, Loss: 0.11384549736976624
step: 900, Loss: 0.11339068412780762
step: 1000, Loss: 0.11421993374824524
step: 1100, Loss: 0.11391398310661316
step: 1200, Loss: 0.11461764574050903
step: 1300, Loss: 0.11412259191274643
step: 1400, Loss: 0.11388421058654785
step: 1500, Loss: 0.11380632221698761
step: 1600, Loss: 0.11307522654533386
step: 1700, Loss: 2.8422799110412598
step: 1800, Loss: 0.186674565076828
step: 1900, Loss: 0.1325053721666336
step: 2000, Loss: 0.4159628450870514
step: 2100, Loss: 0.12134450674057007
step: 2200, Loss: 0.11756729334592819
step: 2300, Loss: 0.11840955913066864
step: 2400, Loss: 0.11628751456737518
step: 2500, Loss: 0.12266673892736435
step: 2600, Loss: 0.12069273740053177
step: 2700, Loss: 0.11634915322065353
step: 2800, Loss: 0.11478284001350403
step: 2900, Loss: 0.11901505291461945
step: 3000, Loss: 0.12054020166397095
step: 3100, Loss: 0.1200709193944931
step: 3200, Loss: 0.11462286859750748
step: 3300, Loss: 0.11695517599582672
step: 3400, Loss: 0.11730580031871796
step: 3500, Loss: 0.11628462374210358
step: 3600, Loss: 0.1154274269938469
step: 3700, Loss: 0.11626946181058884
step: 3800, Loss: 0.11523866653442383
step: 3900, Loss: 0.11616919934749603
step: 4000, Loss: 0.11561775207519531
step: 4100, Loss: 0.1164788156747818
step: 4200, Loss: 0.11774009466171265
step: 4300, Loss: 0.3779594600200653
step: 4400, Loss: 0.11579981446266174
step: 4500, Loss: 0.11477233469486237
step: 4600, Loss: 0.1132095530629158
step: 4700, Loss: 0.11580933630466461
step: 4800, Loss: 0.11406183242797852
step: 4900, Loss: 0.1148756816983223
step: 5000, Loss: 0.114101842045784
step: 5100, Loss: 0.11438682675361633
step: 5200, Loss: 0.1164698377251625
step: 5300, Loss: 0.11417289078235626
step: 5400, Loss: 0.1153985783457756
step: 5500, Loss: 0.1135043352842331
step: 5600, Loss: 0.11466094106435776
step: 5700, Loss: 0.11558198183774948
step: 5800, Loss: 0.11430919915437698
step: 5900, Loss: 0.11454366892576218
step: 5300, Loss: 0.11867037415504456
step: 5400, Loss: 0.11742877960205078
step: 5500, Loss: 0.11524370312690735
step: 5600, Loss: 0.11418391764163971
step: 5700, Loss: 0.1183810755610466
step: 5800, Loss: 0.11553285270929337
step: 5900, Loss: 0.12687139213085175
step: 6000, Loss: 0.1164059117436409
step: 6100, Loss: 0.11688056588172913
step: 6200, Loss: 0.11698209494352341
step: 6300, Loss: 0.11982814222574234
step: 6400, Loss: 0.11494967341423035
step: 6500, Loss: 0.11532796919345856
step: 6600, Loss: 0.11944954097270966
step: 6700, Loss: 0.11386942118406296
step: 6800, Loss: 0.1158985123038292
step: 6900, Loss: 0.11483892798423767
step: 7000, Loss: 0.11865558475255966
step: 7100, Loss: 0.11459526419639587
step: 7200, Loss: 0.11458258330821991
step: 7300, Loss: 0.11648435145616531
step: 7400, Loss: 0.1149417981505394
step: 7500, Loss: 0.11722106486558914
step: 7600, Loss: 0.1170898973941803
step: 7700, Loss: 0.11529987305402756
step: 7800, Loss: 0.11451491713523865
step: 7900, Loss: 0.12009306997060776
step: 8000, Loss: 0.11658176779747009
step: 8100, Loss: 0.11614485085010529
step: 8200, Loss: 0.11418776959180832
step: 8300, Loss: 0.11364154517650604
step: 8400, Loss: 0.11409130692481995
step: 8500, Loss: 0.11411791294813156
step: 8600, Loss: 0.11490920186042786
step: 8700, Loss: 0.11428359895944595
step: 8800, Loss: 0.11405730992555618
step: 8900, Loss: 0.11594054102897644
step: 9000, Loss: 0.11290475726127625
step: 9100, Loss: 0.11804328858852386
step: 9200, Loss: 0.11540907621383667
step: 9300, Loss: 0.11570776253938675
step: 9400, Loss: 0.11524292081594467
step: 9500, Loss: 0.11476326733827591
step: 9600, Loss: 0.1143946498632431
step: 9700, Loss: 0.11425959318876266
step: 9800, Loss: 0.11813823133707047
step: 9900, Loss: 0.12164362519979477
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8
recall: 0.9411764705882353
F_score: 0.8648648648648648
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.47169211506843567
step: 100, Loss: 0.11723595857620239
step: 200, Loss: 0.11439139395952225
step: 300, Loss: 0.1149764209985733
step: 400, Loss: 0.11381090432405472
step: 500, Loss: 0.11535978317260742
step: 600, Loss: 0.11381903290748596
step: 700, Loss: 0.11442628502845764
step: 800, Loss: 0.11470909416675568
step: 900, Loss: 0.11449911445379257
step: 1000, Loss: 0.1142275407910347
step: 1100, Loss: 0.11474210768938065
step: 1200, Loss: 0.11379926651716232
step: 1300, Loss: 0.11452944576740265
step: 1400, Loss: 0.11420218646526337
step: 1500, Loss: 0.11518649011850357
step: 1600, Loss: 0.11389236897230148
step: 1700, Loss: 0.11334782093763351
step: 1800, Loss: 0.11376746743917465
step: 1900, Loss: 0.1131167858839035
step: 2000, Loss: 0.11348357051610947
step: 2100, Loss: 0.11435921490192413
step: 2200, Loss: 0.11441217362880707
step: 2300, Loss: 0.1158650666475296
step: 2400, Loss: 0.11508588492870331
step: 2500, Loss: 0.1150965616106987
step: 2600, Loss: 0.11526262760162354
step: 2700, Loss: 0.11377623677253723
step: 2800, Loss: 0.11622092127799988
step: 2900, Loss: 0.11721538007259369
step: 3000, Loss: 0.1143658235669136
step: 3100, Loss: 0.1142142191529274
step: 3200, Loss: 0.11483991146087646
step: 3300, Loss: 0.11546875536441803
step: 3400, Loss: 0.11362995952367783
step: 3500, Loss: 0.11452256888151169
step: 3600, Loss: 0.11467442661523819
step: 3700, Loss: 0.11439201235771179
step: 3800, Loss: 0.11380917578935623
step: 3900, Loss: 0.11452971398830414
step: 4000, Loss: 0.11544235050678253
step: 4100, Loss: 0.11510006338357925
step: 4200, Loss: 0.11553406715393066
step: 4300, Loss: 0.1134289875626564
step: 4400, Loss: 0.11507242918014526
step: 4500, Loss: 0.11471952497959137
step: 4600, Loss: 0.11556500941514969
step: 4700, Loss: 0.11482411623001099
step: 4800, Loss: 0.11376770585775375
step: 4900, Loss: 0.11342927813529968
step: 5000, Loss: 0.11302552372217178
step: 5100, Loss: 0.11359501630067825
step: 5200, Loss: 3.1210756301879883
step: 5300, Loss: 0.1494521051645279
step: 5400, Loss: 0.12892116606235504
step: 5500, Loss: 0.12768864631652832
step: 5600, Loss: 0.11889663338661194
step: 5700, Loss: 0.12440594285726547
step: 5800, Loss: 0.11718840152025223
step: 5900, Loss: 0.11739762127399445
step: 6000, Loss: 0.1162404865026474
step: 6100, Loss: 0.11972398310899734
step: 6200, Loss: 0.12786442041397095
step: 6300, Loss: 0.11826196312904358
step: 6400, Loss: 0.1146010011434555
step: 6500, Loss: 0.11846446990966797
step: 6600, Loss: 0.11783261597156525
step: 6700, Loss: 0.12035921216011047
step: 6800, Loss: 0.11784148216247559
step: 6900, Loss: 0.11607972532510757
step: 7000, Loss: 0.11604458838701248
step: 7100, Loss: 0.11974118649959564
step: 7200, Loss: 0.1165669858455658
step: 7300, Loss: 0.11605122685432434
step: 7400, Loss: 0.11604678630828857
step: 7500, Loss: 0.11531943082809448
step: 7600, Loss: 0.11741872131824493
step: 7700, Loss: 0.11449217796325684
step: 7800, Loss: 0.11637630313634872
step: 7900, Loss: 0.11479450762271881
step: 8000, Loss: 0.11518123000860214
step: 8100, Loss: 0.11358015984296799
step: 8200, Loss: 0.1139618307352066
step: 8300, Loss: 0.11564312130212784
step: 8400, Loss: 0.11390563100576401
step: 8500, Loss: 0.11478659510612488
step: 8600, Loss: 0.1166883185505867
step: 8700, Loss: 0.12375275790691376
step: 8800, Loss: 0.1138189285993576
step: 8900, Loss: 0.11657489836215973
step: 9000, Loss: 0.11366841942071915
step: 9100, Loss: 0.1163044422864914
step: 9200, Loss: 0.11519511044025421
step: 9300, Loss: 0.11373227834701538
step: 9400, Loss: 0.11540879309177399
step: 9500, Loss: 0.11465638130903244
step: 9600, Loss: 0.11388035118579865
step: 9700, Loss: 0.11344483494758606
step: 9800, Loss: 0.11380067467689514
step: 9900, Loss: 0.11478695273399353
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8666666666666667
recall: 0.8666666666666667
F_score: 0.8666666666666667
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.4009358584880829
step: 100, Loss: 0.11700176447629929
step: 200, Loss: 0.11769174039363861
step: 300, Loss: 0.11421169340610504
step: 400, Loss: 0.11611634492874146
step: 500, Loss: 0.1154002994298935
step: 600, Loss: 0.11524085700511932
step: 700, Loss: 0.11366944015026093
step: 800, Loss: 0.11461363732814789
step: 900, Loss: 0.11590208113193512
step: 1000, Loss: 0.11587661504745483
step: 1100, Loss: 0.11325426399707794
step: 1200, Loss: 0.11532218754291534
step: 1300, Loss: 0.11386070400476456
step: 1400, Loss: 0.1141083762049675
step: 1500, Loss: 0.11380445212125778
step: 1600, Loss: 0.11455665528774261
step: 1700, Loss: 0.11796046048402786
step: 1800, Loss: 0.11391215771436691
step: 1900, Loss: 0.11404195427894592
step: 2000, Loss: 0.1138269305229187
step: 2100, Loss: 0.11468356847763062
step: 2200, Loss: 0.11400630325078964
step: 2300, Loss: 0.11344736814498901
step: 2400, Loss: 0.11374030262231827
step: 2500, Loss: 0.11374148726463318
step: 2600, Loss: 0.11415190994739532
step: 2700, Loss: 0.11400905251502991
step: 2800, Loss: 0.11515455693006516
step: 2900, Loss: 0.11432543396949768
step: 3000, Loss: 0.11391406506299973
step: 3100, Loss: 0.11352163553237915
step: 3200, Loss: 0.11431123316287994
step: 3300, Loss: 0.11421222239732742
step: 3400, Loss: 0.11383190006017685
step: 3500, Loss: 0.11470663547515869
step: 3600, Loss: 0.11471440643072128
step: 3700, Loss: 0.11471850425004959
step: 3800, Loss: 0.1146838366985321
step: 3900, Loss: 0.11598469316959381
step: 4000, Loss: 0.11277736723423004
step: 4100, Loss: 0.11424724012613297
step: 4200, Loss: 0.11445815116167068
step: 4300, Loss: 0.11361706256866455
step: 4400, Loss: 0.11407101154327393
step: 4500, Loss: 0.11502189189195633
step: 4600, Loss: 0.11496297270059586
step: 4700, Loss: 0.11407864838838577
step: 4800, Loss: 0.23777681589126587
step: 4900, Loss: 0.1570163071155548
step: 5000, Loss: 0.12571649253368378
step: 5100, Loss: 0.12707172334194183
step: 5200, Loss: 0.11870063841342926
step: 5300, Loss: 0.13183197379112244
step: 5400, Loss: 0.12850943207740784
step: 5500, Loss: 0.12004946917295456
step: 5600, Loss: 0.11952325701713562
step: 5700, Loss: 0.12116596847772598
step: 6000, Loss: 0.11317197978496552
step: 6100, Loss: 0.11451657116413116
step: 6200, Loss: 0.11291427910327911
step: 6300, Loss: 0.11441776901483536
step: 6400, Loss: 0.11458805948495865
step: 6500, Loss: 0.11472451686859131
step: 6600, Loss: 0.3673543930053711
step: 6700, Loss: 0.11491507291793823
step: 6800, Loss: 0.1146174818277359
step: 6900, Loss: 0.11280835419893265
step: 7000, Loss: 0.11438916623592377
step: 7100, Loss: 0.11484454572200775
step: 7200, Loss: 0.11461872607469559
step: 7300, Loss: 0.16875198483467102
step: 7400, Loss: 0.9688756465911865
step: 7500, Loss: 0.14934644103050232
step: 7600, Loss: 0.14000216126441956
step: 7700, Loss: 0.12766551971435547
step: 7800, Loss: 0.1314285397529602
step: 7900, Loss: 0.1225564256310463
step: 8000, Loss: 0.12155799567699432
step: 8100, Loss: 0.11817093938589096
step: 8200, Loss: 0.11953629553318024
step: 8300, Loss: 0.11774186789989471
step: 8400, Loss: 0.1193971261382103
step: 8500, Loss: 0.1203155443072319
step: 8600, Loss: 0.11617834120988846
step: 8700, Loss: 0.11845822632312775
step: 8800, Loss: 0.11643711477518082
step: 8900, Loss: 0.4145440459251404
step: 9000, Loss: 0.11741673946380615
step: 9100, Loss: 0.1145581379532814
step: 9200, Loss: 0.11602647602558136
step: 9300, Loss: 0.11531983315944672
step: 9400, Loss: 0.11805052310228348
step: 9500, Loss: 0.11964072287082672
step: 9600, Loss: 0.11628172546625137
step: 9700, Loss: 0.11523467302322388
step: 9800, Loss: 0.11401122063398361
step: 9900, Loss: 0.11638793349266052
training successfully ended.
validating...
validate data length:91
acc: 0.9886363636363636
precision: 0.9791666666666666
recall: 1.0
F_score: 0.9894736842105264
******fold 10******

Training... train_data length:821
step: 0, Loss: 0.11520294845104218
step: 100, Loss: 0.11569103598594666
step: 200, Loss: 0.1153319776058197
step: 300, Loss: 0.11442430317401886
step: 400, Loss: 0.11624880880117416
step: 500, Loss: 0.11424486339092255
step: 600, Loss: 0.11475346982479095
step: 700, Loss: 0.11397471278905869
step: 800, Loss: 0.11468586325645447
step: 900, Loss: 0.11463003605604172
step: 1000, Loss: 0.1137160062789917
step: 1100, Loss: 0.11412931233644485
step: 1200, Loss: 0.11332789808511734
step: 1300, Loss: 0.11543124169111252
step: 1400, Loss: 0.11404351890087128
step: 1500, Loss: 0.11369854211807251
step: 1600, Loss: 0.11390578746795654
step: 1700, Loss: 0.11420852690935135
step: 1800, Loss: 0.11459138244390488
step: 1900, Loss: 0.11558081209659576
step: 2000, Loss: 0.3757399618625641
step: 2100, Loss: 0.11379170417785645
step: 2200, Loss: 0.11519356071949005
step: 2300, Loss: 0.11374135315418243
step: 2400, Loss: 0.1132969856262207
step: 2500, Loss: 0.11559131741523743
step: 2600, Loss: 0.11506349593400955
step: 2700, Loss: 0.1137181967496872
step: 2800, Loss: 0.11516738682985306
step: 2900, Loss: 2.952955722808838
step: 3000, Loss: 0.16305862367153168
step: 3100, Loss: 0.13099300861358643
step: 3200, Loss: 0.13719312846660614
step: 3300, Loss: 0.12827715277671814
step: 3400, Loss: 0.11929794400930405
step: 3500, Loss: 0.1277567595243454
step: 3600, Loss: 0.12016377598047256
step: 3700, Loss: 0.11559199541807175
step: 3800, Loss: 0.11707240343093872
step: 3900, Loss: 0.11709190160036087
step: 4000, Loss: 0.12081437557935715
step: 4100, Loss: 0.11610067635774612
step: 4200, Loss: 0.119838647544384
step: 4300, Loss: 0.39996328949928284
step: 4400, Loss: 0.12021107971668243
step: 4500, Loss: 0.1160474419593811
step: 4600, Loss: 0.1147654801607132
step: 4700, Loss: 0.11640816926956177
step: 4800, Loss: 0.11681623756885529
step: 4900, Loss: 0.11557809263467789
step: 5000, Loss: 0.11567451804876328
step: 5100, Loss: 0.113894023001194
step: 5200, Loss: 0.11520655453205109
step: 5300, Loss: 0.11667018383741379
step: 5400, Loss: 0.11387482285499573
step: 5500, Loss: 0.11534124612808228
step: 5600, Loss: 0.1163397878408432
step: 5700, Loss: 0.11630409955978394
step: 5800, Loss: 0.1149723157286644
step: 5900, Loss: 0.11480424553155899
step: 6000, Loss: 0.11487747728824615
step: 6100, Loss: 0.11374334245920181
step: 6200, Loss: 0.11379343271255493
step: 6300, Loss: 0.11533021181821823
step: 6400, Loss: 0.1140436977148056
step: 6500, Loss: 0.11469507962465286
step: 6600, Loss: 0.3678114414215088
step: 6700, Loss: 0.1199585422873497
step: 6800, Loss: 0.1133168637752533
step: 6900, Loss: 0.1142483800649643
step: 7000, Loss: 0.11381885409355164
step: 7100, Loss: 0.11402958631515503
step: 7200, Loss: 0.11499528586864471
step: 7300, Loss: 0.1149919405579567
step: 7400, Loss: 0.11361317336559296
step: 7500, Loss: 0.11465953290462494
step: 7600, Loss: 0.11464209854602814
step: 7700, Loss: 0.1135265901684761
step: 7800, Loss: 0.1137138307094574
step: 7900, Loss: 0.11414213478565216
step: 8000, Loss: 0.11413446068763733
step: 8100, Loss: 0.11322921514511108
step: 8200, Loss: 0.11445704102516174
step: 8300, Loss: 0.11368691921234131
step: 8400, Loss: 0.11518050730228424
step: 8500, Loss: 0.11302675306797028
step: 8600, Loss: 0.1146315410733223
step: 8700, Loss: 0.11330892890691757
step: 8800, Loss: 0.11364908516407013
step: 8900, Loss: 0.3565196394920349
step: 9000, Loss: 0.11514311283826828
step: 9100, Loss: 0.11268050223588943
step: 9200, Loss: 0.11444328725337982
step: 9300, Loss: 0.11506331712007523
step: 9400, Loss: 0.11436186730861664
step: 9500, Loss: 0.11293364316225052
step: 9600, Loss: 0.11390478163957596
step: 9700, Loss: 0.11395420134067535
step: 9800, Loss: 0.11331366747617722
step: 9900, Loss: 0.11387613415718079
training successfully ended.
validating...
validate data length:91
acc: 0.9772727272727273
precision: 0.9743589743589743
recall: 0.9743589743589743
F_score: 0.9743589743589743
subject 2 Avgacc: 0.9534090909090909 Avgfscore: 0.9536001345015336 
 Max acc:0.9886363636363636, Max f score:0.99009900990099
******** mix subject_3 ********

[456, 304]
******fold 1******

Training... train_data length:820
step: 0, Loss: 35.884613037109375
step: 100, Loss: 7.237292289733887
step: 200, Loss: 1.2297992706298828
step: 300, Loss: 0.2050502598285675
step: 400, Loss: 0.4833875298500061
step: 500, Loss: 0.1633743941783905
step: 600, Loss: 0.14979968965053558
step: 700, Loss: 0.14484886825084686
step: 800, Loss: 0.14909635484218597
step: 900, Loss: 0.1456831693649292
step: 1000, Loss: 0.1318368911743164
step: 1100, Loss: 0.12594202160835266
step: 1200, Loss: 0.12631310522556305
step: 1300, Loss: 0.12456433475017548
step: 1400, Loss: 0.12285200506448746
step: 1500, Loss: 0.12955670058727264
step: 1600, Loss: 0.13131609559059143
step: 1700, Loss: 0.12284038960933685
step: 1800, Loss: 0.11812986433506012
step: 1900, Loss: 0.12279253453016281
step: 2000, Loss: 0.3868105411529541
step: 2100, Loss: 0.12113611400127411
step: 2200, Loss: 0.11640970408916473
step: 2300, Loss: 0.11586873978376389
step: 2400, Loss: 0.12224945425987244
step: 2500, Loss: 0.11722718179225922
step: 2600, Loss: 0.11577297002077103
step: 2700, Loss: 0.11850937455892563
step: 2800, Loss: 0.11595527082681656
step: 2900, Loss: 0.11595065891742706
step: 3000, Loss: 0.11876657605171204
step: 3100, Loss: 0.11965423822402954
step: 3200, Loss: 0.11815905570983887
step: 3300, Loss: 0.1181611716747284
step: 3400, Loss: 0.11951357871294022
step: 3500, Loss: 0.1172507107257843
step: 3600, Loss: 0.11919698864221573
step: 3700, Loss: 0.1190008670091629
step: 3800, Loss: 0.114394411444664
step: 3900, Loss: 0.11866677552461624
step: 4000, Loss: 0.11655063182115555
step: 4100, Loss: 0.11512935161590576
step: 4200, Loss: 0.11559834331274033
step: 4300, Loss: 0.41697055101394653
step: 4400, Loss: 0.11952000856399536
step: 4500, Loss: 0.11507941782474518
step: 4600, Loss: 0.11705054342746735
step: 4700, Loss: 0.1154431700706482
step: 4800, Loss: 0.11923285573720932
step: 4900, Loss: 0.11486794054508209
step: 5000, Loss: 0.1196238249540329
step: 5100, Loss: 2.5945115089416504
step: 5200, Loss: 1.4579689502716064
step: 5300, Loss: 0.15960201621055603
step: 5400, Loss: 0.12812450528144836
step: 5500, Loss: 0.137534499168396
step: 5600, Loss: 0.13449016213417053
step: 5700, Loss: 0.13206659257411957
step: 5800, Loss: 0.12857884168624878
step: 5900, Loss: 0.1352713704109192
step: 6000, Loss: 0.12495620548725128
step: 5800, Loss: 0.11725899577140808
step: 5900, Loss: 0.11816651374101639
step: 6000, Loss: 0.11736699938774109
step: 6100, Loss: 0.12396383285522461
step: 6200, Loss: 0.11699596047401428
step: 6300, Loss: 0.12090034782886505
step: 6400, Loss: 0.11593934893608093
step: 6500, Loss: 0.11669585853815079
step: 6600, Loss: 0.11539348214864731
step: 6700, Loss: 0.11562532186508179
step: 6800, Loss: 0.11563488841056824
step: 6900, Loss: 0.11517050862312317
step: 7000, Loss: 0.11665184050798416
step: 7100, Loss: 0.11628720909357071
step: 7200, Loss: 0.1177944615483284
step: 7300, Loss: 0.11732564866542816
step: 7400, Loss: 0.11680394411087036
step: 7500, Loss: 0.1163041740655899
step: 7600, Loss: 0.11592291295528412
step: 7700, Loss: 0.11699352413415909
step: 7800, Loss: 0.11620493978261948
step: 7900, Loss: 0.11585922539234161
step: 8000, Loss: 0.11736756563186646
step: 8100, Loss: 0.11557643860578537
step: 8200, Loss: 0.11465577781200409
step: 8300, Loss: 0.1152830421924591
step: 8400, Loss: 0.11380103975534439
step: 8500, Loss: 0.11448317021131516
step: 8600, Loss: 0.11478261649608612
step: 8700, Loss: 0.11571769416332245
step: 8800, Loss: 0.11427204310894012
step: 8900, Loss: 0.11589724570512772
step: 9000, Loss: 0.11423594504594803
step: 9100, Loss: 0.11352524161338806
step: 9200, Loss: 0.11433445662260056
step: 9300, Loss: 0.11707544326782227
step: 9400, Loss: 0.11398552358150482
step: 9500, Loss: 0.11477863043546677
step: 9600, Loss: 0.11333177983760834
step: 9700, Loss: 0.11449259519577026
step: 9800, Loss: 0.11504196375608444
step: 9900, Loss: 0.11475162208080292
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.8
recall: 0.7058823529411765
F_score: 0.7500000000000001
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.4672892391681671
step: 100, Loss: 0.11680173128843307
step: 200, Loss: 0.11774463206529617
step: 300, Loss: 0.11377552896738052
step: 400, Loss: 0.11576240509748459
step: 500, Loss: 0.11575184762477875
step: 600, Loss: 0.11436475813388824
step: 700, Loss: 0.11663870513439178
step: 800, Loss: 0.11438251286745071
step: 900, Loss: 0.11491771042346954
step: 1000, Loss: 0.11521972715854645
step: 1100, Loss: 0.11351824551820755
step: 1200, Loss: 0.11430039256811142
step: 1300, Loss: 0.11799321323633194
step: 1400, Loss: 0.11469517648220062
step: 1500, Loss: 0.1136709600687027
step: 1600, Loss: 0.1138542890548706
step: 1700, Loss: 0.11701373755931854
step: 1800, Loss: 0.11518841981887817
step: 1900, Loss: 0.1173461377620697
step: 2000, Loss: 0.11401155591011047
step: 2100, Loss: 0.1176857128739357
step: 2200, Loss: 0.11293558776378632
step: 2300, Loss: 0.11556220799684525
step: 2400, Loss: 0.1148926168680191
step: 2500, Loss: 0.11563626676797867
step: 2600, Loss: 0.11459256708621979
step: 2700, Loss: 0.11588671803474426
step: 2800, Loss: 0.1143067479133606
step: 2900, Loss: 0.11658693850040436
step: 3000, Loss: 0.11428150534629822
step: 3100, Loss: 0.11687615513801575
step: 3200, Loss: 0.11358406394720078
step: 3300, Loss: 0.1160169392824173
step: 3400, Loss: 0.11448437720537186
step: 3500, Loss: 0.11543841660022736
step: 3600, Loss: 0.1140449121594429
step: 3700, Loss: 0.11383438110351562
step: 3800, Loss: 0.11456857621669769
step: 3900, Loss: 0.11492178589105606
step: 4000, Loss: 0.11331439763307571
step: 4100, Loss: 0.11763526499271393
step: 4200, Loss: 0.11473595350980759
step: 4300, Loss: 0.11446913331747055
step: 4400, Loss: 0.11600499600172043
step: 4500, Loss: 0.11617064476013184
step: 4600, Loss: 0.11672119796276093
step: 4700, Loss: 0.11568893492221832
step: 4800, Loss: 0.11572878062725067
step: 4900, Loss: 1.9563034772872925
step: 5000, Loss: 0.15734872221946716
step: 5100, Loss: 0.15320435166358948
step: 5200, Loss: 0.1355554312467575
step: 5300, Loss: 0.12581831216812134
step: 5400, Loss: 0.12802936136722565
step: 5500, Loss: 0.12312096357345581
step: 5600, Loss: 0.12114004790782928
step: 5700, Loss: 0.1230664998292923
step: 5800, Loss: 0.11997824162244797
step: 5900, Loss: 0.11835575848817825
step: 6000, Loss: 0.120735764503479
step: 6100, Loss: 0.1261313259601593
step: 6200, Loss: 0.1161385029554367
step: 6300, Loss: 0.12168429046869278
step: 6400, Loss: 0.11675236374139786
step: 6500, Loss: 0.11486983299255371
step: 6600, Loss: 0.12056297808885574
step: 6700, Loss: 0.11607825756072998
step: 6800, Loss: 0.11475242674350739
step: 6900, Loss: 0.11478803306818008
step: 7000, Loss: 0.11638279259204865
step: 7100, Loss: 0.11670134961605072
step: 7200, Loss: 0.11460043489933014
step: 7300, Loss: 0.1156344786286354
step: 7400, Loss: 0.1169222742319107
step: 7500, Loss: 0.11440719664096832
step: 7600, Loss: 0.11414822190999985
step: 7700, Loss: 0.11668426543474197
step: 7800, Loss: 0.11537721008062363
step: 7900, Loss: 0.11817013472318649
step: 8000, Loss: 0.14514485001564026
step: 8100, Loss: 0.11565105617046356
step: 8200, Loss: 0.11459223181009293
step: 8300, Loss: 0.11668599396944046
step: 8400, Loss: 0.11654137820005417
step: 8500, Loss: 0.11496387422084808
step: 8600, Loss: 0.11663233488798141
step: 8700, Loss: 0.12270401418209076
step: 8800, Loss: 0.11456368863582611
step: 8900, Loss: 0.11467614024877548
step: 9000, Loss: 0.1133764311671257
step: 9100, Loss: 0.11436444520950317
step: 9200, Loss: 0.11384391784667969
step: 9300, Loss: 0.1217980682849884
step: 9400, Loss: 0.11490613222122192
step: 9500, Loss: 0.11605197191238403
step: 9600, Loss: 0.1142178475856781
step: 9700, Loss: 0.11460298299789429
step: 9800, Loss: 0.113082155585289
step: 9900, Loss: 0.11485761404037476
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7142857142857143
recall: 0.8333333333333334
F_score: 0.7692307692307692
subject 2 Avgacc: 0.8177083333333334 Avgfscore: 0.8295708856865044 
 Max acc:0.9666666666666667, Max f score:0.9655172413793104
******** mix subject_3 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 44.25096130371094
step: 100, Loss: 0.3664636015892029
step: 200, Loss: 0.14990243315696716
step: 300, Loss: 0.13831974565982819
step: 400, Loss: 0.12528836727142334
step: 500, Loss: 0.13044054806232452
step: 600, Loss: 0.12622559070587158
step: 700, Loss: 0.12018531560897827
step: 800, Loss: 0.1256086826324463
step: 900, Loss: 0.11797325313091278
step: 1000, Loss: 0.12183387577533722
step: 1100, Loss: 0.11751891672611237
step: 1200, Loss: 0.1207992285490036
step: 1300, Loss: 0.11761096119880676
step: 1400, Loss: 0.11840217560529709
step: 1500, Loss: 0.11621329188346863
step: 1600, Loss: 0.1284673660993576
step: 1700, Loss: 0.12180693447589874
step: 1800, Loss: 0.12233838438987732
step: 1900, Loss: 0.11594468355178833
step: 2000, Loss: 0.11988804489374161
step: 2100, Loss: 0.11735836416482925
step: 2200, Loss: 0.11609012633562088
step: 2300, Loss: 0.11568373441696167
step: 2400, Loss: 0.117561936378479
step: 2500, Loss: 0.115354984998703
step: 2600, Loss: 0.1188814640045166
step: 2700, Loss: 0.11635631322860718
step: 2800, Loss: 0.12465228140354156
step: 2900, Loss: 0.11603304743766785
step: 3000, Loss: 0.11821408569812775
step: 3100, Loss: 0.11613152921199799
step: 3200, Loss: 0.11803658306598663
step: 3300, Loss: 0.11501665413379669
step: 3400, Loss: 0.11494722217321396
step: 3500, Loss: 0.11559939384460449
step: 3600, Loss: 0.11695617437362671
step: 3700, Loss: 0.11663901805877686
step: 3800, Loss: 0.11444558203220367
step: 3900, Loss: 0.11630050092935562
step: 4000, Loss: 0.11775670200586319
step: 4100, Loss: 0.12675553560256958
step: 4200, Loss: 1.6574736833572388
step: 4300, Loss: 0.48014524579048157
step: 4400, Loss: 0.1487545371055603
step: 4500, Loss: 0.1392369270324707
step: 4600, Loss: 0.13993115723133087
step: 4700, Loss: 0.12436448037624359
step: 4800, Loss: 0.12970298528671265
step: 4900, Loss: 0.12229472398757935
step: 5000, Loss: 0.1366334706544876
step: 5100, Loss: 0.12187734991312027
step: 5200, Loss: 0.12616874277591705
step: 5300, Loss: 0.11893574893474579
step: 5400, Loss: 0.12432561814785004
step: 5500, Loss: 0.11674248427152634
step: 5600, Loss: 0.12162748724222183
step: 5700, Loss: 0.11577830463647842
step: 5800, Loss: 0.12271174788475037
step: 6100, Loss: 0.12651318311691284
step: 6200, Loss: 0.12205468118190765
step: 6300, Loss: 0.1172768846154213
step: 6400, Loss: 0.12046700716018677
step: 6500, Loss: 0.12065806984901428
step: 6600, Loss: 0.4066275358200073
step: 6700, Loss: 0.11863374710083008
step: 6800, Loss: 0.11646874248981476
step: 6900, Loss: 0.115871362388134
step: 7000, Loss: 0.11619995534420013
step: 7100, Loss: 0.11641058325767517
step: 7200, Loss: 0.11805516481399536
step: 7300, Loss: 0.1189427524805069
step: 7400, Loss: 0.11554545164108276
step: 7500, Loss: 0.11631400883197784
step: 7600, Loss: 0.13533228635787964
step: 7700, Loss: 0.11584676802158356
step: 7800, Loss: 0.11743874847888947
step: 7900, Loss: 0.1156030148267746
step: 8000, Loss: 0.1155969500541687
step: 8100, Loss: 0.11488604545593262
step: 8200, Loss: 0.11832157522439957
step: 8300, Loss: 0.11575746536254883
step: 8400, Loss: 0.11534352600574493
step: 8500, Loss: 0.11738192290067673
step: 8600, Loss: 0.11454172432422638
step: 8700, Loss: 0.11668576300144196
step: 8800, Loss: 0.11487643420696259
step: 8900, Loss: 0.40163394808769226
step: 9000, Loss: 0.1146492213010788
step: 9100, Loss: 0.11458450555801392
step: 9200, Loss: 0.11522847414016724
step: 9300, Loss: 0.116142138838768
step: 9400, Loss: 0.11591801792383194
step: 9500, Loss: 0.11459213495254517
step: 9600, Loss: 0.11552800238132477
step: 9700, Loss: 0.11327344924211502
step: 9800, Loss: 0.11440238356590271
step: 9900, Loss: 0.11467763036489487
training successfully ended.
validating...
validate data length:92
acc: 0.75
precision: 0.6904761904761905
recall: 0.7631578947368421
F_score: 0.725
******fold 2******

Training... train_data length:820
step: 0, Loss: 0.11544205248355865
step: 100, Loss: 0.13167206943035126
step: 200, Loss: 0.1220773458480835
step: 300, Loss: 0.12234221398830414
step: 400, Loss: 0.11913809925317764
step: 500, Loss: 0.11506032943725586
step: 600, Loss: 0.1180916354060173
step: 700, Loss: 0.12141980975866318
step: 800, Loss: 0.12103046476840973
step: 900, Loss: 0.12246754765510559
step: 1000, Loss: 0.11555380374193192
step: 1100, Loss: 0.11431150883436203
step: 1200, Loss: 0.11447305232286453
step: 1300, Loss: 0.11531929671764374
step: 1400, Loss: 0.11478815972805023
step: 1500, Loss: 0.11376617103815079
step: 1600, Loss: 0.11459662020206451
step: 1700, Loss: 0.1141892522573471
step: 1800, Loss: 0.11427638679742813
step: 1900, Loss: 0.1152043342590332
step: 2000, Loss: 0.39042365550994873
step: 2100, Loss: 0.11422258615493774
step: 2200, Loss: 0.11363115906715393
step: 2300, Loss: 0.11438606679439545
step: 2400, Loss: 0.11622054129838943
step: 2500, Loss: 0.11394139379262924
step: 2600, Loss: 0.11540932953357697
step: 2700, Loss: 0.11435209214687347
step: 2800, Loss: 0.11510030180215836
step: 2900, Loss: 0.11437524855136871
step: 3000, Loss: 0.11651858687400818
step: 3100, Loss: 0.11387959867715836
step: 3200, Loss: 0.11604252457618713
step: 3300, Loss: 0.11455253511667252
step: 3400, Loss: 0.11867257952690125
step: 3500, Loss: 0.12429363280534744
step: 3600, Loss: 0.11558966338634491
step: 3700, Loss: 0.11717391759157181
step: 3800, Loss: 0.1155952513217926
step: 3900, Loss: 0.11463582515716553
step: 4000, Loss: 0.11737266182899475
step: 4100, Loss: 0.11769996583461761
step: 4200, Loss: 0.11697983741760254
step: 4300, Loss: 0.4228215515613556
step: 4400, Loss: 0.11502401530742645
step: 4500, Loss: 0.11668755114078522
step: 4600, Loss: 0.11347219347953796
step: 4700, Loss: 0.11445259302854538
step: 4800, Loss: 0.11508561670780182
step: 4900, Loss: 0.8571745157241821
step: 5000, Loss: 0.39261090755462646
step: 5100, Loss: 0.24599145352840424
step: 5200, Loss: 0.12532903254032135
step: 5300, Loss: 0.13659870624542236
step: 5400, Loss: 0.13048678636550903
step: 5500, Loss: 0.12741175293922424
step: 5600, Loss: 0.12978386878967285
step: 5700, Loss: 0.12833744287490845
step: 5800, Loss: 0.13770855963230133
step: 5900, Loss: 0.12487147748470306
step: 6000, Loss: 0.11712995171546936
step: 6100, Loss: 0.12236309051513672
step: 6200, Loss: 0.1218605786561966
step: 6300, Loss: 0.11840122938156128
step: 6400, Loss: 0.11949685961008072
step: 6500, Loss: 0.11920572817325592
step: 6600, Loss: 0.4096296727657318
step: 6700, Loss: 0.11883468925952911
step: 6800, Loss: 0.11526279151439667
step: 6900, Loss: 0.11815347522497177
step: 7000, Loss: 0.11761310696601868
step: 7100, Loss: 0.11535391956567764
step: 7200, Loss: 0.1154000386595726
step: 7300, Loss: 0.1189839169383049
step: 7400, Loss: 0.11410010606050491
step: 7500, Loss: 0.11656850576400757
step: 7600, Loss: 0.12064750492572784
step: 7700, Loss: 0.11352688074111938
step: 7800, Loss: 0.11534018814563751
step: 7900, Loss: 0.11557858437299728
step: 8000, Loss: 0.11474879831075668
step: 8100, Loss: 0.11603428423404694
step: 8200, Loss: 0.113975889980793
step: 8300, Loss: 0.11444169282913208
step: 8400, Loss: 0.11619675159454346
step: 8500, Loss: 0.11544293165206909
step: 8600, Loss: 0.11406996846199036
step: 8700, Loss: 0.11593519896268845
step: 8800, Loss: 0.11510293185710907
step: 8900, Loss: 0.3936735689640045
step: 9000, Loss: 0.11411446332931519
step: 9100, Loss: 0.11553176492452621
step: 9200, Loss: 0.11397707462310791
step: 9300, Loss: 0.11414576321840286
step: 9400, Loss: 0.11609715223312378
step: 9500, Loss: 0.1140555739402771
step: 9600, Loss: 0.1142614558339119
step: 9700, Loss: 0.11552511155605316
step: 9800, Loss: 0.11478842794895172
step: 9900, Loss: 0.11464069783687592
training successfully ended.
validating...
validate data length:92
acc: 0.9431818181818182
precision: 0.9272727272727272
recall: 0.9807692307692307
F_score: 0.9532710280373831
******fold 3******

Training... train_data length:821
step: 0, Loss: 0.16215622425079346
step: 100, Loss: 0.1214500367641449
step: 200, Loss: 0.11782005429267883
step: 300, Loss: 0.11613492667675018
step: 400, Loss: 0.11753764003515244
step: 500, Loss: 0.11581394821405411
step: 600, Loss: 0.11428261548280716
step: 700, Loss: 0.11527985334396362
step: 800, Loss: 0.11487612128257751
step: 900, Loss: 0.11422961950302124
step: 1000, Loss: 0.11465314030647278
step: 1100, Loss: 0.11469331383705139
step: 1200, Loss: 0.11516883224248886
step: 1300, Loss: 0.11446906626224518
step: 1400, Loss: 0.11368580907583237
step: 1500, Loss: 0.11444594711065292
step: 1600, Loss: 0.11364036798477173
step: 1700, Loss: 0.1130562275648117
step: 1800, Loss: 0.11393970251083374
step: 1900, Loss: 0.11494914442300797
step: 2000, Loss: 0.3650277256965637
step: 2100, Loss: 0.11549722403287888
step: 2200, Loss: 0.11864903569221497
step: 2300, Loss: 0.11545475572347641
step: 2400, Loss: 0.11519816517829895
step: 2500, Loss: 0.11413910239934921
step: 2600, Loss: 0.11464270949363708
step: 2700, Loss: 0.11455149203538895
step: 2800, Loss: 0.1143040657043457
step: 2900, Loss: 0.11496542394161224
step: 3000, Loss: 0.11520189046859741
step: 3100, Loss: 0.11367135494947433
step: 3200, Loss: 0.11344302445650101
step: 3300, Loss: 0.116187185049057
step: 3400, Loss: 0.11411816626787186
step: 3500, Loss: 0.11569260060787201
step: 3600, Loss: 0.11594677716493607
step: 3700, Loss: 0.11789298802614212
step: 3800, Loss: 0.11496957391500473
step: 3900, Loss: 0.11467280238866806
step: 4000, Loss: 0.11419679969549179
step: 4100, Loss: 1.6466525793075562
step: 4200, Loss: 0.18916860222816467
step: 4300, Loss: 0.46424347162246704
step: 4400, Loss: 0.13847075402736664
step: 4500, Loss: 0.12723377346992493
step: 4600, Loss: 0.12544944882392883
step: 4700, Loss: 0.1286613494157791
step: 4800, Loss: 0.12392698973417282
step: 4900, Loss: 0.12490332126617432
step: 5000, Loss: 0.12301027029752731
step: 5100, Loss: 0.1205819696187973
step: 5200, Loss: 0.118856281042099
step: 5300, Loss: 0.1247941255569458
step: 5400, Loss: 0.12470243871212006
step: 5500, Loss: 0.11910972744226456
step: 5600, Loss: 0.119510218501091
step: 5700, Loss: 0.1196834072470665
step: 5800, Loss: 0.11552704125642776
step: 5900, Loss: 0.1191047728061676
step: 6000, Loss: 0.11538411676883698
step: 6100, Loss: 0.1184411570429802
step: 6200, Loss: 0.11688262224197388
step: 6300, Loss: 0.11518707126379013
step: 6400, Loss: 0.11740784347057343
step: 6500, Loss: 0.11461126804351807
step: 6600, Loss: 0.3630892038345337step: 5900, Loss: 0.11613321304321289
step: 6000, Loss: 0.12299078702926636
step: 6100, Loss: 0.11599646508693695
step: 6200, Loss: 0.13582560420036316
step: 6300, Loss: 0.11739865690469742
step: 6400, Loss: 0.11839175224304199
step: 6500, Loss: 0.11943980306386948
step: 6600, Loss: 0.11894186586141586
step: 6700, Loss: 0.11379051208496094
step: 6800, Loss: 0.11634813994169235
step: 6900, Loss: 0.11477015912532806
step: 7000, Loss: 0.12331156432628632
step: 7100, Loss: 0.1154315397143364
step: 7200, Loss: 0.11806926131248474
step: 7300, Loss: 0.12271541357040405
step: 7400, Loss: 0.12011756002902985
step: 7500, Loss: 0.11649920046329498
step: 7600, Loss: 0.11794112622737885
step: 7700, Loss: 0.11425754427909851
step: 7800, Loss: 0.11761948466300964
step: 7900, Loss: 0.11561712622642517
step: 8000, Loss: 0.11973047256469727
step: 8100, Loss: 0.11413471400737762
step: 8200, Loss: 0.11820213496685028
step: 8300, Loss: 0.11621661484241486
step: 8400, Loss: 0.11368661373853683
step: 8500, Loss: 0.11732380092144012
step: 8600, Loss: 0.11605572700500488
step: 8700, Loss: 0.1168956533074379
step: 8800, Loss: 0.1165723204612732
step: 8900, Loss: 0.115322545170784
step: 9000, Loss: 0.11557553708553314
step: 9100, Loss: 0.11851073056459427
step: 9200, Loss: 0.11611384898424149
step: 9300, Loss: 0.1142888143658638
step: 9400, Loss: 0.1143978089094162
step: 9500, Loss: 0.1173408105969429
step: 9600, Loss: 0.11402757465839386
step: 9700, Loss: 0.11589577794075012
step: 9800, Loss: 0.11917804181575775
step: 9900, Loss: 0.11578254401683807
training successfully ended.
validating...
validate data length:32
acc: 0.40625
precision: 0.3157894736842105
recall: 0.5
F_score: 0.3870967741935484
******fold 2******

Training... train_data length:280
step: 0, Loss: 1.9663995504379272
step: 100, Loss: 0.13069874048233032
step: 200, Loss: 0.1205817312002182
step: 300, Loss: 0.11940887570381165
step: 400, Loss: 0.11958472430706024
step: 500, Loss: 0.11743579059839249
step: 600, Loss: 0.11863837391138077
step: 700, Loss: 0.11630872637033463
step: 800, Loss: 0.11952526867389679
step: 900, Loss: 0.11416830867528915
step: 1000, Loss: 0.12023916095495224
step: 1100, Loss: 0.11498528718948364
step: 1200, Loss: 0.11570965498685837
step: 1300, Loss: 0.11430608481168747
step: 1400, Loss: 0.11606951802968979
step: 1500, Loss: 0.11426550149917603
step: 1600, Loss: 0.11606021225452423
step: 1700, Loss: 0.1142985001206398
step: 1800, Loss: 0.1167062520980835
step: 1900, Loss: 0.11535991728305817
step: 2000, Loss: 0.12174810469150543
step: 2100, Loss: 0.11497416347265244
step: 2200, Loss: 0.11405637115240097
step: 2300, Loss: 0.11410339176654816
step: 2400, Loss: 0.11630109697580338
step: 2500, Loss: 0.11491449177265167
step: 2600, Loss: 0.11457644402980804
step: 2700, Loss: 0.11437033861875534
step: 2800, Loss: 0.11448144912719727
step: 2900, Loss: 0.11506572365760803
step: 3000, Loss: 0.11578608304262161
step: 3100, Loss: 0.1155005469918251
step: 3200, Loss: 0.11707833409309387
step: 3300, Loss: 0.1149759292602539
step: 3400, Loss: 0.11660075187683105
step: 3500, Loss: 0.11432406306266785
step: 3600, Loss: 0.1147351861000061
step: 3700, Loss: 0.1144934669137001
step: 3800, Loss: 0.11535920202732086
step: 3900, Loss: 0.11532208323478699
step: 4000, Loss: 0.11611385643482208
step: 4100, Loss: 0.11440664529800415
step: 4200, Loss: 0.11680702120065689
step: 4300, Loss: 0.11590851843357086
step: 4400, Loss: 0.11480303108692169
step: 4500, Loss: 0.39163076877593994
step: 4600, Loss: 0.13642212748527527
step: 4700, Loss: 0.13193659484386444
step: 4800, Loss: 0.12324123084545135
step: 4900, Loss: 0.1293184608221054
step: 5000, Loss: 0.12079109996557236
step: 5100, Loss: 0.1252816617488861
step: 5200, Loss: 0.12376752495765686
step: 5300, Loss: 0.1263238936662674
step: 5400, Loss: 0.11769230663776398
step: 5500, Loss: 0.12308803200721741
step: 5600, Loss: 0.12009008228778839
step: 5700, Loss: 0.117777019739151
step: 5800, Loss: 0.11875779926776886
step: 5900, Loss: 0.12185044586658478
step: 6000, Loss: 0.11942877620458603
step: 6100, Loss: 0.11629879474639893
step: 6200, Loss: 0.11607693135738373
step: 6300, Loss: 0.11601150035858154
step: 6400, Loss: 0.11651688814163208
step: 6500, Loss: 0.12498807907104492
step: 6600, Loss: 0.11573438346385956
step: 6700, Loss: 0.11760063469409943
step: 6800, Loss: 0.11554071307182312
step: 6900, Loss: 0.1183210015296936
step: 7000, Loss: 0.1145009696483612
step: 7100, Loss: 0.11559571325778961
step: 7200, Loss: 0.1136951744556427
step: 7300, Loss: 0.11508732289075851
step: 7400, Loss: 0.11679593473672867
step: 7500, Loss: 0.11488477140665054
step: 7600, Loss: 0.11616374552249908
step: 7700, Loss: 0.11549162864685059
step: 7800, Loss: 0.11624349653720856
step: 7900, Loss: 0.11628299951553345
step: 8000, Loss: 0.11332674324512482
step: 8100, Loss: 0.11611166596412659
step: 8200, Loss: 0.11548183113336563
step: 8300, Loss: 0.11527776718139648
step: 8400, Loss: 0.11857273429632187
step: 8500, Loss: 0.11503511667251587
step: 8600, Loss: 0.11351996660232544
step: 8700, Loss: 0.11549702286720276
step: 8800, Loss: 0.11354659497737885
step: 8900, Loss: 0.11525290459394455
step: 9000, Loss: 0.11547361314296722
step: 9100, Loss: 0.11428096890449524
step: 9200, Loss: 0.11736597120761871
step: 9300, Loss: 0.11517892777919769
step: 9400, Loss: 0.11560942232608795
step: 9500, Loss: 0.11811086535453796
step: 9600, Loss: 0.11748416721820831
step: 9700, Loss: 0.1186133325099945
step: 9800, Loss: 0.11737010627985
step: 9900, Loss: 0.1184440404176712
training successfully ended.
validating...
validate data length:32
acc: 0.78125
precision: 0.7647058823529411
recall: 0.8125
F_score: 0.787878787878788
******fold 3******

Training... train_data length:281
step: 0, Loss: 6.359014987945557
step: 100, Loss: 0.12196376919746399
step: 200, Loss: 0.12031422555446625
step: 300, Loss: 0.1179492324590683
step: 400, Loss: 0.11613793671131134
step: 500, Loss: 0.1180567592382431
step: 600, Loss: 0.11589495837688446
step: 700, Loss: 0.12354254722595215
step: 800, Loss: 0.1193794459104538
step: 900, Loss: 0.11486739665269852
step: 1000, Loss: 0.11612456291913986
step: 1100, Loss: 0.11574828624725342
step: 1200, Loss: 0.11435509473085403
step: 1300, Loss: 0.1151287704706192
step: 1400, Loss: 0.11391128599643707
step: 1500, Loss: 0.11532716453075409
step: 1600, Loss: 0.11563754081726074
step: 1700, Loss: 0.11443927884101868
step: 1800, Loss: 0.11404429376125336
step: 1900, Loss: 0.11514369398355484
step: 2000, Loss: 0.11549922823905945
step: 2100, Loss: 0.11782176792621613
step: 2200, Loss: 0.11462850868701935
step: 2300, Loss: 0.1147032082080841
step: 2400, Loss: 0.11574871838092804
step: 2500, Loss: 0.11687734723091125
step: 2600, Loss: 0.11491940170526505
step: 2700, Loss: 0.11353108286857605
step: 2800, Loss: 0.1135648787021637
step: 2900, Loss: 0.11331366747617722
step: 3000, Loss: 0.11418824642896652
step: 3100, Loss: 0.11732731759548187
step: 3200, Loss: 0.11498457193374634
step: 3300, Loss: 0.11634768545627594
step: 3400, Loss: 0.11729200929403305
step: 3500, Loss: 0.11727961152791977
step: 3600, Loss: 0.11572219431400299
step: 3700, Loss: 0.11762960255146027
step: 3800, Loss: 0.1165393814444542
step: 3900, Loss: 0.1138431504368782
step: 4000, Loss: 0.11917522549629211
step: 4100, Loss: 0.15584124624729156
step: 4200, Loss: 0.13324449956417084
step: 4300, Loss: 0.12474184483289719
step: 4400, Loss: 0.12806090712547302
step: 4500, Loss: 0.1227211058139801
step: 4600, Loss: 0.1215280294418335
step: 4700, Loss: 0.12051495909690857
step: 4800, Loss: 0.11774422228336334
step: 4900, Loss: 0.11854063719511032
step: 5000, Loss: 0.1202770546078682
step: 5100, Loss: 0.12032532691955566
step: 5200, Loss: 0.11633311212062836
step: 5300, Loss: 0.11456377804279327
step: 5400, Loss: 0.11885643005371094
step: 5500, Loss: 0.12161803245544434
step: 5600, Loss: 0.11604063957929611
step: 5700, Loss: 0.11507800221443176
step: 5800, Loss: 0.1150059849023819
step: 5900, Loss: 0.11676336079835892
step: 6000, Loss: 0.1153712123632431
step: 6100, Loss: 0.11552178859710693
step: 6200, Loss: 0.1157258003950119
step: 6300, Loss: 0.11468654870986938
step: 6400, Loss: 0.11539875715970993

step: 6700, Loss: 0.12152119725942612
step: 6800, Loss: 0.11493342369794846
step: 6900, Loss: 0.11546199768781662
step: 7000, Loss: 0.11834393441677094
step: 7100, Loss: 0.11349156498908997
step: 7200, Loss: 0.11586623638868332
step: 7300, Loss: 0.11406375467777252
step: 7400, Loss: 0.11472968012094498
step: 7500, Loss: 0.11608849465847015
step: 7600, Loss: 0.11382018029689789
step: 7700, Loss: 0.11354643851518631
step: 7800, Loss: 0.11470770090818405
step: 7900, Loss: 0.11552657186985016
step: 8000, Loss: 0.11488979309797287
step: 8100, Loss: 0.11696360260248184
step: 8200, Loss: 0.11507871001958847
step: 8300, Loss: 0.11599934846162796
step: 8400, Loss: 0.1140044629573822
step: 8500, Loss: 0.11511781811714172
step: 8600, Loss: 0.11539344489574432
step: 8700, Loss: 0.11414118111133575
step: 8800, Loss: 0.11442197859287262
step: 8900, Loss: 0.39387327432632446
step: 9000, Loss: 0.11511393636465073
step: 9100, Loss: 0.11346013098955154
step: 9200, Loss: 0.11390133202075958
step: 9300, Loss: 0.11349839717149734
step: 9400, Loss: 0.11414904147386551
step: 9500, Loss: 0.11355379968881607
step: 9600, Loss: 0.11317944526672363
step: 9700, Loss: 0.11320213973522186
step: 9800, Loss: 0.11484114080667496
step: 9900, Loss: 0.11531363427639008
training successfully ended.
validating...
validate data length:91
acc: 0.9545454545454546
precision: 0.9333333333333333
recall: 0.9767441860465116
F_score: 0.9545454545454545
******fold 4******

Training... train_data length:821
step: 0, Loss: 0.11329157650470734
step: 100, Loss: 0.12264937162399292
step: 200, Loss: 0.11639873683452606
step: 300, Loss: 0.11610326170921326
step: 400, Loss: 0.11701875180006027
step: 500, Loss: 0.11323153972625732
step: 600, Loss: 0.1150234192609787
step: 700, Loss: 0.114041768014431
step: 800, Loss: 0.1140558198094368
step: 900, Loss: 0.1132001280784607
step: 1000, Loss: 0.11699727177619934
step: 1100, Loss: 0.11400096863508224
step: 1200, Loss: 0.11339228600263596
step: 1300, Loss: 0.11880776286125183
step: 1400, Loss: 0.11647087335586548
step: 1500, Loss: 0.11389444768428802
step: 1600, Loss: 0.11531493812799454
step: 1700, Loss: 0.11352644860744476
step: 1800, Loss: 0.11370963603258133
step: 1900, Loss: 0.11399394273757935
step: 2000, Loss: 0.35706931352615356
step: 2100, Loss: 0.11468634754419327
step: 2200, Loss: 0.11436544358730316
step: 2300, Loss: 0.11440277099609375
step: 2400, Loss: 0.11311565339565277
step: 2500, Loss: 0.11313080042600632
step: 2600, Loss: 0.11658985912799835
step: 2700, Loss: 0.114329993724823
step: 2800, Loss: 0.11514358967542648
step: 2900, Loss: 0.11347051709890366
step: 3000, Loss: 0.11234302073717117
step: 3100, Loss: 0.11371245980262756
step: 3200, Loss: 1.0185651779174805
step: 3300, Loss: 0.23593328893184662
step: 3400, Loss: 0.153607577085495
step: 3500, Loss: 0.1872645616531372
step: 3600, Loss: 0.13127359747886658
step: 3700, Loss: 0.13222645223140717
step: 3800, Loss: 0.12624293565750122
step: 3900, Loss: 0.13273271918296814
step: 4000, Loss: 0.12481005489826202
step: 4100, Loss: 0.11875069886445999
step: 4200, Loss: 0.1246991753578186
step: 4300, Loss: 0.3528008460998535
step: 4400, Loss: 0.12166513502597809
step: 4500, Loss: 0.11970017850399017
step: 4600, Loss: 0.11767719686031342
step: 4700, Loss: 0.11624443531036377
step: 4800, Loss: 0.11729846894741058
step: 4900, Loss: 0.11775113642215729
step: 5000, Loss: 0.12081611156463623
step: 5100, Loss: 0.11459001153707504
step: 5200, Loss: 0.11702622473239899
step: 5300, Loss: 0.12403414398431778
step: 5400, Loss: 0.11572221666574478
step: 5500, Loss: 0.1158205047249794
step: 5600, Loss: 0.11464397609233856
step: 5700, Loss: 0.11952979862689972
step: 5800, Loss: 0.1166280135512352
step: 5900, Loss: 0.11914942413568497
step: 6000, Loss: 0.1154731884598732
step: 6100, Loss: 0.11499226838350296
step: 6200, Loss: 0.11413916200399399
step: 6300, Loss: 0.11533835530281067
step: 6400, Loss: 0.11511917412281036
step: 6500, Loss: 0.1173672303557396
step: 6600, Loss: 0.3953647017478943
step: 6700, Loss: 0.11484462022781372
step: 6800, Loss: 0.11579546332359314
step: 6900, Loss: 0.11404405534267426
step: 7000, Loss: 0.11446354538202286
step: 7100, Loss: 0.11391867697238922
step: 7200, Loss: 0.11401022970676422
step: 7300, Loss: 0.11606650799512863
step: 7400, Loss: 0.11609303951263428
step: 7500, Loss: 0.11487316340208054
step: 7600, Loss: 0.11582460254430771
step: 7700, Loss: 0.11406082659959793
step: 7800, Loss: 0.11462397128343582
step: 7900, Loss: 0.1145470067858696
step: 8000, Loss: 0.1141728013753891
step: 8100, Loss: 0.11434121429920197
step: 8200, Loss: 0.11597593128681183
step: 8300, Loss: 0.1139686182141304
step: 8400, Loss: 0.1143374890089035
step: 8500, Loss: 0.11547840386629105
step: 8600, Loss: 0.11489942669868469
step: 8700, Loss: 0.11560320854187012
step: 8800, Loss: 0.11429419368505478
step: 8900, Loss: 0.3708997368812561
step: 9000, Loss: 0.11295221745967865
step: 9100, Loss: 0.11400152742862701
step: 9200, Loss: 0.1134398877620697
step: 9300, Loss: 0.11704190820455551
step: 9400, Loss: 0.116014763712883
step: 9500, Loss: 0.11333473026752472
step: 9600, Loss: 0.11581094563007355
step: 9700, Loss: 0.11408600211143494
step: 9800, Loss: 0.11432429403066635
step: 9900, Loss: 0.11432067304849625
training successfully ended.
validating...
validate data length:91
acc: 0.9204545454545454
precision: 0.8604651162790697
recall: 0.9736842105263158
F_score: 0.9135802469135803
******fold 5******

Training... train_data length:821
step: 0, Loss: 0.11386087536811829
step: 100, Loss: 0.11986649036407471
step: 200, Loss: 0.1226278692483902
step: 300, Loss: 0.11548873037099838
step: 400, Loss: 0.11713865399360657
step: 500, Loss: 0.11452677100896835
step: 600, Loss: 0.11441201716661453
step: 700, Loss: 0.11455696076154709
step: 800, Loss: 0.11364088207483292
step: 900, Loss: 0.11329808086156845
step: 1000, Loss: 0.1140921413898468
step: 1100, Loss: 0.11533187329769135
step: 1200, Loss: 0.11337878555059433
step: 1300, Loss: 0.11536631733179092
step: 1400, Loss: 0.115788534283638
step: 1500, Loss: 0.11339426040649414
step: 1600, Loss: 0.11497965455055237
step: 1700, Loss: 0.11336071789264679
step: 1800, Loss: 0.11417171359062195
step: 1900, Loss: 0.11369123309850693
step: 2000, Loss: 0.3553547263145447
step: 2100, Loss: 0.11587493866682053
step: 2200, Loss: 0.11338347941637039
step: 2300, Loss: 0.1133694127202034
step: 2400, Loss: 0.11394570022821426
step: 2500, Loss: 0.11358746886253357
step: 2600, Loss: 0.11406289786100388
step: 2700, Loss: 0.11635449528694153
step: 2800, Loss: 0.11291959881782532
step: 2900, Loss: 0.11272299289703369
step: 3000, Loss: 0.11796536296606064
step: 3100, Loss: 0.11336584389209747
step: 3200, Loss: 0.11290204524993896
step: 3300, Loss: 0.11283096671104431
step: 3400, Loss: 0.11552499979734421
step: 3500, Loss: 0.11414772272109985
step: 3600, Loss: 0.11692442744970322
step: 3700, Loss: 0.11698295921087265
step: 3800, Loss: 0.11591635644435883
step: 3900, Loss: 0.11334284394979477
step: 4000, Loss: 0.1138085126876831
step: 4100, Loss: 0.11480770260095596
step: 4200, Loss: 0.11520497500896454
step: 4300, Loss: 0.3590821921825409
step: 4400, Loss: 0.11624953895807266
step: 4500, Loss: 0.1173546090722084
step: 4600, Loss: 0.11412173509597778
step: 4700, Loss: 0.11584416031837463
step: 4800, Loss: 0.11526172608137131
step: 4900, Loss: 0.20742078125476837
step: 5000, Loss: 0.16246625781059265
step: 5100, Loss: 0.1267949789762497
step: 5200, Loss: 0.1260596215724945
step: 5300, Loss: 0.12697438895702362
step: 5400, Loss: 0.12109938263893127
step: 5500, Loss: 0.12026844918727875
step: 5600, Loss: 0.11970873922109604
step: 5700, Loss: 0.11586334556341171
step: 5800, Loss: 0.11705679446458817
step: 5900, Loss: 0.12080611288547516
step: 6000, Loss: 0.11624410003423691
step: 6100, Loss: 0.11648239940404892
step: 6200, Loss: 0.1157977432012558
step: 6300, Loss: 0.11826768517494202
step: 6400, Loss: 0.11577451229095459
step: 6500, Loss: 0.11833509802818298
step: 6600, Loss: 0.3668469190597534
step: 6700, Loss: 0.11706095188856125
step: 6800, Loss: 0.11464784294366837
step: 6900, Loss: 0.11594518274068832
step: 7000, Loss: 0.11835815012454987
step: 7100, Loss: 0.11517546325922012
step: 6500, Loss: 0.11714991927146912
step: 6600, Loss: 0.11696699261665344
step: 6700, Loss: 0.11403601616621017
step: 6800, Loss: 0.11472416669130325
step: 6900, Loss: 0.11680193245410919
step: 7000, Loss: 0.11548508703708649
step: 7100, Loss: 0.11493203043937683
step: 7200, Loss: 0.11454500257968903
step: 7300, Loss: 0.11423736065626144
step: 7400, Loss: 0.11381185054779053
step: 7500, Loss: 0.1146751195192337
step: 7600, Loss: 0.11599139869213104
step: 7700, Loss: 0.11392956972122192
step: 7800, Loss: 0.11580604314804077
step: 7900, Loss: 0.11574846506118774
step: 8000, Loss: 0.11609899997711182
step: 8100, Loss: 0.1143227219581604
step: 8200, Loss: 0.11710914969444275
step: 8300, Loss: 0.11318952590227127
step: 8400, Loss: 0.11331919580698013
step: 8500, Loss: 0.11615359783172607
step: 8600, Loss: 0.1150081604719162
step: 8700, Loss: 0.11862463504076004
step: 8800, Loss: 0.11573415994644165
step: 8900, Loss: 0.1155163124203682
step: 9000, Loss: 0.11524896323680878
step: 9100, Loss: 0.11502037942409515
step: 9200, Loss: 0.11591298133134842
step: 9300, Loss: 0.11490847170352936
step: 9400, Loss: 0.11681467294692993
step: 9500, Loss: 0.11626119166612625
step: 9600, Loss: 0.11352512985467911
step: 9700, Loss: 0.11488714069128036
step: 9800, Loss: 0.11526651680469513
step: 9900, Loss: 0.1143496111035347
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7777777777777778
recall: 0.9333333333333333
F_score: 0.8484848484848485
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.5511188507080078
step: 100, Loss: 0.1173458993434906
step: 200, Loss: 0.11509767174720764
step: 300, Loss: 0.11720511317253113
step: 400, Loss: 0.11526449024677277
step: 500, Loss: 0.11635363101959229
step: 600, Loss: 0.11403906345367432
step: 700, Loss: 0.11541111767292023
step: 800, Loss: 0.11620619893074036
step: 900, Loss: 0.11551408469676971
step: 1000, Loss: 0.11522384732961655
step: 1100, Loss: 0.11674400418996811
step: 1200, Loss: 0.11417198181152344
step: 1300, Loss: 0.11487527936697006
step: 1400, Loss: 0.11487342417240143
step: 1500, Loss: 0.1148187518119812
step: 1600, Loss: 0.11493434011936188
step: 1700, Loss: 0.11503816395998001
step: 1800, Loss: 0.11500605195760727
step: 1900, Loss: 0.11377158015966415
step: 2000, Loss: 0.1139359176158905
step: 2100, Loss: 0.1171572357416153
step: 2200, Loss: 0.11435423046350479
step: 2300, Loss: 0.11367777734994888
step: 2400, Loss: 0.11502614617347717
step: 2500, Loss: 0.11464191973209381
step: 2600, Loss: 0.12101677805185318
step: 2700, Loss: 0.11562860012054443
step: 2800, Loss: 0.11623735725879669
step: 2900, Loss: 0.11544442176818848
step: 3000, Loss: 0.11432711780071259
step: 3100, Loss: 0.11504548788070679
step: 3200, Loss: 0.11509526520967484
step: 3300, Loss: 0.11600770056247711
step: 3400, Loss: 0.11411376297473907
step: 3500, Loss: 0.11411964893341064
step: 3600, Loss: 0.11624692380428314
step: 3700, Loss: 0.1134072095155716
step: 3800, Loss: 0.11611465364694595
step: 3900, Loss: 0.11558323353528976
step: 4000, Loss: 0.11505343019962311
step: 4100, Loss: 0.11489029973745346
step: 4200, Loss: 0.11612769961357117
step: 4300, Loss: 0.11466114223003387
step: 4400, Loss: 0.11361237615346909
step: 4500, Loss: 0.11597993969917297
step: 4600, Loss: 0.11401629447937012
step: 4700, Loss: 3.073068141937256
step: 4800, Loss: 0.14272892475128174
step: 4900, Loss: 0.13100402057170868
step: 5000, Loss: 0.12676894664764404
step: 5100, Loss: 0.12256692349910736
step: 5200, Loss: 0.12421365082263947
step: 5300, Loss: 0.12242141366004944
step: 5400, Loss: 0.12044226378202438
step: 5500, Loss: 0.11676749587059021
step: 5600, Loss: 0.12220587581396103
step: 5700, Loss: 0.11480354517698288
step: 5800, Loss: 0.11730928719043732
step: 5900, Loss: 0.11776541918516159
step: 6000, Loss: 0.11712023615837097
step: 6100, Loss: 0.11628583818674088
step: 6200, Loss: 0.11715599149465561
step: 6300, Loss: 0.11724328994750977
step: 6400, Loss: 0.11592233180999756
step: 6500, Loss: 0.11554059386253357
step: 6600, Loss: 0.12519046664237976
step: 6700, Loss: 0.11569695174694061
step: 6800, Loss: 0.11409322172403336
step: 6900, Loss: 0.11438159644603729
step: 7000, Loss: 0.11509434133768082
step: 7100, Loss: 0.11414097249507904
step: 7200, Loss: 0.11529026925563812
step: 7300, Loss: 0.11435430496931076
step: 7400, Loss: 0.11669880896806717
step: 7500, Loss: 0.1141791120171547
step: 7600, Loss: 0.11493388563394547
step: 7700, Loss: 0.11542461812496185
step: 7800, Loss: 0.11653665453195572
step: 7900, Loss: 0.11433403939008713
step: 8000, Loss: 0.11831992864608765
step: 8100, Loss: 0.11544646322727203
step: 8200, Loss: 0.11608432233333588
step: 8300, Loss: 0.11500626802444458
step: 8400, Loss: 0.11456749588251114
step: 8500, Loss: 0.11410653591156006
step: 8600, Loss: 0.11470633000135422
step: 8700, Loss: 0.11415553838014603
step: 8800, Loss: 0.11535058170557022
step: 8900, Loss: 0.11385643482208252
step: 9000, Loss: 0.11542624235153198
step: 9100, Loss: 0.11520744860172272
step: 9200, Loss: 0.1159834936261177
step: 9300, Loss: 0.11329676955938339
step: 9400, Loss: 0.11444779485464096
step: 9500, Loss: 0.11412349343299866
step: 9600, Loss: 0.11702186614274979
step: 9700, Loss: 0.11403099447488785
step: 9800, Loss: 0.11445583403110504
step: 9900, Loss: 0.11410446465015411
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.6166430115699768
step: 100, Loss: 0.11548440903425217
step: 200, Loss: 0.11614862829446793
step: 300, Loss: 0.11458279192447662
step: 400, Loss: 0.11666371673345566
step: 500, Loss: 0.11616328358650208
step: 600, Loss: 0.11416524648666382
step: 700, Loss: 0.11477626860141754
step: 800, Loss: 0.11590197682380676
step: 900, Loss: 0.11481308937072754
step: 1000, Loss: 0.11637041717767715
step: 1100, Loss: 0.11348467320203781
step: 1200, Loss: 0.11434304714202881
step: 1300, Loss: 0.11344165354967117
step: 1400, Loss: 0.1153264045715332
step: 1500, Loss: 0.11322218924760818
step: 1600, Loss: 0.11436428129673004
step: 1700, Loss: 0.11393019556999207
step: 1800, Loss: 0.11315924674272537
step: 1900, Loss: 0.11533188819885254
step: 2000, Loss: 0.11397150158882141
step: 2100, Loss: 0.11520863324403763
step: 2200, Loss: 0.11472806334495544
step: 2300, Loss: 0.11399151384830475
step: 2400, Loss: 0.11499393731355667
step: 2500, Loss: 0.11385586857795715
step: 2600, Loss: 0.11560140550136566
step: 2700, Loss: 0.11640530824661255
step: 2800, Loss: 0.11341434717178345
step: 2900, Loss: 0.11426592618227005
step: 3000, Loss: 0.11594270169734955
step: 3100, Loss: 0.11498917639255524
step: 3200, Loss: 0.11599445343017578
step: 3300, Loss: 0.11380141228437424
step: 3400, Loss: 0.11430975794792175
step: 3500, Loss: 0.114394411444664
step: 3600, Loss: 0.11381703615188599
step: 3700, Loss: 0.11344332247972488
step: 3800, Loss: 0.11398613452911377
step: 3900, Loss: 0.11379283666610718
step: 4000, Loss: 0.11578977108001709
step: 4100, Loss: 0.11471973359584808
step: 4200, Loss: 0.11500062048435211
step: 4300, Loss: 0.11438975483179092
step: 4400, Loss: 0.11697952449321747
step: 4500, Loss: 0.11391092836856842
step: 4600, Loss: 0.11742158234119415
step: 4700, Loss: 0.11361096799373627
step: 4800, Loss: 0.38085222244262695
step: 4900, Loss: 0.13151314854621887
step: 5000, Loss: 0.16422942280769348
step: 5100, Loss: 0.11960865557193756
step: 5200, Loss: 0.1203942522406578
step: 5300, Loss: 0.12088945508003235
step: 5400, Loss: 0.12364979088306427
step: 5500, Loss: 0.11901125311851501
step: 5600, Loss: 0.12420590966939926
step: 5700, Loss: 0.11675702780485153
step: 5800, Loss: 0.11819857358932495
step: 5900, Loss: 0.11541591584682465
step: 6000, Loss: 0.11709858477115631
step: 6100, Loss: 0.117203488945961
step: 6200, Loss: 0.11620843410491943
step: 6300, Loss: 0.11551986634731293
step: 6400, Loss: 0.11603023111820221
step: 6500, Loss: 0.11684560030698776
step: 6600, Loss: 0.12149035185575485
step: 6700, Loss: 0.11616164445877075
step: 6800, Loss: 0.11818179488182068
step: 6900, Loss: 0.11505132913589478
step: 7200, Loss: 0.11681095510721207
step: 7300, Loss: 0.11673126369714737
step: 7400, Loss: 0.11504126340150833
step: 7500, Loss: 0.11498183757066727
step: 7600, Loss: 0.11578154563903809
step: 7700, Loss: 0.11389916390180588
step: 7800, Loss: 0.1131339818239212
step: 7900, Loss: 0.11393651366233826
step: 8000, Loss: 0.11838915944099426
step: 8100, Loss: 0.11528526246547699
step: 8200, Loss: 0.11434881389141083
step: 8300, Loss: 0.11406954377889633
step: 8400, Loss: 0.1145218089222908
step: 8500, Loss: 0.11423605680465698
step: 8600, Loss: 0.11510792374610901
step: 8700, Loss: 0.1160905510187149
step: 8800, Loss: 0.11603888869285583
step: 8900, Loss: 0.37072765827178955
step: 9000, Loss: 0.11682112514972687
step: 9100, Loss: 0.11359646916389465
step: 9200, Loss: 0.11412831395864487
step: 9300, Loss: 0.11383110284805298
step: 9400, Loss: 0.11327023059129715
step: 9500, Loss: 0.11426059901714325
step: 9600, Loss: 0.11341706663370132
step: 9700, Loss: 0.11319342255592346
step: 9800, Loss: 0.1139875277876854
step: 9900, Loss: 0.11332292854785919
training successfully ended.
validating...
validate data length:91
acc: 0.9318181818181818
precision: 0.9166666666666666
recall: 0.9565217391304348
F_score: 0.9361702127659574
******fold 6******

Training... train_data length:821
step: 0, Loss: 0.1134876236319542
step: 100, Loss: 0.12098252028226852
step: 200, Loss: 0.1164683923125267
step: 300, Loss: 0.11639924347400665
step: 400, Loss: 0.12200456857681274
step: 500, Loss: 0.11471356451511383
step: 600, Loss: 0.11419990658760071
step: 700, Loss: 0.11589162051677704
step: 800, Loss: 0.11292499303817749
step: 900, Loss: 0.11409925669431686
step: 1000, Loss: 0.11439920961856842
step: 1100, Loss: 0.11411828547716141
step: 1200, Loss: 0.11513391137123108
step: 1300, Loss: 0.11596952378749847
step: 1400, Loss: 0.1137785017490387
step: 1500, Loss: 0.11335916072130203
step: 1600, Loss: 0.11416029930114746
step: 1700, Loss: 0.11349733918905258
step: 1800, Loss: 0.11390601843595505
step: 1900, Loss: 0.11403684318065643
step: 2000, Loss: 0.36591681838035583
step: 2100, Loss: 0.11638525128364563
step: 2200, Loss: 0.11423549056053162
step: 2300, Loss: 0.11340741813182831
step: 2400, Loss: 0.11353760957717896
step: 2500, Loss: 0.1133234053850174
step: 2600, Loss: 0.11463627219200134
step: 2700, Loss: 0.11369644105434418
step: 2800, Loss: 0.11447861790657043
step: 2900, Loss: 0.11480479687452316
step: 3000, Loss: 0.11409705877304077
step: 3100, Loss: 0.11387630552053452
step: 3200, Loss: 0.11352652311325073
step: 3300, Loss: 0.11428980529308319
step: 3400, Loss: 0.1147812157869339
step: 3500, Loss: 0.11464360356330872
step: 3600, Loss: 0.11430999636650085
step: 3700, Loss: 0.1134665459394455
step: 3800, Loss: 0.11460402607917786
step: 3900, Loss: 0.11437679827213287
step: 4000, Loss: 4.112484931945801
step: 4100, Loss: 0.366515576839447
step: 4200, Loss: 0.14351345598697662
step: 4300, Loss: 0.40448325872421265
step: 4400, Loss: 0.13574597239494324
step: 4500, Loss: 0.12594734132289886
step: 4600, Loss: 0.12262716144323349
step: 4700, Loss: 0.12060213834047318
step: 4800, Loss: 0.12848293781280518
step: 4900, Loss: 0.12295985966920853
step: 5000, Loss: 0.12940821051597595
step: 5100, Loss: 0.12168130278587341
step: 5200, Loss: 0.12500138580799103
step: 5300, Loss: 0.11842334270477295
step: 5400, Loss: 0.11625206470489502
step: 5500, Loss: 0.11690918356180191
step: 5600, Loss: 0.11852969974279404
step: 5700, Loss: 0.1161641925573349
step: 5800, Loss: 0.11720746010541916
step: 5900, Loss: 0.11877382546663284
step: 6000, Loss: 0.11800932884216309
step: 6100, Loss: 0.11660698056221008
step: 6200, Loss: 0.11809659749269485
step: 6300, Loss: 0.11423453688621521
step: 6400, Loss: 0.11706068366765976
step: 6500, Loss: 0.11560365557670593
step: 6600, Loss: 0.3556714355945587
step: 6700, Loss: 0.11582475900650024
step: 6800, Loss: 0.1151607409119606
step: 6900, Loss: 0.11514364182949066
step: 7000, Loss: 0.11653284728527069
step: 7100, Loss: 0.11692686378955841
step: 7200, Loss: 0.116618312895298
step: 7300, Loss: 0.1175033301115036
step: 7400, Loss: 0.11428119987249374
step: 7500, Loss: 0.11488887667655945
step: 7600, Loss: 0.11368264257907867
step: 7700, Loss: 0.11420245468616486
step: 7800, Loss: 0.11490762233734131
step: 7900, Loss: 0.11471320688724518
step: 8000, Loss: 0.11513330042362213
step: 8100, Loss: 0.11503025889396667
step: 8200, Loss: 0.11506780982017517
step: 8300, Loss: 0.11238978803157806
step: 8400, Loss: 0.11501869559288025
step: 8500, Loss: 0.11580991744995117
step: 8600, Loss: 0.11339674144983292
step: 8700, Loss: 0.11365080624818802
step: 8800, Loss: 0.11411941051483154
step: 8900, Loss: 0.39919382333755493
step: 9000, Loss: 0.11382424831390381
step: 9100, Loss: 0.11552279442548752
step: 9200, Loss: 0.114471435546875
step: 9300, Loss: 0.11441581696271896
step: 9400, Loss: 0.11521758139133453
step: 9500, Loss: 0.11352591216564178
step: 9600, Loss: 0.11700329184532166
step: 9700, Loss: 0.11424722522497177
step: 9800, Loss: 0.11452874541282654
step: 9900, Loss: 0.11374443769454956
training successfully ended.
validating...
validate data length:91
acc: 0.9886363636363636
precision: 0.9787234042553191
recall: 1.0
F_score: 0.989247311827957
******fold 7******

Training... train_data length:821
step: 0, Loss: 0.13591301441192627
step: 100, Loss: 0.12865085899829865
step: 200, Loss: 0.11566535383462906
step: 300, Loss: 0.1169344037771225
step: 400, Loss: 0.11577660590410233
step: 500, Loss: 0.11469079554080963
step: 600, Loss: 0.1144997850060463
step: 700, Loss: 0.11512980610132217
step: 800, Loss: 0.11374238133430481
step: 900, Loss: 0.11297732591629028
step: 1000, Loss: 0.11557810008525848
step: 1100, Loss: 0.1143164336681366
step: 1200, Loss: 0.1135440245270729
step: 1300, Loss: 0.11498764157295227
step: 1400, Loss: 0.11368910223245621
step: 1500, Loss: 0.11588838696479797
step: 1600, Loss: 0.11361189931631088
step: 1700, Loss: 0.11499037593603134
step: 1800, Loss: 0.11385421454906464
step: 1900, Loss: 0.11468134820461273
step: 2000, Loss: 0.38154053688049316
step: 2100, Loss: 0.11466096341609955
step: 2200, Loss: 0.1140892505645752
step: 2300, Loss: 0.11368511617183685
step: 2400, Loss: 0.11275874823331833
step: 2500, Loss: 0.11519011855125427
step: 2600, Loss: 0.11424560844898224
step: 2700, Loss: 0.1141316145658493
step: 2800, Loss: 0.1133919209241867
step: 2900, Loss: 0.11494603753089905
step: 3000, Loss: 2.435227632522583
step: 3100, Loss: 0.2206888049840927
step: 3200, Loss: 0.14845040440559387
step: 3300, Loss: 0.13720636069774628
step: 3400, Loss: 0.12477129697799683
step: 3500, Loss: 0.12961576879024506
step: 3600, Loss: 0.12327685952186584
step: 3700, Loss: 0.12639424204826355
step: 3800, Loss: 0.11789825558662415
step: 3900, Loss: 0.12227583676576614
step: 4000, Loss: 0.11971436440944672
step: 4100, Loss: 0.11768423020839691
step: 4200, Loss: 0.12469343096017838
step: 4300, Loss: 0.36089396476745605
step: 4400, Loss: 0.11840508878231049
step: 4500, Loss: 0.1200314462184906
step: 4600, Loss: 0.11722980439662933
step: 4700, Loss: 0.11932182312011719
step: 4800, Loss: 0.16558440029621124
step: 4900, Loss: 0.11670557409524918
step: 5000, Loss: 0.11943325400352478
step: 5100, Loss: 0.1151466816663742
step: 5200, Loss: 0.11698116362094879
step: 5300, Loss: 0.11625894904136658
step: 5400, Loss: 0.11509057879447937
step: 5500, Loss: 0.11571075767278671
step: 5600, Loss: 0.11466540396213531
step: 5700, Loss: 0.11521859467029572
step: 5800, Loss: 0.11411760002374649
step: 5900, Loss: 0.11678223311901093
step: 6000, Loss: 0.11554079502820969
step: 6100, Loss: 0.11517469584941864
step: 6200, Loss: 0.1183050125837326
step: 6300, Loss: 0.11464569717645645
step: 6400, Loss: 0.11374972760677338
step: 6500, Loss: 0.11638680100440979
step: 6600, Loss: 0.3567068874835968
step: 6700, Loss: 0.11605430394411087
step: 6800, Loss: 0.11507631838321686
step: 6900, Loss: 0.11513548344373703
step: 7000, Loss: 0.11451241374015808
step: 7100, Loss: 0.11480306833982468
step: 7200, Loss: 0.1135382354259491
step: 7300, Loss: 0.11527837812900543
step: 7400, Loss: 0.1138814315199852
step: 7500, Loss: 0.11453229188919067
step: 7600, Loss: 0.11521085351705551
step: 7000, Loss: 0.11470907926559448
step: 7100, Loss: 0.11656375229358673
step: 7200, Loss: 0.116688072681427
step: 7300, Loss: 0.11332102119922638
step: 7400, Loss: 0.1184794157743454
step: 7500, Loss: 0.1143311858177185
step: 7600, Loss: 0.11786846816539764
step: 7700, Loss: 0.11386918276548386
step: 7800, Loss: 0.1164044439792633
step: 7900, Loss: 0.1142486184835434
step: 8000, Loss: 0.11475242674350739
step: 8100, Loss: 0.11412134766578674
step: 8200, Loss: 0.11751844733953476
step: 8300, Loss: 0.11376187205314636
step: 8400, Loss: 0.11671110987663269
step: 8500, Loss: 0.11399923264980316
step: 8600, Loss: 0.11808805167675018
step: 8700, Loss: 0.11314406991004944
step: 8800, Loss: 0.11943219602108002
step: 8900, Loss: 0.11373651772737503
step: 9000, Loss: 0.11797326058149338
step: 9100, Loss: 0.11516852676868439
step: 9200, Loss: 0.1154550090432167
step: 9300, Loss: 0.11337848007678986
step: 9400, Loss: 0.1190476045012474
step: 9500, Loss: 0.11454638093709946
step: 9600, Loss: 0.11714405566453934
step: 9700, Loss: 0.11387957632541656
step: 9800, Loss: 0.1147998571395874
step: 9900, Loss: 0.11434321850538254
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.875
recall: 0.7777777777777778
F_score: 0.823529411764706
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.5864717960357666
step: 100, Loss: 0.12087401747703552
step: 200, Loss: 0.1177021861076355
step: 300, Loss: 0.11442717164754868
step: 400, Loss: 0.11690131574869156
step: 500, Loss: 0.11495490372180939
step: 600, Loss: 0.11555299907922745
step: 700, Loss: 0.11728623509407043
step: 800, Loss: 0.11537746340036392
step: 900, Loss: 0.11490694433450699
step: 1000, Loss: 0.11696814745664597
step: 1100, Loss: 0.11556247621774673
step: 1200, Loss: 0.11507095396518707
step: 1300, Loss: 0.11517664045095444
step: 1400, Loss: 0.11521917581558228
step: 1500, Loss: 0.11795572936534882
step: 1600, Loss: 0.11478349566459656
step: 1700, Loss: 0.11430799961090088
step: 1800, Loss: 0.1144518032670021
step: 1900, Loss: 0.11510317027568817
step: 2000, Loss: 0.11480515450239182
step: 2100, Loss: 0.11501234769821167
step: 2200, Loss: 0.1162501722574234
step: 2300, Loss: 0.1145223081111908
step: 2400, Loss: 0.11559465527534485
step: 2500, Loss: 0.11347343027591705
step: 2600, Loss: 0.11466454714536667
step: 2700, Loss: 0.1139119490981102
step: 2800, Loss: 0.11653422564268112
step: 2900, Loss: 0.11749055236577988
step: 3000, Loss: 0.11677032709121704
step: 3100, Loss: 0.11542349308729172
step: 3200, Loss: 0.11414535343647003
step: 3300, Loss: 0.1141241267323494
step: 3400, Loss: 0.11751183122396469
step: 3500, Loss: 0.11516852676868439
step: 3600, Loss: 0.11787960678339005
step: 3700, Loss: 0.1136249378323555
step: 3800, Loss: 0.11610232293605804
step: 3900, Loss: 0.11516439914703369
step: 4000, Loss: 0.11352027952671051
step: 4100, Loss: 0.11436164379119873
step: 4200, Loss: 0.11535415798425674
step: 4300, Loss: 0.11396481841802597
step: 4400, Loss: 0.1183326244354248
step: 4500, Loss: 0.11411920189857483
step: 4600, Loss: 0.8577194213867188
step: 4700, Loss: 0.14822620153427124
step: 4800, Loss: 0.12718652188777924
step: 4900, Loss: 0.12909381091594696
step: 5000, Loss: 0.11914534866809845
step: 5100, Loss: 0.12236203253269196
step: 5200, Loss: 0.12123231589794159
step: 5300, Loss: 0.12666907906532288
step: 5400, Loss: 0.11655586212873459
step: 5500, Loss: 0.12528647482395172
step: 5600, Loss: 0.12284155189990997
step: 5700, Loss: 0.12215085327625275
step: 5800, Loss: 0.11688157916069031
step: 5900, Loss: 0.11967206001281738
step: 6000, Loss: 0.11627815663814545
step: 6100, Loss: 0.11986233294010162
step: 6200, Loss: 0.11836462467908859
step: 6300, Loss: 0.11761375516653061
step: 6400, Loss: 0.1167137622833252
step: 6500, Loss: 0.11802688986063004
step: 6600, Loss: 0.11419123411178589
step: 6700, Loss: 0.11779636144638062
step: 6800, Loss: 0.12419051676988602
step: 6900, Loss: 0.11735988408327103
step: 7000, Loss: 0.11589834839105606
step: 7100, Loss: 0.11523983627557755
step: 7200, Loss: 0.11728757619857788
step: 7300, Loss: 0.11494652926921844
step: 7400, Loss: 0.11780714988708496
step: 7500, Loss: 0.11572813987731934
step: 7600, Loss: 0.11566272377967834
step: 7700, Loss: 0.13001671433448792
step: 7800, Loss: 0.12357800453901291
step: 7900, Loss: 0.11744195967912674
step: 8000, Loss: 0.1144065409898758
step: 8100, Loss: 0.11547211557626724
step: 8200, Loss: 0.11718522012233734
step: 8300, Loss: 0.11552752554416656
step: 8400, Loss: 0.11558820307254791
step: 8500, Loss: 0.11537259817123413
step: 8600, Loss: 0.11542479693889618
step: 8700, Loss: 0.1135793924331665
step: 8800, Loss: 0.11350461095571518
step: 8900, Loss: 0.11435960978269577
step: 9000, Loss: 0.11574789136648178
step: 9100, Loss: 0.11502265930175781
step: 9200, Loss: 0.11454039067029953
step: 9300, Loss: 0.11500848084688187
step: 9400, Loss: 0.11628498136997223
step: 9500, Loss: 0.1161155253648758
step: 9600, Loss: 0.12165884673595428
step: 9700, Loss: 0.1153927817940712
step: 9800, Loss: 0.1159726083278656
step: 9900, Loss: 0.11484839022159576
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8823529411764706
recall: 0.8333333333333334
F_score: 0.8571428571428571
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.5303874015808105
step: 100, Loss: 0.11639565229415894
step: 200, Loss: 0.11676380783319473
step: 300, Loss: 0.11770712584257126
step: 400, Loss: 0.11402910947799683
step: 500, Loss: 0.11488299816846848
step: 600, Loss: 0.11845187097787857
step: 700, Loss: 0.11602190136909485
step: 800, Loss: 0.11387839168310165
step: 900, Loss: 0.11456093192100525
step: 1000, Loss: 0.11479151993989944
step: 1100, Loss: 0.11419360339641571
step: 1200, Loss: 0.11520854383707047
step: 1300, Loss: 0.11406056582927704
step: 1400, Loss: 0.11564575135707855
step: 1500, Loss: 0.1139308363199234
step: 1600, Loss: 0.11437185108661652
step: 1700, Loss: 0.11386043578386307
step: 1800, Loss: 0.11351557075977325
step: 1900, Loss: 0.11515018343925476
step: 2000, Loss: 0.11511703580617905
step: 2100, Loss: 0.11579003930091858
step: 2200, Loss: 0.11601107567548752
step: 2300, Loss: 0.114092618227005
step: 2400, Loss: 0.11384101212024689
step: 2500, Loss: 0.11382488161325455
step: 2600, Loss: 0.11472467333078384
step: 2700, Loss: 0.11401787400245667
step: 2800, Loss: 0.11417704820632935
step: 2900, Loss: 0.11394059658050537
step: 3000, Loss: 0.11464761197566986
step: 3100, Loss: 0.113736093044281
step: 3200, Loss: 0.11600978672504425
step: 3300, Loss: 0.11544628441333771
step: 3400, Loss: 0.11600953340530396
step: 3500, Loss: 0.11344461888074875
step: 3600, Loss: 0.11407257616519928
step: 3700, Loss: 0.11452753841876984
step: 3800, Loss: 0.11570734530687332
step: 3900, Loss: 0.11358232796192169
step: 4000, Loss: 0.11547350883483887
step: 4100, Loss: 0.11442483216524124
step: 4200, Loss: 0.11416932940483093
step: 4300, Loss: 0.11559615284204483
step: 4400, Loss: 0.11440085619688034
step: 4500, Loss: 0.11456029117107391
step: 4600, Loss: 0.11336243897676468
step: 4700, Loss: 0.11745967715978622
step: 4800, Loss: 0.11551814526319504
step: 4900, Loss: 0.11398129165172577
step: 5000, Loss: 0.11469507217407227
step: 5100, Loss: 0.11479630321264267
step: 5200, Loss: 0.11725296825170517
step: 5300, Loss: 0.11433503031730652
step: 5400, Loss: 0.11683503538370132
step: 5500, Loss: 0.11696961522102356
step: 5600, Loss: 0.11856454610824585
step: 5700, Loss: 0.11418961733579636
step: 5800, Loss: 0.11538150161504745
step: 5900, Loss: 0.11342336237430573
step: 6000, Loss: 0.11393415182828903
step: 6100, Loss: 0.18644624948501587
step: 6200, Loss: 0.13683614134788513
step: 6300, Loss: 0.12244225293397903
step: 6400, Loss: 0.12286141514778137
step: 6500, Loss: 0.12320760637521744
step: 6600, Loss: 0.12419435381889343
step: 6700, Loss: 0.11559093743562698
step: 6800, Loss: 0.11863110959529877
step: 6900, Loss: 0.11802975088357925
step: 7000, Loss: 0.11710938811302185
step: 7100, Loss: 0.11676676571369171
step: 7200, Loss: 0.11709847301244736
step: 7300, Loss: 0.12119649350643158
step: 7400, Loss: 0.1196286529302597
step: 7700, Loss: 0.11425130069255829
step: 7800, Loss: 0.1153550073504448
step: 7900, Loss: 0.11447332054376602
step: 8000, Loss: 0.11396870017051697
step: 8100, Loss: 0.11364544928073883
step: 8200, Loss: 0.11368899792432785
step: 8300, Loss: 0.11355378478765488
step: 8400, Loss: 0.11420603096485138
step: 8500, Loss: 0.11301180720329285
step: 8600, Loss: 0.1143118366599083
step: 8700, Loss: 0.11530831456184387
step: 8800, Loss: 0.11418631672859192
step: 8900, Loss: 0.37321722507476807
step: 9000, Loss: 0.1141061931848526
step: 9100, Loss: 0.114547498524189
step: 9200, Loss: 0.11562975496053696
step: 9300, Loss: 0.11343632638454437
step: 9400, Loss: 0.11404348909854889
step: 9500, Loss: 0.11329643428325653
step: 9600, Loss: 0.11439885944128036
step: 9700, Loss: 0.11559797078371048
step: 9800, Loss: 0.11443548649549484
step: 9900, Loss: 0.11414143443107605
training successfully ended.
validating...
validate data length:91
acc: 0.9886363636363636
precision: 0.9803921568627451
recall: 1.0
F_score: 0.99009900990099
******fold 8******

Training... train_data length:821
step: 0, Loss: 0.1308767944574356
step: 100, Loss: 0.11942256987094879
step: 200, Loss: 0.11453229188919067
step: 300, Loss: 0.11483627557754517
step: 400, Loss: 0.11471103131771088
step: 500, Loss: 0.11423400789499283
step: 600, Loss: 0.11469576507806778
step: 700, Loss: 0.11465126276016235
step: 800, Loss: 0.11499375104904175
step: 900, Loss: 0.11470876634120941
step: 1000, Loss: 0.1136956438422203
step: 1100, Loss: 0.11434642970561981
step: 1200, Loss: 0.11284874379634857
step: 1300, Loss: 0.11424367129802704
step: 1400, Loss: 0.11356820166110992
step: 1500, Loss: 0.1138743981719017
step: 1600, Loss: 0.11606944352388382
step: 1700, Loss: 0.11429537087678909
step: 1800, Loss: 0.11419452726840973
step: 1900, Loss: 0.11424479633569717
step: 2000, Loss: 0.37427306175231934
step: 2100, Loss: 0.1152019202709198
step: 2200, Loss: 0.11511851102113724
step: 2300, Loss: 0.11421544849872589
step: 2400, Loss: 0.11417829990386963
step: 2500, Loss: 3.513570547103882
step: 2600, Loss: 0.28094637393951416
step: 2700, Loss: 0.1684456318616867
step: 2800, Loss: 0.13375888764858246
step: 2900, Loss: 0.12918557226657867
step: 3000, Loss: 0.1305009424686432
step: 3100, Loss: 0.12668931484222412
step: 3200, Loss: 0.12173529714345932
step: 3300, Loss: 0.12063511461019516
step: 3400, Loss: 0.12315180897712708
step: 3500, Loss: 0.12174470722675323
step: 3600, Loss: 0.11957346647977829
step: 3700, Loss: 0.11737049371004105
step: 3800, Loss: 0.1176590621471405
step: 3900, Loss: 0.11740177124738693
step: 4000, Loss: 0.11639868468046188
step: 4100, Loss: 0.11561229079961777
step: 4200, Loss: 0.11745508015155792
step: 4300, Loss: 0.3587588667869568
step: 4400, Loss: 0.1176280677318573
step: 4500, Loss: 0.1165623664855957
step: 4600, Loss: 0.11780091375112534
step: 4700, Loss: 0.11508087813854218
step: 4800, Loss: 0.11592233180999756
step: 4900, Loss: 0.11514271795749664
step: 5000, Loss: 0.1183394119143486
step: 5100, Loss: 0.11511765420436859
step: 5200, Loss: 0.11543221771717072
step: 5300, Loss: 0.11692551523447037
step: 5400, Loss: 0.11398263275623322
step: 5500, Loss: 0.11447282135486603
step: 5600, Loss: 0.11470545828342438
step: 5700, Loss: 0.1152617484331131
step: 5800, Loss: 0.11371190845966339
step: 5900, Loss: 0.11565157771110535
step: 6000, Loss: 0.11390699446201324
step: 6100, Loss: 0.11388666927814484
step: 6200, Loss: 0.11332158744335175
step: 6300, Loss: 0.11337043344974518
step: 6400, Loss: 0.1147145926952362
step: 6500, Loss: 0.11463223397731781
step: 6600, Loss: 0.35604560375213623
step: 6700, Loss: 0.11399707943201065
step: 6800, Loss: 0.11504652351140976
step: 6900, Loss: 0.11443357169628143
step: 7000, Loss: 0.11583881080150604
step: 7100, Loss: 0.11957213282585144
step: 7200, Loss: 0.11347997933626175
step: 7300, Loss: 0.11455685645341873
step: 7400, Loss: 0.1132396012544632
step: 7500, Loss: 0.1137237399816513
step: 7600, Loss: 0.11704674363136292
step: 7700, Loss: 0.11415079236030579
step: 7800, Loss: 0.11451520025730133
step: 7900, Loss: 0.1140051856637001
step: 8000, Loss: 0.11478147655725479
step: 8100, Loss: 0.11355563998222351
step: 8200, Loss: 0.11371913552284241
step: 8300, Loss: 0.11453785002231598
step: 8400, Loss: 0.1137763038277626
step: 8500, Loss: 0.11348699778318405
step: 8600, Loss: 0.11378485709428787
step: 8700, Loss: 0.11522194743156433
step: 8800, Loss: 0.11384090036153793
step: 8900, Loss: 0.3585239052772522
step: 9000, Loss: 0.11385022103786469
step: 9100, Loss: 0.11334189772605896
step: 9200, Loss: 0.11348914355039597
step: 9300, Loss: 0.11479641497135162
step: 9400, Loss: 0.11614899337291718
step: 9500, Loss: 0.11400432139635086
step: 9600, Loss: 0.11437360942363739
step: 9700, Loss: 0.11400929093360901
step: 9800, Loss: 0.11463707685470581
step: 9900, Loss: 0.11462322622537613
training successfully ended.
validating...
validate data length:91
acc: 0.9545454545454546
precision: 0.926829268292683
recall: 0.9743589743589743
F_score: 0.9500000000000001
******fold 9******

Training... train_data length:821
step: 0, Loss: 0.13644324243068695
step: 100, Loss: 0.11877318471670151
step: 200, Loss: 0.11574013531208038
step: 300, Loss: 0.11496495455503464
step: 400, Loss: 0.11522643268108368
step: 500, Loss: 0.11304906010627747
step: 600, Loss: 0.11400235444307327
step: 700, Loss: 0.11465862393379211
step: 800, Loss: 0.11320458352565765
step: 900, Loss: 0.1144648864865303
step: 1000, Loss: 0.11388144642114639
step: 1100, Loss: 0.11406170576810837
step: 1200, Loss: 0.11307266354560852
step: 1300, Loss: 0.1136382445693016
step: 1400, Loss: 0.11468006670475006
step: 1500, Loss: 0.11402373760938644
step: 1600, Loss: 0.1136411502957344
step: 1700, Loss: 0.11568507552146912
step: 1800, Loss: 0.11457861959934235
step: 1900, Loss: 0.1189381405711174
step: 2000, Loss: 0.48449796438217163
step: 2100, Loss: 0.214257150888443
step: 2200, Loss: 0.1486208587884903
step: 2300, Loss: 0.13620226085186005
step: 2400, Loss: 0.12505295872688293
step: 2500, Loss: 0.12448106706142426
step: 2600, Loss: 0.12344402819871902
step: 2700, Loss: 0.12488238513469696
step: 2800, Loss: 0.120704784989357
step: 2900, Loss: 0.1210877075791359
step: 3000, Loss: 0.12568879127502441
step: 3100, Loss: 0.12059460580348969
step: 3200, Loss: 0.11653211712837219
step: 3300, Loss: 0.11703624576330185
step: 3400, Loss: 0.11738909035921097
step: 3500, Loss: 0.11662711203098297
step: 3600, Loss: 0.11743131279945374
step: 3700, Loss: 0.11567571759223938
step: 3800, Loss: 0.11798042058944702
step: 3900, Loss: 0.11959095299243927
step: 4000, Loss: 0.11468400806188583
step: 4100, Loss: 0.11532322317361832
step: 4200, Loss: 0.11578049510717392
step: 4300, Loss: 0.35585907101631165
step: 4400, Loss: 0.11924922466278076
step: 4500, Loss: 0.11565039306879044
step: 4600, Loss: 0.11431121826171875
step: 4700, Loss: 0.11562129855155945
step: 4800, Loss: 0.11544576287269592
step: 4900, Loss: 0.1138274073600769
step: 5000, Loss: 0.11589904874563217
step: 5100, Loss: 0.11627297848463058
step: 5200, Loss: 0.11415041983127594
step: 5300, Loss: 0.11491238325834274
step: 5400, Loss: 0.11451774835586548
step: 5500, Loss: 0.1144195944070816
step: 5600, Loss: 0.11347748339176178
step: 5700, Loss: 0.11457030475139618
step: 5800, Loss: 0.11307875066995621
step: 5900, Loss: 0.113240085542202
step: 6000, Loss: 0.11513632535934448
step: 6100, Loss: 0.11418477445840836
step: 6200, Loss: 0.11583336442708969
step: 6300, Loss: 0.11570679396390915
step: 6400, Loss: 0.11414308100938797
step: 6500, Loss: 0.11690250784158707
step: 6600, Loss: 0.3648819327354431
step: 6700, Loss: 0.1155424416065216
step: 6800, Loss: 0.11447776854038239
step: 6900, Loss: 0.11347617954015732
step: 7000, Loss: 0.11488644033670425
step: 7100, Loss: 0.11362921446561813
step: 7200, Loss: 0.11319252103567123
step: 7300, Loss: 0.11410091072320938
step: 7400, Loss: 0.11421694606542587
step: 7500, Loss: 0.11528012901544571
step: 7600, Loss: 0.11425673961639404
step: 7700, Loss: 0.11484947055578232
step: 7800, Loss: 0.11411282420158386
step: 7900, Loss: 0.11380219459533691
step: 8000, Loss: 0.11488795280456543
step: 8100, Loss: 0.11310669034719467
step: 7500, Loss: 0.11523638665676117
step: 7600, Loss: 0.11717551946640015
step: 7700, Loss: 0.11705255508422852
step: 7800, Loss: 0.1176576018333435
step: 7900, Loss: 0.11935433000326157
step: 8000, Loss: 0.1186583936214447
step: 8100, Loss: 0.12043476104736328
step: 8200, Loss: 0.11696527898311615
step: 8300, Loss: 0.11714009940624237
step: 8400, Loss: 0.11558618396520615
step: 8500, Loss: 0.11815735697746277
step: 8600, Loss: 0.11502766609191895
step: 8700, Loss: 0.11547724157571793
step: 8800, Loss: 0.1165473535656929
step: 8900, Loss: 0.11373946815729141
step: 9000, Loss: 0.11863438785076141
step: 9100, Loss: 0.1154826432466507
step: 9200, Loss: 0.1148519366979599
step: 9300, Loss: 0.11503884196281433
step: 9400, Loss: 0.11566296219825745
step: 9500, Loss: 0.11573111265897751
step: 9600, Loss: 0.11579015851020813
step: 9700, Loss: 0.11898992210626602
step: 9800, Loss: 0.11751443147659302
step: 9900, Loss: 0.11519908905029297
training successfully ended.
validating...
validate data length:31
acc: 0.6666666666666666
precision: 0.7058823529411765
recall: 0.7058823529411765
F_score: 0.7058823529411765
******fold 8******

Training... train_data length:281
step: 0, Loss: 12.683340072631836
step: 100, Loss: 0.1666676104068756
step: 200, Loss: 0.14928776025772095
step: 300, Loss: 0.12541218101978302
step: 400, Loss: 0.13053059577941895
step: 500, Loss: 0.1240953654050827
step: 600, Loss: 0.1286441683769226
step: 700, Loss: 0.11567956954240799
step: 800, Loss: 0.12000617384910583
step: 900, Loss: 0.11927786469459534
step: 1000, Loss: 0.11882840096950531
step: 1100, Loss: 0.11790528893470764
step: 1200, Loss: 0.12563109397888184
step: 1300, Loss: 0.11573208123445511
step: 1400, Loss: 0.11924266815185547
step: 1500, Loss: 0.11660823225975037
step: 1600, Loss: 0.11753224581480026
step: 1700, Loss: 0.11483250558376312
step: 1800, Loss: 0.12011617422103882
step: 1900, Loss: 0.11581540107727051
step: 2000, Loss: 0.12387225776910782
step: 2100, Loss: 0.11651122570037842
step: 2200, Loss: 0.11869718879461288
step: 2300, Loss: 0.11684302240610123
step: 2400, Loss: 0.11881793290376663
step: 2500, Loss: 0.11661070585250854
step: 2600, Loss: 0.12159990519285202
step: 2700, Loss: 0.11550548672676086
step: 2800, Loss: 0.12148091197013855
step: 2900, Loss: 0.1181325763463974
step: 3000, Loss: 0.11796285957098007
step: 3100, Loss: 0.12321441620588303
step: 3200, Loss: 0.11476767063140869
step: 3300, Loss: 0.11507973074913025
step: 3400, Loss: 0.11628256738185883
step: 3500, Loss: 0.1164318174123764
step: 3600, Loss: 0.11463786661624908
step: 3700, Loss: 0.11572086066007614
step: 3800, Loss: 0.11636098474264145
step: 3900, Loss: 0.11511576175689697
step: 4000, Loss: 0.1146007776260376
step: 4100, Loss: 0.1148422583937645
step: 4200, Loss: 0.11744926124811172
step: 4300, Loss: 0.11375181376934052
step: 4400, Loss: 0.11682327091693878
step: 4500, Loss: 0.11738923192024231
step: 4600, Loss: 0.11598905920982361
step: 4700, Loss: 0.11568644642829895
step: 4800, Loss: 0.11695968359708786
step: 4900, Loss: 0.11760737001895905
step: 5000, Loss: 0.11668179929256439
step: 5100, Loss: 0.11400763690471649
step: 5200, Loss: 0.11687518656253815
step: 5300, Loss: 0.11684233695268631
step: 5400, Loss: 0.11493706703186035
step: 5500, Loss: 0.11497523635625839
step: 5600, Loss: 0.11463437229394913
step: 5700, Loss: 0.1156700849533081
step: 5800, Loss: 0.11454378813505173
step: 5900, Loss: 1.1891393661499023
step: 6000, Loss: 0.1854003667831421
step: 6100, Loss: 0.13941997289657593
step: 6200, Loss: 0.14748889207839966
step: 6300, Loss: 0.12755443155765533
step: 6400, Loss: 0.1440020501613617
step: 6500, Loss: 0.12814119458198547
step: 6600, Loss: 0.13448312878608704
step: 6700, Loss: 0.14671416580677032
step: 6800, Loss: 0.1232801303267479
step: 6900, Loss: 0.11784471571445465
step: 7000, Loss: 0.1259411871433258
step: 7100, Loss: 0.12007070332765579
step: 7200, Loss: 0.12109395861625671
step: 7300, Loss: 0.11721299588680267
step: 7400, Loss: 0.12167157977819443
step: 7500, Loss: 0.1158091351389885
step: 7600, Loss: 0.12310504168272018
step: 7700, Loss: 0.1239309012889862
step: 7800, Loss: 0.11876712739467621
step: 7900, Loss: 0.12509053945541382
step: 8000, Loss: 0.11762544512748718
step: 8100, Loss: 0.1155736893415451
step: 8200, Loss: 0.12314645946025848
step: 8300, Loss: 0.11490211635828018
step: 8400, Loss: 0.1169876828789711
step: 8500, Loss: 0.1157342866063118
step: 8600, Loss: 0.11575861275196075
step: 8700, Loss: 0.11529109627008438
step: 8800, Loss: 0.11740425229072571
step: 8900, Loss: 0.11600623279809952
step: 9000, Loss: 0.11879459768533707
step: 9100, Loss: 0.1158485859632492
step: 9200, Loss: 0.11572901904582977
step: 9300, Loss: 0.11550501734018326
step: 9400, Loss: 0.11614008247852325
step: 9500, Loss: 0.11580938845872879
step: 9600, Loss: 0.1154177188873291
step: 9700, Loss: 0.11657875031232834
step: 9800, Loss: 0.11579335480928421
step: 9900, Loss: 0.11777862161397934
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7
recall: 0.9333333333333333
F_score: 0.8
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.5416357517242432
step: 100, Loss: 0.11451121419668198
step: 200, Loss: 0.11884782463312149
step: 300, Loss: 0.11573520302772522
step: 400, Loss: 0.11869218945503235
step: 500, Loss: 0.11426257342100143
step: 600, Loss: 0.11636576056480408
step: 700, Loss: 0.11543336510658264
step: 800, Loss: 0.11562356352806091
step: 900, Loss: 0.11455530673265457
step: 1000, Loss: 0.1148514598608017
step: 1100, Loss: 0.11481156200170517
step: 1200, Loss: 0.11512287706136703
step: 1300, Loss: 0.11373737454414368
step: 1400, Loss: 0.11412865668535233
step: 1500, Loss: 0.11462298035621643
step: 1600, Loss: 0.11471149325370789
step: 1700, Loss: 0.11366753280162811
step: 1800, Loss: 0.11537261307239532
step: 1900, Loss: 0.11530760675668716
step: 2000, Loss: 0.1146528348326683
step: 2100, Loss: 0.11473951488733292
step: 2200, Loss: 0.11353136599063873
step: 2300, Loss: 0.11354748904705048
step: 2400, Loss: 0.11505243182182312
step: 2500, Loss: 0.11560241878032684
step: 2600, Loss: 0.11682228744029999
step: 2700, Loss: 0.11519403010606766
step: 2800, Loss: 0.11610062420368195
step: 2900, Loss: 0.11422740668058395
step: 3000, Loss: 0.11500322073698044
step: 3100, Loss: 0.11380302160978317
step: 3200, Loss: 0.11580845713615417
step: 3300, Loss: 0.11438275873661041
step: 3400, Loss: 0.11441551893949509
step: 3500, Loss: 0.1133364886045456
step: 3600, Loss: 0.11522035300731659
step: 3700, Loss: 0.11419837921857834
step: 3800, Loss: 0.11505166441202164
step: 3900, Loss: 0.11643947660923004
step: 4000, Loss: 0.11658743023872375
step: 4100, Loss: 2.8692426681518555
step: 4200, Loss: 0.1475048065185547
step: 4300, Loss: 0.12710930407047272
step: 4400, Loss: 0.1289249062538147
step: 4500, Loss: 0.1308782398700714
step: 4600, Loss: 0.12170986086130142
step: 4700, Loss: 0.11906768381595612
step: 4800, Loss: 0.11812015622854233
step: 4900, Loss: 0.12063133716583252
step: 5000, Loss: 0.12340058386325836
step: 5100, Loss: 0.11631940305233002
step: 5200, Loss: 0.12586742639541626
step: 5300, Loss: 0.11607702821493149
step: 5400, Loss: 0.1215393915772438
step: 5500, Loss: 0.11711551249027252
step: 5600, Loss: 0.1166347786784172
step: 5700, Loss: 0.11399908363819122
step: 5800, Loss: 0.12011934071779251
step: 5900, Loss: 0.11682865023612976
step: 6000, Loss: 0.11806241422891617
step: 6100, Loss: 0.11751708388328552
step: 6200, Loss: 0.11462868750095367
step: 6300, Loss: 0.11474286019802094
step: 6400, Loss: 0.117102712392807
step: 6500, Loss: 0.11590506136417389
step: 6600, Loss: 0.11962196230888367
step: 6700, Loss: 0.11588378995656967
step: 6800, Loss: 0.1164885014295578
step: 6900, Loss: 0.11585638672113419
step: 7000, Loss: 0.1160392239689827
step: 7100, Loss: 0.11480803787708282
step: 7200, Loss: 0.11543464660644531
step: 7300, Loss: 0.11380469799041748
step: 7400, Loss: 0.11784917861223221
step: 7500, Loss: 0.11541426181793213
step: 7600, Loss: 0.11788837611675262
step: 7700, Loss: 0.11418190598487854
step: 7800, Loss: 0.11721640825271606
step: 7900, Loss: 0.11428166925907135
step: 8200, Loss: 0.11358252167701721
step: 8300, Loss: 0.11319593340158463
step: 8400, Loss: 0.11466620117425919
step: 8500, Loss: 0.11418823152780533
step: 8600, Loss: 0.11409587413072586
step: 8700, Loss: 0.11438839882612228
step: 8800, Loss: 0.11474847048521042
step: 8900, Loss: 0.3829745650291443
step: 9000, Loss: 0.11723028123378754
step: 9100, Loss: 0.11543954908847809
step: 9200, Loss: 0.11329475790262222
step: 9300, Loss: 0.11538408696651459
step: 9400, Loss: 0.1129830852150917
step: 9500, Loss: 0.11329053342342377
step: 9600, Loss: 0.11350728571414948
step: 9700, Loss: 0.11538805812597275
step: 9800, Loss: 3.4052140712738037
step: 9900, Loss: 0.16540569067001343
training successfully ended.
validating...
validate data length:91
acc: 0.8977272727272727
precision: 0.9210526315789473
recall: 0.8536585365853658
F_score: 0.8860759493670887
******fold 10******

Training... train_data length:821
step: 0, Loss: 0.14135664701461792
step: 100, Loss: 0.1208050549030304
step: 200, Loss: 0.11335013061761856
step: 300, Loss: 0.11469964683055878
step: 400, Loss: 0.11407776921987534
step: 500, Loss: 0.11465997248888016
step: 600, Loss: 0.11278938502073288
step: 700, Loss: 0.11459777504205704
step: 800, Loss: 0.11448761075735092
step: 900, Loss: 0.11487433314323425
step: 1000, Loss: 0.11399024724960327
step: 1100, Loss: 0.11413764953613281
step: 1200, Loss: 2.55655574798584
step: 1300, Loss: 0.14668452739715576
step: 1400, Loss: 0.13232824206352234
step: 1500, Loss: 0.12796485424041748
step: 1600, Loss: 0.12377333641052246
step: 1700, Loss: 0.1190495640039444
step: 1800, Loss: 0.117347352206707
step: 1900, Loss: 0.12023037672042847
step: 2000, Loss: 0.3567311763763428
step: 2100, Loss: 0.11742717772722244
step: 2200, Loss: 0.11933016777038574
step: 2300, Loss: 0.11747931689023972
step: 2400, Loss: 0.12144297361373901
step: 2500, Loss: 0.1152941957116127
step: 2600, Loss: 0.11753614246845245
step: 2700, Loss: 0.11869078874588013
step: 2800, Loss: 0.11562249064445496
step: 2900, Loss: 0.11821918189525604
step: 3000, Loss: 0.11594145745038986
step: 3100, Loss: 0.11509431898593903
step: 3200, Loss: 0.11540928483009338
step: 3300, Loss: 0.11469104886054993
step: 3400, Loss: 0.11530447751283646
step: 3500, Loss: 0.11453696340322495
step: 3600, Loss: 0.1144791841506958
step: 3700, Loss: 0.1162436231970787
step: 3800, Loss: 0.1135992705821991
step: 3900, Loss: 0.11488792300224304
step: 4000, Loss: 0.11306446045637131
step: 4100, Loss: 0.11412154138088226
step: 4200, Loss: 0.11502529680728912
step: 4300, Loss: 0.3524848222732544
step: 4400, Loss: 0.11578622460365295
step: 4500, Loss: 0.11511678993701935
step: 4600, Loss: 0.113507941365242
step: 4700, Loss: 0.11671526730060577
step: 4800, Loss: 0.11458353698253632
step: 4900, Loss: 0.11724144220352173
step: 5000, Loss: 0.11419633775949478
step: 5100, Loss: 0.11411147564649582
step: 5200, Loss: 0.11441131681203842
step: 5300, Loss: 0.11442921310663223
step: 5400, Loss: 0.11419632285833359
step: 5500, Loss: 0.11322494596242905
step: 5600, Loss: 0.11566406488418579
step: 5700, Loss: 0.11374550312757492
step: 5800, Loss: 0.11470451951026917
step: 5900, Loss: 0.11351484060287476
step: 6000, Loss: 0.11336033046245575
step: 6100, Loss: 0.11365672200918198
step: 6200, Loss: 0.11790506541728973
step: 6300, Loss: 0.11304377764463425
step: 6400, Loss: 0.11423933506011963
step: 6500, Loss: 0.11403851956129074
step: 6600, Loss: 0.3617337942123413
step: 6700, Loss: 0.11381126940250397
step: 6800, Loss: 0.11406031250953674
step: 6900, Loss: 0.11357377469539642
step: 7000, Loss: 0.11295632272958755
step: 7100, Loss: 0.11413699388504028
step: 7200, Loss: 0.11338396370410919
step: 7300, Loss: 0.11508570611476898
step: 7400, Loss: 0.11328376829624176
step: 7500, Loss: 0.11431461572647095
step: 7600, Loss: 0.11447083950042725
step: 7700, Loss: 0.11392576992511749
step: 7800, Loss: 0.1139734536409378
step: 7900, Loss: 0.11368101835250854
step: 8000, Loss: 0.11482929438352585
step: 8100, Loss: 0.11322782188653946
step: 8200, Loss: 0.11442539095878601
step: 8300, Loss: 0.11549149453639984
step: 8400, Loss: 0.11387553066015244
step: 8500, Loss: 0.11403532326221466
step: 8600, Loss: 0.11385062336921692
step: 8700, Loss: 0.11488862335681915
step: 8800, Loss: 0.11323653161525726
step: 8900, Loss: 0.6466473340988159
step: 9000, Loss: 0.14352506399154663
step: 9100, Loss: 0.1577371060848236
step: 9200, Loss: 0.12925028800964355
step: 9300, Loss: 0.12886576354503632
step: 9400, Loss: 0.12128008902072906
step: 9500, Loss: 0.11923307180404663
step: 9600, Loss: 0.12568636238574982
step: 9700, Loss: 0.11897124350070953
step: 9800, Loss: 0.11940816789865494
step: 9900, Loss: 0.12664617598056793
training successfully ended.
validating...
validate data length:91
acc: 0.9318181818181818
precision: 0.9574468085106383
recall: 0.9183673469387755
F_score: 0.9375000000000001
subject 3 Avgacc: 0.9261363636363636 Avgfscore: 0.9235489213358411 
 Max acc:0.9886363636363636, Max f score:0.99009900990099
******** mix subject_4 ********

[247, 513]
******fold 1******

Training... train_data length:923
step: 0, Loss: 47.04888153076172
step: 100, Loss: 4.513730049133301
step: 200, Loss: 2.6177730560302734
step: 300, Loss: 0.1692291647195816
step: 400, Loss: 0.17323489487171173
step: 500, Loss: 0.15549370646476746
step: 600, Loss: 0.1521582305431366
step: 700, Loss: 0.15480956435203552
step: 800, Loss: 0.15459319949150085
step: 900, Loss: 0.14246243238449097
step: 1000, Loss: 0.14404083788394928
step: 1100, Loss: 0.13375608623027802
step: 1200, Loss: 0.13979612290859222
step: 1300, Loss: 0.13351592421531677
step: 1400, Loss: 0.13247478008270264
step: 1500, Loss: 0.1277519017457962
step: 1600, Loss: 0.1266651153564453
step: 1700, Loss: 0.1285209357738495
step: 1800, Loss: 0.12228954583406448
step: 1900, Loss: 0.1252005696296692
step: 2000, Loss: 0.12284725904464722
step: 2100, Loss: 0.12154072523117065
step: 2200, Loss: 0.12108331173658371
step: 2300, Loss: 0.12106701731681824
step: 2400, Loss: 0.11966171860694885
step: 2500, Loss: 0.12007159739732742
step: 2600, Loss: 0.11872237920761108
step: 2700, Loss: 0.11837181448936462
step: 2800, Loss: 0.11764822900295258
step: 2900, Loss: 0.11633018404245377
step: 3000, Loss: 0.11673603951931
step: 3100, Loss: 0.11605337262153625
step: 3200, Loss: 0.11617151647806168
step: 3300, Loss: 0.11670839041471481
step: 3400, Loss: 0.11445792764425278
step: 3500, Loss: 0.11391778290271759
step: 3600, Loss: 0.11548317968845367
step: 3700, Loss: 0.1147809773683548
step: 3800, Loss: 0.113448366522789
step: 3900, Loss: 0.1155872642993927
step: 4000, Loss: 0.11545025557279587
step: 4100, Loss: 0.11361236870288849
step: 4200, Loss: 0.11729520559310913
step: 4300, Loss: 0.11484216153621674
step: 4400, Loss: 0.11924198269844055
step: 4500, Loss: 0.12373426556587219
step: 4600, Loss: 2.9957633018493652
step: 4700, Loss: 0.7482557892799377
step: 4800, Loss: 0.1630975753068924
step: 4900, Loss: 0.143002450466156
step: 5000, Loss: 0.14116723835468292
step: 5100, Loss: 0.133057102560997
step: 5200, Loss: 0.12986719608306885
step: 5300, Loss: 0.13039851188659668
step: 5400, Loss: 0.1260870099067688
step: 5500, Loss: 0.12820065021514893
step: 5600, Loss: 0.12686766684055328
step: 5700, Loss: 0.12517137825489044
step: 5800, Loss: 0.12332053482532501
step: 5900, Loss: 0.12175402045249939
step: 6000, Loss: 0.12245780229568481
step: 6100, Loss: 0.12309222668409348
step: 6200, Loss: 0.11801803857088089
step: 6300, Loss: 0.11880248785018921
step: 6400, Loss: 0.12151117622852325
step: 6500, Loss: 0.12317907810211182
step: 6600, Loss: 0.1177452951669693
step: 6700, Loss: 0.11701932549476624
step: 6800, Loss: 0.1176738366484642
step: 6900, Loss: 0.12066486477851868
step: 7000, Loss: 0.11767824739217758
step: 7100, Loss: 0.11830633133649826
step: 7200, Loss: 0.11593706160783768
step: 7300, Loss: 0.11752347648143768
step: 7400, Loss: 0.11671125888824463
step: 7500, Loss: 0.11755762994289398
step: 7600, Loss: 0.11682230979204178
step: 7700, Loss: 0.11587756127119064
step: 7800, Loss: 0.1153445914387703
step: 7900, Loss: 0.11651681363582611
step: 8000, Loss: 0.11512947082519531
step: 8100, Loss: 0.11555051058530807
step: 8000, Loss: 0.11474572867155075
step: 8100, Loss: 0.11396346241235733
step: 8200, Loss: 0.11519581079483032
step: 8300, Loss: 0.11437461525201797
step: 8400, Loss: 0.11522317677736282
step: 8500, Loss: 0.11430162191390991
step: 8600, Loss: 0.11470665782690048
step: 8700, Loss: 0.11375099420547485
step: 8800, Loss: 0.11720742285251617
step: 8900, Loss: 0.11462968587875366
step: 9000, Loss: 0.11506155878305435
step: 9100, Loss: 0.11478494852781296
step: 9200, Loss: 0.11429621279239655
step: 9300, Loss: 0.11332448571920395
step: 9400, Loss: 0.11576720327138901
step: 9500, Loss: 0.11652666330337524
step: 9600, Loss: 0.11591293662786484
step: 9700, Loss: 0.11581169813871384
step: 9800, Loss: 0.11583681404590607
step: 9900, Loss: 0.11394588649272919
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.9285714285714286
recall: 0.7647058823529411
F_score: 0.8387096774193549
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.67852783203125
step: 100, Loss: 0.11700659990310669
step: 200, Loss: 0.11980642378330231
step: 300, Loss: 0.11500118672847748
step: 400, Loss: 0.11453177034854889
step: 500, Loss: 0.11465076357126236
step: 600, Loss: 0.11473870277404785
step: 700, Loss: 0.11598114669322968
step: 800, Loss: 0.11437603831291199
step: 900, Loss: 0.11416114121675491
step: 1000, Loss: 0.11573371291160583
step: 1100, Loss: 0.114716537296772
step: 1200, Loss: 0.11440403759479523
step: 1300, Loss: 0.11662983894348145
step: 1400, Loss: 0.1168186366558075
step: 1500, Loss: 0.11454226076602936
step: 1600, Loss: 0.11478743702173233
step: 1700, Loss: 0.11504403501749039
step: 1800, Loss: 0.1160290464758873
step: 1900, Loss: 0.11352387070655823
step: 2000, Loss: 0.11490163952112198
step: 2100, Loss: 0.11418347805738449
step: 2200, Loss: 0.1132344901561737
step: 2300, Loss: 0.11516600847244263
step: 2400, Loss: 0.11515536904335022
step: 2500, Loss: 0.1147664487361908
step: 2600, Loss: 0.1138661652803421
step: 2700, Loss: 0.11416763067245483
step: 2800, Loss: 0.11383114755153656
step: 2900, Loss: 0.11448013782501221
step: 3000, Loss: 0.11328274011611938
step: 3100, Loss: 0.11559056490659714
step: 3200, Loss: 0.11351711302995682
step: 3300, Loss: 0.11346469074487686
step: 3400, Loss: 0.11548428237438202
step: 3500, Loss: 0.11351080983877182
step: 3600, Loss: 0.1138325035572052
step: 3700, Loss: 0.11563801020383835
step: 3800, Loss: 0.1143563911318779
step: 3900, Loss: 0.11350882053375244
step: 4000, Loss: 0.11471041291952133
step: 4100, Loss: 0.5835825800895691
step: 4200, Loss: 0.1578403115272522
step: 4300, Loss: 0.12802496552467346
step: 4400, Loss: 0.12802433967590332
step: 4500, Loss: 0.12250597774982452
step: 4600, Loss: 0.13022536039352417
step: 4700, Loss: 0.12696866691112518
step: 4800, Loss: 0.12859892845153809
step: 4900, Loss: 0.11766768991947174
step: 5000, Loss: 0.12127628922462463
step: 5100, Loss: 0.11814939230680466
step: 5200, Loss: 0.12724870443344116
step: 5300, Loss: 0.11704634130001068
step: 5400, Loss: 0.12455759942531586
step: 5500, Loss: 0.11773242056369781
step: 5600, Loss: 0.12156785279512405
step: 5700, Loss: 0.11716417223215103
step: 5800, Loss: 0.12604555487632751
step: 5900, Loss: 0.11667566001415253
step: 6000, Loss: 0.12074870616197586
step: 6100, Loss: 0.11539091914892197
step: 6200, Loss: 0.11755424737930298
step: 6300, Loss: 0.11534591764211655
step: 6400, Loss: 0.11491121351718903
step: 6500, Loss: 0.11388890445232391
step: 6600, Loss: 0.11520151048898697
step: 6700, Loss: 0.11499787867069244
step: 6800, Loss: 0.11650171130895615
step: 6900, Loss: 0.11412528157234192
step: 7000, Loss: 0.11466948688030243
step: 7100, Loss: 0.11678434908390045
step: 7200, Loss: 0.11640936136245728
step: 7300, Loss: 0.11362064629793167
step: 7400, Loss: 0.11531560122966766
step: 7500, Loss: 0.11420757323503494
step: 7600, Loss: 0.11600880324840546
step: 7700, Loss: 0.11459901183843613
step: 7800, Loss: 0.11342332512140274
step: 7900, Loss: 0.11412401497364044
step: 8000, Loss: 0.11596029251813889
step: 8100, Loss: 0.11435717344284058
step: 8200, Loss: 0.1149732694029808
step: 8300, Loss: 0.11340721696615219
step: 8400, Loss: 0.11480584740638733
step: 8500, Loss: 0.11508365720510483
step: 8600, Loss: 0.1154092326760292
step: 8700, Loss: 0.11416971683502197
step: 8800, Loss: 0.11478175222873688
step: 8900, Loss: 0.11371207237243652
step: 9000, Loss: 0.11513897776603699
step: 9100, Loss: 0.11366166174411774
step: 9200, Loss: 0.11364593356847763
step: 9300, Loss: 0.11670216172933578
step: 9400, Loss: 0.11318255215883255
step: 9500, Loss: 0.11557017266750336
step: 9600, Loss: 0.11367252469062805
step: 9700, Loss: 0.11471682786941528
step: 9800, Loss: 0.1134863942861557
step: 9900, Loss: 0.11382076144218445
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8
recall: 1.0
F_score: 0.888888888888889
subject 3 Avgacc: 0.7720833333333333 Avgfscore: 0.7840839405165781 
 Max acc:0.9, Max f score:0.9032258064516129
******** mix subject_4 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 50.94630813598633
step: 100, Loss: 1.0516388416290283
step: 200, Loss: 0.13700440526008606
step: 300, Loss: 0.13866131007671356
step: 400, Loss: 0.12099148333072662
step: 500, Loss: 0.12414604425430298
step: 600, Loss: 0.11797207593917847
step: 700, Loss: 0.11789992451667786
step: 800, Loss: 0.11637933552265167
step: 900, Loss: 0.11872631311416626
step: 1000, Loss: 0.11669222265481949
step: 1100, Loss: 0.11851530522108078
step: 1200, Loss: 0.11679400503635406
step: 1300, Loss: 0.11880835145711899
step: 1400, Loss: 0.11526542156934738
step: 1500, Loss: 0.11407753825187683
step: 1600, Loss: 0.11943258345127106
step: 1700, Loss: 0.11587107926607132
step: 1800, Loss: 0.11360069364309311
step: 1900, Loss: 0.11368970572948456
step: 2000, Loss: 0.11510644853115082
step: 2100, Loss: 0.11437304317951202
step: 2200, Loss: 0.11436150968074799
step: 2300, Loss: 0.11394087225198746
step: 2400, Loss: 0.11404580622911453
step: 2500, Loss: 0.11544521898031235
step: 2600, Loss: 0.11592437326908112
step: 2700, Loss: 0.11600261181592941
step: 2800, Loss: 0.11355014890432358
step: 2900, Loss: 0.11602456867694855
step: 3000, Loss: 0.11350198090076447
step: 3100, Loss: 0.1144242212176323
step: 3200, Loss: 0.11461284011602402
step: 3300, Loss: 0.11509236693382263
step: 3400, Loss: 0.11459844559431076
step: 3500, Loss: 0.11501765996217728
step: 3600, Loss: 0.11593089252710342
step: 3700, Loss: 0.11478620767593384
step: 3800, Loss: 0.1138206273317337
step: 3900, Loss: 0.11837971210479736
step: 4000, Loss: 0.1157577782869339
step: 4100, Loss: 0.11571113765239716
step: 4200, Loss: 0.11558131128549576
step: 4300, Loss: 0.11372963339090347
step: 4400, Loss: 0.11538318544626236
step: 4500, Loss: 0.11481812596321106
step: 4600, Loss: 0.11524558812379837
step: 4700, Loss: 0.11498364061117172
step: 4800, Loss: 0.11406952142715454
step: 4900, Loss: 0.11339403688907623
step: 5000, Loss: 0.11497783660888672
step: 5100, Loss: 0.11546634137630463
step: 5200, Loss: 0.11383181065320969
step: 5300, Loss: 0.1139296367764473
step: 5400, Loss: 0.11380761861801147
step: 5500, Loss: 0.12020255625247955
step: 5600, Loss: 0.11748778820037842
step: 5700, Loss: 0.11397142708301544
step: 5800, Loss: 0.11414888501167297
step: 5900, Loss: 0.11326370388269424
step: 6000, Loss: 0.11523143202066422
step: 6100, Loss: 0.11585575342178345
step: 6200, Loss: 0.1142188161611557
step: 6300, Loss: 0.11385972797870636
step: 6400, Loss: 0.11499299108982086
step: 6500, Loss: 0.11564446985721588
step: 6600, Loss: 0.11403217166662216
step: 6700, Loss: 0.11795161664485931
step: 6800, Loss: 0.11338113248348236
step: 6900, Loss: 0.11406657099723816
step: 7000, Loss: 0.11533062905073166
step: 7100, Loss: 0.20009984076023102
step: 7200, Loss: 0.12444540858268738
step: 7300, Loss: 0.1226600781083107
step: 7400, Loss: 0.1194196343421936
step: 7500, Loss: 0.1272272765636444
step: 7600, Loss: 0.11951574683189392
step: 7700, Loss: 0.12328868359327316
step: 7800, Loss: 0.11628971248865128
step: 7900, Loss: 0.12111745029687881
step: 8000, Loss: 0.11889061331748962
step: 8200, Loss: 0.11569235473871231
step: 8300, Loss: 0.11385747045278549
step: 8400, Loss: 0.11451198905706406
step: 8500, Loss: 0.11515647172927856
step: 8600, Loss: 0.11470533907413483
step: 8700, Loss: 0.11500123888254166
step: 8800, Loss: 0.11452572792768478
step: 8900, Loss: 0.11494048684835434
step: 9000, Loss: 0.11503968387842178
step: 9100, Loss: 0.11385545879602432
step: 9200, Loss: 0.1148347407579422
step: 9300, Loss: 0.11509213596582413
step: 9400, Loss: 0.11443021148443222
step: 9500, Loss: 0.11397528648376465
step: 9600, Loss: 0.11368750780820847
step: 9700, Loss: 0.11461738497018814
step: 9800, Loss: 0.11391275376081467
step: 9900, Loss: 0.11385007947683334
training successfully ended.
validating...
validate data length:103
acc: 0.8645833333333334
precision: 0.7678571428571429
recall: 1.0
F_score: 0.8686868686868687
******fold 2******

Training... train_data length:923
step: 0, Loss: 1.3202999830245972
step: 100, Loss: 0.12428294122219086
step: 200, Loss: 0.12091443687677383
step: 300, Loss: 0.11894375085830688
step: 400, Loss: 0.11771008372306824
step: 500, Loss: 0.11587648093700409
step: 600, Loss: 0.11596079915761948
step: 700, Loss: 0.11534716188907623
step: 800, Loss: 0.11620941013097763
step: 900, Loss: 0.11473292112350464
step: 1000, Loss: 0.11352185904979706
step: 1100, Loss: 0.11454913765192032
step: 1200, Loss: 0.11422378569841385
step: 1300, Loss: 0.11316955089569092
step: 1400, Loss: 0.11312447488307953
step: 1500, Loss: 0.11350888013839722
step: 1600, Loss: 0.11509886384010315
step: 1700, Loss: 0.11321642249822617
step: 1800, Loss: 0.1155857965350151
step: 1900, Loss: 0.11420206725597382
step: 2000, Loss: 0.11387915909290314
step: 2100, Loss: 0.11346287280321121
step: 2200, Loss: 0.11567752063274384
step: 2300, Loss: 0.11582281440496445
step: 2400, Loss: 0.11910843849182129
step: 2500, Loss: 0.11619643867015839
step: 2600, Loss: 0.116300068795681
step: 2700, Loss: 0.12014693021774292
step: 2800, Loss: 0.11800488084554672
step: 2900, Loss: 0.12105944007635117
step: 3000, Loss: 0.12416069954633713
step: 3100, Loss: 0.12426039576530457
step: 3200, Loss: 0.7742921113967896
step: 3300, Loss: 0.16414666175842285
step: 3400, Loss: 0.14229466021060944
step: 3500, Loss: 0.1416558176279068
step: 3600, Loss: 0.12975046038627625
step: 3700, Loss: 0.13612893223762512
step: 3800, Loss: 0.13168606162071228
step: 3900, Loss: 0.13373661041259766
step: 4000, Loss: 0.1370163857936859
step: 4100, Loss: 0.12645475566387177
step: 4200, Loss: 0.12240669876337051
step: 4300, Loss: 0.12489505112171173
step: 4400, Loss: 0.12228938937187195
step: 4500, Loss: 0.12136294692754745
step: 4600, Loss: 0.11811971664428711
step: 4700, Loss: 0.11858977377414703
step: 4800, Loss: 0.1171644851565361
step: 4900, Loss: 0.11726029217243195
step: 5000, Loss: 0.11596687138080597
step: 5100, Loss: 0.11595579981803894
step: 5200, Loss: 0.1171884536743164
step: 5300, Loss: 0.11734470725059509
step: 5400, Loss: 0.11493710428476334
step: 5500, Loss: 0.11597616970539093
step: 5600, Loss: 0.11621274054050446
step: 5700, Loss: 0.11396849900484085
step: 5800, Loss: 0.11676084250211716
step: 5900, Loss: 0.115389883518219
step: 6000, Loss: 0.11586657911539078
step: 6100, Loss: 0.11472497880458832
step: 6200, Loss: 0.11405806243419647
step: 6300, Loss: 0.1150776818394661
step: 6400, Loss: 0.11470558494329453
step: 6500, Loss: 0.11422188580036163
step: 6600, Loss: 0.11472469568252563
step: 6700, Loss: 0.11391396820545197
step: 6800, Loss: 0.11611808836460114
step: 6900, Loss: 0.11367562413215637
step: 7000, Loss: 0.11522693186998367
step: 7100, Loss: 0.1137622594833374
step: 7200, Loss: 0.1161579042673111
step: 7300, Loss: 0.11415253579616547
step: 7400, Loss: 0.11400803178548813
step: 7500, Loss: 0.1142922043800354
step: 7600, Loss: 0.11438417434692383
step: 7700, Loss: 0.11462411284446716
step: 7800, Loss: 0.11461973935365677
step: 7900, Loss: 0.11459522694349289
step: 8000, Loss: 0.11359637975692749
step: 8100, Loss: 0.11445468664169312
step: 8200, Loss: 0.11461777240037918
step: 8300, Loss: 0.11385688930749893
step: 8400, Loss: 0.1130724847316742
step: 8500, Loss: 0.11325349658727646
step: 8600, Loss: 0.11453420668840408
step: 8700, Loss: 0.11385973542928696
step: 8800, Loss: 0.1143968254327774
step: 8900, Loss: 0.11535214632749557
step: 9000, Loss: 0.11445507407188416
step: 9100, Loss: 0.11611741781234741
step: 9200, Loss: 0.11515549570322037
step: 9300, Loss: 0.11618677526712418
step: 9400, Loss: 0.11670459806919098
step: 9500, Loss: 0.11432930827140808
step: 9600, Loss: 0.11366860568523407
step: 9700, Loss: 0.11375513672828674
step: 9800, Loss: 0.11416023969650269
step: 9900, Loss: 0.11637398600578308
training successfully ended.
validating...
validate data length:103
acc: 0.9791666666666666
precision: 0.9574468085106383
recall: 1.0
F_score: 0.9782608695652174
******fold 3******

Training... train_data length:923
step: 0, Loss: 0.12108913064002991
step: 100, Loss: 0.12329046428203583
step: 200, Loss: 0.1171034574508667
step: 300, Loss: 0.11532587558031082
step: 400, Loss: 0.11445271223783493
step: 500, Loss: 0.11609436571598053
step: 600, Loss: 0.11584950238466263
step: 700, Loss: 0.11384499073028564
step: 800, Loss: 0.11398538947105408
step: 900, Loss: 0.11429236084222794
step: 1000, Loss: 0.11329137533903122
step: 1100, Loss: 0.11303406953811646
step: 1200, Loss: 0.11348949372768402
step: 1300, Loss: 0.11468669772148132
step: 1400, Loss: 0.11337634176015854
step: 1500, Loss: 0.11436128616333008
step: 1600, Loss: 0.11317609995603561
step: 1700, Loss: 0.11377143114805222
step: 1800, Loss: 0.11298488825559616
step: 1900, Loss: 0.11321927607059479
step: 2000, Loss: 0.11380992084741592
step: 2100, Loss: 0.11314081400632858
step: 2200, Loss: 0.11324696987867355
step: 2300, Loss: 0.11332033574581146
step: 2400, Loss: 0.11498519778251648
step: 2500, Loss: 0.11516769230365753
step: 2600, Loss: 0.11482885479927063
step: 2700, Loss: 0.11396172642707825
step: 2800, Loss: 0.11437144875526428
step: 2900, Loss: 0.11500854045152664
step: 3000, Loss: 0.11594177037477493
step: 3100, Loss: 0.1138857901096344
step: 3200, Loss: 0.11550112068653107
step: 3300, Loss: 0.11427426338195801
step: 3400, Loss: 0.11348340660333633
step: 3500, Loss: 0.11584033071994781
step: 3600, Loss: 2.1319069862365723
step: 3700, Loss: 0.1900225728750229
step: 3800, Loss: 0.13222335278987885
step: 3900, Loss: 0.13473139703273773
step: 4000, Loss: 0.14052745699882507
step: 4100, Loss: 0.12894222140312195
step: 4200, Loss: 0.12259896099567413
step: 4300, Loss: 0.12741558253765106
step: 4400, Loss: 0.1258740872144699
step: 4500, Loss: 0.1251985877752304
step: 4600, Loss: 0.12238123267889023
step: 4700, Loss: 0.12897755205631256
step: 4800, Loss: 0.11999070644378662
step: 4900, Loss: 0.12234743684530258
step: 5000, Loss: 0.1214381605386734
step: 5100, Loss: 0.1177956759929657
step: 5200, Loss: 0.11744093894958496
step: 5300, Loss: 0.11821430921554565
step: 5400, Loss: 0.11704607307910919
step: 5500, Loss: 0.1186685562133789
step: 5600, Loss: 0.11681371927261353
step: 5700, Loss: 0.11667342483997345
step: 5800, Loss: 0.11711351573467255
step: 5900, Loss: 0.11695078760385513
step: 6000, Loss: 0.11633170396089554
step: 6100, Loss: 0.11657413840293884
step: 6200, Loss: 0.11617325246334076
step: 6300, Loss: 0.11573491990566254
step: 6400, Loss: 0.11635786294937134
step: 6500, Loss: 0.11573001742362976
step: 6600, Loss: 0.11570912599563599
step: 6700, Loss: 0.114547960460186
step: 6800, Loss: 0.11522895842790604
step: 6900, Loss: 0.1151462197303772
step: 7000, Loss: 0.11454159021377563
step: 7100, Loss: 0.11507708579301834
step: 7200, Loss: 0.11580584198236465
step: 7300, Loss: 0.11476956307888031
step: 7400, Loss: 0.11507414281368256
step: 7500, Loss: 0.1142408549785614
step: 7600, Loss: 0.11467055976390839
step: 7700, Loss: 0.1151331439614296
step: 7800, Loss: 0.11383646726608276
step: 7900, Loss: 0.11445491760969162
step: 8000, Loss: 0.11338522285223007
step: 8100, Loss: 0.11398633569478989
step: 8200, Loss: 0.11313334852457047
step: 8300, Loss: 0.11418880522251129
step: 8400, Loss: 0.11342358589172363
step: 8500, Loss: 0.11341176182031631
step: 8600, Loss: 0.1133517473936081
step: 8100, Loss: 0.11666266620159149
step: 8200, Loss: 0.11664794385433197
step: 8300, Loss: 0.11794597655534744
step: 8400, Loss: 0.11541233956813812
step: 8500, Loss: 0.12132829427719116
step: 8600, Loss: 0.11761311441659927
step: 8700, Loss: 0.11940442770719528
step: 8800, Loss: 0.1177627295255661
step: 8900, Loss: 0.11527121067047119
step: 9000, Loss: 0.11513003706932068
step: 9100, Loss: 0.11495143920183182
step: 9200, Loss: 0.11444797366857529
step: 9300, Loss: 0.11561644077301025
step: 9400, Loss: 0.11459079384803772
step: 9500, Loss: 0.11543231457471848
step: 9600, Loss: 0.11466298997402191
step: 9700, Loss: 0.1151040717959404
step: 9800, Loss: 0.11408822238445282
step: 9900, Loss: 0.11535570025444031
training successfully ended.
validating...
validate data length:32
acc: 0.46875
precision: 0.3684210526315789
recall: 0.5833333333333334
F_score: 0.4516129032258065
******fold 2******

Training... train_data length:280
step: 0, Loss: 3.7684905529022217
step: 100, Loss: 0.19725272059440613
step: 200, Loss: 0.11882296204566956
step: 300, Loss: 0.12192965298891068
step: 400, Loss: 0.11789744347333908
step: 500, Loss: 0.11635610461235046
step: 600, Loss: 0.11615544557571411
step: 700, Loss: 0.11738237738609314
step: 800, Loss: 0.11851822584867477
step: 900, Loss: 0.11739861220121384
step: 1000, Loss: 0.11525969952344894
step: 1100, Loss: 0.11359986662864685
step: 1200, Loss: 0.116744264960289
step: 1300, Loss: 0.1153603196144104
step: 1400, Loss: 0.11414173245429993
step: 1500, Loss: 0.11992239952087402
step: 1600, Loss: 0.1148873120546341
step: 1700, Loss: 0.11483735591173172
step: 1800, Loss: 0.11386603116989136
step: 1900, Loss: 0.16433250904083252
step: 2000, Loss: 0.1261678785085678
step: 2100, Loss: 0.1322682499885559
step: 2200, Loss: 0.12194962054491043
step: 2300, Loss: 0.12436211109161377
step: 2400, Loss: 0.12113945186138153
step: 2500, Loss: 0.12235335260629654
step: 2600, Loss: 0.11664078384637833
step: 2700, Loss: 0.12553206086158752
step: 2800, Loss: 0.1272130161523819
step: 2900, Loss: 0.12150433659553528
step: 3000, Loss: 0.1258874535560608
step: 3100, Loss: 0.11969079822301865
step: 3200, Loss: 0.11467969417572021
step: 3300, Loss: 0.12219082564115524
step: 3400, Loss: 0.1196444183588028
step: 3500, Loss: 0.11555233597755432
step: 3600, Loss: 0.12088742107152939
step: 3700, Loss: 0.11901172995567322
step: 3800, Loss: 0.11705359816551208
step: 3900, Loss: 0.11491002887487411
step: 4000, Loss: 0.11659686267375946
step: 4100, Loss: 0.11707192659378052
step: 4200, Loss: 0.11822812259197235
step: 4300, Loss: 0.11645498126745224
step: 4400, Loss: 0.11487400531768799
step: 4500, Loss: 0.11566741019487381
step: 4600, Loss: 0.1144709661602974
step: 4700, Loss: 0.11871396005153656
step: 4800, Loss: 0.11769583821296692
step: 4900, Loss: 0.11671311408281326
step: 5000, Loss: 0.12516812980175018
step: 5100, Loss: 0.1176026463508606
step: 5200, Loss: 0.11686085164546967
step: 5300, Loss: 0.11539481580257416
step: 5400, Loss: 0.11568474769592285
step: 5500, Loss: 0.11447405070066452
step: 5600, Loss: 0.11569844186306
step: 5700, Loss: 0.11480015516281128
step: 5800, Loss: 0.1150350421667099
step: 5900, Loss: 0.11461136490106583
step: 6000, Loss: 0.11603156477212906
step: 6100, Loss: 0.11990605294704437
step: 6200, Loss: 0.11736822128295898
step: 6300, Loss: 0.11580445617437363
step: 6400, Loss: 0.11371984332799911
step: 6500, Loss: 0.1132594421505928
step: 6600, Loss: 0.11509730666875839
step: 6700, Loss: 0.1142096221446991
step: 6800, Loss: 0.11653035879135132
step: 6900, Loss: 0.11482829600572586
step: 7000, Loss: 0.11491691321134567
step: 7100, Loss: 0.11673252284526825
step: 7200, Loss: 0.11277242004871368
step: 7300, Loss: 0.11733801662921906
step: 7400, Loss: 0.11382024735212326
step: 7500, Loss: 0.31967565417289734
step: 7600, Loss: 0.13671979308128357
step: 7700, Loss: 0.14251887798309326
step: 7800, Loss: 0.12118469923734665
step: 7900, Loss: 0.12629085779190063
step: 8000, Loss: 0.11736326664686203
step: 8100, Loss: 0.12248658388853073
step: 8200, Loss: 0.11761603504419327
step: 8300, Loss: 0.11802683025598526
step: 8400, Loss: 0.11659346520900726
step: 8500, Loss: 0.12109865248203278
step: 8600, Loss: 0.11956089735031128
step: 8700, Loss: 0.11898986995220184
step: 8800, Loss: 0.11576364189386368
step: 8900, Loss: 0.11993284523487091
step: 9000, Loss: 0.11663560569286346
step: 9100, Loss: 0.11558814346790314
step: 9200, Loss: 0.11512348800897598
step: 9300, Loss: 0.11533287167549133
step: 9400, Loss: 0.11423244327306747
step: 9500, Loss: 0.11590558290481567
step: 9600, Loss: 0.11529980599880219
step: 9700, Loss: 0.11537548154592514
step: 9800, Loss: 0.11558172106742859
step: 9900, Loss: 0.11615908145904541
training successfully ended.
validating...
validate data length:32
acc: 0.875
precision: 0.875
recall: 0.875
F_score: 0.875
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.24969235062599182
step: 100, Loss: 0.11655613780021667
step: 200, Loss: 0.11479675024747849
step: 300, Loss: 0.11501303315162659
step: 400, Loss: 0.11288776993751526
step: 500, Loss: 0.11454157531261444
step: 600, Loss: 0.11377488076686859
step: 700, Loss: 0.11430531740188599
step: 800, Loss: 0.11417652666568756
step: 900, Loss: 0.11511674523353577
step: 1000, Loss: 0.114138662815094
step: 1100, Loss: 0.11445311456918716
step: 1200, Loss: 0.1137002483010292
step: 1300, Loss: 0.11368704587221146
step: 1400, Loss: 0.11381583660840988
step: 1500, Loss: 0.11283350735902786
step: 1600, Loss: 0.11382593214511871
step: 1700, Loss: 0.11370088160037994
step: 1800, Loss: 0.11334115266799927
step: 1900, Loss: 0.11300042271614075
step: 2000, Loss: 0.11374709755182266
step: 2100, Loss: 0.11414873600006104
step: 2200, Loss: 0.11504584550857544
step: 2300, Loss: 0.11358753591775894
step: 2400, Loss: 0.11441056430339813
step: 2500, Loss: 0.11323782801628113
step: 2600, Loss: 0.11315903067588806
step: 2700, Loss: 0.11437942832708359
step: 2800, Loss: 0.11338812112808228
step: 2900, Loss: 0.11504875868558884
step: 3000, Loss: 0.11398904025554657
step: 3100, Loss: 0.11524546891450882
step: 3200, Loss: 0.11313561350107193
step: 3300, Loss: 0.11346389353275299
step: 3400, Loss: 0.11363000422716141
step: 3500, Loss: 0.11317254602909088
step: 3600, Loss: 0.11435482650995255
step: 3700, Loss: 0.11545636504888535
step: 3800, Loss: 0.1147691160440445
step: 3900, Loss: 0.11410927027463913
step: 4000, Loss: 0.11459224671125412
step: 4100, Loss: 0.1137416809797287
step: 4200, Loss: 0.11347590386867523
step: 4300, Loss: 0.11301600933074951
step: 4400, Loss: 0.1138799786567688
step: 4500, Loss: 0.11364170908927917
step: 4600, Loss: 0.11443381011486053
step: 4700, Loss: 0.11401969939470291
step: 4800, Loss: 0.11274400353431702
step: 4900, Loss: 0.11298863589763641
step: 5000, Loss: 0.1134432777762413
step: 5100, Loss: 0.11296052485704422
step: 5200, Loss: 0.11301775276660919
step: 5300, Loss: 0.11293963342905045
step: 5400, Loss: 0.11348364502191544
step: 5500, Loss: 0.11271790415048599
step: 5600, Loss: 0.11381606757640839
step: 5700, Loss: 0.11305321007966995
step: 5800, Loss: 0.11437743902206421
step: 5900, Loss: 0.11762846261262894
step: 6000, Loss: 0.11289725452661514
step: 6100, Loss: 3.293278217315674
step: 6200, Loss: 0.14828374981880188
step: 6300, Loss: 0.1295117437839508
step: 6400, Loss: 0.13691586256027222
step: 6500, Loss: 0.12245389819145203
step: 6600, Loss: 0.12067919224500656
step: 6700, Loss: 0.12445554882287979
step: 6800, Loss: 0.11996489763259888
step: 6900, Loss: 0.11905375123023987
step: 7000, Loss: 0.12066063284873962
step: 7100, Loss: 0.11823062598705292
step: 7200, Loss: 0.11721651256084442
step: 7300, Loss: 0.11466710269451141
step: 7400, Loss: 0.11747849732637405
step: 7500, Loss: 0.11618766188621521
step: 7600, Loss: 0.11662889271974564
step: 7700, Loss: 0.1137973889708519
step: 7800, Loss: 0.11473202705383301
step: 7900, Loss: 0.11456532031297684
step: 8000, Loss: 0.11613636463880539
step: 8100, Loss: 0.11435647308826447
step: 8200, Loss: 0.11530128121376038
step: 8300, Loss: 0.11504873633384705
step: 8400, Loss: 0.1145881712436676
step: 8500, Loss: 0.11296713352203369
step: 8600, Loss: 0.11545004695653915
step: 8700, Loss: 0.1141698881983757
step: 8800, Loss: 0.11308765411376953
step: 8900, Loss: 0.11350716650485992
step: 9000, Loss: 0.1130087822675705
step: 9100, Loss: 0.11357990652322769
step: 9200, Loss: 0.11388704180717468
step: 9300, Loss: 0.11416848748922348
step: 9400, Loss: 0.11245027184486389
step: 9500, Loss: 0.11361891031265259
step: 9600, Loss: 0.11474397778511047
step: 9700, Loss: 0.11494368314743042
step: 9800, Loss: 0.11320886015892029
step: 9900, Loss: 0.113920658826828
training successfully ended.
validating...
validate data length:103
acc: 0.9895833333333334
precision: 0.9818181818181818
recall: 1.0
F_score: 0.9908256880733944
******fold 4******

Training... train_data length:923
step: 0, Loss: 0.1320413500070572
step: 100, Loss: 0.12047548592090607
step: 200, Loss: 0.11742289364337921
step: 300, Loss: 0.11680800467729568
step: 400, Loss: 0.11780112236738205
step: 500, Loss: 0.11421018093824387
step: 600, Loss: 0.11752861738204956
step: 700, Loss: 0.11639006435871124
step: 800, Loss: 0.11557105928659439
step: 900, Loss: 0.1153416857123375
step: 1000, Loss: 0.11385087668895721
step: 1100, Loss: 0.11521163582801819
step: 1200, Loss: 0.1151110902428627
step: 1300, Loss: 0.1148243322968483
step: 1400, Loss: 0.11339854449033737
step: 1500, Loss: 0.1142774447798729
step: 1600, Loss: 0.11347837001085281
step: 1700, Loss: 0.11322975158691406
step: 1800, Loss: 0.11411383002996445
step: 1900, Loss: 0.11406715214252472
step: 2000, Loss: 0.11273659765720367
step: 2100, Loss: 0.11433501541614532
step: 2200, Loss: 0.11296018958091736
step: 2300, Loss: 0.11406343430280685
step: 2400, Loss: 0.11411631107330322
step: 2500, Loss: 0.1142285019159317
step: 2600, Loss: 0.11346825957298279
step: 2700, Loss: 0.11422601342201233
step: 2800, Loss: 0.11484058946371078
step: 2900, Loss: 0.11419132351875305
step: 3000, Loss: 0.11502145230770111
step: 3100, Loss: 0.11326911300420761
step: 3200, Loss: 0.11479414254426956
step: 3300, Loss: 0.11388199776411057
step: 3400, Loss: 0.11389140784740448
step: 3500, Loss: 0.1138535737991333
step: 3600, Loss: 0.1156170517206192
step: 3700, Loss: 0.11509495973587036
step: 3800, Loss: 0.11414700001478195
step: 3900, Loss: 0.11412040144205093
step: 4000, Loss: 0.11579018831253052
step: 4100, Loss: 0.11475639790296555
step: 4200, Loss: 0.23009426891803741
step: 4300, Loss: 0.1596071422100067
step: 4400, Loss: 0.1369752734899521
step: 4500, Loss: 0.1273566633462906
step: 4600, Loss: 0.1261146515607834
step: 4700, Loss: 0.12672416865825653
step: 4800, Loss: 0.12296970188617706
step: 4900, Loss: 0.12197016924619675
step: 5000, Loss: 0.12104380130767822
step: 5100, Loss: 0.11957693099975586
step: 5200, Loss: 0.1191241592168808
step: 5300, Loss: 0.11724750697612762
step: 5400, Loss: 0.11900920420885086
step: 5500, Loss: 0.11572365462779999
step: 5600, Loss: 0.11545629054307938
step: 5700, Loss: 0.1166732907295227
step: 5800, Loss: 0.11566680669784546
step: 5900, Loss: 0.11751800775527954
step: 6000, Loss: 0.1160469725728035
step: 6100, Loss: 0.11562705039978027
step: 6200, Loss: 0.11759158968925476
step: 6300, Loss: 0.11768820881843567
step: 6400, Loss: 0.11487479507923126
step: 6500, Loss: 0.11644353717565536
step: 6600, Loss: 0.1156238466501236
step: 6700, Loss: 0.11463713645935059
step: 6800, Loss: 0.11428570002317429
step: 6900, Loss: 0.11455242335796356
step: 7000, Loss: 0.11508895456790924
step: 7100, Loss: 0.11516232788562775
step: 7200, Loss: 0.11475248634815216
step: 7300, Loss: 0.1150268018245697
step: 7400, Loss: 0.1145416870713234
step: 7500, Loss: 0.1144205629825592
step: 7600, Loss: 0.11717767268419266
step: 7700, Loss: 0.11347644031047821
step: 7800, Loss: 0.11319711804389954
step: 7900, Loss: 0.11416298896074295
step: 8000, Loss: 0.11358058452606201
step: 8100, Loss: 0.1149909570813179
step: 8200, Loss: 0.11498677730560303
step: 8300, Loss: 0.11391530185937881
step: 8400, Loss: 0.1138901486992836
step: 8500, Loss: 0.11643362045288086
step: 8600, Loss: 0.11329758912324905
step: 8700, Loss: 0.11425460129976273
step: 8800, Loss: 0.11481212079524994
step: 8900, Loss: 0.11283800005912781
step: 9000, Loss: 0.11539936065673828
step: 9100, Loss: 0.11375013738870621
step: 9200, Loss: 0.11486513912677765
step: 9300, Loss: 0.11420246958732605
step: 9400, Loss: 0.11483544111251831
step: 9500, Loss: 0.11359648406505585
step: 9600, Loss: 0.11378056555986404
step: 9700, Loss: 0.11513996124267578
step: 9800, Loss: 0.11321672797203064
step: 9900, Loss: 0.11328259110450745
training successfully ended.
validating...
validate data length:103
acc: 0.96875
precision: 0.9824561403508771
recall: 0.9655172413793104
F_score: 0.9739130434782608
******fold 5******

Training... train_data length:923
step: 0, Loss: 0.12728598713874817
step: 100, Loss: 0.11853578686714172
step: 200, Loss: 0.11726214736700058
step: 300, Loss: 0.11743760108947754
step: 400, Loss: 0.11583728343248367
step: 500, Loss: 0.11585868149995804
step: 600, Loss: 0.11446462571620941
step: 700, Loss: 0.11459068208932877
step: 800, Loss: 0.11438249796628952
step: 900, Loss: 0.11432011425495148
step: 1000, Loss: 0.11422514915466309
step: 1100, Loss: 0.11439421772956848
step: 1200, Loss: 0.11489640921354294
step: 1300, Loss: 0.1144251823425293
step: 1400, Loss: 0.11328969150781631
step: 1500, Loss: 0.11471393704414368
step: 1600, Loss: 0.11382158100605011
step: 1700, Loss: 0.11297765374183655
step: 1800, Loss: 0.11337147653102875
step: 1900, Loss: 0.1144542470574379
step: 2000, Loss: 0.11510873585939407
step: 2100, Loss: 0.11526333540678024
step: 2200, Loss: 0.11349386721849442
step: 2300, Loss: 0.11782699078321457
step: 2400, Loss: 0.11408041417598724
step: 2500, Loss: 0.11371416598558426
step: 2600, Loss: 0.1155841127038002
step: 2700, Loss: 0.11656754463911057
step: 2800, Loss: 0.11556734889745712
step: 2900, Loss: 0.1163703128695488
step: 3000, Loss: 0.11442986875772476
step: 3100, Loss: 0.5060485601425171
step: 3200, Loss: 0.2090483158826828
step: 3300, Loss: 0.1369531899690628
step: 3400, Loss: 0.12971407175064087
step: 3500, Loss: 0.12634801864624023
step: 3600, Loss: 0.12509091198444366
step: 3700, Loss: 0.1178169772028923
step: 3800, Loss: 0.12781625986099243
step: 3900, Loss: 0.12014252692461014
step: 4000, Loss: 0.1196875348687172
step: 4100, Loss: 0.11968305706977844
step: 4200, Loss: 0.11823730915784836
step: 4300, Loss: 0.11825288087129593
step: 4400, Loss: 0.11757748574018478
step: 4500, Loss: 0.11810362339019775
step: 4600, Loss: 0.11898010969161987
step: 4700, Loss: 0.11714041233062744
step: 4800, Loss: 0.11890031397342682
step: 4900, Loss: 0.11581666022539139
step: 5000, Loss: 0.115740105509758
step: 5100, Loss: 0.11678077280521393
step: 5200, Loss: 0.11735460162162781
step: 5300, Loss: 0.1140793189406395
step: 5400, Loss: 0.11688533425331116
step: 5500, Loss: 0.11706836521625519
step: 5600, Loss: 0.11504232883453369
step: 5700, Loss: 0.11542011797428131
step: 5800, Loss: 0.11697143316268921
step: 5900, Loss: 0.11483517289161682
step: 6000, Loss: 0.11490839719772339
step: 6100, Loss: 0.11439186334609985
step: 6200, Loss: 0.11380718648433685
step: 6300, Loss: 0.11419971287250519
step: 6400, Loss: 0.11412089318037033
step: 6500, Loss: 0.11500409990549088
step: 6600, Loss: 0.11575240641832352
step: 6700, Loss: 0.11437328904867172
step: 6800, Loss: 0.11356046795845032
step: 6900, Loss: 0.11591983586549759
step: 7000, Loss: 0.11406676471233368
step: 7100, Loss: 0.11443846672773361
step: 7200, Loss: 0.1143183708190918
step: 7300, Loss: 0.11433883011341095
step: 7400, Loss: 0.11585608124732971
step: 7500, Loss: 0.1134684607386589
step: 7600, Loss: 0.11420394480228424
step: 7700, Loss: 0.114577516913414
step: 7800, Loss: 0.11487925797700882
step: 7900, Loss: 0.11327309906482697
step: 8000, Loss: 0.11361802369356155
step: 8100, Loss: 0.11426527798175812
step: 8200, Loss: 0.11356116831302643
step: 8300, Loss: 0.11345922946929932
step: 8400, Loss: 0.11524233967065811
step: 8500, Loss: 0.11336523294448853
step: 8600, Loss: 0.11380571871995926
step: 8700, Loss: 0.1132570430636406
step: 8800, Loss: 0.11462607979774475
step: 8900, Loss: 0.1131940633058548
step: 9000, Loss: 0.11340417712926865
step: 9100, Loss: 0.11538805812597275
step: 8700, Loss: 0.11438478529453278
step: 8800, Loss: 0.11377429962158203
step: 8900, Loss: 0.11371804773807526
step: 9000, Loss: 0.11394836008548737
step: 9100, Loss: 0.11319640278816223
step: 9200, Loss: 0.11420219391584396
step: 9300, Loss: 0.11350840330123901
step: 9400, Loss: 0.11788834631443024
step: 9500, Loss: 0.11537401378154755
step: 9600, Loss: 0.11343958973884583
step: 9700, Loss: 0.11395301669836044
step: 9800, Loss: 0.11477159708738327
step: 9900, Loss: 0.11445527523756027
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7647058823529411
recall: 0.8666666666666667
F_score: 0.8125
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.24339666962623596
step: 100, Loss: 0.11563852429389954
step: 200, Loss: 0.11505134403705597
step: 300, Loss: 0.11528787016868591
step: 400, Loss: 0.11615173518657684
step: 500, Loss: 0.11477160453796387
step: 600, Loss: 0.11370952427387238
step: 700, Loss: 0.11448831111192703
step: 800, Loss: 0.1141217052936554
step: 900, Loss: 0.11379953473806381
step: 1000, Loss: 0.11416998505592346
step: 1100, Loss: 0.1131647452712059
step: 1200, Loss: 0.11392734199762344
step: 1300, Loss: 0.11356879770755768
step: 1400, Loss: 0.11677948385477066
step: 1500, Loss: 0.1146022230386734
step: 1600, Loss: 0.11377836763858795
step: 1700, Loss: 0.11511686444282532
step: 1800, Loss: 0.11475629359483719
step: 1900, Loss: 0.11350280046463013
step: 2000, Loss: 0.11377038806676865
step: 2100, Loss: 0.11446068435907364
step: 2200, Loss: 0.11547128856182098
step: 2300, Loss: 0.11383625119924545
step: 2400, Loss: 0.11390412598848343
step: 2500, Loss: 0.11506336182355881
step: 2600, Loss: 0.11400233209133148
step: 2700, Loss: 0.11497485637664795
step: 2800, Loss: 0.11370974034070969
step: 2900, Loss: 0.11412495374679565
step: 3000, Loss: 0.11306445300579071
step: 3100, Loss: 0.11379490792751312
step: 3200, Loss: 0.113140769302845
step: 3300, Loss: 0.11494627594947815
step: 3400, Loss: 0.11331876367330551
step: 3500, Loss: 0.1137733906507492
step: 3600, Loss: 0.11392930150032043
step: 3700, Loss: 0.1134585440158844
step: 3800, Loss: 0.11296137422323227
step: 3900, Loss: 0.11409765481948853
step: 4000, Loss: 0.1135958805680275
step: 4100, Loss: 0.1134294718503952
step: 4200, Loss: 0.11406952142715454
step: 4300, Loss: 0.11320101469755173
step: 4400, Loss: 0.11462446302175522
step: 4500, Loss: 0.1136486679315567
step: 4600, Loss: 0.11265851557254791
step: 4700, Loss: 0.11578675359487534
step: 4800, Loss: 0.1127437949180603
step: 4900, Loss: 0.4101279377937317
step: 5000, Loss: 0.1233152374625206
step: 5100, Loss: 0.12927663326263428
step: 5200, Loss: 0.12371425330638885
step: 5300, Loss: 0.12508714199066162
step: 5400, Loss: 0.11671487241983414
step: 5500, Loss: 0.12015068531036377
step: 5600, Loss: 0.11850783973932266
step: 5700, Loss: 0.11761154234409332
step: 5800, Loss: 0.11478689312934875
step: 5900, Loss: 0.11769754439592361
step: 6000, Loss: 0.11573588103055954
step: 6100, Loss: 0.11484275758266449
step: 6200, Loss: 0.1153814047574997
step: 6300, Loss: 0.11364498734474182
step: 6400, Loss: 0.11457136273384094
step: 6500, Loss: 0.11506287753582001
step: 6600, Loss: 0.1143270879983902
step: 6700, Loss: 0.11706554144620895
step: 6800, Loss: 0.11395250260829926
step: 6900, Loss: 0.1158590316772461
step: 7000, Loss: 0.1136317178606987
step: 7100, Loss: 0.11786629259586334
step: 7200, Loss: 0.11408468335866928
step: 7300, Loss: 0.1156463772058487
step: 7400, Loss: 0.1150626614689827
step: 7500, Loss: 0.11351858079433441
step: 7600, Loss: 0.11475948244333267
step: 7700, Loss: 0.1151464655995369
step: 7800, Loss: 0.11459876596927643
step: 7900, Loss: 0.11356714367866516
step: 8000, Loss: 0.11433503031730652
step: 8100, Loss: 0.1145622730255127
step: 8200, Loss: 0.11324062943458557
step: 8300, Loss: 0.11453375965356827
step: 8400, Loss: 0.11404389142990112
step: 8500, Loss: 0.11412263661623001
step: 8600, Loss: 0.11449885368347168
step: 8700, Loss: 0.11364603042602539
step: 8800, Loss: 0.11391443014144897
step: 8900, Loss: 0.11437103897333145
step: 9000, Loss: 0.11435217410326004
step: 9100, Loss: 0.11653540283441544
step: 9200, Loss: 0.11431938409805298
step: 9300, Loss: 0.11431704461574554
step: 9400, Loss: 0.11362610012292862
step: 9500, Loss: 0.11587686836719513
step: 9600, Loss: 0.11361291259527206
step: 9700, Loss: 0.11400331556797028
step: 9800, Loss: 0.11349177360534668
step: 9900, Loss: 0.11375489830970764
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.1138310357928276
step: 100, Loss: 0.1157018393278122
step: 200, Loss: 0.11345434188842773
step: 300, Loss: 0.11468508839607239
step: 400, Loss: 0.11537302285432816
step: 500, Loss: 0.11369331181049347
step: 600, Loss: 0.1137949526309967
step: 700, Loss: 0.11438431590795517
step: 800, Loss: 0.11436107754707336
step: 900, Loss: 0.11457803100347519
step: 1000, Loss: 0.11374733597040176
step: 1100, Loss: 0.11318236589431763
step: 1200, Loss: 0.11386185139417648
step: 1300, Loss: 0.11497624218463898
step: 1400, Loss: 0.11405876278877258
step: 1500, Loss: 0.11571608483791351
step: 1600, Loss: 0.11479979008436203
step: 1700, Loss: 0.11336538940668106
step: 1800, Loss: 0.11368945240974426
step: 1900, Loss: 0.11366628110408783
step: 2000, Loss: 0.11274249851703644
step: 2100, Loss: 0.11328478157520294
step: 2200, Loss: 0.11313290894031525
step: 2300, Loss: 0.11473746597766876
step: 2400, Loss: 0.11356404423713684
step: 2500, Loss: 0.11480796337127686
step: 2600, Loss: 0.11363422125577927
step: 2700, Loss: 0.11473057419061661
step: 2800, Loss: 0.1134873554110527
step: 2900, Loss: 0.11314713209867477
step: 3000, Loss: 0.11445010453462601
step: 3100, Loss: 0.11461582779884338
step: 3200, Loss: 0.11328534781932831
step: 3300, Loss: 0.11333698034286499
step: 3400, Loss: 0.11348192393779755
step: 3500, Loss: 0.11431050300598145
step: 3600, Loss: 0.11407127976417542
step: 3700, Loss: 0.11417116224765778
step: 3800, Loss: 0.11380147933959961
step: 3900, Loss: 0.11361809819936752
step: 4000, Loss: 0.11296364665031433
step: 4100, Loss: 0.11352354288101196
step: 4200, Loss: 0.11506436765193939
step: 4300, Loss: 0.11354174464941025
step: 4400, Loss: 0.11348731815814972
step: 4500, Loss: 2.3905253410339355
step: 4600, Loss: 0.12129253149032593
step: 4700, Loss: 0.13236749172210693
step: 4800, Loss: 0.12192290276288986
step: 4900, Loss: 0.12557312846183777
step: 5000, Loss: 0.11857622116804123
step: 5100, Loss: 0.1270246058702469
step: 5200, Loss: 0.12087170779705048
step: 5300, Loss: 0.11966371536254883
step: 5400, Loss: 0.11618638038635254
step: 5500, Loss: 0.11614210158586502
step: 5600, Loss: 0.12545667588710785
step: 5700, Loss: 0.11949904263019562
step: 5800, Loss: 0.11668508499860764
step: 5900, Loss: 0.11518874019384384
step: 6000, Loss: 0.12373252958059311
step: 6100, Loss: 0.11550553888082504
step: 6200, Loss: 0.11448479443788528
step: 6300, Loss: 0.11396695673465729
step: 6400, Loss: 0.11423318833112717
step: 6500, Loss: 0.11462846398353577
step: 6600, Loss: 0.11476904153823853
step: 6700, Loss: 0.11394630372524261
step: 6800, Loss: 0.11457891762256622
step: 6900, Loss: 0.1153515875339508
step: 7000, Loss: 0.11618669331073761
step: 7100, Loss: 0.11572036147117615
step: 7200, Loss: 0.11401168256998062
step: 7300, Loss: 0.11441849172115326
step: 7400, Loss: 0.11460152268409729
step: 7500, Loss: 0.11451857537031174
step: 7600, Loss: 0.11521797627210617
step: 7700, Loss: 0.11492747813463211
step: 7800, Loss: 0.1154279038310051
step: 7900, Loss: 0.11458458751440048
step: 8000, Loss: 0.1157766729593277
step: 8100, Loss: 0.11476582288742065
step: 8200, Loss: 0.11536822468042374
step: 8300, Loss: 0.1143014058470726
step: 8400, Loss: 0.1147984117269516
step: 8500, Loss: 0.11453928053379059
step: 8600, Loss: 0.11551308631896973
step: 8700, Loss: 0.11506693810224533
step: 8800, Loss: 0.11472206562757492
step: 8900, Loss: 0.11380501091480255
step: 9000, Loss: 0.11432387679815292
step: 9100, Loss: 0.1131146103143692
step: 9200, Loss: 0.11328010261058807
step: 9300, Loss: 0.11439753323793411
step: 9200, Loss: 0.11348813772201538
step: 9300, Loss: 0.11356446892023087
step: 9400, Loss: 0.1144125834107399
step: 9500, Loss: 0.11383707821369171
step: 9600, Loss: 0.11334250867366791
step: 9700, Loss: 0.11380510777235031
step: 9800, Loss: 0.11420723795890808
step: 9900, Loss: 0.11427336931228638
training successfully ended.
validating...
validate data length:103
acc: 0.9895833333333334
precision: 0.98
recall: 1.0
F_score: 0.98989898989899
******fold 6******

Training... train_data length:923
step: 0, Loss: 0.11882958561182022
step: 100, Loss: 0.1192912757396698
step: 200, Loss: 0.11845535039901733
step: 300, Loss: 0.11729519814252853
step: 400, Loss: 0.11581616848707199
step: 500, Loss: 0.11509709805250168
step: 600, Loss: 0.11745356023311615
step: 700, Loss: 0.11496463418006897
step: 800, Loss: 0.11360795050859451
step: 900, Loss: 0.11433454602956772
step: 1000, Loss: 0.1139443963766098
step: 1100, Loss: 0.11604001373052597
step: 1200, Loss: 0.11437036097049713
step: 1300, Loss: 0.11278241872787476
step: 1400, Loss: 0.11433105915784836
step: 1500, Loss: 0.11363305151462555
step: 1600, Loss: 0.11370769143104553
step: 1700, Loss: 0.11288571357727051
step: 1800, Loss: 0.11392689496278763
step: 1900, Loss: 0.11325675994157791
step: 2000, Loss: 0.11330254375934601
step: 2100, Loss: 0.11292693018913269
step: 2200, Loss: 0.11647267639636993
step: 2300, Loss: 0.11388614028692245
step: 2400, Loss: 0.11461415886878967
step: 2500, Loss: 0.11433596163988113
step: 2600, Loss: 0.11762458086013794
step: 2700, Loss: 0.11425222456455231
step: 2800, Loss: 0.11512617766857147
step: 2900, Loss: 0.11365588009357452
step: 3000, Loss: 0.11577001214027405
step: 3100, Loss: 0.1137177124619484
step: 3200, Loss: 0.11383865773677826
step: 3300, Loss: 0.7193642258644104
step: 3400, Loss: 0.40520697832107544
step: 3500, Loss: 0.13215211033821106
step: 3600, Loss: 0.13283798098564148
step: 3700, Loss: 0.12335792928934097
step: 3800, Loss: 0.12424919009208679
step: 3900, Loss: 0.12051486223936081
step: 4000, Loss: 0.12103646248579025
step: 4100, Loss: 0.12048972398042679
step: 4200, Loss: 0.11965598911046982
step: 4300, Loss: 0.119259312748909
step: 4400, Loss: 0.1163632869720459
step: 4500, Loss: 0.11850202083587646
step: 4600, Loss: 0.1167188435792923
step: 4700, Loss: 0.11906393617391586
step: 4800, Loss: 0.11712311953306198
step: 4900, Loss: 0.1146278902888298
step: 5000, Loss: 0.11696071922779083
step: 5100, Loss: 0.11472117155790329
step: 5200, Loss: 0.11571705341339111
step: 5300, Loss: 0.11710220575332642
step: 5400, Loss: 0.11771425604820251
step: 5500, Loss: 0.11774429678916931
step: 5600, Loss: 0.1153770238161087
step: 5700, Loss: 0.11571162939071655
step: 5800, Loss: 0.11471918225288391
step: 5900, Loss: 0.11567142605781555
step: 6000, Loss: 0.11520704627037048
step: 6100, Loss: 0.11295169591903687
step: 6200, Loss: 0.11521197855472565
step: 6300, Loss: 0.11447001993656158
step: 6400, Loss: 0.11630851775407791
step: 6500, Loss: 0.11594490706920624
step: 6600, Loss: 0.11552992463111877
step: 6700, Loss: 0.11425494402647018
step: 6800, Loss: 0.11348914355039597
step: 6900, Loss: 0.11491161584854126
step: 7000, Loss: 0.1140989437699318
step: 7100, Loss: 0.11409910023212433
step: 7200, Loss: 0.11474049091339111
step: 7300, Loss: 0.11348071694374084
step: 7400, Loss: 0.11466637998819351
step: 7500, Loss: 0.11411269754171371
step: 7600, Loss: 0.11310996860265732
step: 7700, Loss: 0.1140628308057785
step: 7800, Loss: 0.11494949460029602
step: 7900, Loss: 0.11458683013916016
step: 8000, Loss: 0.11271136999130249
step: 8100, Loss: 0.11421176791191101
step: 8200, Loss: 0.11408792436122894
step: 8300, Loss: 0.11372169852256775
step: 8400, Loss: 0.11458883434534073
step: 8500, Loss: 0.11360911279916763
step: 8600, Loss: 0.11453274637460709
step: 8700, Loss: 0.11347363889217377
step: 8800, Loss: 0.11360664665699005
step: 8900, Loss: 0.11375202238559723
step: 9000, Loss: 0.11329387873411179
step: 9100, Loss: 0.11349112540483475
step: 9200, Loss: 0.11317690461874008
step: 9300, Loss: 0.11287252604961395
step: 9400, Loss: 0.11380425840616226
step: 9500, Loss: 0.11322282254695892
step: 9600, Loss: 0.11371385306119919
step: 9700, Loss: 0.11412863433361053
step: 9800, Loss: 0.11571219563484192
step: 9900, Loss: 0.11399132013320923
training successfully ended.
validating...
validate data length:103
acc: 0.96875
precision: 0.94
recall: 1.0
F_score: 0.9690721649484536
******fold 7******

Training... train_data length:924
step: 0, Loss: 0.1216050460934639
step: 100, Loss: 0.11960708349943161
step: 200, Loss: 0.11672914028167725
step: 300, Loss: 0.11598212271928787
step: 400, Loss: 0.11459671705961227
step: 500, Loss: 0.11679832637310028
step: 600, Loss: 0.11420544981956482
step: 700, Loss: 0.11434657871723175
step: 800, Loss: 0.11352258920669556
step: 900, Loss: 0.11479552090167999
step: 1000, Loss: 0.11466027796268463
step: 1100, Loss: 0.1132444515824318
step: 1200, Loss: 0.11574305593967438
step: 1300, Loss: 0.1150830090045929
step: 1400, Loss: 0.11374764889478683
step: 1500, Loss: 0.11387515068054199
step: 1600, Loss: 0.11311351507902145
step: 1700, Loss: 0.11507180333137512
step: 1800, Loss: 0.11340447515249252
step: 1900, Loss: 0.11365712434053421
step: 2000, Loss: 0.11385513097047806
step: 2100, Loss: 0.11379355192184448
step: 2200, Loss: 0.11545474082231522
step: 2300, Loss: 0.11613915860652924
step: 2400, Loss: 0.11543601006269455
step: 2500, Loss: 0.11464163661003113
step: 2600, Loss: 0.11340706795454025
step: 2700, Loss: 0.11460713297128677
step: 2800, Loss: 0.11264225095510483
step: 2900, Loss: 0.11380840092897415
step: 3000, Loss: 0.11459039151668549
step: 3100, Loss: 0.11509356647729874
step: 3200, Loss: 0.11419402807950974
step: 3300, Loss: 0.24416176974773407
step: 3400, Loss: 0.13889460265636444
step: 3500, Loss: 0.12661166489124298
step: 3600, Loss: 0.12567783892154694
step: 3700, Loss: 0.12405170500278473
step: 3800, Loss: 0.12308375537395477
step: 3900, Loss: 0.11932513117790222
step: 4000, Loss: 0.12012925744056702
step: 4100, Loss: 0.1218249574303627
step: 4200, Loss: 0.12010963261127472
step: 4300, Loss: 0.12184754014015198
step: 4400, Loss: 0.11621012538671494
step: 4500, Loss: 0.11589188873767853
step: 4600, Loss: 0.12029969692230225
step: 4700, Loss: 0.1191212385892868
step: 4800, Loss: 0.11533728241920471
step: 4900, Loss: 0.11714628338813782
step: 5000, Loss: 0.11661827564239502
step: 5100, Loss: 0.1191546842455864
step: 5200, Loss: 0.11632567644119263
step: 5300, Loss: 0.11619478464126587
step: 5400, Loss: 0.115058533847332
step: 5500, Loss: 0.11586590111255646
step: 5600, Loss: 0.11806336045265198
step: 5700, Loss: 0.11454600840806961
step: 5800, Loss: 0.11612246930599213
step: 5900, Loss: 0.11454830318689346
step: 6000, Loss: 0.1168503388762474
step: 6100, Loss: 0.11374560743570328
step: 6200, Loss: 0.115162193775177
step: 6300, Loss: 0.11370252072811127
step: 6400, Loss: 0.11399785429239273
step: 6500, Loss: 0.11474098265171051
step: 6600, Loss: 0.1131962314248085
step: 6700, Loss: 0.11436890065670013
step: 6800, Loss: 0.11488083004951477
step: 6900, Loss: 0.11420249938964844
step: 7000, Loss: 0.11392153799533844
step: 7100, Loss: 0.11438164860010147
step: 7200, Loss: 0.1140998899936676
step: 7300, Loss: 0.11353642493486404
step: 7400, Loss: 0.11435046792030334
step: 7500, Loss: 0.11367513239383698
step: 7600, Loss: 0.11407776176929474
step: 7700, Loss: 0.11372216045856476
step: 7800, Loss: 0.11356060206890106
step: 7900, Loss: 0.11303537338972092
step: 8000, Loss: 0.11378490924835205
step: 8100, Loss: 0.11442174762487411
step: 8200, Loss: 0.11326304823160172
step: 8300, Loss: 0.11509522050619125
step: 8400, Loss: 0.11331649869680405
step: 8500, Loss: 0.1128547266125679
step: 8600, Loss: 0.11246895790100098
step: 8700, Loss: 0.11362767964601517
step: 8800, Loss: 0.11267682164907455
step: 8900, Loss: 0.11309568583965302
step: 9000, Loss: 0.11290016770362854
step: 9100, Loss: 0.11381837725639343
step: 9200, Loss: 0.11455461382865906
step: 9300, Loss: 0.11287566274404526
step: 9400, Loss: 0.11457622051239014
step: 9500, Loss: 0.1126839891076088
step: 9600, Loss: 0.1141420304775238
step: 9700, Loss: 0.11454334855079651
step: 9400, Loss: 0.11486921459436417
step: 9500, Loss: 0.11691674590110779
step: 9600, Loss: 0.11364039778709412
step: 9700, Loss: 0.11336618661880493
step: 9800, Loss: 0.1135692223906517
step: 9900, Loss: 0.1130916103720665
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8823529411764706
recall: 0.8333333333333334
F_score: 0.8571428571428571
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.11454811692237854
step: 100, Loss: 0.11527856439352036
step: 200, Loss: 0.11421260237693787
step: 300, Loss: 0.11533386260271072
step: 400, Loss: 0.11432372033596039
step: 500, Loss: 0.11450602859258652
step: 600, Loss: 0.11497364193201065
step: 700, Loss: 0.11557383090257645
step: 800, Loss: 0.11383061856031418
step: 900, Loss: 0.11351694911718369
step: 1000, Loss: 0.11312206089496613
step: 1100, Loss: 0.11378809809684753
step: 1200, Loss: 0.11323206126689911
step: 1300, Loss: 0.11407481133937836
step: 1400, Loss: 0.11403403431177139
step: 1500, Loss: 0.11367400735616684
step: 1600, Loss: 0.1131623312830925
step: 1700, Loss: 0.11341234296560287
step: 1800, Loss: 0.11299140751361847
step: 1900, Loss: 0.11422068625688553
step: 2000, Loss: 0.1129002571105957
step: 2100, Loss: 0.11445820331573486
step: 2200, Loss: 0.11436822265386581
step: 2300, Loss: 0.11599896103143692
step: 2400, Loss: 0.11320929229259491
step: 2500, Loss: 0.11353563517332077
step: 2600, Loss: 0.1138596385717392
step: 2700, Loss: 0.11309897154569626
step: 2800, Loss: 0.1142830029129982
step: 2900, Loss: 0.11373162269592285
step: 3000, Loss: 0.11334116756916046
step: 3100, Loss: 0.11379960924386978
step: 3200, Loss: 0.11302531510591507
step: 3300, Loss: 0.11330747604370117
step: 3400, Loss: 0.1141928881406784
step: 3500, Loss: 0.11372128874063492
step: 3600, Loss: 0.11251696944236755
step: 3700, Loss: 0.11358539015054703
step: 3800, Loss: 0.11417698860168457
step: 3900, Loss: 0.11396744847297668
step: 4000, Loss: 0.11318600922822952
step: 4100, Loss: 0.11293049156665802
step: 4200, Loss: 0.11424227058887482
step: 4300, Loss: 0.11337132751941681
step: 4400, Loss: 0.11413342505693436
step: 4500, Loss: 0.11517180502414703
step: 4600, Loss: 0.11349605768918991
step: 4700, Loss: 0.11556476354598999
step: 4800, Loss: 0.11301188915967941
step: 4900, Loss: 0.1132139042019844
step: 5000, Loss: 0.11481905728578568
step: 5100, Loss: 0.11362292617559433
step: 5200, Loss: 0.11301706731319427
step: 5300, Loss: 0.11301277577877045
step: 5400, Loss: 0.1151699423789978
step: 5500, Loss: 0.12757644057273865
step: 5600, Loss: 0.13419459760189056
step: 5700, Loss: 0.12333190441131592
step: 5800, Loss: 0.11618276685476303
step: 5900, Loss: 0.125118687748909
step: 6000, Loss: 0.11978576332330704
step: 6100, Loss: 0.11641109734773636
step: 6200, Loss: 0.12323282659053802
step: 6300, Loss: 0.11799593269824982
step: 6400, Loss: 0.11651193350553513
step: 6500, Loss: 0.11541192978620529
step: 6600, Loss: 0.1160898506641388
step: 6700, Loss: 0.11422207951545715
step: 6800, Loss: 0.11431155353784561
step: 6900, Loss: 0.11436663568019867
step: 7000, Loss: 0.11396501958370209
step: 7100, Loss: 0.11490082740783691
step: 7200, Loss: 0.11566749215126038
step: 7300, Loss: 0.11426889896392822
step: 7400, Loss: 0.11605662107467651
step: 7500, Loss: 0.1128118559718132
step: 7600, Loss: 0.1177443116903305
step: 7700, Loss: 0.11354967951774597
step: 7800, Loss: 0.11436044424772263
step: 7900, Loss: 0.11364322900772095
step: 8000, Loss: 0.11579591780900955
step: 8100, Loss: 0.11430519819259644
step: 8200, Loss: 0.11516311019659042
step: 8300, Loss: 0.11281858384609222
step: 8400, Loss: 0.11471697688102722
step: 8500, Loss: 0.11377958208322525
step: 8600, Loss: 0.11543172597885132
step: 8700, Loss: 0.11406047642230988
step: 8800, Loss: 0.11358311772346497
step: 8900, Loss: 0.11428166925907135
step: 9000, Loss: 0.11471771448850632
step: 9100, Loss: 0.11312703043222427
step: 9200, Loss: 0.11419899016618729
step: 9300, Loss: 0.1131078228354454
step: 9400, Loss: 0.11340577155351639
step: 9500, Loss: 0.11451514065265656
step: 9600, Loss: 0.11344142258167267
step: 9700, Loss: 0.11487994343042374
step: 9800, Loss: 0.11369232833385468
step: 9900, Loss: 0.1142292320728302
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8823529411764706
recall: 0.8333333333333334
F_score: 0.8571428571428571
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.1145135909318924
step: 100, Loss: 0.11509637534618378
step: 200, Loss: 0.11490171402692795
step: 300, Loss: 0.1145830824971199
step: 400, Loss: 0.11343500763177872
step: 500, Loss: 0.1145009845495224
step: 600, Loss: 0.11403204500675201
step: 700, Loss: 0.11337359249591827
step: 800, Loss: 0.11335328221321106
step: 900, Loss: 0.11498412489891052
step: 1000, Loss: 0.11390861123800278
step: 1100, Loss: 0.11365975439548492
step: 1200, Loss: 0.11335709691047668
step: 1300, Loss: 0.11368163675069809
step: 1400, Loss: 0.1135292500257492
step: 1500, Loss: 0.11496506631374359
step: 1600, Loss: 0.11452428251504898
step: 1700, Loss: 0.11392852663993835
step: 1800, Loss: 0.11526428163051605
step: 1900, Loss: 0.11473293602466583
step: 2000, Loss: 0.11297541856765747
step: 2100, Loss: 0.13018497824668884
step: 2200, Loss: 0.11330348253250122
step: 2300, Loss: 0.11372362077236176
step: 2400, Loss: 0.11373213678598404
step: 2500, Loss: 0.1134144738316536
step: 2600, Loss: 0.11432158201932907
step: 2700, Loss: 0.1141161248087883
step: 2800, Loss: 0.1135471761226654
step: 2900, Loss: 0.11346166580915451
step: 3000, Loss: 0.1132233589887619
step: 3100, Loss: 0.11430428922176361
step: 3200, Loss: 0.11324767768383026
step: 3300, Loss: 0.11312179267406464
step: 3400, Loss: 0.1139393076300621
step: 3500, Loss: 0.11606805771589279
step: 3600, Loss: 0.11421329528093338
step: 3700, Loss: 0.1143365204334259
step: 3800, Loss: 0.11477202922105789
step: 3900, Loss: 0.11400104314088821
step: 4000, Loss: 0.1142406016588211
step: 4100, Loss: 0.11369696259498596
step: 4200, Loss: 0.1134304404258728
step: 4300, Loss: 0.11278115212917328
step: 4400, Loss: 0.11327710747718811
step: 4500, Loss: 0.1137399971485138
step: 4600, Loss: 0.11244437098503113
step: 4700, Loss: 0.11503759026527405
step: 4800, Loss: 0.13948072493076324
step: 4900, Loss: 0.12385566532611847
step: 5000, Loss: 0.11705338209867477
step: 5100, Loss: 0.11663801968097687
step: 5200, Loss: 0.11718514561653137
step: 5300, Loss: 0.11659013479948044
step: 5400, Loss: 0.11523376405239105
step: 5500, Loss: 0.11670538783073425
step: 5600, Loss: 0.11860699951648712
step: 5700, Loss: 0.11485254019498825
step: 5800, Loss: 0.11706005036830902
step: 5900, Loss: 0.115333192050457
step: 6000, Loss: 0.11611577868461609
step: 6100, Loss: 0.11653144657611847
step: 6200, Loss: 0.11359868943691254
step: 6300, Loss: 0.11524446308612823
step: 6400, Loss: 0.11432316154241562
step: 6500, Loss: 0.11413296312093735
step: 6600, Loss: 0.11358790844678879
step: 6700, Loss: 0.1203305572271347
step: 6800, Loss: 0.11367425322532654
step: 6900, Loss: 0.12079787254333496
step: 7000, Loss: 0.11431939154863358
step: 7100, Loss: 0.11527840793132782
step: 7200, Loss: 0.1158834770321846
step: 7300, Loss: 0.11575455218553543
step: 7400, Loss: 0.11398644000291824
step: 7500, Loss: 0.11362964659929276
step: 7600, Loss: 0.1148112416267395
step: 7700, Loss: 0.11424045264720917
step: 7800, Loss: 0.11428774148225784
step: 7900, Loss: 0.11530226469039917
step: 8000, Loss: 0.11356868594884872
step: 8100, Loss: 0.11779540032148361
step: 8200, Loss: 0.11540999263525009
step: 8300, Loss: 0.11367163062095642
step: 8400, Loss: 0.11458271741867065
step: 8500, Loss: 0.11349864304065704
step: 8600, Loss: 0.11292994022369385
step: 8700, Loss: 0.11469113826751709
step: 8800, Loss: 0.11456188559532166
step: 8900, Loss: 0.11283993721008301
step: 9000, Loss: 0.11354795843362808
step: 9100, Loss: 0.1134943887591362
step: 9200, Loss: 0.11487515270709991
step: 9300, Loss: 0.11469227075576782
step: 9400, Loss: 0.11448279768228531
step: 9500, Loss: 0.11475154757499695
step: 9600, Loss: 0.11402835696935654
step: 9700, Loss: 0.11358252167701721
step: 9800, Loss: 0.11384102702140808
step: 9800, Loss: 0.11351136118173599
step: 9900, Loss: 0.11334399878978729
training successfully ended.
validating...
validate data length:102
acc: 0.9791666666666666
precision: 0.9534883720930233
recall: 1.0
F_score: 0.9761904761904763
******fold 8******

Training... train_data length:924
step: 0, Loss: 0.1184341311454773
step: 100, Loss: 0.12137530744075775
step: 200, Loss: 0.11801648885011673
step: 300, Loss: 0.11507429927587509
step: 400, Loss: 0.11550161987543106
step: 500, Loss: 0.11633703857660294
step: 600, Loss: 0.11454645544290543
step: 700, Loss: 0.11406633257865906
step: 800, Loss: 0.11406469345092773
step: 900, Loss: 0.11440518498420715
step: 1000, Loss: 0.11351452767848969
step: 1100, Loss: 0.1132848858833313
step: 1200, Loss: 0.11423753947019577
step: 1300, Loss: 0.11507199704647064
step: 1400, Loss: 0.1144423708319664
step: 1500, Loss: 0.11336301267147064
step: 1600, Loss: 0.11453817039728165
step: 1700, Loss: 0.11631844192743301
step: 1800, Loss: 0.11407066136598587
step: 1900, Loss: 0.1140146553516388
step: 2000, Loss: 0.11467694491147995
step: 2100, Loss: 0.11400216072797775
step: 2200, Loss: 0.11395414918661118
step: 2300, Loss: 0.11433113366365433
step: 2400, Loss: 0.1151992678642273
step: 2500, Loss: 0.1149555891752243
step: 2600, Loss: 0.11457350850105286
step: 2700, Loss: 0.11433388292789459
step: 2800, Loss: 0.9487550854682922
step: 2900, Loss: 0.16260337829589844
step: 3000, Loss: 0.133194237947464
step: 3100, Loss: 0.13155299425125122
step: 3200, Loss: 0.12494553625583649
step: 3300, Loss: 0.12186501920223236
step: 3400, Loss: 0.12460799515247345
step: 3500, Loss: 0.12067259848117828
step: 3600, Loss: 0.11994241178035736
step: 3700, Loss: 0.11934781819581985
step: 3800, Loss: 0.12159749865531921
step: 3900, Loss: 0.11729277670383453
step: 4000, Loss: 0.11659972369670868
step: 4100, Loss: 0.11544005572795868
step: 4200, Loss: 0.1170971691608429
step: 4300, Loss: 0.1170806959271431
step: 4400, Loss: 0.11603334546089172
step: 4500, Loss: 0.11778299510478973
step: 4600, Loss: 0.11797711253166199
step: 4700, Loss: 0.11532418429851532
step: 4800, Loss: 0.11629828810691833
step: 4900, Loss: 0.11516249924898148
step: 5000, Loss: 0.11795980483293533
step: 5100, Loss: 0.11376700550317764
step: 5200, Loss: 0.11466016620397568
step: 5300, Loss: 0.1137167438864708
step: 5400, Loss: 0.11513037979602814
step: 5500, Loss: 0.11542269587516785
step: 5600, Loss: 0.11494569480419159
step: 5700, Loss: 0.11444422602653503
step: 5800, Loss: 0.11352650821208954
step: 5900, Loss: 0.11441783607006073
step: 6000, Loss: 0.11451082676649094
step: 6100, Loss: 0.11532984673976898
step: 6200, Loss: 0.11640602350234985
step: 6300, Loss: 0.1132303774356842
step: 6400, Loss: 0.11311987042427063
step: 6500, Loss: 0.11463703215122223
step: 6600, Loss: 0.1142304539680481
step: 6700, Loss: 0.11368528753519058
step: 6800, Loss: 0.11516700685024261
step: 6900, Loss: 0.11371298134326935
step: 7000, Loss: 0.11465594917535782
step: 7100, Loss: 0.112930528819561
step: 7200, Loss: 0.1142091229557991
step: 7300, Loss: 0.1152055561542511
step: 7400, Loss: 0.11321967095136642
step: 7500, Loss: 0.1138775497674942
step: 7600, Loss: 0.11334825307130814
step: 7700, Loss: 0.11337807029485703
step: 7800, Loss: 0.1140465959906578
step: 7900, Loss: 0.11374938488006592
step: 8000, Loss: 0.11411531269550323
step: 8100, Loss: 0.11415252834558487
step: 8200, Loss: 0.11512896418571472
step: 8300, Loss: 0.11339913308620453
step: 8400, Loss: 0.1138916164636612
step: 8500, Loss: 0.11391220986843109
step: 8600, Loss: 0.11368148773908615
step: 8700, Loss: 0.11461308598518372
step: 8800, Loss: 0.11346118152141571
step: 8900, Loss: 0.11361432075500488
step: 9000, Loss: 0.11313609778881073
step: 9100, Loss: 0.11313477158546448
step: 9200, Loss: 0.11307796090841293
step: 9300, Loss: 0.11507797986268997
step: 9400, Loss: 0.11385627090930939
step: 9500, Loss: 0.11514097452163696
step: 9600, Loss: 0.11410477757453918
step: 9700, Loss: 0.11400580406188965
step: 9800, Loss: 0.11456546187400818
step: 9900, Loss: 0.1140318214893341
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:924
step: 0, Loss: 0.11409589648246765
step: 100, Loss: 0.11651439964771271
step: 200, Loss: 0.1143788993358612
step: 300, Loss: 0.11368511617183685
step: 400, Loss: 0.11367163062095642
step: 500, Loss: 0.11461175978183746
step: 600, Loss: 0.11461715400218964
step: 700, Loss: 0.1132509708404541
step: 800, Loss: 0.11354636400938034
step: 900, Loss: 0.11469170451164246
step: 1000, Loss: 0.11422787606716156
step: 1100, Loss: 0.7218267917633057
step: 1200, Loss: 0.14655596017837524
step: 1300, Loss: 0.12736906111240387
step: 1400, Loss: 0.1245599165558815
step: 1500, Loss: 0.12403375655412674
step: 1600, Loss: 0.12213532626628876
step: 1700, Loss: 0.12033011764287949
step: 1800, Loss: 0.1191580519080162
step: 1900, Loss: 0.11954548209905624
step: 2000, Loss: 0.11978858709335327
step: 2100, Loss: 0.12017156928777695
step: 2200, Loss: 0.11818160861730576
step: 2300, Loss: 0.11770663410425186
step: 2400, Loss: 0.11809813976287842
step: 2500, Loss: 0.11736646294593811
step: 2600, Loss: 0.11940561234951019
step: 2700, Loss: 0.11521033197641373
step: 2800, Loss: 0.1184120923280716
step: 2900, Loss: 0.11606350541114807
step: 3000, Loss: 0.11795438826084137
step: 3100, Loss: 0.11698020249605179
step: 3200, Loss: 0.11756081879138947
step: 3300, Loss: 0.11555854231119156
step: 3400, Loss: 0.11471131443977356
step: 3500, Loss: 0.11479044705629349
step: 3600, Loss: 0.11595252901315689
step: 3700, Loss: 0.11524459719657898
step: 3800, Loss: 0.11471639573574066
step: 3900, Loss: 0.11470159888267517
step: 4000, Loss: 0.11423760652542114
step: 4100, Loss: 0.11393867433071136
step: 4200, Loss: 0.11647950857877731
step: 4300, Loss: 0.11431456357240677
step: 4400, Loss: 0.11475732922554016
step: 4500, Loss: 0.11649268865585327
step: 4600, Loss: 0.11377386748790741
step: 4700, Loss: 0.11395113915205002
step: 4800, Loss: 0.11319142580032349
step: 4900, Loss: 0.11427031457424164
step: 5000, Loss: 0.11290515214204788
step: 5100, Loss: 0.11324465274810791
step: 5200, Loss: 0.1139826849102974
step: 5300, Loss: 0.11467980593442917
step: 5400, Loss: 0.11382565647363663
step: 5500, Loss: 0.11334608495235443
step: 5600, Loss: 0.11289653182029724
step: 5700, Loss: 0.11302007734775543
step: 5800, Loss: 0.11380806565284729
step: 5900, Loss: 0.1135350912809372
step: 6000, Loss: 0.11371847242116928
step: 6100, Loss: 0.11348189413547516
step: 6200, Loss: 0.11302889138460159
step: 6300, Loss: 0.11259926855564117
step: 6400, Loss: 0.11286702752113342
step: 6500, Loss: 0.11350102722644806
step: 6600, Loss: 0.1147475615143776
step: 6700, Loss: 0.11404167115688324
step: 6800, Loss: 0.11281836777925491
step: 6900, Loss: 0.11308127641677856
step: 7000, Loss: 0.11389646679162979
step: 7100, Loss: 0.11439716070890427
step: 7200, Loss: 0.11343753337860107
step: 7300, Loss: 0.11280178278684616
step: 7400, Loss: 0.1150960922241211
step: 7500, Loss: 0.11324343085289001
step: 7600, Loss: 0.11398079246282578
step: 7700, Loss: 0.11369537562131882
step: 7800, Loss: 0.11347102373838425
step: 7900, Loss: 0.1138935387134552
step: 8000, Loss: 0.11355781555175781
step: 8100, Loss: 0.11445352435112
step: 8200, Loss: 0.11339055746793747
step: 8300, Loss: 0.11442568153142929
step: 8400, Loss: 0.11369659751653671
step: 8500, Loss: 0.11510790884494781
step: 8600, Loss: 0.11311089992523193
step: 8700, Loss: 0.1135840192437172
step: 8800, Loss: 0.11286130547523499
step: 8900, Loss: 0.11391831189393997
step: 9000, Loss: 0.11447706818580627
step: 9100, Loss: 0.11295564472675323
step: 9200, Loss: 0.11500780284404755
step: 9300, Loss: 0.11439252644777298
step: 9400, Loss: 0.11414439976215363
step: 9500, Loss: 0.11425639688968658
step: 9600, Loss: 2.3228797912597656
step: 9700, Loss: 0.19614791870117188
step: 9800, Loss: 0.13657021522521973
step: 9900, Loss: 0.13565786182880402
training successfully ended.
validating...
validate data length:102
acc: 0.96875
precision: 0.9464285714285714
recall: 1.0
F_score: 0.9724770642201834
******fold 10******

step: 9900, Loss: 0.1131984069943428
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7777777777777778
recall: 0.8235294117647058
F_score: 0.7999999999999999
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.11292906850576401
step: 100, Loss: 0.11487115174531937
step: 200, Loss: 0.11617612838745117
step: 300, Loss: 0.11627468466758728
step: 400, Loss: 0.11580413579940796
step: 500, Loss: 0.11431150138378143
step: 600, Loss: 0.11380486190319061
step: 700, Loss: 0.1140076220035553
step: 800, Loss: 0.11311548948287964
step: 900, Loss: 0.11355264484882355
step: 1000, Loss: 0.11387921869754791
step: 1100, Loss: 0.11320552974939346
step: 1200, Loss: 0.11518015712499619
step: 1300, Loss: 0.11345059424638748
step: 1400, Loss: 0.11483583599328995
step: 1500, Loss: 0.11659219861030579
step: 1600, Loss: 0.11399808526039124
step: 1700, Loss: 0.11392643302679062
step: 1800, Loss: 0.11328417807817459
step: 1900, Loss: 0.11435197293758392
step: 2000, Loss: 0.11356715857982635
step: 2100, Loss: 0.11425501853227615
step: 2200, Loss: 0.11459195613861084
step: 2300, Loss: 0.1145615205168724
step: 2400, Loss: 0.11421433091163635
step: 2500, Loss: 0.11393449455499649
step: 2600, Loss: 0.11281302571296692
step: 2700, Loss: 0.11428560316562653
step: 2800, Loss: 0.11416817456483841
step: 2900, Loss: 0.11530318856239319
step: 3000, Loss: 0.11424888670444489
step: 3100, Loss: 0.11404459178447723
step: 3200, Loss: 0.11381204426288605
step: 3300, Loss: 0.1147661879658699
step: 3400, Loss: 0.11283641308546066
step: 3500, Loss: 0.11428338289260864
step: 3600, Loss: 0.11332643777132034
step: 3700, Loss: 0.11351682245731354
step: 3800, Loss: 0.1146419495344162
step: 3900, Loss: 0.11407448351383209
step: 4000, Loss: 0.1127384752035141
step: 4100, Loss: 0.11357288807630539
step: 4200, Loss: 0.11324506998062134
step: 4300, Loss: 0.11647170037031174
step: 4400, Loss: 0.11365777254104614
step: 4500, Loss: 0.1159534901380539
step: 4600, Loss: 0.1135137528181076
step: 4700, Loss: 0.1137181892991066
step: 4800, Loss: 0.26708850264549255
step: 4900, Loss: 0.12860076129436493
step: 5000, Loss: 0.12512685358524323
step: 5100, Loss: 0.12143512815237045
step: 5200, Loss: 0.11838747560977936
step: 5300, Loss: 0.11864765733480453
step: 5400, Loss: 0.11577393114566803
step: 5500, Loss: 0.11590559780597687
step: 5600, Loss: 0.11880707740783691
step: 5700, Loss: 0.1176634207367897
step: 5800, Loss: 0.1158168762922287
step: 5900, Loss: 0.12386536598205566
step: 6000, Loss: 0.1152099072933197
step: 6100, Loss: 0.1230393797159195
step: 6200, Loss: 0.11518232524394989
step: 6300, Loss: 0.11639010161161423
step: 6400, Loss: 0.11568092554807663
step: 6500, Loss: 0.1149081289768219
step: 6600, Loss: 0.11320401728153229
step: 6700, Loss: 0.11605238169431686
step: 6800, Loss: 0.11276211589574814
step: 6900, Loss: 0.11511901021003723
step: 7000, Loss: 0.11668837070465088
step: 7100, Loss: 0.11450963467359543
step: 7200, Loss: 0.11385326087474823
step: 7300, Loss: 0.114525206387043
step: 7400, Loss: 0.1177021935582161
step: 7500, Loss: 0.11631868779659271
step: 7600, Loss: 0.11360898613929749
step: 7700, Loss: 0.11516548693180084
step: 7800, Loss: 0.11543747782707214
step: 7900, Loss: 0.11687677353620529
step: 8000, Loss: 0.11371809989213943
step: 8100, Loss: 0.1152922660112381
step: 8200, Loss: 0.11361908167600632
step: 8300, Loss: 0.11616566777229309
step: 8400, Loss: 0.11380045115947723
step: 8500, Loss: 0.11323535442352295
step: 8600, Loss: 0.11417558044195175
step: 8700, Loss: 0.1137612909078598
step: 8800, Loss: 0.11491324007511139
step: 8900, Loss: 0.11400933563709259
step: 9000, Loss: 0.11495010554790497
step: 9100, Loss: 0.11300985515117645
step: 9200, Loss: 0.11408092081546783
step: 9300, Loss: 0.11385604739189148
step: 9400, Loss: 0.11416158825159073
step: 9500, Loss: 0.11438394337892532
step: 9600, Loss: 0.11397583037614822
step: 9700, Loss: 0.11390179395675659
step: 9800, Loss: 0.1132417693734169
step: 9900, Loss: 0.11439070105552673
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.9090909090909091
recall: 0.6666666666666666
F_score: 0.7692307692307692
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.113531693816185
step: 100, Loss: 0.11688031256198883
step: 200, Loss: 0.11456809937953949
step: 300, Loss: 0.11505025625228882
step: 400, Loss: 0.11452185362577438
step: 500, Loss: 0.11309891939163208
step: 600, Loss: 0.11348489671945572
step: 700, Loss: 0.11356019973754883
step: 800, Loss: 0.11420200020074844
step: 900, Loss: 0.11367490887641907
step: 1000, Loss: 0.11360117048025131
step: 1100, Loss: 0.11305657774209976
step: 1200, Loss: 0.11311359703540802
step: 1300, Loss: 0.11566203832626343
step: 1400, Loss: 0.1139456033706665
step: 1500, Loss: 0.11870687454938889
step: 1600, Loss: 0.11353415250778198
step: 1700, Loss: 0.1131414845585823
step: 1800, Loss: 0.11337298899888992
step: 1900, Loss: 0.11350233852863312
step: 2000, Loss: 0.11270105838775635
step: 2100, Loss: 0.11433453112840652
step: 2200, Loss: 0.1142895370721817
step: 2300, Loss: 0.1140112578868866
step: 2400, Loss: 0.11321597546339035
step: 2500, Loss: 0.11641686409711838
step: 2600, Loss: 0.11321082711219788
step: 2700, Loss: 0.11341451853513718
step: 2800, Loss: 0.11313538253307343
step: 2900, Loss: 0.11436856538057327
step: 3000, Loss: 0.11305510997772217
step: 3100, Loss: 0.11350204050540924
step: 3200, Loss: 0.11428207159042358
step: 3300, Loss: 0.11466780304908752
step: 3400, Loss: 0.11405421793460846
step: 3500, Loss: 0.11600804328918457
step: 3600, Loss: 0.1141405776143074
step: 3700, Loss: 0.1140320897102356
step: 3800, Loss: 0.11548801511526108
step: 3900, Loss: 0.11328588426113129
step: 4000, Loss: 0.1132277399301529
step: 4100, Loss: 0.11347267031669617
step: 4200, Loss: 0.11290131509304047
step: 4300, Loss: 0.1137886494398117
step: 4400, Loss: 0.11377829313278198
step: 4500, Loss: 0.11391819268465042
step: 4600, Loss: 0.11402839422225952
step: 4700, Loss: 0.6957230567932129
step: 4800, Loss: 0.12575018405914307
step: 4900, Loss: 0.11816342175006866
step: 5000, Loss: 0.12126807868480682
step: 5100, Loss: 0.11760547012090683
step: 5200, Loss: 0.11707707494497299
step: 5300, Loss: 0.11654624342918396
step: 5400, Loss: 0.11530562490224838
step: 5500, Loss: 0.11622430384159088
step: 5600, Loss: 0.1179501861333847
step: 5700, Loss: 0.11491362750530243
step: 5800, Loss: 0.115291528403759
step: 5900, Loss: 0.11783839017152786
step: 6000, Loss: 0.11650864779949188
step: 6100, Loss: 0.11304059624671936
step: 6200, Loss: 0.1162230372428894
step: 6300, Loss: 0.11648020148277283
step: 6400, Loss: 0.11584379523992538
step: 6500, Loss: 0.11546887457370758
step: 6600, Loss: 0.11431533098220825
step: 6700, Loss: 0.11625996232032776
step: 6800, Loss: 0.11481452733278275
step: 6900, Loss: 0.11476044356822968
step: 7000, Loss: 0.11407146602869034
step: 7100, Loss: 0.11560549587011337
step: 7200, Loss: 0.11430035531520844
step: 7300, Loss: 0.11351631581783295
step: 7400, Loss: 0.11505548655986786
step: 7500, Loss: 0.11435674875974655
step: 7600, Loss: 0.11348246037960052
step: 7700, Loss: 0.11385184526443481
step: 7800, Loss: 0.11408396810293198
step: 7900, Loss: 0.1144242063164711
step: 8000, Loss: 0.1166534423828125
step: 8100, Loss: 0.11496029049158096
step: 8200, Loss: 0.11300430446863174
step: 8300, Loss: 0.1148766279220581
step: 8400, Loss: 0.11395865678787231
step: 8500, Loss: 0.11421045660972595
step: 8600, Loss: 0.11306729912757874
step: 8700, Loss: 0.11451556533575058
step: 8800, Loss: 0.11490899324417114
step: 8900, Loss: 0.11444836109876633
step: 9000, Loss: 0.11397037655115128
step: 9100, Loss: 0.11293120682239532
step: 9200, Loss: 0.11374430358409882
step: 9300, Loss: 0.11429093778133392
step: 9400, Loss: 0.11391309648752213
step: 9500, Loss: 0.11441193521022797
step: 9600, Loss: 0.11633069068193436
step: 9700, Loss: 0.11385179311037064
step: 9800, Loss: 0.11263363063335419
step: 9900, Loss: 0.11310754716396332
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.875
recall: 0.8235294117647058
F_score: 0.8484848484848485
Training... train_data length:924
step: 0, Loss: 0.11431380361318588
step: 100, Loss: 0.11431441456079483
step: 200, Loss: 0.11387650668621063
step: 300, Loss: 0.11285483837127686
step: 400, Loss: 0.11394844949245453
step: 500, Loss: 0.1135900467634201
step: 600, Loss: 0.11522417515516281
step: 700, Loss: 0.1131385788321495
step: 800, Loss: 0.11355853080749512
step: 900, Loss: 0.1142275258898735
step: 1000, Loss: 0.11430700868368149
step: 1100, Loss: 0.11341142654418945
step: 1200, Loss: 0.11407031118869781
step: 1300, Loss: 0.11409150063991547
step: 1400, Loss: 0.11305591464042664
step: 1500, Loss: 0.11454353481531143
step: 1600, Loss: 0.11406190693378448
step: 1700, Loss: 0.21942201256752014
step: 1800, Loss: 0.22631725668907166
step: 1900, Loss: 0.1248568668961525
step: 2000, Loss: 0.12479604780673981
step: 2100, Loss: 0.12119428813457489
step: 2200, Loss: 0.11768651753664017
step: 2300, Loss: 0.11591900885105133
step: 2400, Loss: 0.11511335521936417
step: 2500, Loss: 0.12042675167322159
step: 2600, Loss: 0.11536796391010284
step: 2700, Loss: 0.11638526618480682
step: 2800, Loss: 0.11665299534797668
step: 2900, Loss: 0.11721085757017136
step: 3000, Loss: 0.11617152392864227
step: 3100, Loss: 0.11568675935268402
step: 3200, Loss: 0.11621765792369843
step: 3300, Loss: 0.115829236805439
step: 3400, Loss: 0.11409565806388855
step: 3500, Loss: 0.11556543409824371
step: 3600, Loss: 0.1144343763589859
step: 3700, Loss: 0.11452117562294006
step: 3800, Loss: 0.11519667506217957
step: 3900, Loss: 0.11625584959983826
step: 4000, Loss: 0.1145683005452156
step: 4100, Loss: 0.11452506482601166
step: 4200, Loss: 0.115438312292099
step: 4300, Loss: 0.11427618563175201
step: 4400, Loss: 0.11460506170988083
step: 4500, Loss: 0.11463804543018341
step: 4600, Loss: 0.11392098665237427
step: 4700, Loss: 0.11463432013988495
step: 4800, Loss: 0.11311578750610352
step: 4900, Loss: 0.11337295174598694
step: 5000, Loss: 0.11381754279136658
step: 5100, Loss: 0.11285918951034546
step: 5200, Loss: 0.11533288657665253
step: 5300, Loss: 0.1137055829167366
step: 5400, Loss: 0.11329862475395203
step: 5500, Loss: 0.11439615488052368
step: 5600, Loss: 0.11464453488588333
step: 5700, Loss: 0.1132633239030838
step: 5800, Loss: 0.11352462321519852
step: 5900, Loss: 0.11428694427013397
step: 6000, Loss: 0.1128852516412735
step: 6100, Loss: 0.11324921250343323
step: 6200, Loss: 0.11336927860975266
step: 6300, Loss: 0.11286281049251556
step: 6400, Loss: 0.11231912672519684
step: 6500, Loss: 0.11444197595119476
step: 6600, Loss: 0.11379292607307434
step: 6700, Loss: 0.11407411098480225
step: 6800, Loss: 0.1131875216960907
step: 6900, Loss: 0.11399916559457779
step: 7000, Loss: 0.11427284777164459
step: 7100, Loss: 0.11349663138389587
step: 7200, Loss: 0.1131780669093132
step: 7300, Loss: 0.11356352269649506
step: 7400, Loss: 0.11383005231618881
step: 7500, Loss: 0.1136293113231659
step: 7600, Loss: 0.11499340832233429
step: 7700, Loss: 0.11377308517694473
step: 7800, Loss: 0.11328975111246109
step: 7900, Loss: 0.11349397897720337
step: 8000, Loss: 0.11357593536376953
step: 8100, Loss: 0.11315032094717026
step: 8200, Loss: 0.11364482343196869
step: 8300, Loss: 0.11363228410482407
step: 8400, Loss: 0.11321821808815002
step: 8500, Loss: 0.11365193128585815
step: 8600, Loss: 0.11349908262491226
step: 8700, Loss: 0.11354727298021317
step: 8800, Loss: 0.11404666304588318
step: 8900, Loss: 0.11261303722858429
step: 9000, Loss: 0.11416122317314148
step: 9100, Loss: 0.11341657489538193
step: 9200, Loss: 0.11523710936307907
step: 9300, Loss: 0.11401265859603882
step: 9400, Loss: 0.1144682988524437
step: 9500, Loss: 0.11663144826889038
step: 9600, Loss: 0.11365493386983871
step: 9700, Loss: 0.11344271153211594
step: 9800, Loss: 0.11334299296140671
step: 9900, Loss: 0.11308952420949936
training successfully ended.
validating...
validate data length:102
acc: 0.9895833333333334
precision: 0.9777777777777777
recall: 1.0
F_score: 0.9887640449438202
subject 4 Avgacc: 0.9697916666666668 Avgfscore: 0.9708089210005666 
 Max acc:1.0, Max f score:1.0
******** mix subject_5 ********

[266, 494]
******fold 1******

Training... train_data length:889
step: 0, Loss: 41.414424896240234
step: 100, Loss: 4.579486846923828
step: 200, Loss: 3.1104416847229004
step: 300, Loss: 0.23290975391864777
step: 400, Loss: 0.16214986145496368
step: 500, Loss: 0.1510593742132187
step: 600, Loss: 0.13121072947978973
step: 700, Loss: 0.1374615728855133
step: 800, Loss: 0.13731083273887634
step: 900, Loss: 0.1420598328113556
step: 1000, Loss: 0.139301598072052
step: 1100, Loss: 0.1312137246131897
step: 1200, Loss: 0.12258969992399216
step: 1300, Loss: 0.12874624133110046
step: 1400, Loss: 0.12446686625480652
step: 1500, Loss: 0.12826988101005554
step: 1600, Loss: 0.1265917420387268
step: 1700, Loss: 0.12114228308200836
step: 1800, Loss: 0.11904886364936829
step: 1900, Loss: 0.12258818745613098
step: 2000, Loss: 0.11877896636724472
step: 2100, Loss: 0.12259998917579651
step: 2200, Loss: 0.1235455572605133
step: 2300, Loss: 0.1186201274394989
step: 2400, Loss: 0.11748737096786499
step: 2500, Loss: 0.12047721445560455
step: 2600, Loss: 0.11588841676712036
step: 2700, Loss: 0.11942635476589203
step: 2800, Loss: 0.11739281564950943
step: 2900, Loss: 0.11740230023860931
step: 3000, Loss: 0.11621270328760147
step: 3100, Loss: 0.11729981005191803
step: 3200, Loss: 0.1150519996881485
step: 3300, Loss: 0.11866018176078796
step: 3400, Loss: 0.11822954565286636
step: 3500, Loss: 0.11620505899190903
step: 3600, Loss: 0.11495380103588104
step: 3700, Loss: 0.11765452474355698
step: 3800, Loss: 0.1170053631067276
step: 3900, Loss: 0.11498134583234787
step: 4000, Loss: 0.11664816737174988
step: 4100, Loss: 0.11735713481903076
step: 4200, Loss: 0.1145702451467514
step: 4300, Loss: 0.11523841321468353
step: 4400, Loss: 0.11515622586011887
step: 4500, Loss: 0.11606666445732117
step: 4600, Loss: 0.11437268555164337
step: 4700, Loss: 0.11518928408622742
step: 4800, Loss: 0.11561080068349838
step: 4900, Loss: 0.11371596157550812
step: 5000, Loss: 0.1152850016951561
step: 5100, Loss: 0.11535703390836716
step: 5200, Loss: 0.11452943086624146
step: 5300, Loss: 0.11515159904956818
step: 5400, Loss: 0.1155773401260376
step: 5500, Loss: 0.11465194821357727
step: 5600, Loss: 0.1155528724193573
step: 5700, Loss: 0.1142100989818573
step: 5800, Loss: 0.11535823345184326
step: 5900, Loss: 0.11413612216711044
step: 6000, Loss: 0.11535994708538055
step: 6100, Loss: 9.50650691986084
step: 6200, Loss: 1.9839824438095093
step: 6300, Loss: 0.17916393280029297
step: 6400, Loss: 0.13303443789482117
step: 6500, Loss: 0.14104825258255005
step: 6600, Loss: 0.13936758041381836
step: 6700, Loss: 0.12897467613220215
step: 6800, Loss: 0.12269958853721619
step: 6900, Loss: 0.12909339368343353
step: 7000, Loss: 0.12305010110139847
step: 7100, Loss: 0.1263163536787033
step: 7200, Loss: 0.12106212973594666
step: 7300, Loss: 0.11810950934886932
step: 7400, Loss: 0.11985369771718979
step: 7500, Loss: 0.12277989089488983
step: 7600, Loss: 0.11855912953615189
step: 7700, Loss: 0.11927725374698639
step: 7800, Loss: 0.11858119070529938
step: 7900, Loss: 0.1172766461968422
step: 8000, Loss: 0.11828585714101791
step: 8100, Loss: 0.11898461729288101
step: 8200, Loss: 0.12032359838485718
step: 8300, Loss: 0.12064751982688904
step: 8400, Loss: 0.11944954842329025
step: 8500, Loss: 0.1168605238199234
step: 8600, Loss: 0.1164308562874794
step: 8700, Loss: 0.11696656793355942
step: 8800, Loss: 0.11677899956703186
step: 8900, Loss: 2.4829299449920654
step: 9000, Loss: 0.46928608417510986
step: 9100, Loss: 0.1407046765089035
step: 9200, Loss: 0.131964772939682
step: 9300, Loss: 0.13854296505451202
step: 9400, Loss: 0.11750245094299316
step: 9500, Loss: 0.12685078382492065
step: 9600, Loss: 0.12886786460876465
step: 9700, Loss: 0.12291894108057022
step: 9800, Loss: 0.1223021075129509
step: 9900, Loss: 0.12094301730394363
training successfully ended.
validating...
validate data length:99
acc: 0.8854166666666666
precision: 0.9122807017543859
recall: 0.896551724137931
F_score: 0.9043478260869565
******fold 2******

Training... train_data length:889
step: 0, Loss: 0.3364037275314331
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.11435748636722565
step: 100, Loss: 0.11396276205778122
step: 200, Loss: 0.11415915936231613
step: 300, Loss: 0.1125357523560524
step: 400, Loss: 0.11393700540065765
step: 500, Loss: 0.11444514244794846
step: 600, Loss: 0.11447224020957947
step: 700, Loss: 0.11331051588058472
step: 800, Loss: 0.11538863182067871
step: 900, Loss: 0.11548472940921783
step: 1000, Loss: 0.11381907016038895
step: 1100, Loss: 0.11351649463176727
step: 1200, Loss: 0.11378884315490723
step: 1300, Loss: 0.11415554583072662
step: 1400, Loss: 0.11402890086174011
step: 1500, Loss: 0.11402083933353424
step: 1600, Loss: 0.11288081854581833
step: 1700, Loss: 0.11343877017498016
step: 1800, Loss: 0.11351088434457779
step: 1900, Loss: 0.11396156251430511
step: 2000, Loss: 0.11362002044916153
step: 2100, Loss: 0.11334088444709778
step: 2200, Loss: 0.11449576169252396
step: 2300, Loss: 0.11962538957595825
step: 2400, Loss: 0.11285829544067383
step: 2500, Loss: 0.11324424296617508
step: 2600, Loss: 0.11367130279541016
step: 2700, Loss: 0.11384313553571701
step: 2800, Loss: 0.11416268348693848
step: 2900, Loss: 0.11451287567615509
step: 3000, Loss: 0.11317325383424759
step: 3100, Loss: 0.11420533061027527
step: 3200, Loss: 0.1136309877038002
step: 3300, Loss: 0.11356239020824432
step: 3400, Loss: 0.11525314301252365
step: 3500, Loss: 0.11249619722366333
step: 3600, Loss: 0.11412198841571808
step: 3700, Loss: 0.11387231200933456
step: 3800, Loss: 0.11391846835613251
step: 3900, Loss: 0.1135173812508583
step: 4000, Loss: 0.11478672176599503
step: 4100, Loss: 0.11362685263156891
step: 4200, Loss: 0.14910060167312622
step: 4300, Loss: 0.12208256125450134
step: 4400, Loss: 0.1193990632891655
step: 4500, Loss: 0.1180734634399414
step: 4600, Loss: 0.12010844796895981
step: 4700, Loss: 0.11711324751377106
step: 4800, Loss: 0.11703959852457047
step: 4900, Loss: 0.11752445250749588
step: 5000, Loss: 0.11746912449598312
step: 5100, Loss: 0.11529211699962616
step: 5200, Loss: 0.1176140159368515
step: 5300, Loss: 0.11647774279117584
step: 5400, Loss: 0.11450909823179245
step: 5500, Loss: 0.11647729575634003
step: 5600, Loss: 0.118727907538414
step: 5700, Loss: 0.1144334152340889
step: 5800, Loss: 0.11455194652080536
step: 5900, Loss: 0.11382051557302475
step: 6000, Loss: 0.11831609904766083
step: 6100, Loss: 0.11619945615530014
step: 6200, Loss: 0.11607059836387634
step: 6300, Loss: 0.11255002021789551
step: 6400, Loss: 0.11378321796655655
step: 6500, Loss: 0.11580057442188263
step: 6600, Loss: 0.11381712555885315
step: 6700, Loss: 0.1165720522403717
step: 6800, Loss: 0.1147226095199585
step: 6900, Loss: 0.11437874287366867
step: 7000, Loss: 0.11428417265415192
step: 7100, Loss: 0.11376165598630905
step: 7200, Loss: 0.11313052475452423
step: 7300, Loss: 0.11389120668172836
step: 7400, Loss: 0.11469477415084839
step: 7500, Loss: 0.11457333713769913
step: 7600, Loss: 0.1133682131767273
step: 7700, Loss: 0.11402180045843124
step: 7800, Loss: 0.1136687621474266
step: 7900, Loss: 0.11393693089485168
step: 8000, Loss: 0.11405913531780243
step: 8100, Loss: 0.11435414850711823
step: 8200, Loss: 0.11352475732564926
step: 8300, Loss: 0.11417438089847565
step: 8400, Loss: 0.11371442675590515
step: 8500, Loss: 0.11422055214643478
step: 8600, Loss: 0.11367874592542648
step: 8700, Loss: 0.11282012611627579
step: 8800, Loss: 0.11610249429941177
step: 8900, Loss: 0.1130455806851387
step: 9000, Loss: 0.11411372572183609
step: 9100, Loss: 0.11287245154380798
step: 9200, Loss: 0.11336755007505417
step: 9300, Loss: 0.11424608528614044
step: 9400, Loss: 0.11257485300302505
step: 9500, Loss: 0.1135869100689888
step: 9600, Loss: 0.11395248770713806
step: 9700, Loss: 0.11299572885036469
step: 9800, Loss: 0.1151033490896225
step: 9900, Loss: 0.11373595893383026
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.7857142857142857
recall: 0.9166666666666666
F_score: 0.8461538461538461
subject 4 Avgacc: 0.8077083333333333 Avgfscore: 0.8117268081380985 
 Max acc:1.0, Max f score:1.0
******** mix subject_5 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 61.66053009033203
step: 100, Loss: 1.0163609981536865
step: 200, Loss: 0.13964806497097015
step: 300, Loss: 0.12417905777692795
step: 400, Loss: 0.12675337493419647
step: 500, Loss: 0.12617619335651398
step: 600, Loss: 0.13290655612945557
step: 700, Loss: 0.12322637438774109
step: 800, Loss: 0.12494542449712753
step: 900, Loss: 0.1213175505399704
step: 1000, Loss: 2.0958681106567383
step: 1100, Loss: 0.1311621069908142
step: 1200, Loss: 0.13416704535484314
step: 1300, Loss: 0.12109711021184921
step: 1400, Loss: 0.13455231487751007
step: 1500, Loss: 0.11820933222770691
step: 1600, Loss: 0.14812250435352325
step: 1700, Loss: 0.11924250423908234
step: 1800, Loss: 0.12249232083559036
step: 1900, Loss: 0.12004037946462631
step: 2000, Loss: 0.12106142938137054
step: 2100, Loss: 0.11613832414150238
step: 2200, Loss: 0.12808334827423096
step: 2300, Loss: 0.11824624985456467
step: 2400, Loss: 0.11879634857177734
step: 2500, Loss: 0.1170637235045433
step: 2600, Loss: 0.11587660014629364
step: 2700, Loss: 0.11490313708782196
step: 2800, Loss: 0.11642113327980042
step: 2900, Loss: 0.11564305424690247
step: 3000, Loss: 0.11704949289560318
step: 3100, Loss: 0.1295369267463684
step: 3200, Loss: 0.11777006089687347
step: 3300, Loss: 0.11963718384504318
step: 3400, Loss: 0.11520638316869736
step: 3500, Loss: 0.11983861774206161
step: 3600, Loss: 0.11404283344745636
step: 3700, Loss: 0.1166667491197586
step: 3800, Loss: 0.11534494906663895
step: 3900, Loss: 0.12322761118412018
step: 4000, Loss: 0.11556124687194824
step: 4100, Loss: 0.11709926277399063
step: 4200, Loss: 0.11589387059211731
step: 4300, Loss: 0.11453710496425629
step: 4400, Loss: 0.11775513738393784
step: 4500, Loss: 0.11389907449483871
step: 4600, Loss: 0.11746976524591446
step: 4700, Loss: 0.11627312004566193
step: 4800, Loss: 0.11375055462121964
step: 4900, Loss: 0.11511021107435226
step: 5000, Loss: 0.11472567915916443
step: 5100, Loss: 0.11529114097356796
step: 5200, Loss: 0.1145702600479126
step: 5300, Loss: 0.11728005111217499
step: 5400, Loss: 0.115515798330307
step: 5500, Loss: 0.11969958245754242
step: 5600, Loss: 0.11746983230113983
step: 5700, Loss: 0.11709430068731308
step: 5800, Loss: 0.11690415441989899
step: 5900, Loss: 0.11567079275846481
step: 6000, Loss: 0.11835628002882004
step: 6100, Loss: 0.12177272140979767
step: 6200, Loss: 0.11479506641626358
step: 6300, Loss: 0.11596041917800903
step: 6400, Loss: 0.1174122616648674
step: 6500, Loss: 5.859465599060059
step: 6600, Loss: 0.1314094513654709
step: 6700, Loss: 0.12529784440994263
step: 6800, Loss: 0.1257878988981247
step: 6900, Loss: 0.11904437839984894
step: 7000, Loss: 0.12444819509983063
step: 7100, Loss: 0.11838346719741821
step: 7200, Loss: 0.12885785102844238
step: 7300, Loss: 0.11727351695299149
step: 7400, Loss: 0.11772070080041885
step: 7500, Loss: 0.11466388404369354
step: 7600, Loss: 0.1173567846417427
step: 7700, Loss: 0.11509251594543457
step: 7800, Loss: 0.11621006578207016
step: 7900, Loss: 0.114209845662117
step: 8000, Loss: 0.11386039853096008
step: 8100, Loss: 0.11431404948234558
step: 8200, Loss: 0.1148398369550705
step: 8300, Loss: 0.11598019301891327
step: 8400, Loss: 0.11564349383115768
step: 8500, Loss: 0.11758303642272949
step: 8600, Loss: 0.12946513295173645
step: 8700, Loss: 0.11450184136629105
step: 8800, Loss: 0.11609914153814316
step: 8900, Loss: 0.11581994593143463
step: 9000, Loss: 0.11627303063869476
step: 9100, Loss: 0.11551724374294281
step: 9200, Loss: 0.11814028024673462
step: 9300, Loss: 0.11580252647399902
step: 9400, Loss: 0.11415933072566986
step: 9500, Loss: 0.11468144506216049
step: 9600, Loss: 0.11420612782239914
step: 9700, Loss: 0.11401691287755966
step: 9800, Loss: 0.11578112095594406
step: 9900, Loss: 0.11432473361492157
training successfully ended.
validating...
validate data length:32
acc: 0.46875
precision: 0.3684210526315789
recall: 0.5833333333333334
F_score: 0.4516129032258065
******fold 2******

Training... train_data length:280
step: 100, Loss: 0.1269906759262085
step: 200, Loss: 0.13021869957447052
step: 300, Loss: 0.1274288147687912
step: 400, Loss: 0.12248894572257996
step: 500, Loss: 0.1253260374069214
step: 600, Loss: 0.12047063559293747
step: 700, Loss: 0.11870752274990082
step: 800, Loss: 0.11841171979904175
step: 900, Loss: 0.12092507630586624
step: 1000, Loss: 0.11718542128801346
step: 1100, Loss: 0.11779062449932098
step: 1200, Loss: 0.12015490233898163
step: 1300, Loss: 0.11538515239953995
step: 1400, Loss: 0.11835350096225739
step: 1500, Loss: 0.11749112606048584
step: 1600, Loss: 0.11446353793144226
step: 1700, Loss: 0.11644168943166733
step: 1800, Loss: 0.11594626307487488
step: 1900, Loss: 0.11601569503545761
step: 2000, Loss: 0.11493298411369324
step: 2100, Loss: 0.11447811871767044
step: 2200, Loss: 0.11350389569997787
step: 2300, Loss: 0.11511195451021194
step: 2400, Loss: 0.11441987752914429
step: 2500, Loss: 0.1142166331410408
step: 2600, Loss: 0.11618005484342575
step: 2700, Loss: 0.11516404896974564
step: 2800, Loss: 0.11324754357337952
step: 2900, Loss: 0.115055613219738
step: 3000, Loss: 0.11427897214889526
step: 3100, Loss: 0.11496653407812119
step: 3200, Loss: 0.11508728563785553
step: 3300, Loss: 0.11425779014825821
step: 3400, Loss: 0.11353672295808792
step: 3500, Loss: 0.11676818877458572
step: 3600, Loss: 0.11522495001554489
step: 3700, Loss: 0.11512740701436996
step: 3800, Loss: 0.11360640823841095
step: 3900, Loss: 0.11431477218866348
step: 4000, Loss: 0.11503554880619049
step: 4100, Loss: 0.11404907703399658
step: 4200, Loss: 0.11396518349647522
step: 4300, Loss: 0.11481354385614395
step: 4400, Loss: 0.11396024376153946
step: 4500, Loss: 0.11342163383960724
step: 4600, Loss: 0.11361733078956604
step: 4700, Loss: 0.11384104192256927
step: 4800, Loss: 0.11322711408138275
step: 4900, Loss: 0.11292899399995804
step: 5000, Loss: 0.1138332337141037
step: 5100, Loss: 0.11442200094461441
step: 5200, Loss: 0.1128029152750969
step: 5300, Loss: 0.11509644240140915
step: 5400, Loss: 0.1149490475654602
step: 5500, Loss: 0.11515210568904877
step: 5600, Loss: 0.11347202211618423
step: 5700, Loss: 0.11466266214847565
step: 5800, Loss: 0.113329216837883
step: 5900, Loss: 0.11370940506458282
step: 6000, Loss: 0.7124146223068237
step: 6100, Loss: 0.21578022837638855
step: 6200, Loss: 0.1323351413011551
step: 6300, Loss: 0.14211389422416687
step: 6400, Loss: 0.12236674129962921
step: 6500, Loss: 0.12506960332393646
step: 6600, Loss: 0.12032660841941833
step: 6700, Loss: 0.12376813590526581
step: 6800, Loss: 0.1209074854850769
step: 6900, Loss: 0.11876557767391205
step: 7000, Loss: 0.11958737671375275
step: 7100, Loss: 0.1228107288479805
step: 7200, Loss: 0.11793407797813416
step: 7300, Loss: 0.11936437338590622
step: 7400, Loss: 0.11617130041122437
step: 7500, Loss: 0.11611951887607574
step: 7600, Loss: 0.11640354245901108
step: 7700, Loss: 0.1172131672501564
step: 7800, Loss: 0.11665463447570801
step: 7900, Loss: 0.11566244065761566
step: 8000, Loss: 0.11510802805423737
step: 8100, Loss: 0.1155739352107048
step: 8200, Loss: 0.11611287295818329
step: 8300, Loss: 0.11760064959526062
step: 8400, Loss: 0.11536084115505219
step: 8500, Loss: 0.11500456929206848
step: 8600, Loss: 0.11512809991836548
step: 8700, Loss: 0.11677617579698563
step: 8800, Loss: 0.11553125083446503
step: 8900, Loss: 0.11443906277418137
step: 9000, Loss: 0.11514928936958313
step: 9100, Loss: 0.11429852992296219
step: 9200, Loss: 0.11489139497280121
step: 9300, Loss: 0.11463623493909836
step: 9400, Loss: 0.11455558985471725
step: 9500, Loss: 0.1146872341632843
step: 9600, Loss: 0.11496654152870178
step: 9700, Loss: 0.11282060295343399
step: 9800, Loss: 0.11309459060430527
step: 9900, Loss: 0.1151144802570343
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.9166666666666666
recall: 1.0
F_score: 0.9565217391304348
******fold 3******

Training... train_data length:889
step: 0, Loss: 3.571812391281128
step: 100, Loss: 0.11954513192176819
step: 200, Loss: 0.1191704198718071
step: 300, Loss: 0.1167161613702774
step: 400, Loss: 0.11687660217285156
step: 500, Loss: 0.1147938072681427
step: 600, Loss: 0.22698155045509338
step: 700, Loss: 0.1139058992266655
step: 800, Loss: 0.11485427618026733
step: 900, Loss: 0.11690494418144226
step: 1000, Loss: 0.11556055396795273
step: 1100, Loss: 0.11448809504508972
step: 1200, Loss: 0.37188756465911865
step: 1300, Loss: 0.11404266953468323
step: 1400, Loss: 0.11524087190628052
step: 1500, Loss: 0.1143423467874527
step: 1600, Loss: 0.11323700100183487
step: 1700, Loss: 0.11478517204523087
step: 1800, Loss: 0.14366990327835083
step: 1900, Loss: 0.1151009351015091
step: 2000, Loss: 0.1150214746594429
step: 2100, Loss: 0.11489731073379517
step: 2200, Loss: 0.11505373567342758
step: 2300, Loss: 0.11665458977222443
step: 2400, Loss: 0.13531357049942017
step: 2500, Loss: 0.11370690912008286
step: 2600, Loss: 0.1139254942536354
step: 2700, Loss: 0.11338159441947937
step: 2800, Loss: 0.11424112319946289
step: 2900, Loss: 0.11321550607681274
step: 3000, Loss: 0.2213532030582428
step: 3100, Loss: 0.11466286331415176
step: 3200, Loss: 0.11538658291101456
step: 3300, Loss: 0.11592426896095276
step: 3400, Loss: 0.1139235869050026
step: 3500, Loss: 0.11377426236867905
step: 3600, Loss: 0.12658502161502838
step: 3700, Loss: 0.1144176721572876
step: 3800, Loss: 0.11430421471595764
step: 3900, Loss: 0.11471186578273773
step: 4000, Loss: 0.11374156922101974
step: 4100, Loss: 0.11416447907686234
step: 4200, Loss: 0.12615808844566345
step: 4300, Loss: 0.11490034312009811
step: 4400, Loss: 0.11493226885795593
step: 4500, Loss: 0.11398090422153473
step: 4600, Loss: 0.1136435717344284
step: 4700, Loss: 0.11504441499710083
step: 4800, Loss: 0.13483403623104095
step: 4900, Loss: 0.39812159538269043
step: 5000, Loss: 0.20250993967056274
step: 5100, Loss: 0.14825943112373352
step: 5200, Loss: 0.1400907039642334
step: 5300, Loss: 0.12108785659074783
step: 5400, Loss: 0.12151934206485748
step: 5500, Loss: 0.11979401856660843
step: 5600, Loss: 0.12148258090019226
step: 5700, Loss: 0.12242923676967621
step: 5800, Loss: 0.12265871465206146
step: 5900, Loss: 0.11804448813199997
step: 6000, Loss: 0.12541984021663666
step: 6100, Loss: 0.1203753724694252
step: 6200, Loss: 0.11702561378479004
step: 6300, Loss: 0.11902390420436859
step: 6400, Loss: 0.1198788583278656
step: 6500, Loss: 0.11609116196632385
step: 6600, Loss: 0.11820565164089203
step: 6700, Loss: 0.11800987273454666
step: 6800, Loss: 0.11648847907781601
step: 6900, Loss: 0.11511503159999847
step: 7000, Loss: 0.11596067994832993
step: 7100, Loss: 0.11475306749343872
step: 7200, Loss: 0.11820444464683533
step: 7300, Loss: 0.11819113045930862
step: 7400, Loss: 0.11466723680496216
step: 7500, Loss: 0.11426368355751038
step: 7600, Loss: 0.11532045900821686
step: 7700, Loss: 0.11567233502864838
step: 7800, Loss: 0.11476350575685501
step: 7900, Loss: 0.11681220680475235
step: 8000, Loss: 0.11569086462259293
step: 8100, Loss: 0.11480472981929779
step: 8200, Loss: 0.11463059484958649
step: 8300, Loss: 0.11405251920223236
step: 8400, Loss: 0.11383811384439468
step: 8500, Loss: 0.11583726853132248
step: 8600, Loss: 0.11440459638834
step: 8700, Loss: 0.11368125677108765
step: 8800, Loss: 0.11638395488262177
step: 8900, Loss: 0.11449943482875824
step: 9000, Loss: 0.11354716122150421
step: 9100, Loss: 0.11485783010721207
step: 9200, Loss: 0.11548389494419098
step: 9300, Loss: 0.11511017382144928
step: 9400, Loss: 0.1148567646741867
step: 9500, Loss: 0.11306127905845642
step: 9600, Loss: 0.11384742707014084
step: 9700, Loss: 0.11302635818719864
step: 9800, Loss: 0.11483892798423767
step: 9900, Loss: 0.11488991975784302
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.9615384615384616
recall: 1.0
F_score: 0.9803921568627451
******fold 4******

Training... train_data length:889
step: 0, Loss: 0.11412390321493149
step: 100, Loss: 0.12363877147436142
step: 200, Loss: 0.11787361651659012
step: 300, Loss: 0.11358147859573364
step: 400, Loss: 0.11492364853620529
step: 500, Loss: 0.11453405767679214
step: 600, Loss: 0.11623991280794144
step: 0, Loss: 0.1151605173945427
step: 100, Loss: 0.13055221736431122
step: 200, Loss: 0.12749046087265015
step: 300, Loss: 0.1197349950671196
step: 400, Loss: 0.12212042510509491
step: 500, Loss: 0.12004628032445908
step: 600, Loss: 0.12036984413862228
step: 700, Loss: 0.1168154776096344
step: 800, Loss: 0.1185566857457161
step: 900, Loss: 0.11597468703985214
step: 1000, Loss: 0.11828590929508209
step: 1100, Loss: 0.11598369479179382
step: 1200, Loss: 0.11378616094589233
step: 1300, Loss: 0.11638716608285904
step: 1400, Loss: 0.11584025621414185
step: 1500, Loss: 0.11502531915903091
step: 1600, Loss: 0.11524921655654907
step: 1700, Loss: 0.11404882371425629
step: 1800, Loss: 0.11586293578147888
step: 1900, Loss: 0.1212148517370224
step: 2000, Loss: 0.11660537123680115
step: 2100, Loss: 0.11554253846406937
step: 2200, Loss: 0.11446662247180939
step: 2300, Loss: 0.11581338942050934
step: 2400, Loss: 0.11923764646053314
step: 2500, Loss: 0.1152203381061554
step: 2600, Loss: 0.11550761759281158
step: 2700, Loss: 0.11513161659240723
step: 2800, Loss: 0.11437267810106277
step: 2900, Loss: 0.11432813853025436
step: 3000, Loss: 0.11438263952732086
step: 3100, Loss: 0.11549466848373413
step: 3200, Loss: 0.11552512645721436
step: 3300, Loss: 0.11562120169401169
step: 3400, Loss: 0.11599583178758621
step: 3500, Loss: 0.11537988483905792
step: 3600, Loss: 0.11641810834407806
step: 3700, Loss: 0.11999779939651489
step: 3800, Loss: 0.11790414899587631
step: 3900, Loss: 0.11413431167602539
step: 4000, Loss: 0.11530338227748871
step: 4100, Loss: 0.11767688393592834
step: 4200, Loss: 0.11533588171005249
step: 4300, Loss: 0.11630763858556747
step: 4400, Loss: 0.11816908419132233
step: 4500, Loss: 0.11384451389312744
step: 4600, Loss: 0.11475352942943573
step: 4700, Loss: 0.12077300250530243
step: 4800, Loss: 0.11552152782678604
step: 4900, Loss: 0.11691609025001526
step: 5000, Loss: 0.21033120155334473
step: 5100, Loss: 0.11948145925998688
step: 5200, Loss: 0.11899367719888687
step: 5300, Loss: 0.1169876754283905
step: 5400, Loss: 0.12452542036771774
step: 5500, Loss: 0.11714520305395126
step: 5600, Loss: 0.1185239776968956
step: 5700, Loss: 0.12072242051362991
step: 5800, Loss: 0.11816491186618805
step: 5900, Loss: 0.11406653374433517
step: 6000, Loss: 0.11881543695926666
step: 6100, Loss: 0.11716970056295395
step: 6200, Loss: 0.1186029314994812
step: 6300, Loss: 0.11710570007562637
step: 6400, Loss: 0.1209888607263565
step: 6500, Loss: 0.11809296905994415
step: 6600, Loss: 0.11838619410991669
step: 6700, Loss: 0.11535866558551788
step: 6800, Loss: 0.11744965612888336
step: 6900, Loss: 0.115960493683815
step: 7000, Loss: 0.11616924405097961
step: 7100, Loss: 0.11541767418384552
step: 7200, Loss: 0.11752115190029144
step: 7300, Loss: 0.11614295840263367
step: 7400, Loss: 0.11476979404687881
step: 7500, Loss: 0.11493604630231857
step: 7600, Loss: 0.11394289135932922
step: 7700, Loss: 0.1161719262599945
step: 7800, Loss: 0.1193789690732956
step: 7900, Loss: 0.11404204368591309
step: 8000, Loss: 0.1151556596159935
step: 8100, Loss: 0.11551996320486069
step: 8200, Loss: 0.11385495960712433
step: 8300, Loss: 0.11696557700634003
step: 8400, Loss: 0.11404464393854141
step: 8500, Loss: 0.11445516347885132
step: 8600, Loss: 0.11447189748287201
step: 8700, Loss: 0.11727728694677353
step: 8800, Loss: 0.11499707400798798
step: 8900, Loss: 0.11597730219364166
step: 9000, Loss: 0.11651188880205154
step: 9100, Loss: 0.11982595920562744
step: 9200, Loss: 0.11545708775520325
step: 9300, Loss: 0.11753230541944504
step: 9400, Loss: 0.11656764894723892
step: 9500, Loss: 0.11370351165533066
step: 9600, Loss: 0.12090051919221878
step: 9700, Loss: 0.11546619236469269
step: 9800, Loss: 0.11388696730136871
step: 9900, Loss: 0.11578094959259033
training successfully ended.
validating...
validate data length:32
acc: 0.84375
precision: 0.7894736842105263
recall: 0.9375
F_score: 0.8571428571428572
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.2639482915401459
step: 100, Loss: 0.11622833460569382
step: 200, Loss: 0.11640394479036331
step: 300, Loss: 0.1148797795176506
step: 400, Loss: 0.11770488321781158
step: 500, Loss: 0.11725734174251556
step: 600, Loss: 0.11463650315999985
step: 700, Loss: 0.1151449978351593
step: 800, Loss: 0.11683174967765808
step: 900, Loss: 0.11644717305898666
step: 1000, Loss: 0.11558403074741364
step: 1100, Loss: 0.11548677831888199
step: 1200, Loss: 0.11580076068639755
step: 1300, Loss: 0.12016788125038147
step: 1400, Loss: 0.11457297205924988
step: 1500, Loss: 0.11586397886276245
step: 1600, Loss: 0.12891165912151337
step: 1700, Loss: 0.11384053528308868
step: 1800, Loss: 0.11679801344871521
step: 1900, Loss: 0.11501790583133698
step: 2000, Loss: 0.1151401698589325
step: 2100, Loss: 0.11679115891456604
step: 2200, Loss: 0.11591282486915588
step: 2300, Loss: 0.11399032920598984
step: 2400, Loss: 0.11688461154699326
step: 2500, Loss: 0.11459414660930634
step: 2600, Loss: 0.11522774398326874
step: 2700, Loss: 0.11837072670459747
step: 2800, Loss: 0.11530225723981857
step: 2900, Loss: 0.1182175725698471
step: 3000, Loss: 0.11518676578998566
step: 3100, Loss: 0.11449361592531204
step: 3200, Loss: 0.11817706376314163
step: 3300, Loss: 0.11561430990695953
step: 3400, Loss: 0.1146330013871193
step: 3500, Loss: 0.11581050604581833
step: 3600, Loss: 0.11868114769458771
step: 3700, Loss: 0.1165231391787529
step: 3800, Loss: 0.11597567796707153
step: 3900, Loss: 0.11636120080947876
step: 4000, Loss: 0.11587855219841003
step: 4100, Loss: 0.11976178735494614
step: 4200, Loss: 0.13113458454608917
step: 4300, Loss: 0.13891194760799408
step: 4400, Loss: 0.12629817426204681
step: 4500, Loss: 0.12128613889217377
step: 4600, Loss: 0.12245498597621918
step: 4700, Loss: 0.11958742141723633
step: 4800, Loss: 0.11697133630514145
step: 4900, Loss: 0.11755979061126709
step: 5000, Loss: 0.11792220175266266
step: 5100, Loss: 0.1183084025979042
step: 5200, Loss: 0.12041179835796356
step: 5300, Loss: 0.11481660604476929
step: 5400, Loss: 0.11867766827344894
step: 5500, Loss: 0.11636881530284882
step: 5600, Loss: 0.11512763798236847
step: 5700, Loss: 0.11878730356693268
step: 5800, Loss: 0.11453523486852646
step: 5900, Loss: 0.11353861540555954
step: 6000, Loss: 0.11838775128126144
step: 6100, Loss: 0.11458303779363632
step: 6200, Loss: 0.11580081284046173
step: 6300, Loss: 0.11599750816822052
step: 6400, Loss: 0.11648338288068771
step: 6500, Loss: 0.11681859940290451
step: 6600, Loss: 0.11409866809844971
step: 6700, Loss: 0.11443238705396652
step: 6800, Loss: 0.11395484209060669
step: 6900, Loss: 0.11420996487140656
step: 7000, Loss: 0.11517946422100067
step: 7100, Loss: 0.11668848991394043
step: 7200, Loss: 0.11590392142534256
step: 7300, Loss: 0.11494891345500946
step: 7400, Loss: 0.11406292021274567
step: 7500, Loss: 0.11596546322107315
step: 7600, Loss: 0.11528220772743225
step: 7700, Loss: 0.11672207713127136
step: 7800, Loss: 0.11631486564874649
step: 7900, Loss: 0.1168396919965744
step: 8000, Loss: 0.11457762122154236
step: 8100, Loss: 0.11447229236364365
step: 8200, Loss: 0.11543269455432892
step: 8300, Loss: 0.11443069577217102
step: 8400, Loss: 0.11402459442615509
step: 8500, Loss: 0.11487847566604614
step: 8600, Loss: 0.1132953017950058
step: 8700, Loss: 0.11375239491462708
step: 8800, Loss: 0.11380220204591751
step: 8900, Loss: 0.11587650328874588
step: 9000, Loss: 0.1170654296875
step: 9100, Loss: 0.11454442143440247
step: 9200, Loss: 0.11599705368280411
step: 9300, Loss: 0.11628632247447968
step: 9400, Loss: 0.1151796281337738
step: 9500, Loss: 0.11546540260314941
step: 9600, Loss: 0.7289187908172607
step: 9700, Loss: 0.14807160198688507
step: 9800, Loss: 0.1289391666650772
step: 9900, Loss: 0.1216692104935646
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.6666666666666666
recall: 0.9333333333333333
F_score: 0.7777777777777778
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.2499081790447235
step: 100, Loss: 0.1207500696182251
step: 200, Loss: 0.11690349131822586
step: 300, Loss: 0.11698675155639648
step: 400, Loss: 0.12837818264961243
step: 700, Loss: 0.11450603604316711
step: 800, Loss: 0.11374486982822418
step: 900, Loss: 0.11383019387722015
step: 1000, Loss: 0.11394328624010086
step: 1100, Loss: 0.1146707609295845
step: 1200, Loss: 0.11467332392930984
step: 1300, Loss: 0.11362776160240173
step: 1400, Loss: 0.11389901489019394
step: 1500, Loss: 0.11519142985343933
step: 1600, Loss: 0.11389940977096558
step: 1700, Loss: 0.11447284370660782
step: 1800, Loss: 0.11305636912584305
step: 1900, Loss: 0.11387111246585846
step: 2000, Loss: 0.11293168365955353
step: 2100, Loss: 0.11325239390134811
step: 2200, Loss: 0.11343216896057129
step: 2300, Loss: 0.1150193139910698
step: 2400, Loss: 0.114316925406456
step: 2500, Loss: 0.11367866396903992
step: 2600, Loss: 0.11317440122365952
step: 2700, Loss: 0.11443296074867249
step: 2800, Loss: 0.1147397831082344
step: 2900, Loss: 0.1139187216758728
step: 3000, Loss: 0.11320843547582626
step: 3100, Loss: 0.11364301294088364
step: 3200, Loss: 0.11421200633049011
step: 3300, Loss: 0.11485797166824341
step: 3400, Loss: 0.11335452646017075
step: 3500, Loss: 0.1138659417629242
step: 3600, Loss: 0.11280746012926102
step: 3700, Loss: 0.11436375975608826
step: 3800, Loss: 0.11372803151607513
step: 3900, Loss: 0.11436565220355988
step: 4000, Loss: 0.1137513518333435
step: 4100, Loss: 0.11444834619760513
step: 4200, Loss: 0.1159481406211853
step: 4300, Loss: 0.11333202570676804
step: 4400, Loss: 0.11402338743209839
step: 4500, Loss: 0.11477498710155487
step: 4600, Loss: 0.11354759335517883
step: 4700, Loss: 0.11337658762931824
step: 4800, Loss: 0.1137915700674057
step: 4900, Loss: 0.113392174243927
step: 5000, Loss: 0.11360890418291092
step: 5100, Loss: 0.11301454156637192
step: 5200, Loss: 0.11384128034114838
step: 5300, Loss: 0.1141650378704071
step: 5400, Loss: 0.11441217362880707
step: 5500, Loss: 0.1150631532073021
step: 5600, Loss: 0.1138186901807785
step: 5700, Loss: 0.11336396634578705
step: 5800, Loss: 0.11411777883768082
step: 5900, Loss: 0.11354173719882965
step: 6000, Loss: 0.11379389464855194
step: 6100, Loss: 2.2627346515655518
step: 6200, Loss: 0.2358158379793167
step: 6300, Loss: 0.13271212577819824
step: 6400, Loss: 0.12142202258110046
step: 6500, Loss: 0.12173846364021301
step: 6600, Loss: 0.12292522192001343
step: 6700, Loss: 0.11846953630447388
step: 6800, Loss: 0.11659815162420273
step: 6900, Loss: 0.12558312714099884
step: 7000, Loss: 0.11850825697183609
step: 7100, Loss: 0.11904877424240112
step: 7200, Loss: 0.11943934857845306
step: 7300, Loss: 0.11791358888149261
step: 7400, Loss: 0.11739534139633179
step: 7500, Loss: 0.12056205421686172
step: 7600, Loss: 0.11658226698637009
step: 7700, Loss: 0.11500728130340576
step: 7800, Loss: 0.11501289904117584
step: 7900, Loss: 0.11639904975891113
step: 8000, Loss: 0.11617302149534225
step: 8100, Loss: 0.11655854433774948
step: 8200, Loss: 0.114762082695961
step: 8300, Loss: 0.11487854272127151
step: 8400, Loss: 0.1147758886218071
step: 8500, Loss: 0.1159767210483551
step: 8600, Loss: 0.11647449433803558
step: 8700, Loss: 0.11522845178842545
step: 8800, Loss: 0.11414410173892975
step: 8900, Loss: 0.11485716700553894
step: 9000, Loss: 0.11415792256593704
step: 9100, Loss: 0.11521420627832413
step: 9200, Loss: 0.11442197114229202
step: 9300, Loss: 0.11399998515844345
step: 9400, Loss: 0.11589284986257553
step: 9500, Loss: 0.11459089070558548
step: 9600, Loss: 0.11342350393533707
step: 9700, Loss: 0.11439691483974457
step: 9800, Loss: 0.11364062130451202
step: 9900, Loss: 0.11457868665456772
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.9803921568627451
recall: 0.9803921568627451
F_score: 0.9803921568627451
******fold 5******

Training... train_data length:889
step: 0, Loss: 0.11597089469432831
step: 100, Loss: 0.1169646605849266
step: 200, Loss: 0.11603140830993652
step: 300, Loss: 0.11537091434001923
step: 400, Loss: 0.11412858963012695
step: 500, Loss: 0.11563950031995773
step: 600, Loss: 0.11453033983707428
step: 700, Loss: 0.11498153209686279
step: 800, Loss: 0.11340564489364624
step: 900, Loss: 0.11395654082298279
step: 1000, Loss: 0.11415952444076538
step: 1100, Loss: 0.11332385987043381
step: 1200, Loss: 0.1132209524512291
step: 1300, Loss: 0.11390659213066101
step: 1400, Loss: 0.11488080024719238
step: 1500, Loss: 0.1143898144364357
step: 1600, Loss: 0.11336041986942291
step: 1700, Loss: 0.11308027058839798
step: 1800, Loss: 0.11434535682201385
step: 1900, Loss: 0.1146298423409462
step: 2000, Loss: 0.11418931186199188
step: 2100, Loss: 0.11439051479101181
step: 2200, Loss: 0.11271806061267853
step: 2300, Loss: 0.11317058652639389
step: 2400, Loss: 0.11411788314580917
step: 2500, Loss: 0.11368797719478607
step: 2600, Loss: 0.11455634981393814
step: 2700, Loss: 0.11362074315547943
step: 2800, Loss: 0.11398579925298691
step: 2900, Loss: 0.11424754559993744
step: 3000, Loss: 0.1132444366812706
step: 3100, Loss: 0.11457828432321548
step: 3200, Loss: 0.1142706647515297
step: 3300, Loss: 0.11402734369039536
step: 3400, Loss: 0.11499423533678055
step: 3500, Loss: 0.11394919455051422
step: 3600, Loss: 0.11452099680900574
step: 3700, Loss: 0.11461802572011948
step: 3800, Loss: 0.11279235780239105
step: 3900, Loss: 0.11317093670368195
step: 4000, Loss: 0.11315582692623138
step: 4100, Loss: 0.1131640374660492
step: 4200, Loss: 0.11402331292629242
step: 4300, Loss: 0.1137990653514862
step: 4400, Loss: 0.11529822647571564
step: 4500, Loss: 0.11534623801708221
step: 4600, Loss: 0.11465848237276077
step: 4700, Loss: 2.295179605484009
step: 4800, Loss: 0.15749387443065643
step: 4900, Loss: 0.12386536598205566
step: 5000, Loss: 0.12322626262903214
step: 5100, Loss: 0.12207378447055817
step: 5200, Loss: 0.11790091544389725
step: 5300, Loss: 0.12029911577701569
step: 5400, Loss: 0.11686984449625015
step: 5500, Loss: 0.1181715577840805
step: 5600, Loss: 0.117587611079216
step: 5700, Loss: 0.11591310054063797
step: 5800, Loss: 0.11859899759292603
step: 5900, Loss: 0.11556810885667801
step: 6000, Loss: 0.11592613160610199
step: 6100, Loss: 0.11595723032951355
step: 6200, Loss: 0.11573917418718338
step: 6300, Loss: 0.11561139672994614
step: 6400, Loss: 0.114781953394413
step: 6500, Loss: 0.11564218997955322
step: 6600, Loss: 0.11516191065311432
step: 6700, Loss: 0.11536642909049988
step: 6800, Loss: 0.11393503099679947
step: 6900, Loss: 0.11765003949403763
step: 7000, Loss: 0.11560793220996857
step: 7100, Loss: 0.11331357806921005
step: 7200, Loss: 0.11372102051973343
step: 7300, Loss: 0.11430395394563675
step: 7400, Loss: 0.11437684297561646
step: 7500, Loss: 0.11514917016029358
step: 7600, Loss: 0.11437888443470001
step: 7700, Loss: 0.11357540637254715
step: 7800, Loss: 0.11360397934913635
step: 7900, Loss: 0.1147649735212326
step: 8000, Loss: 0.11439801752567291
step: 8100, Loss: 0.11418129503726959
step: 8200, Loss: 0.11467442661523819
step: 8300, Loss: 0.11489271372556686
step: 8400, Loss: 0.11387772858142853
step: 8500, Loss: 0.11532985419034958
step: 8600, Loss: 0.11339567601680756
step: 8700, Loss: 0.1143207773566246
step: 8800, Loss: 0.11364917457103729
step: 8900, Loss: 0.11389781534671783
step: 9000, Loss: 0.1147490069270134
step: 9100, Loss: 0.11400280892848969
step: 9200, Loss: 0.1156931146979332
step: 9300, Loss: 0.11526327580213547
step: 9400, Loss: 0.11563841998577118
step: 9500, Loss: 0.11493107676506042
step: 9600, Loss: 0.11481480300426483
step: 9700, Loss: 0.11278629302978516
step: 9800, Loss: 0.11408140510320663
step: 9900, Loss: 0.11372663825750351
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.96
recall: 1.0
F_score: 0.9795918367346939
******fold 6******

Training... train_data length:889
step: 0, Loss: 0.11330835521221161
step: 100, Loss: 0.11984841525554657
step: 200, Loss: 0.11510124802589417
step: 300, Loss: 0.1155547946691513
step: 400, Loss: 0.11451053619384766
step: 500, Loss: 0.11469288170337677
step: 600, Loss: 0.1147877648472786
step: 700, Loss: 0.11465650051832199
step: 800, Loss: 0.11364839971065521
step: 900, Loss: 0.11448631435632706
step: 1000, Loss: 0.11272160708904266
step: 1100, Loss: 0.11519429087638855
step: 1200, Loss: 0.1144583448767662
step: 500, Loss: 0.11729196459054947
step: 600, Loss: 0.11550424993038177
step: 700, Loss: 0.11421038210391998
step: 800, Loss: 0.11407414823770523
step: 900, Loss: 0.117939792573452
step: 1000, Loss: 0.11541008949279785
step: 1100, Loss: 0.1196846067905426
step: 1200, Loss: 0.11826018989086151
step: 1300, Loss: 0.11477703601121902
step: 1400, Loss: 0.11413251608610153
step: 1500, Loss: 0.11559565365314484
step: 1600, Loss: 0.11636290699243546
step: 1700, Loss: 0.1132320985198021
step: 1800, Loss: 0.11644887924194336
step: 1900, Loss: 0.11517228186130524
step: 2000, Loss: 0.11565232276916504
step: 2100, Loss: 0.11576653271913528
step: 2200, Loss: 0.1136624813079834
step: 2300, Loss: 0.11438388377428055
step: 2400, Loss: 0.11382877826690674
step: 2500, Loss: 0.11695285141468048
step: 2600, Loss: 0.1146337240934372
step: 2700, Loss: 0.11388630419969559
step: 2800, Loss: 0.11744718998670578
step: 2900, Loss: 0.1144653707742691
step: 3000, Loss: 0.11903119832277298
step: 3100, Loss: 0.11411828547716141
step: 3200, Loss: 0.11522197723388672
step: 3300, Loss: 0.11740631610155106
step: 3400, Loss: 0.11578670144081116
step: 3500, Loss: 0.11558116972446442
step: 3600, Loss: 0.11644144356250763
step: 3700, Loss: 0.11989995837211609
step: 3800, Loss: 0.11815240979194641
step: 3900, Loss: 0.11501078307628632
step: 4000, Loss: 0.11518586426973343
step: 4100, Loss: 0.11648193001747131
step: 4200, Loss: 0.8111016154289246
step: 4300, Loss: 0.12893344461917877
step: 4400, Loss: 0.12666237354278564
step: 4500, Loss: 0.121519073843956
step: 4600, Loss: 0.11986525356769562
step: 4700, Loss: 0.1201857328414917
step: 4800, Loss: 0.12054921686649323
step: 4900, Loss: 0.11864423006772995
step: 5000, Loss: 0.12072139233350754
step: 5100, Loss: 0.11888953298330307
step: 5200, Loss: 0.11866313219070435
step: 5300, Loss: 0.11985336989164352
step: 5400, Loss: 0.11712297797203064
step: 5500, Loss: 0.11651410162448883
step: 5600, Loss: 0.11729951202869415
step: 5700, Loss: 0.11542636156082153
step: 5800, Loss: 0.11773216724395752
step: 5900, Loss: 0.11832621693611145
step: 6000, Loss: 0.11875930428504944
step: 6100, Loss: 0.11496810615062714
step: 6200, Loss: 0.11539238691329956
step: 6300, Loss: 0.11645248532295227
step: 6400, Loss: 0.11669944226741791
step: 6500, Loss: 0.11660704761743546
step: 6600, Loss: 0.11463531851768494
step: 6700, Loss: 0.11655152589082718
step: 6800, Loss: 0.11848245561122894
step: 6900, Loss: 0.11564673483371735
step: 7000, Loss: 0.11441443860530853
step: 7100, Loss: 0.12054024636745453
step: 7200, Loss: 0.11645183712244034
step: 7300, Loss: 0.11802859604358673
step: 7400, Loss: 0.1166878491640091
step: 7500, Loss: 0.1162928119301796
step: 7600, Loss: 0.11554038524627686
step: 7700, Loss: 0.11562799662351608
step: 7800, Loss: 0.11672105640172958
step: 7900, Loss: 0.11493106186389923
step: 8000, Loss: 0.11798539012670517
step: 8100, Loss: 0.11556503176689148
step: 8200, Loss: 0.11486287415027618
step: 8300, Loss: 0.11909908801317215
step: 8400, Loss: 0.11882756650447845
step: 8500, Loss: 0.11522291600704193
step: 8600, Loss: 0.11420107632875443
step: 8700, Loss: 0.11588289588689804
step: 8800, Loss: 0.1159939095377922
step: 8900, Loss: 0.11584062874317169
step: 9000, Loss: 0.11583014577627182
step: 9100, Loss: 0.11354919523000717
step: 9200, Loss: 0.11582951247692108
step: 9300, Loss: 0.11583657562732697
step: 9400, Loss: 0.11541035771369934
step: 9500, Loss: 0.11546647548675537
step: 9600, Loss: 0.11477865278720856
step: 9700, Loss: 0.11455557495355606
step: 9800, Loss: 0.1156923770904541
step: 9900, Loss: 0.1145717054605484
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.2568143308162689
step: 100, Loss: 0.12160725891590118
step: 200, Loss: 0.11589385569095612
step: 300, Loss: 0.1171824038028717
step: 400, Loss: 0.11738227307796478
step: 500, Loss: 0.12001443654298782
step: 600, Loss: 0.11484105885028839
step: 700, Loss: 0.11535850167274475
step: 800, Loss: 0.11459995806217194
step: 900, Loss: 0.1272299587726593
step: 1000, Loss: 0.11378338932991028
step: 1100, Loss: 0.11464688181877136
step: 1200, Loss: 0.11409325897693634
step: 1300, Loss: 0.11650890856981277
step: 1400, Loss: 0.11572960019111633
step: 1500, Loss: 0.1313963532447815
step: 1600, Loss: 0.11398625373840332
step: 1700, Loss: 0.12227511405944824
step: 1800, Loss: 0.11421232670545578
step: 1900, Loss: 0.11559253185987473
step: 2000, Loss: 0.11582334339618683
step: 2100, Loss: 0.11670070886611938
step: 2200, Loss: 0.11550553888082504
step: 2300, Loss: 0.11633476614952087
step: 2400, Loss: 0.11719618737697601
step: 2500, Loss: 0.11944561451673508
step: 2600, Loss: 0.11494404077529907
step: 2700, Loss: 0.11438582092523575
step: 2800, Loss: 0.11901304125785828
step: 2900, Loss: 0.1207612007856369
step: 3000, Loss: 0.11484909802675247
step: 3100, Loss: 0.11737748980522156
step: 3200, Loss: 0.11572003364562988
step: 3300, Loss: 0.1179209053516388
step: 3400, Loss: 0.11389536410570145
step: 3500, Loss: 0.11817079037427902
step: 3600, Loss: 0.11661943048238754
step: 3700, Loss: 0.1150747686624527
step: 3800, Loss: 0.11357027292251587
step: 3900, Loss: 0.1168346107006073
step: 4000, Loss: 0.11490112543106079
step: 4100, Loss: 0.11779379844665527
step: 4200, Loss: 0.11396485567092896
step: 4300, Loss: 0.11674173921346664
step: 4400, Loss: 0.11589471250772476
step: 4500, Loss: 0.11426395922899246
step: 4600, Loss: 0.11624455451965332
step: 4700, Loss: 0.1142600029706955
step: 4800, Loss: 2.195636749267578
step: 4900, Loss: 0.1383635699748993
step: 5000, Loss: 0.12639804184436798
step: 5100, Loss: 0.1321294903755188
step: 5200, Loss: 0.12306971848011017
step: 5300, Loss: 0.12001100182533264
step: 5400, Loss: 0.11918532848358154
step: 5500, Loss: 0.12205812335014343
step: 5600, Loss: 0.12094872444868088
step: 5700, Loss: 0.12432242184877396
step: 5800, Loss: 0.11773230880498886
step: 5900, Loss: 0.11892790347337723
step: 6000, Loss: 0.11774636059999466
step: 6100, Loss: 0.11920183897018433
step: 6200, Loss: 0.11684295535087585
step: 6300, Loss: 0.1202276349067688
step: 6400, Loss: 0.11784210801124573
step: 6500, Loss: 0.11863928288221359
step: 6600, Loss: 0.15322448313236237
step: 6700, Loss: 0.12047122418880463
step: 6800, Loss: 0.11669401824474335
step: 6900, Loss: 0.12553536891937256
step: 7000, Loss: 0.11475758254528046
step: 7100, Loss: 0.11610987037420273
step: 7200, Loss: 0.1154288649559021
step: 7300, Loss: 0.118871308863163
step: 7400, Loss: 0.11689254641532898
step: 7500, Loss: 0.11473769694566727
step: 7600, Loss: 0.11430811882019043
step: 7700, Loss: 0.11793221533298492
step: 7800, Loss: 0.11531530320644379
step: 7900, Loss: 0.11656972020864487
step: 8000, Loss: 0.11981523036956787
step: 8100, Loss: 0.13519099354743958
step: 8200, Loss: 0.11460906267166138
step: 8300, Loss: 0.11625785380601883
step: 8400, Loss: 0.11586214601993561
step: 8500, Loss: 0.11547206342220306
step: 8600, Loss: 0.11505758762359619
step: 8700, Loss: 0.1153116375207901
step: 8800, Loss: 0.11377900093793869
step: 8900, Loss: 0.11592700332403183
step: 9000, Loss: 0.11678065359592438
step: 9100, Loss: 0.1136200800538063
step: 9200, Loss: 0.11686870455741882
step: 9300, Loss: 0.11532572656869888
step: 9400, Loss: 0.1156405359506607
step: 9500, Loss: 0.12748944759368896
step: 9600, Loss: 0.11386673152446747
step: 9700, Loss: 0.11383148282766342
step: 9800, Loss: 0.1143149733543396
step: 9900, Loss: 0.1177508756518364
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8421052631578947
recall: 0.8888888888888888
F_score: 0.8648648648648649
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.23650255799293518
step: 100, Loss: 0.11803417652845383
step: 200, Loss: 0.11731657385826111
step: 300, Loss: 0.11912716925144196
step: 400, Loss: 0.11810822784900665
step: 500, Loss: 0.11745736002922058
step: 600, Loss: 0.11705189943313599
step: 700, Loss: 0.11801035702228546
step: 800, Loss: 0.11716314405202866
step: 900, Loss: 0.1145104467868805
step: 1300, Loss: 0.11365453153848648
step: 1400, Loss: 0.11330460011959076
step: 1500, Loss: 0.11642111837863922
step: 1600, Loss: 0.11407127231359482
step: 1700, Loss: 0.11360354721546173
step: 1800, Loss: 0.11395271122455597
step: 1900, Loss: 0.11398334056138992
step: 2000, Loss: 0.1139117032289505
step: 2100, Loss: 0.11347904056310654
step: 2200, Loss: 0.11467770487070084
step: 2300, Loss: 0.11403754353523254
step: 2400, Loss: 0.11282586306333542
step: 2500, Loss: 0.11420921981334686
step: 2600, Loss: 0.11438309401273727
step: 2700, Loss: 0.11392168700695038
step: 2800, Loss: 0.11435133218765259
step: 2900, Loss: 0.11359115689992905
step: 3000, Loss: 0.11395299434661865
step: 3100, Loss: 0.11271565407514572
step: 3200, Loss: 0.11425963044166565
step: 3300, Loss: 0.11435573548078537
step: 3400, Loss: 0.11471892893314362
step: 3500, Loss: 0.11289463192224503
step: 3600, Loss: 0.11320436745882034
step: 3700, Loss: 0.11424935609102249
step: 3800, Loss: 0.11290611326694489
step: 3900, Loss: 0.11360049247741699
step: 4000, Loss: 0.11295651644468307
step: 4100, Loss: 0.11445635557174683
step: 4200, Loss: 0.11346178501844406
step: 4300, Loss: 0.11298885941505432
step: 4400, Loss: 0.11391081660985947
step: 4500, Loss: 0.11412376165390015
step: 4600, Loss: 0.11454424262046814
step: 4700, Loss: 0.11355385184288025
step: 4800, Loss: 0.11361899971961975
step: 4900, Loss: 0.11424338072538376
step: 5000, Loss: 0.11374378204345703
step: 5100, Loss: 0.11285663396120071
step: 5200, Loss: 0.11401068419218063
step: 5300, Loss: 0.11546775698661804
step: 5400, Loss: 0.11326335370540619
step: 5500, Loss: 0.11468066275119781
step: 5600, Loss: 2.360522508621216
step: 5700, Loss: 0.1437995582818985
step: 5800, Loss: 0.12091024965047836
step: 5900, Loss: 0.12003076076507568
step: 6000, Loss: 0.12222883850336075
step: 6100, Loss: 0.11801164597272873
step: 6200, Loss: 0.11770738661289215
step: 6300, Loss: 0.11872243881225586
step: 6400, Loss: 0.11477454006671906
step: 6500, Loss: 0.11715785413980484
step: 6600, Loss: 0.1200278103351593
step: 6700, Loss: 0.11742642521858215
step: 6800, Loss: 0.11648274958133698
step: 6900, Loss: 0.11912564933300018
step: 7000, Loss: 0.11650417745113373
step: 7100, Loss: 0.11607635021209717
step: 7200, Loss: 0.11612364649772644
step: 7300, Loss: 0.11485004425048828
step: 7400, Loss: 0.11621766537427902
step: 7500, Loss: 0.11796805262565613
step: 7600, Loss: 0.11363676190376282
step: 7700, Loss: 0.11500594764947891
step: 7800, Loss: 0.11460115015506744
step: 7900, Loss: 0.11357897520065308
step: 8000, Loss: 0.11484134942293167
step: 8100, Loss: 0.11662594228982925
step: 8200, Loss: 0.1139226108789444
step: 8300, Loss: 0.11643257737159729
step: 8400, Loss: 0.11455579847097397
step: 8500, Loss: 0.11303769797086716
step: 8600, Loss: 0.11563538759946823
step: 8700, Loss: 0.11545803397893906
step: 8800, Loss: 0.11454825848340988
step: 8900, Loss: 0.11496368050575256
step: 9000, Loss: 0.11453087627887726
step: 9100, Loss: 0.11399468779563904
step: 9200, Loss: 0.1157648041844368
step: 9300, Loss: 0.11439114809036255
step: 9400, Loss: 0.11449497938156128
step: 9500, Loss: 0.11415910720825195
step: 9600, Loss: 0.11304555088281631
step: 9700, Loss: 0.1148928850889206
step: 9800, Loss: 0.11450111865997314
step: 9900, Loss: 0.11436997354030609
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.92
recall: 1.0
F_score: 0.9583333333333334
******fold 7******

Training... train_data length:889
step: 0, Loss: 0.11604702472686768
step: 100, Loss: 0.11898783594369888
step: 200, Loss: 0.11537487059831619
step: 300, Loss: 0.11595755815505981
step: 400, Loss: 0.11551252752542496
step: 500, Loss: 0.11404106020927429
step: 600, Loss: 0.11555696278810501
step: 700, Loss: 0.11478186398744583
step: 800, Loss: 0.11359123885631561
step: 900, Loss: 0.11385075002908707
step: 1000, Loss: 0.11399810016155243
step: 1100, Loss: 0.11359234154224396
step: 1200, Loss: 0.1151358112692833
step: 1300, Loss: 0.112842857837677
step: 1400, Loss: 0.11420480906963348
step: 1500, Loss: 0.11425528675317764
step: 1600, Loss: 0.11353673785924911
step: 1700, Loss: 0.11345656961202621
step: 1800, Loss: 0.11353586614131927
step: 1900, Loss: 0.11340607702732086
step: 2000, Loss: 0.11390002071857452
step: 2100, Loss: 0.11463788151741028
step: 2200, Loss: 0.11329734325408936
step: 2300, Loss: 0.1141153946518898
step: 2400, Loss: 0.11322295665740967
step: 2500, Loss: 0.1138504147529602
step: 2600, Loss: 0.11501916497945786
step: 2700, Loss: 0.11381882429122925
step: 2800, Loss: 0.1134999543428421
step: 2900, Loss: 0.11394171416759491
step: 3000, Loss: 0.11301970481872559
step: 3100, Loss: 0.11316242814064026
step: 3200, Loss: 0.11360883712768555
step: 3300, Loss: 0.1128632202744484
step: 3400, Loss: 0.11256474256515503
step: 3500, Loss: 0.11389151215553284
step: 3600, Loss: 0.11357102543115616
step: 3700, Loss: 0.11638613045215607
step: 3800, Loss: 4.789624214172363
step: 3900, Loss: 0.1873472034931183
step: 4000, Loss: 0.17283830046653748
step: 4100, Loss: 0.1347203254699707
step: 4200, Loss: 0.1270170509815216
step: 4300, Loss: 0.12571394443511963
step: 4400, Loss: 0.1288554072380066
step: 4500, Loss: 0.12089657038450241
step: 4600, Loss: 0.1178029328584671
step: 4700, Loss: 0.11582193523645401
step: 4800, Loss: 0.11829818785190582
step: 4900, Loss: 0.11810116469860077
step: 5000, Loss: 0.11994500458240509
step: 5100, Loss: 0.11774104833602905
step: 5200, Loss: 0.1166277676820755
step: 5300, Loss: 0.11839625239372253
step: 5400, Loss: 0.11647678911685944
step: 5500, Loss: 0.11480662226676941
step: 5600, Loss: 0.11876928806304932
step: 5700, Loss: 0.11505071818828583
step: 5800, Loss: 0.11765442788600922
step: 5900, Loss: 0.11557146906852722
step: 6000, Loss: 0.11515497416257858
step: 6100, Loss: 0.11449408531188965
step: 6200, Loss: 0.11477025598287582
step: 6300, Loss: 0.11429688334465027
step: 6400, Loss: 0.11664621531963348
step: 6500, Loss: 0.11419674754142761
step: 6600, Loss: 0.11518236994743347
step: 6700, Loss: 0.11434370279312134
step: 6800, Loss: 0.11435334384441376
step: 6900, Loss: 0.1139182448387146
step: 7000, Loss: 0.11346973478794098
step: 7100, Loss: 0.11379845440387726
step: 7200, Loss: 0.11649394035339355
step: 7300, Loss: 0.11375363916158676
step: 7400, Loss: 0.11468924582004547
step: 7500, Loss: 0.11362045258283615
step: 7600, Loss: 0.11409065127372742
step: 7700, Loss: 0.11446879804134369
step: 7800, Loss: 0.1143273338675499
step: 7900, Loss: 0.11439944058656693
step: 8000, Loss: 0.11609314382076263
step: 8100, Loss: 0.11493700742721558
step: 8200, Loss: 0.11380715668201447
step: 8300, Loss: 0.11610773205757141
step: 8400, Loss: 0.11508871614933014
step: 8500, Loss: 0.11422178149223328
step: 8600, Loss: 0.1152983084321022
step: 8700, Loss: 0.11418403685092926
step: 8800, Loss: 0.11426044255495071
step: 8900, Loss: 0.11389923840761185
step: 9000, Loss: 0.1137366071343422
step: 9100, Loss: 0.11417694389820099
step: 9200, Loss: 0.11390569806098938
step: 9300, Loss: 0.1139674037694931
step: 9400, Loss: 0.11266611516475677
step: 9500, Loss: 0.11429938673973083
step: 9600, Loss: 0.11316884309053421
step: 9700, Loss: 0.11462928354740143
step: 9800, Loss: 0.11471763253211975
step: 9900, Loss: 0.11331192404031754
training successfully ended.
validating...
validate data length:99
acc: 0.9895833333333334
precision: 0.9777777777777777
recall: 1.0
F_score: 0.9887640449438202
******fold 8******

Training... train_data length:889
step: 0, Loss: 0.11395125091075897
step: 100, Loss: 0.11627264320850372
step: 200, Loss: 0.11725841462612152
step: 300, Loss: 0.11378537863492966
step: 400, Loss: 0.11659377068281174
step: 500, Loss: 0.11421974748373032
step: 600, Loss: 0.1145736575126648
step: 700, Loss: 0.11460994184017181
step: 800, Loss: 0.11446812748908997
step: 900, Loss: 0.11318758875131607
step: 1000, Loss: 0.11392375826835632
step: 1100, Loss: 0.11414752155542374
step: 1200, Loss: 0.11371806263923645
step: 1300, Loss: 0.11445783823728561
step: 1400, Loss: 0.11429193615913391
step: 1500, Loss: 0.11330704391002655
step: 1600, Loss: 0.1133391484618187
step: 1700, Loss: 0.11437683552503586
step: 1800, Loss: 0.11472150683403015
step: 1000, Loss: 0.11742555350065231
step: 1100, Loss: 0.11505728960037231
step: 1200, Loss: 0.1143999695777893
step: 1300, Loss: 0.11594226956367493
step: 1400, Loss: 0.11509758234024048
step: 1500, Loss: 0.11560553312301636
step: 1600, Loss: 0.11451414227485657
step: 1700, Loss: 0.11703522503376007
step: 1800, Loss: 0.11557760834693909
step: 1900, Loss: 0.11476989090442657
step: 2000, Loss: 0.11476444453001022
step: 2100, Loss: 0.11569834500551224
step: 2200, Loss: 0.11633709073066711
step: 2300, Loss: 0.1149696633219719
step: 2400, Loss: 0.11522985994815826
step: 2500, Loss: 0.1147041991353035
step: 2600, Loss: 0.11537232249975204
step: 2700, Loss: 0.11694388836622238
step: 2800, Loss: 0.11659368127584457
step: 2900, Loss: 0.11774701625108719
step: 3000, Loss: 0.12069618701934814
step: 3100, Loss: 0.11587613821029663
step: 3200, Loss: 0.11491090059280396
step: 3300, Loss: 0.11720667779445648
step: 3400, Loss: 0.11657175421714783
step: 3500, Loss: 0.1170060932636261
step: 3600, Loss: 0.11500824242830276
step: 3700, Loss: 0.11644820123910904
step: 3800, Loss: 0.1181720644235611
step: 3900, Loss: 0.11657403409481049
step: 4000, Loss: 0.11565709859132767
step: 4100, Loss: 0.11640685796737671
step: 4200, Loss: 0.11867707967758179
step: 4300, Loss: 0.11652503907680511
step: 4400, Loss: 0.11820952594280243
step: 4500, Loss: 0.1153310239315033
step: 4600, Loss: 0.12152542173862457
step: 4700, Loss: 0.13344505429267883
step: 4800, Loss: 0.12598448991775513
step: 4900, Loss: 0.12014719098806381
step: 5000, Loss: 0.12574438750743866
step: 5100, Loss: 0.11574243009090424
step: 5200, Loss: 0.1192016676068306
step: 5300, Loss: 0.11753183603286743
step: 5400, Loss: 0.12063778191804886
step: 5500, Loss: 0.12189394235610962
step: 5600, Loss: 0.12262952327728271
step: 5700, Loss: 0.11968765407800674
step: 5800, Loss: 0.11797818541526794
step: 5900, Loss: 0.11608308553695679
step: 6000, Loss: 0.12000444531440735
step: 6100, Loss: 0.11533509194850922
step: 6200, Loss: 0.11791536211967468
step: 6300, Loss: 0.11556532979011536
step: 6400, Loss: 0.11600936204195023
step: 6500, Loss: 0.11632254719734192
step: 6600, Loss: 0.11406496167182922
step: 6700, Loss: 0.11464337259531021
step: 6800, Loss: 0.11582309752702713
step: 6900, Loss: 0.11923627555370331
step: 7000, Loss: 0.11519479006528854
step: 7100, Loss: 0.11660349369049072
step: 7200, Loss: 0.11673546582460403
step: 7300, Loss: 0.11524619162082672
step: 7400, Loss: 0.11452911049127579
step: 7500, Loss: 0.1164511889219284
step: 7600, Loss: 0.11548199504613876
step: 7700, Loss: 0.1136210709810257
step: 7800, Loss: 0.11765415221452713
step: 7900, Loss: 0.11461780220270157
step: 8000, Loss: 0.11676960438489914
step: 8100, Loss: 0.11702276021242142
step: 8200, Loss: 0.11414201557636261
step: 8300, Loss: 0.11455196887254715
step: 8400, Loss: 0.11613581329584122
step: 8500, Loss: 0.11597536504268646
step: 8600, Loss: 0.11495479941368103
step: 8700, Loss: 0.11447317898273468
step: 8800, Loss: 0.11606034636497498
step: 8900, Loss: 0.11495430767536163
step: 9000, Loss: 0.11560230702161789
step: 9100, Loss: 0.11618680506944656
step: 9200, Loss: 0.1157548725605011
step: 9300, Loss: 0.1151062622666359
step: 9400, Loss: 0.11371398717164993
step: 9500, Loss: 0.11454779654741287
step: 9600, Loss: 0.11441388726234436
step: 9700, Loss: 0.1139986515045166
step: 9800, Loss: 0.11544506996870041
step: 9900, Loss: 0.11576125770807266
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8888888888888888
recall: 0.8888888888888888
F_score: 0.8888888888888888
******fold 7******

Training... train_data length:281
step: 0, Loss: 1.9697952270507812
step: 100, Loss: 0.11817348003387451
step: 200, Loss: 0.11977577954530716
step: 300, Loss: 0.11795683950185776
step: 400, Loss: 0.11945778131484985
step: 500, Loss: 0.11706824600696564
step: 600, Loss: 0.11739150434732437
step: 700, Loss: 0.11445612460374832
step: 800, Loss: 0.11562136560678482
step: 900, Loss: 0.1150137409567833
step: 1000, Loss: 0.11482569575309753
step: 1100, Loss: 0.11950281262397766
step: 1200, Loss: 0.11979396641254425
step: 1300, Loss: 0.11669997870922089
step: 1400, Loss: 0.11434457451105118
step: 1500, Loss: 0.12054047733545303
step: 1600, Loss: 0.1151021271944046
step: 1700, Loss: 0.11394498497247696
step: 1800, Loss: 0.11415409296751022
step: 1900, Loss: 0.114640973508358
step: 2000, Loss: 0.11482957750558853
step: 2100, Loss: 0.12532901763916016
step: 2200, Loss: 0.1142982542514801
step: 2300, Loss: 0.1135983020067215
step: 2400, Loss: 0.1155073270201683
step: 2500, Loss: 0.11476021260023117
step: 2600, Loss: 0.11415388435125351
step: 2700, Loss: 0.11458131670951843
step: 2800, Loss: 0.11760629713535309
step: 2900, Loss: 0.11561790853738785
step: 3000, Loss: 0.11957728862762451
step: 3100, Loss: 0.11539126932621002
step: 3200, Loss: 0.1165299043059349
step: 3300, Loss: 0.11539803445339203
step: 3400, Loss: 0.11534657329320908
step: 3500, Loss: 0.11440664529800415
step: 3600, Loss: 0.11747639626264572
step: 3700, Loss: 0.11498569697141647
step: 3800, Loss: 0.11483059823513031
step: 3900, Loss: 0.11655506491661072
step: 4000, Loss: 0.11600292474031448
step: 4100, Loss: 0.11432904750108719
step: 4200, Loss: 0.11471531540155411
step: 4300, Loss: 0.11533298343420029
step: 4400, Loss: 0.11432410031557083
step: 4500, Loss: 0.11637815088033676
step: 4600, Loss: 0.2005632519721985
step: 4700, Loss: 0.13609039783477783
step: 4800, Loss: 0.12846603989601135
step: 4900, Loss: 0.11939191073179245
step: 5000, Loss: 0.12282733619213104
step: 5100, Loss: 0.13412226736545563
step: 5200, Loss: 0.11932942271232605
step: 5300, Loss: 0.11617366224527359
step: 5400, Loss: 0.1188819408416748
step: 5500, Loss: 0.1178150624036789
step: 5600, Loss: 0.12245453894138336
step: 5700, Loss: 0.11714847385883331
step: 5800, Loss: 0.11671549081802368
step: 5900, Loss: 0.1193220466375351
step: 6000, Loss: 0.11806023120880127
step: 6100, Loss: 0.1176876500248909
step: 6200, Loss: 0.12174654006958008
step: 6300, Loss: 0.11842560023069382
step: 6400, Loss: 0.11556984484195709
step: 6500, Loss: 0.11852420121431351
step: 6600, Loss: 0.11536058783531189
step: 6700, Loss: 0.11537522822618484
step: 6800, Loss: 0.11432312428951263
step: 6900, Loss: 0.11348418891429901
step: 7000, Loss: 0.11628897488117218
step: 7100, Loss: 0.11409062892198563
step: 7200, Loss: 0.11625935137271881
step: 7300, Loss: 0.11423467099666595
step: 7400, Loss: 0.11551564931869507
step: 7500, Loss: 0.11400460451841354
step: 7600, Loss: 0.11667825281620026
step: 7700, Loss: 0.11432044208049774
step: 7800, Loss: 0.11583884060382843
step: 7900, Loss: 0.11409680545330048
step: 8000, Loss: 0.11461842060089111
step: 8100, Loss: 0.11614511162042618
step: 8200, Loss: 0.11442277580499649
step: 8300, Loss: 0.11669628322124481
step: 8400, Loss: 0.1171833947300911
step: 8500, Loss: 0.11398783326148987
step: 8600, Loss: 0.1154441311955452
step: 8700, Loss: 0.11718647181987762
step: 8800, Loss: 0.1164182797074318
step: 8900, Loss: 0.11561715602874756
step: 9000, Loss: 0.11509720236063004
step: 9100, Loss: 0.12210340797901154
step: 9200, Loss: 0.1140061542391777
step: 9300, Loss: 0.1158047616481781
step: 9400, Loss: 0.11704652011394501
step: 9500, Loss: 0.11499066650867462
step: 9600, Loss: 0.11367989331483841
step: 9700, Loss: 0.11533132195472717
step: 9800, Loss: 0.11534442752599716
step: 9900, Loss: 0.11545254290103912
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8235294117647058
recall: 0.8235294117647058
F_score: 0.8235294117647058
******fold 8******

Training... train_data length:281
step: 0, Loss: 1.5063281059265137
step: 100, Loss: 0.1164267286658287
step: 200, Loss: 0.11645711958408356
step: 300, Loss: 0.1183292344212532
step: 400, Loss: 0.11805231869220734
step: 500, Loss: 0.11445312201976776
step: 600, Loss: 0.11701815575361252
step: 700, Loss: 0.11585292965173721
step: 800, Loss: 0.11413310468196869
step: 900, Loss: 0.1271725296974182
step: 1000, Loss: 0.11474418640136719
step: 1100, Loss: 0.11436580121517181
step: 1200, Loss: 0.11490683257579803
step: 1300, Loss: 0.120325468480587
step: 1400, Loss: 0.11432435363531113
step: 1900, Loss: 0.11310525238513947
step: 2000, Loss: 0.11261793971061707
step: 2100, Loss: 0.11379168182611465
step: 2200, Loss: 0.11392131447792053
step: 2300, Loss: 0.11371500790119171
step: 2400, Loss: 0.11448557674884796
step: 2500, Loss: 0.1137656569480896
step: 2600, Loss: 0.11422410607337952
step: 2700, Loss: 0.11545227468013763
step: 2800, Loss: 0.11311574280261993
step: 2900, Loss: 0.11284919083118439
step: 3000, Loss: 0.11350157856941223
step: 3100, Loss: 0.11375042796134949
step: 3200, Loss: 0.11315295845270157
step: 3300, Loss: 0.11444848775863647
step: 3400, Loss: 0.11379601806402206
step: 3500, Loss: 0.11273135989904404
step: 3600, Loss: 0.11294642835855484
step: 3700, Loss: 0.11247957497835159
step: 3800, Loss: 0.11376714706420898
step: 3900, Loss: 0.11362730711698532
step: 4000, Loss: 0.11314687877893448
step: 4100, Loss: 0.11439645290374756
step: 4200, Loss: 0.1134047657251358
step: 4300, Loss: 0.11386667937040329
step: 4400, Loss: 0.1143159493803978
step: 4500, Loss: 0.11382395029067993
step: 4600, Loss: 0.11361521482467651
step: 4700, Loss: 2.7818238735198975
step: 4800, Loss: 0.145219087600708
step: 4900, Loss: 0.1514667421579361
step: 5000, Loss: 0.12294010072946548
step: 5100, Loss: 0.12049724161624908
step: 5200, Loss: 0.12434611469507217
step: 5300, Loss: 0.11630740761756897
step: 5400, Loss: 0.11816564947366714
step: 5500, Loss: 0.11645014584064484
step: 5600, Loss: 0.11404183506965637
step: 5700, Loss: 0.11651835590600967
step: 5800, Loss: 0.11837095022201538
step: 5900, Loss: 0.12037232518196106
step: 6000, Loss: 0.11766024678945541
step: 6100, Loss: 0.114894799888134
step: 6200, Loss: 0.11573301255702972
step: 6300, Loss: 0.1147707924246788
step: 6400, Loss: 0.11538257449865341
step: 6500, Loss: 0.12110145390033722
step: 6600, Loss: 0.11471380293369293
step: 6700, Loss: 0.11480638384819031
step: 6800, Loss: 0.11549659818410873
step: 6900, Loss: 0.11582103371620178
step: 7000, Loss: 0.1187487319111824
step: 7100, Loss: 0.11498419940471649
step: 7200, Loss: 0.1152845174074173
step: 7300, Loss: 0.11324068903923035
step: 7400, Loss: 0.11430954933166504
step: 7500, Loss: 0.11451651901006699
step: 7600, Loss: 0.11437655240297318
step: 7700, Loss: 0.1153520867228508
step: 7800, Loss: 0.11569976806640625
step: 7900, Loss: 0.1139480248093605
step: 8000, Loss: 0.11588156223297119
step: 8100, Loss: 0.11454994231462479
step: 8200, Loss: 0.11402536928653717
step: 8300, Loss: 0.11387202143669128
step: 8400, Loss: 0.11415283381938934
step: 8500, Loss: 0.1150975227355957
step: 8600, Loss: 0.11374153196811676
step: 8700, Loss: 0.11552025377750397
step: 8800, Loss: 0.1150718703866005
step: 8900, Loss: 0.11435094475746155
step: 9000, Loss: 0.11345730721950531
step: 9100, Loss: 0.1144261434674263
step: 9200, Loss: 0.1142377033829689
step: 9300, Loss: 0.1151561439037323
step: 9400, Loss: 0.11264055222272873
step: 9500, Loss: 0.11313736438751221
step: 9600, Loss: 0.11377441138029099
step: 9700, Loss: 0.11315889656543732
step: 9800, Loss: 0.11351940780878067
step: 9900, Loss: 0.11510541290044785
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:890
step: 0, Loss: 0.11367365717887878
step: 100, Loss: 0.15353724360466003
step: 200, Loss: 0.11774075776338577
step: 300, Loss: 0.1163739413022995
step: 400, Loss: 0.11528048664331436
step: 500, Loss: 0.11432551592588425
step: 600, Loss: 0.1166968047618866
step: 700, Loss: 0.1141849011182785
step: 800, Loss: 0.11439809203147888
step: 900, Loss: 0.11734138429164886
step: 1000, Loss: 0.11610795557498932
step: 1100, Loss: 0.11473347246646881
step: 1200, Loss: 0.11462831497192383
step: 1300, Loss: 0.11525112390518188
step: 1400, Loss: 0.11500920355319977
step: 1500, Loss: 0.11466902494430542
step: 1600, Loss: 0.11445863544940948
step: 1700, Loss: 0.11417119204998016
step: 1800, Loss: 0.11488689482212067
step: 1900, Loss: 0.11482497304677963
step: 2000, Loss: 0.11389294266700745
step: 2100, Loss: 0.11402706056833267
step: 2200, Loss: 0.11489888280630112
step: 2300, Loss: 0.113364577293396
step: 2400, Loss: 0.11378389596939087
step: 2500, Loss: 0.11477528512477875
step: 2600, Loss: 0.11564384400844574
step: 2700, Loss: 0.11410078406333923
step: 2800, Loss: 0.11576106399297714
step: 2900, Loss: 0.1136123389005661
step: 3000, Loss: 0.11536428332328796
step: 3100, Loss: 0.114131398499012
step: 3200, Loss: 0.11391841620206833
step: 3300, Loss: 0.11342416703701019
step: 3400, Loss: 0.11305155605077744
step: 3500, Loss: 0.11339303106069565
step: 3600, Loss: 0.11371637880802155
step: 3700, Loss: 0.11392644047737122
step: 3800, Loss: 0.11402145028114319
step: 3900, Loss: 0.11497096717357635
step: 4000, Loss: 0.11336520314216614
step: 4100, Loss: 0.11443151533603668
step: 4200, Loss: 0.11531778424978256
step: 4300, Loss: 0.11290118843317032
step: 4400, Loss: 0.11410794407129288
step: 4500, Loss: 0.11830774694681168
step: 4600, Loss: 0.1146058589220047
step: 4700, Loss: 0.11327904462814331
step: 4800, Loss: 0.11396731436252594
step: 4900, Loss: 0.1127505749464035
step: 5000, Loss: 0.11550179123878479
step: 5100, Loss: 0.11365554481744766
step: 5200, Loss: 0.11283159255981445
step: 5300, Loss: 0.11340567469596863
step: 5400, Loss: 0.11364540457725525
step: 5500, Loss: 0.11441530287265778
step: 5600, Loss: 0.11398487538099289
step: 5700, Loss: 0.1149589940905571
step: 5800, Loss: 0.11359263956546783
step: 5900, Loss: 0.12205484509468079
step: 6000, Loss: 0.4135943055152893
step: 6100, Loss: 0.14932747185230255
step: 6200, Loss: 0.13411909341812134
step: 6300, Loss: 0.12162235379219055
step: 6400, Loss: 0.12263333797454834
step: 6500, Loss: 0.1197047233581543
step: 6600, Loss: 0.11986834555864334
step: 6700, Loss: 0.11939441412687302
step: 6800, Loss: 0.11617933213710785
step: 6900, Loss: 0.11793112009763718
step: 7000, Loss: 0.11522082984447479
step: 7100, Loss: 0.11732690781354904
step: 7200, Loss: 0.11621375381946564
step: 7300, Loss: 0.11683215945959091
step: 7400, Loss: 0.11525534093379974
step: 7500, Loss: 0.12010718882083893
step: 7600, Loss: 0.115877166390419
step: 7700, Loss: 0.11685554683208466
step: 7800, Loss: 0.1155378520488739
step: 7900, Loss: 0.11584947258234024
step: 8000, Loss: 0.11748916655778885
step: 8100, Loss: 0.11636089533567429
step: 8200, Loss: 0.11556243151426315
step: 8300, Loss: 0.11475804448127747
step: 8400, Loss: 0.11434466391801834
step: 8500, Loss: 0.11623218655586243
step: 8600, Loss: 0.11391488462686539
step: 8700, Loss: 0.1129990816116333
step: 8800, Loss: 0.11583549529314041
step: 8900, Loss: 0.11387332528829575
step: 9000, Loss: 0.11558391898870468
step: 9100, Loss: 0.11413512378931046
step: 9200, Loss: 0.1140870600938797
step: 9300, Loss: 0.11584736406803131
step: 9400, Loss: 0.11480794101953506
step: 9500, Loss: 0.11409283429384232
step: 9600, Loss: 0.11423797160387039
step: 9700, Loss: 0.11454610526561737
step: 9800, Loss: 0.11608022451400757
step: 9900, Loss: 0.1158444806933403
training successfully ended.
validating...
validate data length:98
acc: 0.9791666666666666
precision: 0.9622641509433962
recall: 1.0
F_score: 0.9807692307692307
******fold 10******

Training... train_data length:890
step: 0, Loss: 0.11429920792579651
step: 100, Loss: 0.19536234438419342
step: 200, Loss: 0.1159818097949028
step: 300, Loss: 0.11647123098373413
step: 400, Loss: 0.11340437829494476
step: 500, Loss: 0.11784036457538605
step: 600, Loss: 0.11524590104818344
step: 700, Loss: 0.11464335769414902
step: 800, Loss: 0.1150425374507904
step: 900, Loss: 0.11508295685052872
step: 1000, Loss: 0.11554720252752304
step: 1100, Loss: 0.11720895767211914
step: 1200, Loss: 0.1138402670621872
step: 1300, Loss: 0.11573704332113266
step: 1400, Loss: 0.11516894400119781
step: 1500, Loss: 0.11384784430265427
step: 1600, Loss: 0.11467928439378738
step: 1700, Loss: 0.11445088684558868
step: 1800, Loss: 0.11448922008275986
step: 1900, Loss: 0.11434206366539001
step: 2000, Loss: 0.11536829173564911
step: 2100, Loss: 0.1141323372721672
step: 2200, Loss: 0.11451593786478043
step: 2300, Loss: 0.11387989670038223
step: 2400, Loss: 0.11419643461704254
step: 2500, Loss: 0.11432073265314102
step: 1500, Loss: 0.11486724019050598
step: 1600, Loss: 0.1155799925327301
step: 1700, Loss: 0.11447809636592865
step: 1800, Loss: 0.11425790935754776
step: 1900, Loss: 0.11532606184482574
step: 2000, Loss: 0.11624763906002045
step: 2100, Loss: 0.11449426412582397
step: 2200, Loss: 0.11394052952528
step: 2300, Loss: 0.11867709457874298
step: 2400, Loss: 0.11465433984994888
step: 2500, Loss: 0.11478254944086075
step: 2600, Loss: 0.1175626665353775
step: 2700, Loss: 0.11586077511310577
step: 2800, Loss: 0.11505909264087677
step: 2900, Loss: 0.1146111860871315
step: 3000, Loss: 0.11500892043113708
step: 3100, Loss: 0.11470229923725128
step: 3200, Loss: 0.11801432073116302
step: 3300, Loss: 0.12180715799331665
step: 3400, Loss: 0.11570369452238083
step: 3500, Loss: 0.11501911282539368
step: 3600, Loss: 0.11473722010850906
step: 3700, Loss: 0.11428892612457275
step: 3800, Loss: 0.11571397632360458
step: 3900, Loss: 0.11418876051902771
step: 4000, Loss: 0.11446723341941833
step: 4100, Loss: 0.11661479622125626
step: 4200, Loss: 3.0433740615844727
step: 4300, Loss: 0.1319744884967804
step: 4400, Loss: 0.1255463808774948
step: 4500, Loss: 0.13214011490345
step: 4600, Loss: 0.12362776696681976
step: 4700, Loss: 0.12349898368120193
step: 4800, Loss: 0.12015780061483383
step: 4900, Loss: 0.12420932203531265
step: 5000, Loss: 0.11841379851102829
step: 5100, Loss: 0.12030419707298279
step: 5200, Loss: 0.11861138045787811
step: 5300, Loss: 0.1173996850848198
step: 5400, Loss: 0.11920055747032166
step: 5500, Loss: 0.11675705760717392
step: 5600, Loss: 0.11501350998878479
step: 5700, Loss: 0.11786981672048569
step: 5800, Loss: 0.11610302329063416
step: 5900, Loss: 0.11720950901508331
step: 6000, Loss: 0.11586528271436691
step: 6100, Loss: 0.11981463432312012
step: 6200, Loss: 0.1163126677274704
step: 6300, Loss: 0.11677226424217224
step: 6400, Loss: 0.11601506918668747
step: 6500, Loss: 0.11569059640169144
step: 6600, Loss: 0.11978884041309357
step: 6700, Loss: 0.11501358449459076
step: 6800, Loss: 0.11803632974624634
step: 6900, Loss: 0.11572058498859406
step: 7000, Loss: 0.11442364752292633
step: 7100, Loss: 0.1136237308382988
step: 7200, Loss: 0.11687752604484558
step: 7300, Loss: 0.11332409828901291
step: 7400, Loss: 0.11723820865154266
step: 7500, Loss: 0.11526723206043243
step: 7600, Loss: 0.11393467336893082
step: 7700, Loss: 0.11512866616249084
step: 7800, Loss: 0.11690741777420044
step: 7900, Loss: 0.11371642351150513
step: 8000, Loss: 0.11611572653055191
step: 8100, Loss: 0.11528418958187103
step: 8200, Loss: 0.11389429867267609
step: 8300, Loss: 0.11541515588760376
step: 8400, Loss: 0.11369052529335022
step: 8500, Loss: 0.11614642292261124
step: 8600, Loss: 0.11536796391010284
step: 8700, Loss: 0.11637594550848007
step: 8800, Loss: 0.11544223129749298
step: 8900, Loss: 0.11691826581954956
step: 9000, Loss: 0.11560353636741638
step: 9100, Loss: 0.1147858053445816
step: 9200, Loss: 0.11526666581630707
step: 9300, Loss: 0.11685198545455933
step: 9400, Loss: 0.11479243636131287
step: 9500, Loss: 0.1159989982843399
step: 9600, Loss: 0.11475735902786255
step: 9700, Loss: 0.11563324183225632
step: 9800, Loss: 0.11490266025066376
step: 9900, Loss: 0.11507114768028259
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8666666666666667
recall: 0.8666666666666667
F_score: 0.8666666666666667
******fold 9******

Training... train_data length:281
step: 0, Loss: 2.2257466316223145
step: 100, Loss: 0.12188407778739929
step: 200, Loss: 0.11605111509561539
step: 300, Loss: 0.11610522121191025
step: 400, Loss: 0.11673519015312195
step: 500, Loss: 0.11561006307601929
step: 600, Loss: 0.117221400141716
step: 700, Loss: 0.11525841057300568
step: 800, Loss: 0.11482945829629898
step: 900, Loss: 0.11757957190275192
step: 1000, Loss: 0.11624462902545929
step: 1100, Loss: 0.1171116828918457
step: 1200, Loss: 0.1146530956029892
step: 1300, Loss: 0.11647938936948776
step: 1400, Loss: 0.11480638384819031
step: 1500, Loss: 0.11467600613832474
step: 1600, Loss: 0.1164216697216034
step: 1700, Loss: 0.11690235137939453
step: 1800, Loss: 0.11524549871683121
step: 1900, Loss: 0.11528488993644714
step: 2000, Loss: 0.11518119275569916
step: 2100, Loss: 0.11652744561433792
step: 2200, Loss: 0.11593955755233765
step: 2300, Loss: 0.1157713383436203
step: 2400, Loss: 0.11407866328954697
step: 2500, Loss: 0.11599048227071762
step: 2600, Loss: 0.11589522659778595
step: 2700, Loss: 0.11513219773769379
step: 2800, Loss: 0.11644426733255386
step: 2900, Loss: 0.1145397201180458
step: 3000, Loss: 0.11746522039175034
step: 3100, Loss: 0.1206250786781311
step: 3200, Loss: 0.11505968868732452
step: 3300, Loss: 0.11691852658987045
step: 3400, Loss: 0.11904209107160568
step: 3500, Loss: 0.11726111173629761
step: 3600, Loss: 0.11785493791103363
step: 3700, Loss: 0.11535446345806122
step: 3800, Loss: 0.11477774381637573
step: 3900, Loss: 0.11382438242435455
step: 4000, Loss: 0.11520231515169144
step: 4100, Loss: 0.11382567137479782
step: 4200, Loss: 0.11431649327278137
step: 4300, Loss: 0.11694835126399994
step: 4400, Loss: 0.11398673057556152
step: 4500, Loss: 0.11467516422271729
step: 4600, Loss: 0.11584573984146118
step: 4700, Loss: 0.11512589454650879
step: 4800, Loss: 0.11464337259531021
step: 4900, Loss: 0.11332610994577408
step: 5000, Loss: 0.12068149447441101
step: 5100, Loss: 0.11570757627487183
step: 5200, Loss: 7.769851207733154
step: 5300, Loss: 0.17500990629196167
step: 5400, Loss: 0.1301005780696869
step: 5500, Loss: 0.12267588078975677
step: 5600, Loss: 0.1201210618019104
step: 5700, Loss: 0.11984319984912872
step: 5800, Loss: 0.12385233491659164
step: 5900, Loss: 0.12184666097164154
step: 6000, Loss: 0.11634354293346405
step: 6100, Loss: 0.12090840935707092
step: 6200, Loss: 0.11989021301269531
step: 6300, Loss: 0.11763034760951996
step: 6400, Loss: 0.11687391996383667
step: 6500, Loss: 0.11966297030448914
step: 6600, Loss: 0.11726392060518265
step: 6700, Loss: 0.119785375893116
step: 6800, Loss: 0.11811868101358414
step: 6900, Loss: 0.12403557449579239
step: 7000, Loss: 0.11544238775968552
step: 7100, Loss: 0.11419690400362015
step: 7200, Loss: 0.11665176600217819
step: 7300, Loss: 0.11603666841983795
step: 7400, Loss: 0.11881119757890701
step: 7500, Loss: 0.11939963698387146
step: 7600, Loss: 0.11538484692573547
step: 7700, Loss: 0.11952687799930573
step: 7800, Loss: 0.11587952822446823
step: 7900, Loss: 0.11536799371242523
step: 8000, Loss: 0.11536082625389099
step: 8100, Loss: 0.12018764764070511
step: 8200, Loss: 0.11580434441566467
step: 8300, Loss: 0.11568307876586914
step: 8400, Loss: 0.11562155187129974
step: 8500, Loss: 0.1150275245308876
step: 8600, Loss: 0.11580351740121841
step: 8700, Loss: 0.11487054824829102
step: 8800, Loss: 0.11449844390153885
step: 8900, Loss: 0.11437123268842697
step: 9000, Loss: 0.11481720954179764
step: 9100, Loss: 0.11473443359136581
step: 9200, Loss: 0.11674203723669052
step: 9300, Loss: 0.11472651362419128
step: 9400, Loss: 0.11654532700777054
step: 9500, Loss: 0.11693379282951355
step: 9600, Loss: 0.1144353374838829
step: 9700, Loss: 0.1149604469537735
step: 9800, Loss: 0.11414466798305511
step: 9900, Loss: 0.11590763926506042
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.8
recall: 0.7058823529411765
F_score: 0.7500000000000001
******fold 10******

Training... train_data length:281
step: 0, Loss: 2.1083271503448486
step: 100, Loss: 0.12001954019069672
step: 200, Loss: 0.11896783113479614
step: 300, Loss: 0.12754230201244354
step: 400, Loss: 0.11658702790737152
step: 500, Loss: 0.11455156654119492
step: 600, Loss: 0.11578155308961868
step: 700, Loss: 0.11781881004571915
step: 800, Loss: 0.11504937708377838
step: 900, Loss: 0.11511659622192383
step: 1000, Loss: 0.11396141350269318
step: 1100, Loss: 0.11822807788848877
step: 1200, Loss: 0.11456535011529922
step: 1300, Loss: 0.11785884201526642
step: 1400, Loss: 0.11476144194602966
step: 1500, Loss: 0.1178777739405632
step: 1600, Loss: 0.11318597197532654
step: 1700, Loss: 0.1154235303401947
step: 1800, Loss: 0.11462473124265671
step: 1900, Loss: 0.11451481282711029
step: 2600, Loss: 0.11428220570087433
step: 2700, Loss: 0.11454585194587708
step: 2800, Loss: 0.11493232846260071
step: 2900, Loss: 0.11403969675302505
step: 3000, Loss: 0.11504480242729187
step: 3100, Loss: 0.11333170533180237
step: 3200, Loss: 0.11485236883163452
step: 3300, Loss: 0.11308503150939941
step: 3400, Loss: 0.11335352063179016
step: 3500, Loss: 0.11297188699245453
step: 3600, Loss: 0.11327706277370453
step: 3700, Loss: 0.11474286019802094
step: 3800, Loss: 0.11400983482599258
step: 3900, Loss: 0.11291354894638062
step: 4000, Loss: 0.11352092027664185
step: 4100, Loss: 0.11367776989936829
step: 4200, Loss: 0.11369727551937103
step: 4300, Loss: 0.11541092395782471
step: 4400, Loss: 0.11442290246486664
step: 4500, Loss: 0.11354868859052658
step: 4600, Loss: 0.11368623375892639
step: 4700, Loss: 0.11415878683328629
step: 4800, Loss: 0.11459322273731232
step: 4900, Loss: 8.539275169372559
step: 5000, Loss: 0.3315982222557068
step: 5100, Loss: 0.13892191648483276
step: 5200, Loss: 0.12961052358150482
step: 5300, Loss: 0.1223667562007904
step: 5400, Loss: 0.12131905555725098
step: 5500, Loss: 0.11766570061445236
step: 5600, Loss: 0.11853397637605667
step: 5700, Loss: 0.12143004685640335
step: 5800, Loss: 0.11834023892879486
step: 5900, Loss: 0.1209508627653122
step: 6000, Loss: 0.11731274425983429
step: 6100, Loss: 0.11590258032083511
step: 6200, Loss: 0.11659759283065796
step: 6300, Loss: 0.11564188450574875
step: 6400, Loss: 0.11705119162797928
step: 6500, Loss: 0.11475826799869537
step: 6600, Loss: 0.116142138838768
step: 6700, Loss: 0.11583122611045837
step: 6800, Loss: 0.11637397855520248
step: 6900, Loss: 0.11434921622276306
step: 7000, Loss: 0.11424539238214493
step: 7100, Loss: 0.1149148941040039
step: 7200, Loss: 0.11512625962495804
step: 7300, Loss: 0.11536809802055359
step: 7400, Loss: 0.11414119601249695
step: 7500, Loss: 0.11365459114313126
step: 7600, Loss: 0.11592432856559753
step: 7700, Loss: 0.11393950134515762
step: 7800, Loss: 0.11366917937994003
step: 7900, Loss: 0.11584483832120895
step: 8000, Loss: 0.11497479677200317
step: 8100, Loss: 0.11733999848365784
step: 8200, Loss: 0.11453662067651749
step: 8300, Loss: 0.11522173136472702
step: 8400, Loss: 0.11347632855176926
step: 8500, Loss: 0.1140114814043045
step: 8600, Loss: 0.1147112026810646
step: 8700, Loss: 0.11283045262098312
step: 8800, Loss: 0.11314407736063004
step: 8900, Loss: 0.11427926272153854
step: 9000, Loss: 0.11363371461629868
step: 9100, Loss: 0.11386691778898239
step: 9200, Loss: 0.11436592042446136
step: 9300, Loss: 0.11431124061346054
step: 9400, Loss: 0.11426293104887009
step: 9500, Loss: 0.11470361053943634
step: 9600, Loss: 0.11409892141819
step: 9700, Loss: 0.11397264152765274
step: 9800, Loss: 0.11335311830043793
step: 9900, Loss: 0.11446602642536163
training successfully ended.
validating...
validate data length:98
acc: 0.96875
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
subject 5 Avgacc: 0.9677083333333332 Avgfscore: 0.9696854260207829 
 Max acc:1.0, Max f score:1.0
******** mix subject_6 ********

[190, 570]
******fold 1******

Training... train_data length:1026
step: 0, Loss: 42.4913215637207
step: 100, Loss: 0.8674920797348022
step: 200, Loss: 0.17084871232509613
step: 300, Loss: 0.14800375699996948
step: 400, Loss: 0.15392892062664032
step: 500, Loss: 0.141031414270401
step: 600, Loss: 0.13319048285484314
step: 700, Loss: 0.13592645525932312
step: 800, Loss: 0.12906308472156525
step: 900, Loss: 0.1319912075996399
step: 1000, Loss: 0.12570497393608093
step: 1100, Loss: 0.12736713886260986
step: 1200, Loss: 0.1282641887664795
step: 1300, Loss: 0.12113162130117416
step: 1400, Loss: 0.12340771406888962
step: 1500, Loss: 0.11692625284194946
step: 1600, Loss: 0.11934962123632431
step: 1700, Loss: 0.12107320129871368
step: 1800, Loss: 0.12076282501220703
step: 1900, Loss: 0.12186561524868011
step: 2000, Loss: 0.11979959160089493
step: 2100, Loss: 0.12012510746717453
step: 2200, Loss: 0.11475435644388199
step: 2300, Loss: 0.11653967201709747
step: 2400, Loss: 0.11525178700685501
step: 2500, Loss: 0.11485005915164948
step: 2600, Loss: 0.11881078034639359
step: 2700, Loss: 0.11782076954841614
step: 2800, Loss: 0.11405908316373825
step: 2900, Loss: 0.11387357860803604
step: 3000, Loss: 0.11579078435897827
step: 3100, Loss: 0.11700542271137238
step: 3200, Loss: 0.11548750102519989
step: 3300, Loss: 0.1150285005569458
step: 3400, Loss: 0.11562193930149078
step: 3500, Loss: 0.1154252290725708
step: 3600, Loss: 0.1152726411819458
step: 3700, Loss: 0.115385502576828
step: 3800, Loss: 0.11533571779727936
step: 3900, Loss: 0.11430738121271133
step: 4000, Loss: 4.291045665740967
step: 4100, Loss: 0.1756172478199005
step: 4200, Loss: 0.13182970881462097
step: 4300, Loss: 0.13034190237522125
step: 4400, Loss: 0.12748777866363525
step: 4500, Loss: 0.12008317559957504
step: 4600, Loss: 0.12886367738246918
step: 4700, Loss: 0.12192605435848236
step: 4800, Loss: 0.12327396124601364
step: 4900, Loss: 0.11954228579998016
step: 5000, Loss: 0.11851420998573303
step: 5100, Loss: 0.12004382908344269
step: 5200, Loss: 0.11809058487415314
step: 5300, Loss: 0.1168585941195488
step: 5400, Loss: 0.11716832965612411
step: 5500, Loss: 0.11839242279529572
step: 5600, Loss: 0.1157015711069107
step: 5700, Loss: 0.1150798425078392
step: 5800, Loss: 0.11527705192565918
step: 5900, Loss: 0.11599153280258179
step: 6000, Loss: 0.11557317525148392
step: 6100, Loss: 0.1158394142985344
step: 6200, Loss: 0.11500482261180878
step: 6300, Loss: 0.11607328057289124
step: 6400, Loss: 0.11506372690200806
step: 6500, Loss: 0.1148967295885086
step: 6600, Loss: 0.11435350030660629
step: 6700, Loss: 0.11438349634408951
step: 6800, Loss: 0.11403868347406387
step: 6900, Loss: 0.11914849281311035
step: 7000, Loss: 0.11531418561935425
step: 7100, Loss: 0.11499036848545074
step: 7200, Loss: 0.11425010114908218
step: 7300, Loss: 0.11510241031646729
step: 7400, Loss: 0.11531789600849152
step: 7500, Loss: 0.11383824050426483
step: 7600, Loss: 0.11414767056703568
step: 7700, Loss: 0.11534177511930466
step: 7800, Loss: 0.11420857906341553
step: 7900, Loss: 0.11559172719717026
step: 8000, Loss: 0.11376985162496567
step: 8100, Loss: 0.11432390660047531
step: 8200, Loss: 0.11403827369213104
step: 8300, Loss: 0.11469864845275879
step: 8400, Loss: 0.11370880901813507
step: 8500, Loss: 0.11453907936811447
step: 8600, Loss: 0.11430002003908157
step: 8700, Loss: 0.11362972110509872
step: 8800, Loss: 0.11363039910793304
step: 8900, Loss: 0.113832987844944
step: 9000, Loss: 0.11402885615825653
step: 9100, Loss: 0.11386385560035706
step: 9200, Loss: 0.11455880105495453
step: 9300, Loss: 0.11437994241714478
step: 9400, Loss: 0.11400965601205826
step: 9500, Loss: 0.1148681566119194
step: 9600, Loss: 0.11418356746435165
step: 9700, Loss: 0.11254029721021652
step: 9800, Loss: 0.1133347600698471
step: 9900, Loss: 0.11423791199922562
training successfully ended.
validating...
validate data length:114
acc: 0.9553571428571429
precision: 0.9206349206349206
recall: 1.0
F_score: 0.9586776859504132
******fold 2******

Training... train_data length:1026
step: 0, Loss: 0.1129595935344696
step: 100, Loss: 0.11917190253734589
step: 200, Loss: 0.11677930504083633
step: 300, Loss: 0.11775004118680954
step: 400, Loss: 0.33673056960105896
step: 500, Loss: 0.11462554335594177
step: 600, Loss: 0.11380869150161743
step: 700, Loss: 0.11301387846469879
step: 800, Loss: 0.11620767414569855
step: 900, Loss: 0.1139780580997467
step: 1000, Loss: 0.11364779621362686
step: 1100, Loss: 0.24229228496551514
step: 1200, Loss: 0.11480726301670074
step: 1300, Loss: 0.11499911546707153
step: 1400, Loss: 0.11347176134586334
step: 1500, Loss: 0.11365285515785217
step: 1600, Loss: 0.11450590938329697
step: 1700, Loss: 0.11395037919282913
step: 1800, Loss: 0.1719677448272705
step: 1900, Loss: 0.11492755264043808
step: 2000, Loss: 0.11409592628479004
step: 2100, Loss: 0.11417526751756668
step: 2200, Loss: 0.11378559470176697
step: 2300, Loss: 0.1152176707983017
step: 2400, Loss: 0.11318938434123993
step: 2500, Loss: 0.15314413607120514
step: 2600, Loss: 0.11350434273481369
step: 2700, Loss: 0.1138228252530098
step: 2000, Loss: 0.11824516206979752
step: 2100, Loss: 0.11690720915794373
step: 2200, Loss: 0.11441346257925034
step: 2300, Loss: 0.1151939332485199
step: 2400, Loss: 0.11459051072597504
step: 2500, Loss: 0.11778220534324646
step: 2600, Loss: 0.11662765592336655
step: 2700, Loss: 0.11599718034267426
step: 2800, Loss: 0.11545330286026001
step: 2900, Loss: 0.1157430112361908
step: 3000, Loss: 0.11925847828388214
step: 3100, Loss: 0.11659146845340729
step: 3200, Loss: 0.11464264243841171
step: 3300, Loss: 0.11506938189268112
step: 3400, Loss: 0.11494368314743042
step: 3500, Loss: 0.11542411148548126
step: 3600, Loss: 0.11395346373319626
step: 3700, Loss: 0.11449651420116425
step: 3800, Loss: 0.11538854241371155
step: 3900, Loss: 0.116277314722538
step: 4000, Loss: 0.11427205055952072
step: 4100, Loss: 1.2358146905899048
step: 4200, Loss: 0.13606300950050354
step: 4300, Loss: 0.13352128863334656
step: 4400, Loss: 0.13060587644577026
step: 4500, Loss: 0.12305674701929092
step: 4600, Loss: 0.11994397640228271
step: 4700, Loss: 0.11965183913707733
step: 4800, Loss: 0.1182379275560379
step: 4900, Loss: 0.12071342766284943
step: 5000, Loss: 0.12529946863651276
step: 5100, Loss: 0.11697100102901459
step: 5200, Loss: 0.11560770869255066
step: 5300, Loss: 0.1191006749868393
step: 5400, Loss: 0.1207336038351059
step: 5500, Loss: 0.12481220811605453
step: 5600, Loss: 0.11838985979557037
step: 5700, Loss: 0.12048281729221344
step: 5800, Loss: 0.11441324651241302
step: 5900, Loss: 0.11676453053951263
step: 6000, Loss: 0.11966659128665924
step: 6100, Loss: 0.11770981550216675
step: 6200, Loss: 0.11530008912086487
step: 6300, Loss: 0.11535869538784027
step: 6400, Loss: 0.11554031074047089
step: 6500, Loss: 0.11917947232723236
step: 6600, Loss: 0.11431884765625
step: 6700, Loss: 0.11755520105361938
step: 6800, Loss: 0.11526353657245636
step: 6900, Loss: 0.11646279692649841
step: 7000, Loss: 0.11486764997243881
step: 7100, Loss: 0.1165120080113411
step: 7200, Loss: 0.11454509198665619
step: 7300, Loss: 0.11562282592058182
step: 7400, Loss: 0.11426454037427902
step: 7500, Loss: 0.11582349985837936
step: 7600, Loss: 0.11371861398220062
step: 7700, Loss: 0.11600397527217865
step: 7800, Loss: 0.11572761833667755
step: 7900, Loss: 0.11555357277393341
step: 8000, Loss: 0.11540263891220093
step: 8100, Loss: 0.11495727300643921
step: 8200, Loss: 0.11698008328676224
step: 8300, Loss: 0.11564086377620697
step: 8400, Loss: 0.11578590422868729
step: 8500, Loss: 0.1146327331662178
step: 8600, Loss: 0.11439687013626099
step: 8700, Loss: 0.11557041853666306
step: 8800, Loss: 0.11599532514810562
step: 8900, Loss: 0.1156773716211319
step: 9000, Loss: 0.11598490178585052
step: 9100, Loss: 0.11452639102935791
step: 9200, Loss: 0.11417299509048462
step: 9300, Loss: 0.11608579754829407
step: 9400, Loss: 0.11399141699075699
step: 9500, Loss: 0.1138438805937767
step: 9600, Loss: 0.11428973078727722
step: 9700, Loss: 0.11662919074296951
step: 9800, Loss: 0.11453238874673843
step: 9900, Loss: 0.11744634807109833
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 5 Avgacc: 0.7979166666666667 Avgfscore: 0.8119193047750922 
 Max acc:1.0, Max f score:1.0
******** mix subject_6 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 35.12025833129883
step: 100, Loss: 0.5496400594711304
step: 200, Loss: 0.14963960647583008
step: 300, Loss: 0.13209140300750732
step: 400, Loss: 0.12578622996807098
step: 500, Loss: 0.1207730621099472
step: 600, Loss: 0.11737795174121857
step: 700, Loss: 0.11923201382160187
step: 800, Loss: 0.11488598585128784
step: 900, Loss: 0.11907334625720978
step: 1000, Loss: 0.1168818473815918
step: 1100, Loss: 0.11693091690540314
step: 1200, Loss: 0.11456140875816345
step: 1300, Loss: 0.1162647157907486
step: 1400, Loss: 0.11507099866867065
step: 1500, Loss: 0.11699770390987396
step: 1600, Loss: 0.11800773441791534
step: 1700, Loss: 0.1169232577085495
step: 1800, Loss: 0.11698909103870392
step: 1900, Loss: 0.1195855438709259
step: 2000, Loss: 0.11602744460105896
step: 2100, Loss: 0.11544124037027359
step: 2200, Loss: 0.11618141084909439
step: 2300, Loss: 0.11447422206401825
step: 2400, Loss: 0.11411014199256897
step: 2500, Loss: 0.11696135997772217
step: 2600, Loss: 0.11462878435850143
step: 2700, Loss: 0.11772274971008301
step: 2800, Loss: 0.11516997218132019
step: 2900, Loss: 0.11398331820964813
step: 3000, Loss: 0.11600536853075027
step: 3100, Loss: 0.1168484091758728
step: 3200, Loss: 0.11859428882598877
step: 3300, Loss: 0.11625487357378006
step: 3400, Loss: 0.11479158699512482
step: 3500, Loss: 0.1145123839378357
step: 3600, Loss: 0.11753532290458679
step: 3700, Loss: 0.1144254058599472
step: 3800, Loss: 0.11458340287208557
step: 3900, Loss: 0.11551781743764877
step: 4000, Loss: 0.11536648124456406
step: 4100, Loss: 0.11549441516399384
step: 4200, Loss: 0.11550669372081757
step: 4300, Loss: 0.11393971741199493
step: 4400, Loss: 0.11655176430940628
step: 4500, Loss: 0.11567336320877075
step: 4600, Loss: 0.11591134965419769
step: 4700, Loss: 0.11628888547420502
step: 4800, Loss: 0.11541495472192764
step: 4900, Loss: 0.11684700846672058
step: 5000, Loss: 0.11536162346601486
step: 5100, Loss: 0.15759575366973877
step: 5200, Loss: 0.14152412116527557
step: 5300, Loss: 0.12852884829044342
step: 5400, Loss: 0.1272590160369873
step: 5500, Loss: 0.1267775297164917
step: 5600, Loss: 0.13116523623466492
step: 5700, Loss: 0.1253494769334793
step: 5800, Loss: 0.12244990468025208
step: 5900, Loss: 0.11988929659128189
step: 6000, Loss: 0.11996564269065857
step: 6100, Loss: 0.11829245090484619
step: 6200, Loss: 0.11958955228328705
step: 6300, Loss: 0.11895108968019485
step: 6400, Loss: 0.11723411828279495
step: 6500, Loss: 0.1182771697640419
step: 6600, Loss: 0.1160481721162796
step: 6700, Loss: 0.11827890574932098
step: 6800, Loss: 0.12840546667575836
step: 6900, Loss: 0.11818794161081314
step: 7000, Loss: 0.11697651445865631
step: 7100, Loss: 0.1182117834687233
step: 7200, Loss: 0.11671792715787888
step: 7300, Loss: 0.11694449931383133
step: 7400, Loss: 0.11562646925449371
step: 7500, Loss: 0.11697206646203995
step: 7600, Loss: 0.11482277512550354
step: 7700, Loss: 0.11480295658111572
step: 7800, Loss: 0.11555022746324539
step: 7900, Loss: 0.11540088057518005
step: 8000, Loss: 0.11584502458572388
step: 8100, Loss: 0.11572211980819702
step: 8200, Loss: 0.11666795611381531
step: 8300, Loss: 0.1156921312212944
step: 8400, Loss: 0.1167253702878952
step: 8500, Loss: 0.11580747365951538
step: 8600, Loss: 0.11642396450042725
step: 8700, Loss: 0.11554048955440521
step: 8800, Loss: 0.11826205253601074
step: 8900, Loss: 0.1162286028265953
step: 9000, Loss: 0.11409328132867813
step: 9100, Loss: 0.11458377540111542
step: 9200, Loss: 0.11656082421541214
step: 9300, Loss: 0.11487118154764175
step: 9400, Loss: 0.1159718781709671
step: 9500, Loss: 0.11634524166584015
step: 9600, Loss: 0.12667113542556763
step: 9700, Loss: 0.11577144265174866
step: 9800, Loss: 0.11459309607744217
step: 9900, Loss: 0.11445698142051697
training successfully ended.
validating...
validate data length:32
acc: 0.40625
precision: 0.29411764705882354
recall: 0.4166666666666667
F_score: 0.3448275862068966
******fold 2******

Training... train_data length:280
step: 0, Loss: 3.333016872406006
step: 100, Loss: 0.12498754262924194
step: 200, Loss: 0.1190832331776619
step: 300, Loss: 0.11741739511489868
step: 400, Loss: 0.11887909471988678
step: 500, Loss: 0.11688042432069778
step: 600, Loss: 0.11839762330055237
step: 700, Loss: 0.11619386821985245
step: 800, Loss: 0.11449558287858963
step: 900, Loss: 0.11613593995571136
step: 1000, Loss: 0.11485299468040466
step: 1100, Loss: 0.11337946355342865
step: 1200, Loss: 0.11391492187976837
step: 1300, Loss: 0.1154976487159729
step: 1400, Loss: 0.11491403728723526
step: 1500, Loss: 0.1160481721162796
step: 1600, Loss: 0.11301742494106293
step: 1700, Loss: 0.1154094934463501
step: 1800, Loss: 0.1157456487417221
step: 1900, Loss: 0.11410936713218689
step: 2000, Loss: 0.11675722151994705
step: 2100, Loss: 0.11460202187299728
step: 2200, Loss: 0.11501133441925049
step: 2800, Loss: 0.11462865769863129
step: 2900, Loss: 0.1150413304567337
step: 3000, Loss: 0.1141049787402153
step: 3100, Loss: 0.11474700272083282
step: 3200, Loss: 0.14156612753868103
step: 3300, Loss: 0.11415264010429382
step: 3400, Loss: 0.11321431398391724
step: 3500, Loss: 2.1390902996063232
step: 3600, Loss: 0.1878112256526947
step: 3700, Loss: 0.12520436942577362
step: 3800, Loss: 0.12573041021823883
step: 3900, Loss: 0.11951272189617157
step: 4000, Loss: 0.12331543862819672
step: 4100, Loss: 0.12154077738523483
step: 4200, Loss: 0.11871401220560074
step: 4300, Loss: 0.11851108819246292
step: 4400, Loss: 0.11916898190975189
step: 4500, Loss: 0.11656828224658966
step: 4600, Loss: 0.11695411056280136
step: 4700, Loss: 0.11656740307807922
step: 4800, Loss: 0.1185484528541565
step: 4900, Loss: 0.1163644939661026
step: 5000, Loss: 0.11755788326263428
step: 5100, Loss: 0.11847462505102158
step: 5200, Loss: 0.11713627725839615
step: 5300, Loss: 0.11584272980690002
step: 5400, Loss: 0.11518364399671555
step: 5500, Loss: 0.11807326227426529
step: 5600, Loss: 0.1160353273153305
step: 5700, Loss: 0.11506961286067963
step: 5800, Loss: 0.11596477776765823
step: 5900, Loss: 0.11443104594945908
step: 6000, Loss: 0.11505328863859177
step: 6100, Loss: 0.11524619907140732
step: 6200, Loss: 0.11491170525550842
step: 6300, Loss: 0.11549343168735504
step: 6400, Loss: 0.11532134562730789
step: 6500, Loss: 0.11497657746076584
step: 6600, Loss: 0.11668935418128967
step: 6700, Loss: 0.113366037607193
step: 6800, Loss: 0.11471550911664963
step: 6900, Loss: 0.11415467411279678
step: 7000, Loss: 0.11388718336820602
step: 7100, Loss: 0.11509735882282257
step: 7200, Loss: 0.1160922572016716
step: 7300, Loss: 0.11538770794868469
step: 7400, Loss: 0.11426805704832077
step: 7500, Loss: 0.1134585365653038
step: 7600, Loss: 0.11484992504119873
step: 7700, Loss: 0.11399676650762558
step: 7800, Loss: 0.11464894562959671
step: 7900, Loss: 0.11448535323143005
step: 8000, Loss: 0.11405286192893982
step: 8100, Loss: 0.11448697745800018
step: 8200, Loss: 0.11483322083950043
step: 8300, Loss: 0.11386770009994507
step: 8400, Loss: 0.11390368640422821
step: 8500, Loss: 0.1127019003033638
step: 8600, Loss: 0.11281468719244003
step: 8700, Loss: 0.11364058405160904
step: 8800, Loss: 0.11342258751392365
step: 8900, Loss: 0.11507939547300339
step: 9000, Loss: 0.11336588859558105
step: 9100, Loss: 0.1138973981142044
step: 9200, Loss: 0.11431507021188736
step: 9300, Loss: 0.11347436159849167
step: 9400, Loss: 0.11329643428325653
step: 9500, Loss: 0.11470415443181992
step: 9600, Loss: 0.11376795172691345
step: 9700, Loss: 0.113190658390522
step: 9800, Loss: 0.11301619559526443
step: 9900, Loss: 0.11350144445896149
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******

Training... train_data length:1026
step: 0, Loss: 0.11407148838043213
step: 100, Loss: 0.11412376910448074
step: 200, Loss: 0.11396793276071548
step: 300, Loss: 0.11416045576334
step: 400, Loss: 0.113490529358387
step: 500, Loss: 0.11395855247974396
step: 600, Loss: 0.11385800689458847
step: 700, Loss: 0.11281958967447281
step: 800, Loss: 0.11442825198173523
step: 900, Loss: 0.113142229616642
step: 1000, Loss: 0.11395520716905594
step: 1100, Loss: 0.11424856632947922
step: 1200, Loss: 0.1141686886548996
step: 1300, Loss: 0.11303208768367767
step: 1400, Loss: 0.11434207856655121
step: 1500, Loss: 0.11403917521238327
step: 1600, Loss: 0.11351512372493744
step: 1700, Loss: 0.1134302169084549
step: 1800, Loss: 0.11443586647510529
step: 1900, Loss: 0.11444104462862015
step: 2000, Loss: 0.11437376588582993
step: 2100, Loss: 0.11315155029296875
step: 2200, Loss: 0.11313706636428833
step: 2300, Loss: 0.11408630758523941
step: 2400, Loss: 0.1143859252333641
step: 2500, Loss: 0.11302393674850464
step: 2600, Loss: 0.11379293352365494
step: 2700, Loss: 0.11429888755083084
step: 2800, Loss: 0.11486423015594482
step: 2900, Loss: 0.11433574557304382
step: 3000, Loss: 0.11339853703975677
step: 3100, Loss: 0.11389565467834473
step: 3200, Loss: 0.11378226429224014
step: 3300, Loss: 0.11269709467887878
step: 3400, Loss: 0.11410598456859589
step: 3500, Loss: 0.11397670954465866
step: 3600, Loss: 0.11291639506816864
step: 3700, Loss: 0.11320536583662033
step: 3800, Loss: 0.11360229551792145
step: 3900, Loss: 0.11453688144683838
step: 4000, Loss: 0.11275259405374527
step: 4100, Loss: 0.11336421966552734
step: 4200, Loss: 0.11310666799545288
step: 4300, Loss: 0.11341466009616852
step: 4400, Loss: 0.11340915411710739
step: 4500, Loss: 0.11515544354915619
step: 4600, Loss: 0.11570515483617783
step: 4700, Loss: 0.11340202391147614
step: 4800, Loss: 0.11326035857200623
step: 4900, Loss: 0.11301593482494354
step: 5000, Loss: 0.112873874604702
step: 5100, Loss: 0.113755002617836
step: 5200, Loss: 0.11300458014011383
step: 5300, Loss: 0.11406631767749786
step: 5400, Loss: 0.11287422478199005
step: 5500, Loss: 0.11312387883663177
step: 5600, Loss: 0.1140231117606163
step: 5700, Loss: 0.15447266399860382
step: 5800, Loss: 0.15595319867134094
step: 5900, Loss: 0.12774847447872162
step: 6000, Loss: 0.12051184475421906
step: 6100, Loss: 0.12103947252035141
step: 6200, Loss: 0.11989032477140427
step: 6300, Loss: 0.11799593269824982
step: 6400, Loss: 0.11643460392951965
step: 6500, Loss: 0.12061366438865662
step: 6600, Loss: 0.1182829886674881
step: 6700, Loss: 0.11650059372186661
step: 6800, Loss: 0.1182527244091034
step: 6900, Loss: 0.11637507379055023
step: 7000, Loss: 0.11622890830039978
step: 7100, Loss: 0.11474251747131348
step: 7200, Loss: 0.11613714694976807
step: 7300, Loss: 0.1165882796049118
step: 7400, Loss: 0.11394118517637253
step: 7500, Loss: 0.11658258736133575
step: 7600, Loss: 0.11511465907096863
step: 7700, Loss: 0.11514017730951309
step: 7800, Loss: 0.11411586403846741
step: 7900, Loss: 0.11320320516824722
step: 8000, Loss: 0.11467675864696503
step: 8100, Loss: 0.11602996289730072
step: 8200, Loss: 0.11591789126396179
step: 8300, Loss: 0.11526501178741455
step: 8400, Loss: 0.1158277690410614
step: 8500, Loss: 0.11438022553920746
step: 8600, Loss: 0.11395052075386047
step: 8700, Loss: 0.11662957072257996
step: 8800, Loss: 0.11478114873170853
step: 8900, Loss: 0.11467638611793518
step: 9000, Loss: 0.11517175287008286
step: 9100, Loss: 0.11431950330734253
step: 9200, Loss: 0.11415489763021469
step: 9300, Loss: 0.11432793736457825
step: 9400, Loss: 0.11336594074964523
step: 9500, Loss: 0.11446063220500946
step: 9600, Loss: 0.11380891501903534
step: 9700, Loss: 0.113400399684906
step: 9800, Loss: 0.11548154056072235
step: 9900, Loss: 0.11462134122848511
training successfully ended.
validating...
validate data length:114
acc: 0.9910714285714286
precision: 0.9836065573770492
recall: 1.0
F_score: 0.9917355371900827
******fold 4******

Training... train_data length:1026
step: 0, Loss: 0.11481945961713791
step: 100, Loss: 0.11632329225540161
step: 200, Loss: 0.11398319154977798
step: 300, Loss: 0.11459408700466156
step: 400, Loss: 0.11467806994915009
step: 500, Loss: 0.11508870124816895
step: 600, Loss: 0.11406086385250092
step: 700, Loss: 0.11457840353250504
step: 800, Loss: 0.1138782948255539
step: 900, Loss: 0.1145748496055603
step: 1000, Loss: 0.11349441111087799
step: 1100, Loss: 0.11230714619159698
step: 1200, Loss: 0.11646338552236557
step: 1300, Loss: 0.11307091265916824
step: 1400, Loss: 0.11375395208597183
step: 1500, Loss: 0.11307665705680847
step: 1600, Loss: 0.11312942206859589
step: 1700, Loss: 0.11296346038579941
step: 1800, Loss: 0.11313833296298981
step: 1900, Loss: 0.11280945688486099
step: 2000, Loss: 0.113501638174057
step: 2100, Loss: 0.11347520351409912
step: 2200, Loss: 0.11448594927787781
step: 2300, Loss: 0.11334231495857239
step: 2400, Loss: 0.11428611725568771
step: 2500, Loss: 0.11406965553760529
step: 2600, Loss: 0.11331264674663544
step: 2700, Loss: 0.11607672274112701
step: 2800, Loss: 0.1143733561038971
step: 2900, Loss: 0.11309759318828583
step: 3000, Loss: 0.1133120208978653
step: 3100, Loss: 0.11431236565113068
step: 3200, Loss: 0.11305125057697296
step: 3300, Loss: 0.11347156763076782
step: 3400, Loss: 0.11374896764755249
step: 3500, Loss: 0.11268594861030579
step: 3600, Loss: 0.11333902180194855
step: 3700, Loss: 0.11329937726259232
step: 3800, Loss: 0.11309388279914856
step: 3900, Loss: 0.11433202773332596
step: 4000, Loss: 0.11324900388717651
step: 4100, Loss: 0.11245425045490265
step: 4200, Loss: 0.11354582011699677
step: 4300, Loss: 0.11498688906431198
step: 4400, Loss: 0.11373211443424225
step: 4500, Loss: 0.1134919598698616
step: 4600, Loss: 0.11323227733373642
step: 4700, Loss: 0.11400356888771057
step: 4800, Loss: 0.11358796805143356
step: 4900, Loss: 0.11348335444927216
step: 5000, Loss: 0.11429573595523834
step: 5100, Loss: 0.11354386806488037
step: 5200, Loss: 0.11303451657295227
step: 5300, Loss: 0.11311788856983185
step: 5400, Loss: 0.43855956196784973
step: 5500, Loss: 0.15630702674388885
step: 5600, Loss: 0.12613266706466675
step: 5700, Loss: 0.12051977217197418
step: 5800, Loss: 0.1246916800737381
step: 5900, Loss: 0.12348207086324692
step: 6000, Loss: 0.12429149448871613
step: 6100, Loss: 0.1196485087275505
step: 6200, Loss: 0.11913296580314636
step: 6300, Loss: 0.11811858415603638
step: 6400, Loss: 0.1164429634809494
step: 6500, Loss: 0.11494776606559753
step: 6600, Loss: 0.12327183037996292
step: 6700, Loss: 0.11614188551902771
step: 6800, Loss: 0.11779101938009262
step: 6900, Loss: 0.11716347187757492
step: 7000, Loss: 0.11513037979602814
step: 7100, Loss: 0.11607925593852997
step: 7200, Loss: 0.1155649945139885
step: 7300, Loss: 0.11566386371850967
step: 7400, Loss: 0.11405207216739655
step: 7500, Loss: 0.11379106342792511
step: 7600, Loss: 0.11423422396183014
step: 7700, Loss: 0.11471277475357056
step: 7800, Loss: 0.11474769562482834
step: 7900, Loss: 0.11513519287109375
step: 8000, Loss: 0.11592993140220642
step: 8100, Loss: 0.11384063214063644
step: 8200, Loss: 0.11516433209180832
step: 8300, Loss: 0.11722263693809509
step: 8400, Loss: 0.11432777345180511
step: 8500, Loss: 0.11486616730690002
step: 8600, Loss: 0.1168389618396759
step: 8700, Loss: 0.11435461044311523
step: 8800, Loss: 0.11395016312599182
step: 8900, Loss: 0.11482022702693939
step: 9000, Loss: 0.11411881446838379
step: 9100, Loss: 0.11364670842885971
step: 9200, Loss: 0.11409695446491241
step: 9300, Loss: 0.11845259368419647
step: 9400, Loss: 0.11508218944072723
step: 9500, Loss: 0.11467984318733215
step: 9600, Loss: 0.1127619594335556
step: 9700, Loss: 0.11519624292850494
step: 9800, Loss: 0.11376984417438507
step: 9900, Loss: 0.11438722908496857
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:1026
step: 0, Loss: 0.1132703572511673
step: 100, Loss: 0.11510962247848511
step: 200, Loss: 0.11576001346111298
step: 300, Loss: 0.11634478718042374
step: 400, Loss: 0.11549220234155655
step: 500, Loss: 0.11492868512868881
step: 600, Loss: 0.11354202032089233
step: 700, Loss: 0.11336073279380798
step: 800, Loss: 0.11259594559669495
step: 900, Loss: 0.11582627892494202
step: 1000, Loss: 0.11368618905544281
step: 1100, Loss: 0.11355195939540863
step: 1200, Loss: 0.11367787420749664
step: 1300, Loss: 0.11407491564750671
step: 1400, Loss: 0.11335675418376923
step: 1500, Loss: 0.11529289931058884
step: 1600, Loss: 0.11418720334768295
step: 1700, Loss: 0.1130017563700676
step: 1800, Loss: 0.11267019808292389
step: 1900, Loss: 0.11456053704023361
step: 2000, Loss: 0.11499884724617004
step: 2100, Loss: 0.11379972845315933
step: 2200, Loss: 0.11334177106618881
step: 2300, Loss: 0.11290480196475983
step: 2400, Loss: 0.11314194649457932
step: 2500, Loss: 0.11320114135742188
step: 2600, Loss: 0.11363496631383896
step: 2700, Loss: 0.11312269419431686
step: 2800, Loss: 0.11330456286668777
step: 2900, Loss: 0.11315576732158661
step: 3000, Loss: 0.112825408577919
step: 3100, Loss: 0.11273866891860962
step: 3200, Loss: 0.11305244266986847
step: 3300, Loss: 0.11429722607135773
step: 3400, Loss: 0.11405870318412781
step: 3500, Loss: 0.11361143738031387
step: 3600, Loss: 0.11356937885284424
step: 3700, Loss: 0.113376185297966
step: 3800, Loss: 0.11282681673765182
step: 3900, Loss: 0.11435812711715698
step: 4000, Loss: 0.11469574272632599
step: 4100, Loss: 0.11294247955083847
step: 4200, Loss: 0.11286941915750504
step: 4300, Loss: 0.11366724222898483
step: 4400, Loss: 0.11360278725624084
step: 4500, Loss: 0.11422815173864365
step: 4600, Loss: 0.11474471539258957
step: 4700, Loss: 0.1133510023355484
step: 4800, Loss: 0.11396580189466476
step: 4900, Loss: 0.11311239749193192
step: 5000, Loss: 0.1140165850520134
step: 5100, Loss: 0.11281456053256989
step: 5200, Loss: 0.11256223917007446
step: 5300, Loss: 0.11514479666948318
step: 5400, Loss: 0.11329951137304306
step: 5500, Loss: 0.1138954609632492
step: 5600, Loss: 0.12036032974720001
step: 5700, Loss: 0.1873217076063156
step: 5800, Loss: 0.13689573109149933
step: 5900, Loss: 0.1376994550228119
step: 6000, Loss: 0.12241494655609131
step: 6100, Loss: 0.12697820365428925
step: 6200, Loss: 0.1240597516298294
step: 6300, Loss: 0.12305306643247604
step: 6400, Loss: 0.12029281258583069
step: 6500, Loss: 0.11777056753635406
step: 6600, Loss: 0.1183784008026123
step: 6700, Loss: 0.11659611016511917
step: 6800, Loss: 0.11619331687688828
step: 6900, Loss: 0.11822319775819778
step: 7000, Loss: 0.11549688130617142
step: 7100, Loss: 0.11776278913021088
step: 7200, Loss: 0.11490616202354431
step: 7300, Loss: 0.11493059247732162
step: 7400, Loss: 0.11527027189731598
step: 7500, Loss: 0.11413625627756119
step: 7600, Loss: 0.11715083569288254
step: 7700, Loss: 0.11560076475143433
step: 7800, Loss: 0.11501705646514893
step: 7900, Loss: 0.11460774391889572
step: 8000, Loss: 0.11577821522951126
step: 8100, Loss: 0.11389674246311188
step: 8200, Loss: 0.11291567981243134
step: 8300, Loss: 0.11575210094451904
step: 8400, Loss: 0.11508578807115555
step: 8500, Loss: 0.11497009545564651
step: 8600, Loss: 0.1142290011048317
step: 8700, Loss: 0.11375904083251953
step: 8800, Loss: 0.11387675255537033
step: 8900, Loss: 0.11351286619901657
step: 9000, Loss: 0.1140172928571701
step: 9100, Loss: 0.11392750591039658
step: 9200, Loss: 0.11349831521511078
step: 9300, Loss: 0.11381875723600388
step: 9400, Loss: 0.11319807171821594
step: 9500, Loss: 0.11419221013784409
step: 9600, Loss: 0.11469492316246033
step: 9700, Loss: 0.11382859945297241
step: 9800, Loss: 0.11345840245485306
step: 9900, Loss: 0.1128956750035286
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

Training... train_data length:1026
step: 0, Loss: 0.11315149813890457
step: 100, Loss: 0.11595630645751953
step: 200, Loss: 0.11505396664142609
step: 300, Loss: 0.11388081312179565
step: 400, Loss: 0.11348621547222137
step: 500, Loss: 0.11296548694372177
step: 600, Loss: 0.11530589312314987
step: 700, Loss: 0.11409985274076462
step: 800, Loss: 0.11322814971208572
step: 900, Loss: 0.11413887143135071
step: 1000, Loss: 0.11358270049095154
step: 1100, Loss: 0.11423303186893463
step: 1200, Loss: 0.11324352025985718
step: 1300, Loss: 0.11312927305698395
step: 1400, Loss: 0.1144113540649414
step: 1500, Loss: 0.11366550624370575
step: 1600, Loss: 0.11410193145275116
step: 1700, Loss: 0.1141270250082016
step: 1800, Loss: 0.11358952522277832
step: 1900, Loss: 0.11321783065795898
step: 2000, Loss: 0.11371709406375885
step: 2100, Loss: 0.11403965950012207
step: 2200, Loss: 0.11375690996646881
step: 2300, Loss: 0.11399729549884796
step: 2400, Loss: 0.11282994598150253
step: 2500, Loss: 0.11292870342731476
step: 2600, Loss: 0.11425654590129852
step: 2700, Loss: 0.11416503041982651
step: 2800, Loss: 0.11307741701602936
step: 2900, Loss: 0.11409946531057358
step: 3000, Loss: 0.11335275322198868
step: 3100, Loss: 0.11363997310400009
step: 3200, Loss: 0.11297253519296646
step: 3300, Loss: 0.11397770047187805
step: 3400, Loss: 0.11334755271673203
step: 3500, Loss: 0.11355580389499664
step: 3600, Loss: 0.11317498236894608
step: 3700, Loss: 0.1137755960226059
step: 3800, Loss: 0.11411762237548828
step: 3900, Loss: 0.11360610276460648
step: 4000, Loss: 0.590469479560852
step: 4100, Loss: 0.14983034133911133
step: 2300, Loss: 0.11684432625770569
step: 2400, Loss: 0.11522742360830307
step: 2500, Loss: 0.11639866232872009
step: 2600, Loss: 0.11607228219509125
step: 2700, Loss: 0.1147427037358284
step: 2800, Loss: 0.11490301787853241
step: 2900, Loss: 0.11471600830554962
step: 3000, Loss: 0.11431339383125305
step: 3100, Loss: 0.11467351019382477
step: 3200, Loss: 0.11542299389839172
step: 3300, Loss: 0.11568976938724518
step: 3400, Loss: 0.11433838307857513
step: 3500, Loss: 0.11341220885515213
step: 3600, Loss: 0.11426841467618942
step: 3700, Loss: 0.11512739211320877
step: 3800, Loss: 0.11756493151187897
step: 3900, Loss: 0.11872050166130066
step: 4000, Loss: 0.117616668343544
step: 4100, Loss: 0.11572679877281189
step: 4200, Loss: 0.1145908311009407
step: 4300, Loss: 0.11935846507549286
step: 4400, Loss: 0.11657077819108963
step: 4500, Loss: 0.11499864608049393
step: 4600, Loss: 0.11622454226016998
step: 4700, Loss: 0.11543812602758408
step: 4800, Loss: 0.11500625312328339
step: 4900, Loss: 0.11461078375577927
step: 5000, Loss: 0.11753149330615997
step: 5100, Loss: 0.11669880896806717
step: 5200, Loss: 0.11539234220981598
step: 5300, Loss: 0.11428134143352509
step: 5400, Loss: 0.1135803759098053
step: 5500, Loss: 0.11614993214607239
step: 5600, Loss: 0.11830726265907288
step: 5700, Loss: 0.11476050317287445
step: 5800, Loss: 0.5748809576034546
step: 5900, Loss: 0.12902602553367615
step: 6000, Loss: 0.12365969270467758
step: 6100, Loss: 0.12245728075504303
step: 6200, Loss: 0.11879290640354156
step: 6300, Loss: 0.11950480192899704
step: 6400, Loss: 0.11895114928483963
step: 6500, Loss: 0.11952410638332367
step: 6600, Loss: 0.11931004375219345
step: 6700, Loss: 0.12280736863613129
step: 6800, Loss: 0.1197623535990715
step: 6900, Loss: 0.12528149783611298
step: 7000, Loss: 0.1171717494726181
step: 7100, Loss: 0.11615334451198578
step: 7200, Loss: 0.11649156361818314
step: 7300, Loss: 0.1165790855884552
step: 7400, Loss: 0.11948522925376892
step: 7500, Loss: 0.1188332587480545
step: 7600, Loss: 0.11476186662912369
step: 7700, Loss: 0.11666065454483032
step: 7800, Loss: 0.11938309669494629
step: 7900, Loss: 0.11531290411949158
step: 8000, Loss: 0.11423088610172272
step: 8100, Loss: 0.11935330927371979
step: 8200, Loss: 0.11562531441450119
step: 8300, Loss: 0.1164199486374855
step: 8400, Loss: 0.11564253270626068
step: 8500, Loss: 0.11389757692813873
step: 8600, Loss: 0.1149069294333458
step: 8700, Loss: 0.11844077706336975
step: 8800, Loss: 0.11482800543308258
step: 8900, Loss: 0.11574366688728333
step: 9000, Loss: 0.11481853574514389
step: 9100, Loss: 0.11593129485845566
step: 9200, Loss: 0.11564543098211288
step: 9300, Loss: 0.11767488718032837
step: 9400, Loss: 0.11567017436027527
step: 9500, Loss: 0.11449190229177475
step: 9600, Loss: 0.11569865047931671
step: 9700, Loss: 0.1149488314986229
step: 9800, Loss: 0.11562972515821457
step: 9900, Loss: 0.1159604862332344
training successfully ended.
validating...
validate data length:32
acc: 0.875
precision: 0.8333333333333334
recall: 0.9375
F_score: 0.8823529411764706
******fold 3******

Training... train_data length:281
step: 0, Loss: 7.1325883865356445
step: 100, Loss: 0.12042982876300812
step: 200, Loss: 0.1166631206870079
step: 300, Loss: 0.11557401716709137
step: 400, Loss: 0.11722689121961594
step: 500, Loss: 0.1159421056509018
step: 600, Loss: 0.1138911247253418
step: 700, Loss: 0.11374560743570328
step: 800, Loss: 0.11771778762340546
step: 900, Loss: 0.11520075798034668
step: 1000, Loss: 0.1157829612493515
step: 1100, Loss: 0.1141602173447609
step: 1200, Loss: 0.11793288588523865
step: 1300, Loss: 0.11333579570055008
step: 1400, Loss: 0.11430443823337555
step: 1500, Loss: 0.11415836215019226
step: 1600, Loss: 0.11553804576396942
step: 1700, Loss: 0.11425177752971649
step: 1800, Loss: 0.11421370506286621
step: 1900, Loss: 0.11560770124197006
step: 2000, Loss: 0.11406317353248596
step: 2100, Loss: 0.11416801810264587
step: 2200, Loss: 0.11429644376039505
step: 2300, Loss: 0.11453022062778473
step: 2400, Loss: 0.11404342949390411
step: 2500, Loss: 0.1151186153292656
step: 2600, Loss: 0.1135612428188324
step: 2700, Loss: 0.11450626701116562
step: 2800, Loss: 0.11525727808475494
step: 2900, Loss: 0.11432015895843506
step: 3000, Loss: 0.1165856271982193
step: 3100, Loss: 0.11468607932329178
step: 3200, Loss: 0.11683550477027893
step: 3300, Loss: 0.11393849551677704
step: 3400, Loss: 0.11745582520961761
step: 3500, Loss: 0.115442655980587
step: 3600, Loss: 0.11582722514867783
step: 3700, Loss: 0.11497469991445541
step: 3800, Loss: 0.11701192706823349
step: 3900, Loss: 0.11492964625358582
step: 4000, Loss: 0.11423948407173157
step: 4100, Loss: 0.11367370188236237
step: 4200, Loss: 0.11496590077877045
step: 4300, Loss: 0.11928670108318329
step: 4400, Loss: 0.11402491480112076
step: 4500, Loss: 0.11440926790237427
step: 4600, Loss: 0.11558286845684052
step: 4700, Loss: 0.4278096854686737
step: 4800, Loss: 0.14612260460853577
step: 4900, Loss: 0.1466459035873413
step: 5000, Loss: 0.12476959079504013
step: 5100, Loss: 0.1237582117319107
step: 5200, Loss: 0.12072109431028366
step: 5300, Loss: 0.12007219344377518
step: 5400, Loss: 0.11943958699703217
step: 5500, Loss: 0.11668457835912704
step: 5600, Loss: 0.12008598446846008
step: 5700, Loss: 0.11634866148233414
step: 5800, Loss: 0.11765939742326736
step: 5900, Loss: 0.11504855751991272
step: 6000, Loss: 0.11462917923927307
step: 6100, Loss: 0.11552639305591583
step: 6200, Loss: 0.1159701943397522
step: 6300, Loss: 0.11479165405035019
step: 6400, Loss: 0.11479030549526215
step: 6500, Loss: 0.11391694843769073
step: 6600, Loss: 0.12096813321113586
step: 6700, Loss: 0.11609582602977753
step: 6800, Loss: 0.11627931892871857
step: 6900, Loss: 0.1148468554019928
step: 7000, Loss: 0.11401864141225815
step: 7100, Loss: 0.11586138606071472
step: 7200, Loss: 0.11597056686878204
step: 7300, Loss: 0.11562968790531158
step: 7400, Loss: 0.11468334496021271
step: 7500, Loss: 0.11455335468053818
step: 7600, Loss: 0.11656558513641357
step: 7700, Loss: 0.11501877009868622
step: 7800, Loss: 0.11451899260282516
step: 7900, Loss: 0.1138605996966362
step: 8000, Loss: 0.11577668786048889
step: 8100, Loss: 0.11366478353738785
step: 8200, Loss: 0.1144300177693367
step: 8300, Loss: 0.11517679691314697
step: 8400, Loss: 0.11401055753231049
step: 8500, Loss: 0.11491794884204865
step: 8600, Loss: 0.115825355052948
step: 8700, Loss: 0.11514531821012497
step: 8800, Loss: 0.11401141434907913
step: 8900, Loss: 0.11603204160928726
step: 9000, Loss: 0.11300317943096161
step: 9100, Loss: 0.1139068603515625
step: 9200, Loss: 0.11455145478248596
step: 9300, Loss: 0.11372452229261398
step: 9400, Loss: 0.11494800448417664
step: 9500, Loss: 0.11598028242588043
step: 9600, Loss: 0.11400076001882553
step: 9700, Loss: 0.1140870600938797
step: 9800, Loss: 0.11465428024530411
step: 9900, Loss: 0.11407841742038727
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8
recall: 0.8
F_score: 0.8000000000000002
******fold 4******

Training... train_data length:281
step: 0, Loss: 7.585191249847412
step: 100, Loss: 0.1181410700082779
step: 200, Loss: 0.11512396484613419
step: 300, Loss: 0.11523177474737167
step: 400, Loss: 0.12148460000753403
step: 500, Loss: 0.11462925374507904
step: 600, Loss: 0.11581101268529892
step: 700, Loss: 0.11736264079809189
step: 800, Loss: 0.11625388264656067
step: 900, Loss: 0.11509674787521362
step: 1000, Loss: 0.11476586014032364
step: 1100, Loss: 0.1150909811258316
step: 1200, Loss: 0.11377609521150589
step: 1300, Loss: 0.1141367182135582
step: 1400, Loss: 0.11727090179920197
step: 1500, Loss: 0.11538048833608627
step: 1600, Loss: 0.11384269595146179
step: 1700, Loss: 0.11792254447937012
step: 1800, Loss: 0.11431394517421722
step: 1900, Loss: 0.11431963741779327
step: 2000, Loss: 0.11367480456829071
step: 2100, Loss: 0.11456965655088425
step: 2200, Loss: 0.11432953178882599
step: 2300, Loss: 0.11816077679395676
step: 2400, Loss: 0.11566285043954849
step: 2500, Loss: 0.11646092683076859
step: 2600, Loss: 0.11460747569799423
step: 2700, Loss: 0.11533267796039581
step: 2800, Loss: 0.11477573961019516
step: 2900, Loss: 0.11653229594230652
step: 4200, Loss: 0.1285816729068756
step: 4300, Loss: 0.1256561577320099
step: 4400, Loss: 0.11855502426624298
step: 4500, Loss: 0.1200108453631401
step: 4600, Loss: 0.12710198760032654
step: 4700, Loss: 0.11806056648492813
step: 4800, Loss: 0.118597611784935
step: 4900, Loss: 0.11678468436002731
step: 5000, Loss: 0.11565393954515457
step: 5100, Loss: 0.11438123881816864
step: 5200, Loss: 0.11781544238328934
step: 5300, Loss: 0.11474473774433136
step: 5400, Loss: 0.11643113195896149
step: 5500, Loss: 0.11550714820623398
step: 5600, Loss: 0.11506626009941101
step: 5700, Loss: 0.11395465582609177
step: 5800, Loss: 0.11450457572937012
step: 5900, Loss: 0.11640599370002747
step: 6000, Loss: 0.1140914112329483
step: 6100, Loss: 0.11701349914073944
step: 6200, Loss: 0.11538828909397125
step: 6300, Loss: 0.11622095853090286
step: 6400, Loss: 0.11383965611457825
step: 6500, Loss: 0.11458750069141388
step: 6600, Loss: 0.11497726291418076
step: 6700, Loss: 0.11581842601299286
step: 6800, Loss: 0.11591317504644394
step: 6900, Loss: 0.11576678603887558
step: 7000, Loss: 0.11412859708070755
step: 7100, Loss: 0.11446061730384827
step: 7200, Loss: 0.11567270755767822
step: 7300, Loss: 0.11447514593601227
step: 7400, Loss: 0.11440951377153397
step: 7500, Loss: 0.11381933838129044
step: 7600, Loss: 0.1144975870847702
step: 7700, Loss: 0.1142100840806961
step: 7800, Loss: 0.11306250840425491
step: 7900, Loss: 0.11443653702735901
step: 8000, Loss: 0.11340279132127762
step: 8100, Loss: 0.11347302049398422
step: 8200, Loss: 0.11538638919591904
step: 8300, Loss: 0.11493431031703949
step: 8400, Loss: 0.1145315170288086
step: 8500, Loss: 0.11421994119882584
step: 8600, Loss: 0.11295224726200104
step: 8700, Loss: 0.11350693553686142
step: 8800, Loss: 0.11450961232185364
step: 8900, Loss: 0.11348509788513184
step: 9000, Loss: 0.11322248727083206
step: 9100, Loss: 0.11449971795082092
step: 9200, Loss: 0.1140126883983612
step: 9300, Loss: 0.114761121571064
step: 9400, Loss: 0.11403355002403259
step: 9500, Loss: 0.11473680287599564
step: 9600, Loss: 0.11371729522943497
step: 9700, Loss: 0.11329731345176697
step: 9800, Loss: 0.11397124081850052
step: 9900, Loss: 0.11410672962665558
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:1026
step: 0, Loss: 0.11363393813371658
step: 100, Loss: 0.11445974558591843
step: 200, Loss: 0.11368114501237869
step: 300, Loss: 0.11443349719047546
step: 400, Loss: 0.1137317568063736
step: 500, Loss: 0.1155809536576271
step: 600, Loss: 0.1139657199382782
step: 700, Loss: 0.11353151500225067
step: 800, Loss: 0.11326585710048676
step: 900, Loss: 0.11301155388355255
step: 1000, Loss: 0.11330875009298325
step: 1100, Loss: 0.11311108618974686
step: 1200, Loss: 0.11336895078420639
step: 1300, Loss: 0.11366403102874756
step: 1400, Loss: 0.11298928409814835
step: 1500, Loss: 0.11348560452461243
step: 1600, Loss: 0.1138281524181366
step: 1700, Loss: 0.11385998129844666
step: 1800, Loss: 0.11383205652236938
step: 1900, Loss: 0.1138579472899437
step: 2000, Loss: 0.11297108978033066
step: 2100, Loss: 0.1137920543551445
step: 2200, Loss: 0.113691546022892
step: 2300, Loss: 0.11425132304430008
step: 2400, Loss: 0.1128259152173996
step: 2500, Loss: 0.11349871754646301
step: 2600, Loss: 0.11339159309864044
step: 2700, Loss: 0.11320997029542923
step: 2800, Loss: 0.11371737718582153
step: 2900, Loss: 0.11460303515195847
step: 3000, Loss: 0.1136682778596878
step: 3100, Loss: 0.11337149143218994
step: 3200, Loss: 0.11313840746879578
step: 3300, Loss: 0.113258957862854
step: 3400, Loss: 0.11401478946208954
step: 3500, Loss: 0.1132173165678978
step: 3600, Loss: 0.11337317526340485
step: 3700, Loss: 0.1130954772233963
step: 3800, Loss: 0.11428658664226532
step: 3900, Loss: 0.11370468884706497
step: 4000, Loss: 0.11347075551748276
step: 4100, Loss: 0.11374928802251816
step: 4200, Loss: 0.11291390657424927
step: 4300, Loss: 0.11338648200035095
step: 4400, Loss: 0.11426837742328644
step: 4500, Loss: 0.11405842751264572
step: 4600, Loss: 0.11464054882526398
step: 4700, Loss: 0.11401460319757462
step: 4800, Loss: 0.11335882544517517
step: 4900, Loss: 0.11467155069112778
step: 5000, Loss: 0.16357575356960297
step: 5100, Loss: 0.12402711808681488
step: 5200, Loss: 0.12860959768295288
step: 5300, Loss: 0.11978000402450562
step: 5400, Loss: 0.1199788823723793
step: 5500, Loss: 0.12282560765743256
step: 5600, Loss: 0.11844059824943542
step: 5700, Loss: 0.11683958023786545
step: 5800, Loss: 0.11504500359296799
step: 5900, Loss: 0.11837838590145111
step: 6000, Loss: 0.11666423082351685
step: 6100, Loss: 0.11658795177936554
step: 6200, Loss: 0.11612051725387573
step: 6300, Loss: 0.11504019796848297
step: 6400, Loss: 0.11393415927886963
step: 6500, Loss: 0.11535340547561646
step: 6600, Loss: 0.11619886755943298
step: 6700, Loss: 0.11437559872865677
step: 6800, Loss: 0.11461253464221954
step: 6900, Loss: 0.11555518954992294
step: 7000, Loss: 0.11489543318748474
step: 7100, Loss: 0.11505065113306046
step: 7200, Loss: 0.11384972184896469
step: 7300, Loss: 0.11422492563724518
step: 7400, Loss: 0.11348134279251099
step: 7500, Loss: 0.11515451967716217
step: 7600, Loss: 0.11556683480739594
step: 7700, Loss: 0.11619720607995987
step: 7800, Loss: 0.1142154261469841
step: 7900, Loss: 0.11465872079133987
step: 8000, Loss: 0.11356450617313385
step: 8100, Loss: 0.11437709629535675
step: 8200, Loss: 0.11314966529607773
step: 8300, Loss: 0.11531390249729156
step: 8400, Loss: 0.11386731266975403
step: 8500, Loss: 0.11330290883779526
step: 8600, Loss: 0.11381712555885315
step: 8700, Loss: 0.11412595957517624
step: 8800, Loss: 0.11442430317401886
step: 8900, Loss: 0.11472605168819427
step: 9000, Loss: 0.11403420567512512
step: 9100, Loss: 0.11396757513284683
step: 9200, Loss: 0.11439907550811768
step: 9300, Loss: 0.1122632622718811
step: 9400, Loss: 0.11418768018484116
step: 9500, Loss: 0.113898366689682
step: 9600, Loss: 0.114010289311409
step: 9700, Loss: 0.11254172027111053
step: 9800, Loss: 0.11386548727750778
step: 9900, Loss: 0.11379221081733704
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:1026
step: 0, Loss: 0.11312229931354523
step: 100, Loss: 0.11478602886199951
step: 200, Loss: 0.11385685950517654
step: 300, Loss: 0.11403492093086243
step: 400, Loss: 0.11378822475671768
step: 500, Loss: 0.11487126350402832
step: 600, Loss: 0.11281389743089676
step: 700, Loss: 0.11415570974349976
step: 800, Loss: 0.11353576928377151
step: 900, Loss: 0.11421188712120056
step: 1000, Loss: 0.11440406739711761
step: 1100, Loss: 0.11378704756498337
step: 1200, Loss: 0.1127253919839859
step: 1300, Loss: 0.11411578953266144
step: 1400, Loss: 0.11324191093444824
step: 1500, Loss: 0.11416134983301163
step: 1600, Loss: 0.11425638198852539
step: 1700, Loss: 0.11292502284049988
step: 1800, Loss: 0.11330883949995041
step: 1900, Loss: 0.11314023286104202
step: 2000, Loss: 0.11295221000909805
step: 2100, Loss: 0.11357174068689346
step: 2200, Loss: 0.11406847834587097
step: 2300, Loss: 0.11437878012657166
step: 2400, Loss: 0.11430928111076355
step: 2500, Loss: 0.11458510160446167
step: 2600, Loss: 0.1139661967754364
step: 2700, Loss: 0.11518656462430954
step: 2800, Loss: 0.11356314271688461
step: 2900, Loss: 0.11365197598934174
step: 3000, Loss: 0.11402752995491028
step: 3100, Loss: 0.1125989481806755
step: 3200, Loss: 0.1131276786327362
step: 3300, Loss: 0.1135224923491478
step: 3400, Loss: 0.11352908611297607
step: 3500, Loss: 0.11380869150161743
step: 3600, Loss: 0.11316975206136703
step: 3700, Loss: 0.11381204426288605
step: 3800, Loss: 0.11363596469163895
step: 3900, Loss: 0.11489937454462051
step: 4000, Loss: 0.11524102091789246
step: 4100, Loss: 0.11421071738004684
step: 4200, Loss: 0.11607450991868973
step: 4300, Loss: 0.11434578895568848
step: 4400, Loss: 0.11300606280565262
step: 4500, Loss: 2.749251365661621
step: 4600, Loss: 0.12629112601280212
step: 4700, Loss: 0.13142432272434235
step: 4800, Loss: 0.1326361745595932
step: 4900, Loss: 0.11914390325546265
step: 3000, Loss: 0.11513324826955795
step: 3100, Loss: 0.11448994278907776
step: 3200, Loss: 0.11366426199674606
step: 3300, Loss: 0.11288420855998993
step: 3400, Loss: 0.1143059954047203
step: 3500, Loss: 0.11377432942390442
step: 3600, Loss: 0.11721450090408325
step: 3700, Loss: 0.11475048214197159
step: 3800, Loss: 0.115181103348732
step: 3900, Loss: 0.11531639844179153
step: 4000, Loss: 0.11445173621177673
step: 4100, Loss: 0.11549565941095352
step: 4200, Loss: 0.1139904037117958
step: 4300, Loss: 0.11437340080738068
step: 4400, Loss: 0.11463949829339981
step: 4500, Loss: 0.11429240554571152
step: 4600, Loss: 0.11439425498247147
step: 4700, Loss: 0.11480490863323212
step: 4800, Loss: 0.11600160598754883
step: 4900, Loss: 0.11388623714447021
step: 5000, Loss: 0.16848939657211304
step: 5100, Loss: 0.12647698819637299
step: 5200, Loss: 0.12867328524589539
step: 5300, Loss: 0.12285593152046204
step: 5400, Loss: 0.1194319874048233
step: 5500, Loss: 0.11654311418533325
step: 5600, Loss: 0.1192745715379715
step: 5700, Loss: 0.11458395421504974
step: 5800, Loss: 0.11952749639749527
step: 5900, Loss: 0.11747404932975769
step: 6000, Loss: 0.11832647025585175
step: 6100, Loss: 0.11615046858787537
step: 6200, Loss: 0.11703947186470032
step: 6300, Loss: 0.11502954363822937
step: 6400, Loss: 0.11786967515945435
step: 6500, Loss: 0.11590660363435745
step: 6600, Loss: 0.11673737317323685
step: 6700, Loss: 0.1150948777794838
step: 6800, Loss: 0.11590205878019333
step: 6900, Loss: 0.11540184170007706
step: 7000, Loss: 0.11599540710449219
step: 7100, Loss: 0.11479312181472778
step: 7200, Loss: 0.11762995272874832
step: 7300, Loss: 0.11558090150356293
step: 7400, Loss: 0.11420175433158875
step: 7500, Loss: 0.11942249536514282
step: 7600, Loss: 0.11524523049592972
step: 7700, Loss: 0.11393235623836517
step: 7800, Loss: 0.11649781465530396
step: 7900, Loss: 0.11468121409416199
step: 8000, Loss: 0.11828810721635818
step: 8100, Loss: 0.11413155496120453
step: 8200, Loss: 0.11603708565235138
step: 8300, Loss: 0.11446908861398697
step: 8400, Loss: 0.11572092026472092
step: 8500, Loss: 0.11706073582172394
step: 8600, Loss: 0.11607798933982849
step: 8700, Loss: 0.1199520081281662
step: 8800, Loss: 0.11605540663003922
step: 8900, Loss: 0.11802223324775696
step: 9000, Loss: 0.11353160440921783
step: 9100, Loss: 0.1140962764620781
step: 9200, Loss: 0.11458785831928253
step: 9300, Loss: 0.11368384957313538
step: 9400, Loss: 0.11884994059801102
step: 9500, Loss: 0.11607624590396881
step: 9600, Loss: 0.11434177309274673
step: 9700, Loss: 0.11603645980358124
step: 9800, Loss: 0.115236297249794
step: 9900, Loss: 0.11428467929363251
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7333333333333333
recall: 0.7333333333333333
F_score: 0.7333333333333333
******fold 5******

Training... train_data length:281
step: 0, Loss: 7.113596439361572
step: 100, Loss: 0.11892770230770111
step: 200, Loss: 0.1171279326081276
step: 300, Loss: 0.11458810418844223
step: 400, Loss: 0.1145879328250885
step: 500, Loss: 0.11398458480834961
step: 600, Loss: 0.11719951033592224
step: 700, Loss: 0.11461377888917923
step: 800, Loss: 0.11500187963247299
step: 900, Loss: 0.1183469295501709
step: 1000, Loss: 0.11631964147090912
step: 1100, Loss: 0.11377377063035965
step: 1200, Loss: 0.11684238910675049
step: 1300, Loss: 0.1149936392903328
step: 1400, Loss: 0.114254891872406
step: 1500, Loss: 0.11630021035671234
step: 1600, Loss: 0.11575260758399963
step: 1700, Loss: 0.11454521864652634
step: 1800, Loss: 0.11373667418956757
step: 1900, Loss: 0.11647794395685196
step: 2000, Loss: 0.11641523987054825
step: 2100, Loss: 0.11358218640089035
step: 2200, Loss: 0.1155368834733963
step: 2300, Loss: 0.11328516155481339
step: 2400, Loss: 0.11492444574832916
step: 2500, Loss: 0.11509493738412857
step: 2600, Loss: 0.11422745138406754
step: 2700, Loss: 0.1141357272863388
step: 2800, Loss: 0.11542269587516785
step: 2900, Loss: 0.11441216617822647
step: 3000, Loss: 0.11481431126594543
step: 3100, Loss: 0.11393428593873978
step: 3200, Loss: 0.1147327721118927
step: 3300, Loss: 0.1146637573838234
step: 3400, Loss: 0.11419052630662918
step: 3500, Loss: 0.11544488370418549
step: 3600, Loss: 0.11540458351373672
step: 3700, Loss: 0.11751237511634827
step: 3800, Loss: 0.11586432158946991
step: 3900, Loss: 0.11380735039710999
step: 4000, Loss: 0.11518750339746475
step: 4100, Loss: 0.11517821997404099
step: 4200, Loss: 0.11418522894382477
step: 4300, Loss: 0.11514586955308914
step: 4400, Loss: 0.1172047033905983
step: 4500, Loss: 0.11539497971534729
step: 4600, Loss: 0.15752890706062317
step: 4700, Loss: 0.12564298510551453
step: 4800, Loss: 0.12185380607843399
step: 4900, Loss: 0.12521907687187195
step: 5000, Loss: 0.12255952507257462
step: 5100, Loss: 0.11691157519817352
step: 5200, Loss: 0.12043362855911255
step: 5300, Loss: 0.11905159801244736
step: 5400, Loss: 0.11805085837841034
step: 5500, Loss: 0.12199385464191437
step: 5600, Loss: 0.12406985461711884
step: 5700, Loss: 0.12108389288187027
step: 5800, Loss: 0.11773791909217834
step: 5900, Loss: 0.11748912930488586
step: 6000, Loss: 0.11679607629776001
step: 6100, Loss: 0.11735224723815918
step: 6200, Loss: 0.11660812795162201
step: 6300, Loss: 0.11560769379138947
step: 6400, Loss: 0.11435409635305405
step: 6500, Loss: 0.11760561913251877
step: 6600, Loss: 0.11406559497117996
step: 6700, Loss: 0.11595254391431808
step: 6800, Loss: 0.11803088337182999
step: 6900, Loss: 0.11554458737373352
step: 7000, Loss: 0.11649610847234726
step: 7100, Loss: 0.115651935338974
step: 7200, Loss: 0.11675648391246796
step: 7300, Loss: 0.11542186886072159
step: 7400, Loss: 0.11766938865184784
step: 7500, Loss: 0.1167488768696785
step: 7600, Loss: 0.11694876849651337
step: 7700, Loss: 0.11390785872936249
step: 7800, Loss: 0.1168668195605278
step: 7900, Loss: 0.11382728815078735
step: 8000, Loss: 0.1136133000254631
step: 8100, Loss: 0.11381061375141144
step: 8200, Loss: 0.11451280117034912
step: 8300, Loss: 0.11622948199510574
step: 8400, Loss: 0.11614085733890533
step: 8500, Loss: 0.11455205827951431
step: 8600, Loss: 0.11532125622034073
step: 8700, Loss: 0.11468138545751572
step: 8800, Loss: 0.11514505743980408
step: 8900, Loss: 0.11366100609302521
step: 9000, Loss: 0.11450716853141785
step: 9100, Loss: 0.11465597152709961
step: 9200, Loss: 0.1161845326423645
step: 9300, Loss: 0.11425789445638657
step: 9400, Loss: 0.11481691896915436
step: 9500, Loss: 0.11351054161787033
step: 9600, Loss: 0.11379311233758926
step: 9700, Loss: 0.11374413222074509
step: 9800, Loss: 0.11422820389270782
step: 9900, Loss: 0.11484941840171814
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8333333333333334
recall: 0.8333333333333334
F_score: 0.8333333333333334
******fold 6******

Training... train_data length:281
step: 0, Loss: 6.7814202308654785
step: 100, Loss: 0.11824776232242584
step: 200, Loss: 0.11793310195207596
step: 300, Loss: 0.11499431729316711
step: 400, Loss: 0.1170448362827301
step: 500, Loss: 0.11497746407985687
step: 600, Loss: 0.11514656245708466
step: 700, Loss: 0.1184825673699379
step: 800, Loss: 0.11542259156703949
step: 900, Loss: 0.11488591134548187
step: 1000, Loss: 0.11695108562707901
step: 1100, Loss: 0.1161249577999115
step: 1200, Loss: 0.11553551256656647
step: 1300, Loss: 0.11512191593647003
step: 1400, Loss: 0.11363659799098969
step: 1500, Loss: 0.11430850625038147
step: 1600, Loss: 0.11889225244522095
step: 1700, Loss: 0.11548858880996704
step: 1800, Loss: 0.11921998858451843
step: 1900, Loss: 0.11621162295341492
step: 2000, Loss: 0.11480122804641724
step: 2100, Loss: 0.1153818741440773
step: 2200, Loss: 0.11319581419229507
step: 2300, Loss: 0.11608091741800308
step: 2400, Loss: 0.11435838788747787
step: 2500, Loss: 0.1148751899600029
step: 2600, Loss: 0.11432595551013947
step: 2700, Loss: 0.11435981839895248
step: 2800, Loss: 0.11486755311489105
step: 2900, Loss: 0.11439789831638336
step: 3000, Loss: 0.11642952263355255
step: 3100, Loss: 0.11687718331813812
step: 3200, Loss: 0.11486035585403442
step: 3300, Loss: 0.11587370932102203
step: 3400, Loss: 0.11484694480895996
step: 5000, Loss: 0.11824063956737518
step: 5100, Loss: 0.12080637365579605
step: 5200, Loss: 0.11899999529123306
step: 5300, Loss: 0.11676869541406631
step: 5400, Loss: 0.11701460927724838
step: 5500, Loss: 0.116055428981781
step: 5600, Loss: 0.11727219820022583
step: 5700, Loss: 0.11672407388687134
step: 5800, Loss: 0.11443174630403519
step: 5900, Loss: 0.115163654088974
step: 6000, Loss: 0.1146097332239151
step: 6100, Loss: 0.11609100550413132
step: 6200, Loss: 0.11784140020608902
step: 6300, Loss: 0.11471927911043167
step: 6400, Loss: 0.11761609464883804
step: 6500, Loss: 0.11468971520662308
step: 6600, Loss: 0.1169508621096611
step: 6700, Loss: 0.11352922767400742
step: 6800, Loss: 0.11383648961782455
step: 6900, Loss: 0.11497732251882553
step: 7000, Loss: 0.1141863614320755
step: 7100, Loss: 0.11455885320901871
step: 7200, Loss: 0.11434461176395416
step: 7300, Loss: 0.11462684720754623
step: 7400, Loss: 0.11439923942089081
step: 7500, Loss: 0.11384860426187515
step: 7600, Loss: 0.11566165089607239
step: 7700, Loss: 0.11423401534557343
step: 7800, Loss: 0.11432934552431107
step: 7900, Loss: 0.11421504616737366
step: 8000, Loss: 0.11517811566591263
step: 8100, Loss: 0.11353684216737747
step: 8200, Loss: 0.11444328725337982
step: 8300, Loss: 0.11457633227109909
step: 8400, Loss: 0.11419838666915894
step: 8500, Loss: 0.11426221579313278
step: 8600, Loss: 0.11388161033391953
step: 8700, Loss: 0.11509396135807037
step: 8800, Loss: 0.11390363425016403
step: 8900, Loss: 0.11309065669775009
step: 9000, Loss: 0.11634360253810883
step: 9100, Loss: 0.11405916512012482
step: 9200, Loss: 0.11401141434907913
step: 9300, Loss: 0.11378544569015503
step: 9400, Loss: 0.11448003351688385
step: 9500, Loss: 0.11319656670093536
step: 9600, Loss: 0.1137930378317833
step: 9700, Loss: 0.11405674368143082
step: 9800, Loss: 0.11435622721910477
step: 9900, Loss: 0.11447815597057343
training successfully ended.
validating...
validate data length:114
acc: 0.9910714285714286
precision: 0.9807692307692307
recall: 1.0
F_score: 0.9902912621359222
******fold 9******

Training... train_data length:1026
step: 0, Loss: 0.11484655737876892
step: 100, Loss: 0.1138463169336319
step: 200, Loss: 0.11306052654981613
step: 300, Loss: 0.11340044438838959
step: 400, Loss: 0.11365346610546112
step: 500, Loss: 0.11329130083322525
step: 600, Loss: 0.11335838586091995
step: 700, Loss: 0.11387871205806732
step: 800, Loss: 0.11321976035833359
step: 900, Loss: 0.11419832706451416
step: 1000, Loss: 0.11363521218299866
step: 1100, Loss: 0.11382968723773956
step: 1200, Loss: 0.11527962982654572
step: 1300, Loss: 0.11403527110815048
step: 1400, Loss: 0.11416932195425034
step: 1500, Loss: 0.11499959975481033
step: 1600, Loss: 0.8884624242782593
step: 1700, Loss: 0.13248804211616516
step: 1800, Loss: 0.12377836555242538
step: 1900, Loss: 0.12296894937753677
step: 2000, Loss: 0.12065593153238297
step: 2100, Loss: 0.11752120405435562
step: 2200, Loss: 0.116402767598629
step: 2300, Loss: 0.12016209959983826
step: 2400, Loss: 0.1157713234424591
step: 2500, Loss: 0.11872237175703049
step: 2600, Loss: 0.11911831796169281
step: 2700, Loss: 0.11538466811180115
step: 2800, Loss: 0.11664682626724243
step: 2900, Loss: 0.11669178307056427
step: 3000, Loss: 0.11497478187084198
step: 3100, Loss: 0.11494684219360352
step: 3200, Loss: 0.11653466522693634
step: 3300, Loss: 0.11495368927717209
step: 3400, Loss: 0.11584662646055222
step: 3500, Loss: 0.11361876875162125
step: 3600, Loss: 0.11519477516412735
step: 3700, Loss: 0.11466062813997269
step: 3800, Loss: 0.11381009966135025
step: 3900, Loss: 0.11391142010688782
step: 4000, Loss: 0.11549092829227448
step: 4100, Loss: 0.11426015198230743
step: 4200, Loss: 0.11549009382724762
step: 4300, Loss: 0.11339107155799866
step: 4400, Loss: 0.1155802458524704
step: 4500, Loss: 0.11431483179330826
step: 4600, Loss: 0.11415498703718185
step: 4700, Loss: 0.11495207995176315
step: 4800, Loss: 0.11445125937461853
step: 4900, Loss: 0.11495593935251236
step: 5000, Loss: 0.11490882188081741
step: 5100, Loss: 0.11401240527629852
step: 5200, Loss: 0.1141570657491684
step: 5300, Loss: 0.11279448866844177
step: 5400, Loss: 0.11526954919099808
step: 5500, Loss: 0.1138290986418724
step: 5600, Loss: 0.11317507922649384
step: 5700, Loss: 0.11431627720594406
step: 5800, Loss: 0.11280355602502823
step: 5900, Loss: 0.11336469650268555
step: 6000, Loss: 0.11544312536716461
step: 6100, Loss: 0.1142268180847168
step: 6200, Loss: 0.11520910263061523
step: 6300, Loss: 0.11400572210550308
step: 6400, Loss: 0.11349046975374222
step: 6500, Loss: 0.11388047784566879
step: 6600, Loss: 0.11391600966453552
step: 6700, Loss: 0.11318671703338623
step: 6800, Loss: 0.11337809264659882
step: 6900, Loss: 0.1143416315317154
step: 7000, Loss: 0.1146814152598381
step: 7100, Loss: 0.11441431939601898
step: 7200, Loss: 0.11351701617240906
step: 7300, Loss: 0.11391526460647583
step: 7400, Loss: 0.11355776339769363
step: 7500, Loss: 0.11436080187559128
step: 7600, Loss: 0.11548391729593277
step: 7700, Loss: 0.11453448235988617
step: 7800, Loss: 0.11279778927564621
step: 7900, Loss: 0.11479649692773819
step: 8000, Loss: 0.1138334646821022
step: 8100, Loss: 0.1131213903427124
step: 8200, Loss: 0.112676702439785
step: 8300, Loss: 0.11579760909080505
step: 8400, Loss: 0.11322622746229172
step: 8500, Loss: 0.11371178925037384
step: 8600, Loss: 0.11401679366827011
step: 8700, Loss: 0.11273504793643951
step: 8800, Loss: 0.1130283921957016
step: 8900, Loss: 0.11335580050945282
step: 9000, Loss: 0.11386655271053314
step: 9100, Loss: 0.11377166956663132
step: 9200, Loss: 0.11372816562652588
step: 9300, Loss: 0.113655686378479
step: 9400, Loss: 0.11376237124204636
step: 9500, Loss: 0.11423073709011078
step: 9600, Loss: 0.11517638713121414
step: 9700, Loss: 0.11428661644458771
step: 9800, Loss: 0.11424931138753891
step: 9900, Loss: 0.11374805122613907
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:1026
step: 0, Loss: 0.11344006657600403
step: 100, Loss: 0.11765208095312119
step: 200, Loss: 0.1151600107550621
step: 300, Loss: 0.11378268152475357
step: 400, Loss: 0.11522877961397171
step: 500, Loss: 0.11463946849107742
step: 600, Loss: 0.11585075408220291
step: 700, Loss: 0.11435500532388687
step: 800, Loss: 0.11303185671567917
step: 900, Loss: 0.11368009448051453
step: 1000, Loss: 0.11397696286439896
step: 1100, Loss: 0.1145084947347641
step: 1200, Loss: 0.11382784694433212
step: 1300, Loss: 0.11343295127153397
step: 1400, Loss: 0.1151019036769867
step: 1500, Loss: 0.11474425345659256
step: 1600, Loss: 0.11502321064472198
step: 1700, Loss: 0.1144571453332901
step: 1800, Loss: 0.1144547164440155
step: 1900, Loss: 0.11399082094430923
step: 2000, Loss: 0.11393742263317108
step: 2100, Loss: 0.1132342517375946
step: 2200, Loss: 0.11290915310382843
step: 2300, Loss: 0.11405259370803833
step: 2400, Loss: 0.1141524463891983
step: 2500, Loss: 0.11246664077043533
step: 2600, Loss: 0.11426092684268951
step: 2700, Loss: 0.11473330855369568
step: 2800, Loss: 0.1144639328122139
step: 2900, Loss: 0.1129375696182251
step: 3000, Loss: 0.113345667719841
step: 3100, Loss: 0.11295236647129059
step: 3200, Loss: 0.11442027986049652
step: 3300, Loss: 0.11376192420721054
step: 3400, Loss: 0.11525682359933853
step: 3500, Loss: 0.11348550766706467
step: 3600, Loss: 0.1141858696937561
step: 3700, Loss: 0.11355254054069519
step: 3800, Loss: 0.11330726742744446
step: 3900, Loss: 0.11369317024946213
step: 4000, Loss: 0.11301101744174957
step: 4100, Loss: 0.11361720412969589
step: 4200, Loss: 0.11346957832574844
step: 4300, Loss: 0.1141374260187149
step: 4400, Loss: 0.11365851014852524
step: 4500, Loss: 0.11633092164993286
step: 4600, Loss: 0.11319632828235626
step: 4700, Loss: 0.1134183257818222
step: 4800, Loss: 0.11349800229072571
step: 4900, Loss: 0.11277834326028824
step: 5000, Loss: 0.1133815124630928
step: 5100, Loss: 0.11428795754909515
step: 5200, Loss: 0.11238151043653488
step: 5300, Loss: 0.11289361864328384
step: 5400, Loss: 0.11297527700662613
step: 5500, Loss: 0.11411765962839127
step: 5600, Loss: 0.11338450759649277
step: 3500, Loss: 0.11405659466981888
step: 3600, Loss: 0.11395279318094254
step: 3700, Loss: 0.11449393630027771
step: 3800, Loss: 0.11468181014060974
step: 3900, Loss: 0.11551781743764877
step: 4000, Loss: 0.11460588872432709
step: 4100, Loss: 0.11486205458641052
step: 4200, Loss: 0.11448962241411209
step: 4300, Loss: 0.11583247780799866
step: 4400, Loss: 0.1156756654381752
step: 4500, Loss: 0.16261182725429535
step: 4600, Loss: 0.1376916617155075
step: 4700, Loss: 0.1381884217262268
step: 4800, Loss: 0.13090987503528595
step: 4900, Loss: 0.11795135587453842
step: 5000, Loss: 0.11832791566848755
step: 5100, Loss: 0.11842717230319977
step: 5200, Loss: 0.11614323407411575
step: 5300, Loss: 0.11768056452274323
step: 5400, Loss: 0.11522743105888367
step: 5500, Loss: 0.11818760633468628
step: 5600, Loss: 0.11720295995473862
step: 5700, Loss: 0.11796542257070541
step: 5800, Loss: 0.11825616657733917
step: 5900, Loss: 0.11865691840648651
step: 6000, Loss: 0.11671102792024612
step: 6100, Loss: 0.11604732275009155
step: 6200, Loss: 0.11552087962627411
step: 6300, Loss: 0.115228570997715
step: 6400, Loss: 0.11404276639223099
step: 6500, Loss: 0.11858983337879181
step: 6600, Loss: 0.11533106863498688
step: 6700, Loss: 0.11555887013673782
step: 6800, Loss: 0.11378556489944458
step: 6900, Loss: 0.11595975607633591
step: 7000, Loss: 0.11516385525465012
step: 7100, Loss: 0.11513987928628922
step: 7200, Loss: 0.11566897481679916
step: 7300, Loss: 0.11629192531108856
step: 7400, Loss: 0.1155533716082573
step: 7500, Loss: 0.11433117091655731
step: 7600, Loss: 0.11618541926145554
step: 7700, Loss: 0.11577871441841125
step: 7800, Loss: 0.11513327807188034
step: 7900, Loss: 0.14300651848316193
step: 8000, Loss: 0.11454874277114868
step: 8100, Loss: 0.11437021940946579
step: 8200, Loss: 0.1143999844789505
step: 8300, Loss: 0.11687207221984863
step: 8400, Loss: 0.11388011276721954
step: 8500, Loss: 0.11535544693470001
step: 8600, Loss: 0.11494564265012741
step: 8700, Loss: 0.11485618352890015
step: 8800, Loss: 0.11484381556510925
step: 8900, Loss: 0.11713238060474396
step: 9000, Loss: 0.11466626822948456
step: 9100, Loss: 0.11613185703754425
step: 9200, Loss: 0.11618940532207489
step: 9300, Loss: 0.11468814313411713
step: 9400, Loss: 0.11577577888965607
step: 9500, Loss: 0.11581245809793472
step: 9600, Loss: 0.11336952447891235
step: 9700, Loss: 0.11719516664743423
step: 9800, Loss: 0.11585815250873566
step: 9900, Loss: 0.11505692452192307
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.8666666666666667
recall: 0.7222222222222222
F_score: 0.7878787878787877
******fold 7******

Training... train_data length:281
step: 0, Loss: 7.1931328773498535
step: 100, Loss: 0.12319699674844742
step: 200, Loss: 0.11744020879268646
step: 300, Loss: 0.11521528661251068
step: 400, Loss: 0.11567471921443939
step: 500, Loss: 0.11569197475910187
step: 600, Loss: 0.11915533244609833
step: 700, Loss: 0.11671306937932968
step: 800, Loss: 0.11386890709400177
step: 900, Loss: 0.11500492691993713
step: 1000, Loss: 0.11428835988044739
step: 1100, Loss: 0.11660260707139969
step: 1200, Loss: 0.11562653630971909
step: 1300, Loss: 0.11334548890590668
step: 1400, Loss: 0.11553290486335754
step: 1500, Loss: 0.11538739502429962
step: 1600, Loss: 0.1146295890212059
step: 1700, Loss: 0.11404921859502792
step: 1800, Loss: 0.11724327504634857
step: 1900, Loss: 0.11298102885484695
step: 2000, Loss: 0.11381068825721741
step: 2100, Loss: 0.11502955853939056
step: 2200, Loss: 0.1139305830001831
step: 2300, Loss: 0.11492530256509781
step: 2400, Loss: 0.11537279188632965
step: 2500, Loss: 0.1175839751958847
step: 2600, Loss: 0.1143178641796112
step: 2700, Loss: 0.11445365846157074
step: 2800, Loss: 0.1157969981431961
step: 2900, Loss: 0.11706122756004333
step: 3000, Loss: 0.11560756713151932
step: 3100, Loss: 0.11573754996061325
step: 3200, Loss: 0.11488273739814758
step: 3300, Loss: 0.11532867699861526
step: 3400, Loss: 0.11531239002943039
step: 3500, Loss: 0.11502954363822937
step: 3600, Loss: 0.11596725136041641
step: 3700, Loss: 0.11538784205913544
step: 3800, Loss: 0.11612237244844437
step: 3900, Loss: 0.11502354592084885
step: 4000, Loss: 0.11586514860391617
step: 4100, Loss: 0.11656922101974487
step: 4200, Loss: 0.11385045945644379
step: 4300, Loss: 1.6718860864639282
step: 4400, Loss: 0.12802466750144958
step: 4500, Loss: 0.13795284926891327
step: 4600, Loss: 0.12246602773666382
step: 4700, Loss: 0.12048371136188507
step: 4800, Loss: 0.11988642811775208
step: 4900, Loss: 0.12055054306983948
step: 5000, Loss: 0.12130971252918243
step: 5100, Loss: 0.1213511973619461
step: 5200, Loss: 0.11833538115024567
step: 5300, Loss: 0.1196150928735733
step: 5400, Loss: 0.12093161791563034
step: 5500, Loss: 0.11765803396701813
step: 5600, Loss: 0.11678269505500793
step: 5700, Loss: 0.11546389013528824
step: 5800, Loss: 0.11578891426324844
step: 5900, Loss: 0.11461632698774338
step: 6000, Loss: 0.11434970796108246
step: 6100, Loss: 0.1171175017952919
step: 6200, Loss: 0.11597012728452682
step: 6300, Loss: 0.11572903394699097
step: 6400, Loss: 0.11696218699216843
step: 6500, Loss: 0.11701969802379608
step: 6600, Loss: 0.1141795963048935
step: 6700, Loss: 0.11658994853496552
step: 6800, Loss: 0.11435143649578094
step: 6900, Loss: 0.11615327000617981
step: 7000, Loss: 0.11489097774028778
step: 7100, Loss: 0.1160384938120842
step: 7200, Loss: 0.11471321433782578
step: 7300, Loss: 0.11596696078777313
step: 7400, Loss: 0.11490101367235184
step: 7500, Loss: 0.11459875851869583
step: 7600, Loss: 0.11519128829240799
step: 7700, Loss: 0.11666899919509888
step: 7800, Loss: 0.11492785811424255
step: 7900, Loss: 0.11542177200317383
step: 8000, Loss: 0.12162555009126663
step: 8100, Loss: 0.11333653330802917
step: 8200, Loss: 0.11479829996824265
step: 8300, Loss: 0.11483687907457352
step: 8400, Loss: 0.11404144763946533
step: 8500, Loss: 0.1138271614909172
step: 8600, Loss: 0.11392058432102203
step: 8700, Loss: 0.1157282367348671
step: 8800, Loss: 0.11371293663978577
step: 8900, Loss: 0.11538182944059372
step: 9000, Loss: 0.11457010358572006
step: 9100, Loss: 0.11483664810657501
step: 9200, Loss: 0.11423549801111221
step: 9300, Loss: 0.11370038986206055
step: 9400, Loss: 0.1144094169139862
step: 9500, Loss: 0.11509077250957489
step: 9600, Loss: 0.11412855237722397
step: 9700, Loss: 0.11470050364732742
step: 9800, Loss: 0.11590452492237091
step: 9900, Loss: 0.11558370292186737
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.8947368421052632
recall: 1.0
F_score: 0.9444444444444444
******fold 8******

Training... train_data length:281
step: 0, Loss: 1.2060834169387817
step: 100, Loss: 0.1163637712597847
step: 200, Loss: 0.11688102036714554
step: 300, Loss: 0.11598878353834152
step: 400, Loss: 0.1156267374753952
step: 500, Loss: 0.11827203631401062
step: 600, Loss: 0.11586976051330566
step: 700, Loss: 0.11469456553459167
step: 800, Loss: 0.11566304415464401
step: 900, Loss: 0.11408110707998276
step: 1000, Loss: 0.1169678345322609
step: 1100, Loss: 0.11668519675731659
step: 1200, Loss: 0.11584485322237015
step: 1300, Loss: 0.11457788944244385
step: 1400, Loss: 0.11373594403266907
step: 1500, Loss: 0.11442728340625763
step: 1600, Loss: 0.11466769874095917
step: 1700, Loss: 0.11452213674783707
step: 1800, Loss: 0.11422345042228699
step: 1900, Loss: 0.11400160193443298
step: 2000, Loss: 0.11439243704080582
step: 2100, Loss: 0.11664562672376633
step: 2200, Loss: 0.11851610243320465
step: 2300, Loss: 0.11417396366596222
step: 2400, Loss: 0.11899088323116302
step: 2500, Loss: 0.11651523411273956
step: 2600, Loss: 0.11482592672109604
step: 2700, Loss: 0.11495672166347504
step: 2800, Loss: 0.1145850419998169
step: 2900, Loss: 0.11598134785890579
step: 3000, Loss: 0.11441771686077118
step: 3100, Loss: 0.1134597510099411
step: 3200, Loss: 0.11633799970149994
step: 3300, Loss: 0.11398255825042725
step: 3400, Loss: 0.11402030289173126
step: 3500, Loss: 0.11813239008188248
step: 3600, Loss: 0.11666594445705414
step: 3700, Loss: 0.11412163078784943
step: 3800, Loss: 0.11800502240657806
step: 3900, Loss: 0.11402695626020432
step: 5700, Loss: 0.9723246097564697
step: 5800, Loss: 0.14504320919513702
step: 5900, Loss: 0.13126787543296814
step: 6000, Loss: 0.12367894500494003
step: 6100, Loss: 0.11866643279790878
step: 6200, Loss: 0.1287790834903717
step: 6300, Loss: 0.11903668940067291
step: 6400, Loss: 0.11739642918109894
step: 6500, Loss: 0.11459770798683167
step: 6600, Loss: 0.11659552156925201
step: 6700, Loss: 0.1150132566690445
step: 6800, Loss: 0.11810927838087082
step: 6900, Loss: 0.11532124876976013
step: 7000, Loss: 0.11545873433351517
step: 7100, Loss: 0.11605550348758698
step: 7200, Loss: 0.1151919737458229
step: 7300, Loss: 0.11593539267778397
step: 7400, Loss: 0.11529263854026794
step: 7500, Loss: 0.11448845267295837
step: 7600, Loss: 0.11703959852457047
step: 7700, Loss: 0.11560928076505661
step: 7800, Loss: 0.11553288996219635
step: 7900, Loss: 0.11499422043561935
step: 8000, Loss: 0.11491797864437103
step: 8100, Loss: 0.11377254128456116
step: 8200, Loss: 0.11361682415008545
step: 8300, Loss: 0.11445821076631546
step: 8400, Loss: 0.11437201499938965
step: 8500, Loss: 0.11477840691804886
step: 8600, Loss: 0.11440172791481018
step: 8700, Loss: 0.11494947969913483
step: 8800, Loss: 0.11431513726711273
step: 8900, Loss: 0.1147809699177742
step: 9000, Loss: 0.11629181355237961
step: 9100, Loss: 0.11457592248916626
step: 9200, Loss: 0.11455260217189789
step: 9300, Loss: 0.11325456202030182
step: 9400, Loss: 0.1136784702539444
step: 9500, Loss: 0.11425130069255829
step: 9600, Loss: 0.11411896347999573
step: 9700, Loss: 0.1144450232386589
step: 9800, Loss: 0.11309795081615448
step: 9900, Loss: 0.11345825344324112
training successfully ended.
validating...
validate data length:114
acc: 0.9910714285714286
precision: 0.9807692307692307
recall: 1.0
F_score: 0.9902912621359222
subject 6 Avgacc: 0.9928571428571429 Avgfscore: 0.9930995747412341 
 Max acc:1.0, Max f score:1.0
******** mix subject_7 ********

[380, 380]
******fold 1******

Training... train_data length:684
step: 0, Loss: 51.013450622558594
step: 100, Loss: 8.605006217956543
step: 200, Loss: 2.2819132804870605
step: 300, Loss: 2.5236148834228516
step: 400, Loss: 6.513556957244873
step: 500, Loss: 0.20632368326187134
step: 600, Loss: 0.16458535194396973
step: 700, Loss: 0.16637611389160156
step: 800, Loss: 0.17944352328777313
step: 900, Loss: 0.14036664366722107
step: 1000, Loss: 0.14143399894237518
step: 1100, Loss: 0.14321307837963104
step: 1200, Loss: 0.14120790362358093
step: 1300, Loss: 0.146569162607193
step: 1400, Loss: 0.14845997095108032
step: 1500, Loss: 0.20904725790023804
step: 1600, Loss: 0.13802500069141388
step: 1700, Loss: 0.1461513787508011
step: 1800, Loss: 0.12769223749637604
step: 1900, Loss: 0.13588029146194458
step: 2000, Loss: 0.12934893369674683
step: 2100, Loss: 0.12241612374782562
step: 2200, Loss: 0.12428195774555206
step: 2300, Loss: 0.12563636898994446
step: 2400, Loss: 0.1257244497537613
step: 2500, Loss: 0.12082158029079437
step: 2600, Loss: 0.12217433750629425
step: 2700, Loss: 0.12445645034313202
step: 2800, Loss: 0.12096384167671204
step: 2900, Loss: 0.12931948900222778
step: 3000, Loss: 0.11995068192481995
step: 3100, Loss: 0.1173442006111145
step: 3200, Loss: 0.12056662142276764
step: 3300, Loss: 0.12131919711828232
step: 3400, Loss: 0.2031538188457489
step: 3500, Loss: 0.1163279190659523
step: 3600, Loss: 0.12307573854923248
step: 3700, Loss: 0.11494731158018112
step: 3800, Loss: 0.12139542400836945
step: 3900, Loss: 0.1163230612874031
step: 4000, Loss: 0.11705887317657471
step: 4100, Loss: 0.11487473547458649
step: 4200, Loss: 0.11701404303312302
step: 4300, Loss: 0.11389463394880295
step: 4400, Loss: 0.11710904538631439
step: 4500, Loss: 0.11508040130138397
step: 4600, Loss: 0.11760324239730835
step: 4700, Loss: 0.1174793392419815
step: 4800, Loss: 0.11616580188274384
step: 4900, Loss: 0.11669766902923584
step: 5000, Loss: 0.11703397333621979
step: 5100, Loss: 0.11476599425077438
step: 5200, Loss: 0.11764057725667953
step: 5300, Loss: 0.20143085718154907
step: 5400, Loss: 0.11607467383146286
step: 5500, Loss: 0.11523667722940445
step: 5600, Loss: 0.11418817192316055
step: 5700, Loss: 0.117562435567379
step: 5800, Loss: 0.11610299348831177
step: 5900, Loss: 0.11744683980941772
step: 6000, Loss: 0.11866474896669388
step: 6100, Loss: 0.9855471849441528
step: 6200, Loss: 0.3666050434112549
step: 6300, Loss: 0.16260410845279694
step: 6400, Loss: 0.1568368524312973
step: 6500, Loss: 0.15562140941619873
step: 6600, Loss: 0.15207698941230774
step: 6700, Loss: 0.13742327690124512
step: 6800, Loss: 0.13436615467071533
step: 6900, Loss: 0.13242509961128235
step: 7000, Loss: 0.13175004720687866
step: 7100, Loss: 0.12988415360450745
step: 7200, Loss: 0.2063349187374115
step: 7300, Loss: 0.1255699098110199
step: 7400, Loss: 0.1352120041847229
step: 7500, Loss: 0.12763851881027222
step: 7600, Loss: 0.12599696218967438
step: 7700, Loss: 0.12453064322471619
step: 7800, Loss: 0.11934687197208405
step: 7900, Loss: 0.11680921167135239
step: 8000, Loss: 0.12198904156684875
step: 8100, Loss: 0.11919206380844116
step: 8200, Loss: 0.11831393092870712
step: 8300, Loss: 0.1221412941813469
step: 8400, Loss: 0.12386168539524078
step: 8500, Loss: 0.11563048511743546
step: 8600, Loss: 0.11727578192949295
step: 8700, Loss: 0.11742348968982697
step: 8800, Loss: 0.11779206991195679
step: 8900, Loss: 0.11893883347511292
step: 9000, Loss: 0.11793245375156403
step: 9100, Loss: 0.2044057846069336
step: 9200, Loss: 0.11572905629873276
step: 9300, Loss: 0.1213107779622078
step: 9400, Loss: 0.12016074359416962
step: 9500, Loss: 0.11974863708019257
step: 9600, Loss: 0.11600463092327118
step: 9700, Loss: 0.11378410458564758
step: 9800, Loss: 0.11543520539999008
step: 9900, Loss: 0.11458097398281097
training successfully ended.
validating...
validate data length:76
acc: 0.7916666666666666
precision: 0.7045454545454546
recall: 0.9393939393939394
F_score: 0.8051948051948052
******fold 2******

Training... train_data length:684
step: 0, Loss: 4.179877758026123
step: 100, Loss: 0.16664043068885803
step: 200, Loss: 0.13079534471035004
step: 300, Loss: 0.12947773933410645
step: 400, Loss: 0.15089741349220276
step: 500, Loss: 0.11867193877696991
step: 600, Loss: 0.12007074803113937
step: 700, Loss: 0.11807422339916229
step: 800, Loss: 0.11731796711683273
step: 900, Loss: 0.11816207319498062
step: 1000, Loss: 0.11736927926540375
step: 1100, Loss: 0.12045834958553314
step: 1200, Loss: 0.11680017411708832
step: 1300, Loss: 0.1176367700099945
step: 1400, Loss: 0.1200101226568222
step: 1500, Loss: 0.19803205132484436
step: 1600, Loss: 0.11424570530653
step: 1700, Loss: 0.12034806609153748
step: 1800, Loss: 0.11526899039745331
step: 1900, Loss: 0.11550860852003098
step: 2000, Loss: 0.11519834399223328
step: 2100, Loss: 0.1159529760479927
step: 2200, Loss: 0.1151919960975647
step: 2300, Loss: 0.11395194381475449
step: 2400, Loss: 0.11774127930402756
step: 2500, Loss: 0.1159374862909317
step: 2600, Loss: 0.11504840105772018
step: 2700, Loss: 0.1148267537355423
step: 2800, Loss: 0.11694659292697906
step: 2900, Loss: 0.1157037690281868
step: 3000, Loss: 0.11336379498243332
step: 3100, Loss: 0.11475329101085663
step: 3200, Loss: 0.11437470465898514
step: 3300, Loss: 0.11533485352993011
step: 3400, Loss: 0.19674068689346313
step: 3500, Loss: 0.11584866046905518
step: 3600, Loss: 0.115306057035923
step: 3700, Loss: 0.11462408304214478
step: 3800, Loss: 0.11488475650548935
step: 3900, Loss: 0.11591579765081406
step: 4000, Loss: 0.11572399735450745
step: 4100, Loss: 0.11383457481861115
step: 4200, Loss: 0.1492251455783844
step: 4300, Loss: 0.12510819733142853
step: 4400, Loss: 0.24847660958766937
step: 4500, Loss: 0.3693138360977173
step: 4600, Loss: 0.14220167696475983
step: 4700, Loss: 0.13949993252754211
step: 4800, Loss: 0.12998753786087036
step: 4900, Loss: 0.13832716643810272
step: 5000, Loss: 0.12547585368156433
step: 5100, Loss: 0.13139717280864716
step: 5200, Loss: 0.12264624238014221
step: 5300, Loss: 0.20844438672065735
step: 5400, Loss: 0.11782638728618622
step: 5500, Loss: 0.11999104171991348
step: 5600, Loss: 0.12197021394968033
step: 5700, Loss: 0.119380421936512
step: 5800, Loss: 0.11835651844739914
step: 4000, Loss: 0.11566019803285599
step: 4100, Loss: 0.11503644287586212
step: 4200, Loss: 0.11642830073833466
step: 4300, Loss: 0.11480671912431717
step: 4400, Loss: 0.11331551522016525
step: 4500, Loss: 0.11600938439369202
step: 4600, Loss: 0.11379382759332657
step: 4700, Loss: 0.11421062052249908
step: 4800, Loss: 0.11679426580667496
step: 4900, Loss: 0.11461158841848373
step: 5000, Loss: 0.11699137091636658
step: 5100, Loss: 0.11391302198171616
step: 5200, Loss: 0.1150982528924942
step: 5300, Loss: 0.11575593054294586
step: 5400, Loss: 0.11437491327524185
step: 5500, Loss: 0.1147405132651329
step: 5600, Loss: 0.15971899032592773
step: 5700, Loss: 0.13204063475131989
step: 5800, Loss: 0.12983737885951996
step: 5900, Loss: 0.13414518535137177
step: 6000, Loss: 0.12957999110221863
step: 6100, Loss: 0.11898558586835861
step: 6200, Loss: 0.11976401507854462
step: 6300, Loss: 0.11842409521341324
step: 6400, Loss: 0.12416204065084457
step: 6500, Loss: 0.1167619377374649
step: 6600, Loss: 0.11968763172626495
step: 6700, Loss: 0.11794938892126083
step: 6800, Loss: 0.11714731156826019
step: 6900, Loss: 0.1172884851694107
step: 7000, Loss: 0.11724599450826645
step: 7100, Loss: 0.11766603589057922
step: 7200, Loss: 0.11622190475463867
step: 7300, Loss: 0.11422386765480042
step: 7400, Loss: 0.11549729853868484
step: 7500, Loss: 0.11488185822963715
step: 7600, Loss: 0.11414188891649246
step: 7700, Loss: 0.11617981642484665
step: 7800, Loss: 0.11777419596910477
step: 7900, Loss: 0.11534596234560013
step: 8000, Loss: 0.11600466072559357
step: 8100, Loss: 0.11492261290550232
step: 8200, Loss: 0.11434018611907959
step: 8300, Loss: 0.11311420798301697
step: 8400, Loss: 0.1145814061164856
step: 8500, Loss: 0.11488190293312073
step: 8600, Loss: 0.11436909437179565
step: 8700, Loss: 0.11345769464969635
step: 8800, Loss: 0.1149054691195488
step: 8900, Loss: 0.11489184945821762
step: 9000, Loss: 0.11368569731712341
step: 9100, Loss: 0.11395597457885742
step: 9200, Loss: 0.11357975006103516
step: 9300, Loss: 0.114958256483078
step: 9400, Loss: 0.11436847597360611
step: 9500, Loss: 0.11421355605125427
step: 9600, Loss: 0.118832066655159
step: 9700, Loss: 0.11316428333520889
step: 9800, Loss: 0.115134097635746
step: 9900, Loss: 0.11320442706346512
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.6144177913665771
step: 100, Loss: 0.12041832506656647
step: 200, Loss: 0.1199198067188263
step: 300, Loss: 0.11593513190746307
step: 400, Loss: 0.11575712263584137
step: 500, Loss: 0.11470641195774078
step: 600, Loss: 0.11508350819349289
step: 700, Loss: 0.11454357951879501
step: 800, Loss: 0.11550547182559967
step: 900, Loss: 0.11646341532468796
step: 1000, Loss: 0.11547734588384628
step: 1100, Loss: 0.11448320001363754
step: 1200, Loss: 0.11299659311771393
step: 1300, Loss: 0.11465427279472351
step: 1400, Loss: 0.1149420440196991
step: 1500, Loss: 0.11442838609218597
step: 1600, Loss: 0.11436627805233002
step: 1700, Loss: 0.11437423527240753
step: 1800, Loss: 0.11341908574104309
step: 1900, Loss: 0.11301650106906891
step: 2000, Loss: 0.11470654606819153
step: 2100, Loss: 0.11542541533708572
step: 2200, Loss: 0.1142544224858284
step: 2300, Loss: 0.1138841062784195
step: 2400, Loss: 0.11380724608898163
step: 2500, Loss: 0.11703860759735107
step: 2600, Loss: 0.11574900150299072
step: 2700, Loss: 0.1153443455696106
step: 2800, Loss: 0.11399199068546295
step: 2900, Loss: 0.11797812581062317
step: 3000, Loss: 0.1141078770160675
step: 3100, Loss: 0.11443106830120087
step: 3200, Loss: 0.11496420204639435
step: 3300, Loss: 0.11412210017442703
step: 3400, Loss: 0.11493141949176788
step: 3500, Loss: 0.11430655419826508
step: 3600, Loss: 0.11425850540399551
step: 3700, Loss: 0.11372417211532593
step: 3800, Loss: 0.11515819281339645
step: 3900, Loss: 0.11368121206760406
step: 4000, Loss: 0.11424145102500916
step: 4100, Loss: 0.11542394012212753
step: 4200, Loss: 0.11488210409879684
step: 4300, Loss: 0.11433295160531998
step: 4400, Loss: 0.1155846118927002
step: 4500, Loss: 0.1140865758061409
step: 4600, Loss: 0.11434608697891235
step: 4700, Loss: 0.11582715809345245
step: 4800, Loss: 0.11508902162313461
step: 4900, Loss: 0.1195431500673294
step: 5000, Loss: 0.11553996056318283
step: 5100, Loss: 0.16869902610778809
step: 5200, Loss: 0.13354989886283875
step: 5300, Loss: 0.12964880466461182
step: 5400, Loss: 0.12393244355916977
step: 5500, Loss: 0.1258724480867386
step: 5600, Loss: 0.11970655620098114
step: 5700, Loss: 0.12018372863531113
step: 5800, Loss: 0.138542041182518
step: 5900, Loss: 0.12096631526947021
step: 6000, Loss: 0.11897169798612595
step: 6100, Loss: 0.11707450449466705
step: 6200, Loss: 0.11918002367019653
step: 6300, Loss: 0.11764328181743622
step: 6400, Loss: 0.11918013542890549
step: 6500, Loss: 0.11828479915857315
step: 6600, Loss: 0.11594224721193314
step: 6700, Loss: 0.11654968559741974
step: 6800, Loss: 0.11702047288417816
step: 6900, Loss: 0.11589210480451584
step: 7000, Loss: 0.11558620631694794
step: 7100, Loss: 0.11834283173084259
step: 7200, Loss: 0.11602315306663513
step: 7300, Loss: 0.11634894460439682
step: 7400, Loss: 0.11663918942213058
step: 7500, Loss: 0.11579220741987228
step: 7600, Loss: 0.11634014546871185
step: 7700, Loss: 0.11693929880857468
step: 7800, Loss: 0.11664318293333054
step: 7900, Loss: 0.11444788426160812
step: 8000, Loss: 0.11444202065467834
step: 8100, Loss: 0.1149989515542984
step: 8200, Loss: 0.11702165752649307
step: 8300, Loss: 0.11456415802240372
step: 8400, Loss: 0.1159222275018692
step: 8500, Loss: 0.11449714004993439
step: 8600, Loss: 0.11499156802892685
step: 8700, Loss: 0.11579208821058273
step: 8800, Loss: 0.11644341796636581
step: 8900, Loss: 0.11561442911624908
step: 9000, Loss: 0.11347182095050812
step: 9100, Loss: 0.11617299914360046
step: 9200, Loss: 0.11531110852956772
step: 9300, Loss: 0.11579562723636627
step: 9400, Loss: 0.11661815643310547
step: 9500, Loss: 0.11489264667034149
step: 9600, Loss: 0.11497879028320312
step: 9700, Loss: 0.1151125580072403
step: 9800, Loss: 0.11667214334011078
step: 9900, Loss: 0.11558811366558075
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7619047619047619
recall: 0.9411764705882353
F_score: 0.8421052631578947
******fold 10******

Training... train_data length:281
step: 0, Loss: 1.4477486610412598
step: 100, Loss: 0.11829666793346405
step: 200, Loss: 0.11783339828252792
step: 300, Loss: 0.11631815880537033
step: 400, Loss: 0.11581818759441376
step: 500, Loss: 0.11457476764917374
step: 600, Loss: 0.11387987434864044
step: 700, Loss: 0.11384285241365433
step: 800, Loss: 0.11372781544923782
step: 900, Loss: 0.11369931697845459
step: 1000, Loss: 0.11494605988264084
step: 1100, Loss: 0.11468851566314697
step: 1200, Loss: 0.1151973158121109
step: 1300, Loss: 0.11398167163133621
step: 1400, Loss: 0.11545141041278839
step: 1500, Loss: 0.11476187407970428
step: 1600, Loss: 0.11453837156295776
step: 1700, Loss: 0.11486205458641052
step: 1800, Loss: 0.11367495357990265
step: 1900, Loss: 0.11505363136529922
step: 2000, Loss: 0.11436747759580612
step: 2100, Loss: 0.11365779489278793
step: 2200, Loss: 0.11514952778816223
step: 2300, Loss: 0.11281123012304306
step: 2400, Loss: 0.11413177102804184
step: 2500, Loss: 0.11477658152580261
step: 2600, Loss: 0.11515942215919495
step: 2700, Loss: 0.11529567837715149
step: 2800, Loss: 0.1160539984703064
step: 2900, Loss: 0.11413049697875977
step: 3000, Loss: 0.116001658141613
step: 3100, Loss: 0.11341074854135513
step: 3200, Loss: 0.11611086875200272
step: 3300, Loss: 0.11336668580770493
step: 3400, Loss: 0.11536721885204315
step: 3500, Loss: 0.11387196183204651
step: 3600, Loss: 0.11534373462200165
step: 3700, Loss: 0.11517884582281113
step: 3800, Loss: 0.11544626951217651
step: 3900, Loss: 0.11397553980350494
step: 4000, Loss: 0.11416540294885635
step: 4100, Loss: 0.11485551297664642
step: 4200, Loss: 0.11512356996536255
step: 4300, Loss: 0.11611966043710709
step: 4400, Loss: 0.11537636816501617
step: 4500, Loss: 0.11406215280294418
step: 5900, Loss: 0.11686109006404877
step: 6000, Loss: 0.11771067976951599
step: 6100, Loss: 0.11572210490703583
step: 6200, Loss: 0.11544446647167206
step: 6300, Loss: 0.11721083521842957
step: 6400, Loss: 0.11610112339258194
step: 6500, Loss: 0.12134966254234314
step: 6600, Loss: 0.11856511235237122
step: 6700, Loss: 0.11542236804962158
step: 6800, Loss: 0.11684583127498627
step: 6900, Loss: 0.11419416218996048
step: 7000, Loss: 0.11556927859783173
step: 7100, Loss: 0.11718004941940308
step: 7200, Loss: 0.1982976645231247
step: 7300, Loss: 0.11498688906431198
step: 7400, Loss: 0.11742015182971954
step: 7500, Loss: 0.11443066596984863
step: 7600, Loss: 0.11637695133686066
step: 7700, Loss: 0.11478513479232788
step: 7800, Loss: 0.1146409660577774
step: 7900, Loss: 0.11524881422519684
step: 8000, Loss: 0.11437816917896271
step: 8100, Loss: 0.11354640871286392
step: 8200, Loss: 0.11367791891098022
step: 8300, Loss: 0.11540666967630386
step: 8400, Loss: 0.11379837989807129
step: 8500, Loss: 0.1155974268913269
step: 8600, Loss: 0.11425827443599701
step: 8700, Loss: 0.11463649570941925
step: 8800, Loss: 0.1143965870141983
step: 8900, Loss: 0.11289741098880768
step: 9000, Loss: 0.11533325910568237
step: 9100, Loss: 0.1927652657032013
step: 9200, Loss: 0.11322734504938126
step: 9300, Loss: 0.11625196784734726
step: 9400, Loss: 0.11450908333063126
step: 9500, Loss: 0.11520522832870483
step: 9600, Loss: 0.11376716941595078
step: 9700, Loss: 0.11374811083078384
step: 9800, Loss: 0.1134614571928978
step: 9900, Loss: 0.11440642178058624
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.9444444444444444
recall: 0.918918918918919
F_score: 0.9315068493150684
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.13396114110946655
step: 100, Loss: 0.11900703608989716
step: 200, Loss: 0.11756788194179535
step: 300, Loss: 0.11603779345750809
step: 400, Loss: 0.11694108694791794
step: 500, Loss: 0.1166856661438942
step: 600, Loss: 0.11505607515573502
step: 700, Loss: 0.11606055498123169
step: 800, Loss: 0.11616351455450058
step: 900, Loss: 0.1157286986708641
step: 1000, Loss: 0.11419668793678284
step: 1100, Loss: 0.11300601810216904
step: 1200, Loss: 0.11428605765104294
step: 1300, Loss: 0.11419625580310822
step: 1400, Loss: 0.11529548466205597
step: 1500, Loss: 0.19043056666851044
step: 1600, Loss: 0.11412285268306732
step: 1700, Loss: 0.1150321215391159
step: 1800, Loss: 0.1147346943616867
step: 1900, Loss: 0.11465379595756531
step: 2000, Loss: 0.11306885629892349
step: 2100, Loss: 0.1144096627831459
step: 2200, Loss: 0.11433298140764236
step: 2300, Loss: 0.11393353343009949
step: 2400, Loss: 0.11338553577661514
step: 2500, Loss: 0.11430831253528595
step: 2600, Loss: 0.11401745676994324
step: 2700, Loss: 0.11403796821832657
step: 2800, Loss: 0.11389070749282837
step: 2900, Loss: 0.11445224285125732
step: 3000, Loss: 0.11354417353868484
step: 3100, Loss: 0.11448284238576889
step: 3200, Loss: 0.11286389082670212
step: 3300, Loss: 0.11391198635101318
step: 3400, Loss: 0.1931602656841278
step: 3500, Loss: 0.11407429724931717
step: 3600, Loss: 0.11384210735559464
step: 3700, Loss: 0.1141882985830307
step: 3800, Loss: 0.11379964649677277
step: 3900, Loss: 0.11333493143320084
step: 4000, Loss: 0.11447000503540039
step: 4100, Loss: 0.11400320380926132
step: 4200, Loss: 0.11535932123661041
step: 4300, Loss: 0.11447974294424057
step: 4400, Loss: 0.11438654363155365
step: 4500, Loss: 0.11404737085103989
step: 4600, Loss: 0.11295483261346817
step: 4700, Loss: 0.11460943520069122
step: 4800, Loss: 0.11413536220788956
step: 4900, Loss: 0.11400207877159119
step: 5000, Loss: 0.11386236548423767
step: 5100, Loss: 0.11443882435560226
step: 5200, Loss: 0.1149534285068512
step: 5300, Loss: 1.5777349472045898
step: 5400, Loss: 0.18440668284893036
step: 5500, Loss: 0.13517211377620697
step: 5600, Loss: 0.13149797916412354
step: 5700, Loss: 0.1280650645494461
step: 5800, Loss: 0.13078884780406952
step: 5900, Loss: 0.12445364892482758
step: 6000, Loss: 0.1259184032678604
step: 6100, Loss: 0.12469010055065155
step: 6200, Loss: 0.12667430937290192
step: 6300, Loss: 0.12120648473501205
step: 6400, Loss: 0.12246650457382202
step: 6500, Loss: 0.1193041205406189
step: 6600, Loss: 0.12021756172180176
step: 6700, Loss: 0.11935588717460632
step: 6800, Loss: 0.11925310641527176
step: 6900, Loss: 0.11663305014371872
step: 7000, Loss: 0.11792995035648346
step: 7100, Loss: 0.11801972985267639
step: 7200, Loss: 0.20135368406772614
step: 7300, Loss: 0.11525014787912369
step: 7400, Loss: 0.11617285758256912
step: 7500, Loss: 0.11503064632415771
step: 7600, Loss: 0.1165972501039505
step: 7700, Loss: 0.1161300539970398
step: 7800, Loss: 0.11476270854473114
step: 7900, Loss: 0.11646582186222076
step: 8000, Loss: 0.11549334228038788
step: 8100, Loss: 0.11495224386453629
step: 8200, Loss: 0.11412424594163895
step: 8300, Loss: 0.11647982150316238
step: 8400, Loss: 0.11507739126682281
step: 8500, Loss: 0.11633198708295822
step: 8600, Loss: 0.11328617483377457
step: 8700, Loss: 0.11545497179031372
step: 8800, Loss: 0.1157173439860344
step: 8900, Loss: 0.11516030132770538
step: 9000, Loss: 0.11512322723865509
step: 9100, Loss: 0.19262132048606873
step: 9200, Loss: 0.11464764177799225
step: 9300, Loss: 0.11327008903026581
step: 9400, Loss: 0.11267377436161041
step: 9500, Loss: 0.11431930214166641
step: 9600, Loss: 0.11431107670068741
step: 9700, Loss: 0.11317697167396545
step: 9800, Loss: 0.11326828598976135
step: 9900, Loss: 0.11389417201280594
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.9
recall: 0.9473684210526315
F_score: 0.9230769230769231
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.14235447347164154
step: 100, Loss: 0.1187470331788063
step: 200, Loss: 0.11813483387231827
step: 300, Loss: 0.11678393185138702
step: 400, Loss: 0.11634641885757446
step: 500, Loss: 0.1176580861210823
step: 600, Loss: 0.11753096431493759
step: 700, Loss: 0.11484561115503311
step: 800, Loss: 0.11706627160310745
step: 900, Loss: 0.11428695172071457
step: 1000, Loss: 0.11470694839954376
step: 1100, Loss: 0.11420578509569168
step: 1200, Loss: 0.11470327526330948
step: 1300, Loss: 0.1148046925663948
step: 1400, Loss: 0.11419734358787537
step: 1500, Loss: 0.19630499184131622
step: 1600, Loss: 0.11362598836421967
step: 1700, Loss: 0.11429464817047119
step: 1800, Loss: 0.11375851929187775
step: 1900, Loss: 0.11310237646102905
step: 2000, Loss: 0.11426039785146713
step: 2100, Loss: 0.11430482566356659
step: 2200, Loss: 0.11403752118349075
step: 2300, Loss: 0.11290354281663895
step: 2400, Loss: 0.11315202713012695
step: 2500, Loss: 0.11518672853708267
step: 2600, Loss: 0.11540329456329346
step: 2700, Loss: 0.1135806292295456
step: 2800, Loss: 0.11402107030153275
step: 2900, Loss: 0.11366608738899231
step: 3000, Loss: 0.11300364136695862
step: 3100, Loss: 0.11380608379840851
step: 3200, Loss: 0.11511266976594925
step: 3300, Loss: 0.11406873166561127
step: 3400, Loss: 0.19093255698680878
step: 3500, Loss: 0.11325383931398392
step: 3600, Loss: 0.11338049918413162
step: 3700, Loss: 0.1137353852391243
step: 3800, Loss: 0.11317110806703568
step: 3900, Loss: 0.11530342698097229
step: 4000, Loss: 0.11320318281650543
step: 4100, Loss: 0.11392244696617126
step: 4200, Loss: 0.11490096896886826
step: 4300, Loss: 0.11375978589057922
step: 4400, Loss: 0.11374303698539734
step: 4500, Loss: 0.11406475305557251
step: 4600, Loss: 0.11268506944179535
step: 4700, Loss: 0.11467789113521576
step: 4800, Loss: 0.11375463008880615
step: 4900, Loss: 0.11343604326248169
step: 5000, Loss: 0.11416645348072052
step: 5100, Loss: 0.11365925520658493
step: 5200, Loss: 5.279269218444824
step: 5300, Loss: 1.621779203414917
step: 5400, Loss: 0.41008126735687256
step: 5500, Loss: 0.1404777467250824
step: 5600, Loss: 0.12511762976646423
step: 5700, Loss: 0.1297495812177658
step: 5800, Loss: 0.1368563175201416
step: 5900, Loss: 0.12344106286764145
step: 6000, Loss: 0.12267115712165833
step: 6100, Loss: 0.12119439244270325
step: 6200, Loss: 0.12089087814092636
step: 6300, Loss: 0.1194148138165474
step: 4600, Loss: 0.11467481404542923
step: 4700, Loss: 0.11556506901979446
step: 4800, Loss: 0.11575213819742203
step: 4900, Loss: 0.11420847475528717
step: 5000, Loss: 0.11597733199596405
step: 5100, Loss: 0.11678075045347214
step: 5200, Loss: 0.11337868124246597
step: 5300, Loss: 0.1142757385969162
step: 5400, Loss: 0.11481605470180511
step: 5500, Loss: 0.11455163359642029
step: 5600, Loss: 0.11592012643814087
step: 5700, Loss: 0.11575306951999664
step: 5800, Loss: 0.11356232315301895
step: 5900, Loss: 0.11519575119018555
step: 6000, Loss: 0.11793308705091476
step: 6100, Loss: 0.11498831957578659
step: 6200, Loss: 0.220738023519516
step: 6300, Loss: 0.13687951862812042
step: 6400, Loss: 0.12867142260074615
step: 6500, Loss: 0.12162825465202332
step: 6600, Loss: 0.1228417158126831
step: 6700, Loss: 0.1176011860370636
step: 6800, Loss: 0.11991588771343231
step: 6900, Loss: 0.12083408236503601
step: 7000, Loss: 0.11622465401887894
step: 7100, Loss: 0.12550179660320282
step: 7200, Loss: 0.11771780252456665
step: 7300, Loss: 0.11818721890449524
step: 7400, Loss: 0.11729735136032104
step: 7500, Loss: 0.11868799477815628
step: 7600, Loss: 0.11739667505025864
step: 7700, Loss: 0.1192605048418045
step: 7800, Loss: 0.1159416064620018
step: 7900, Loss: 0.11540379375219345
step: 8000, Loss: 0.12208374589681625
step: 8100, Loss: 0.11674551665782928
step: 8200, Loss: 0.11573243886232376
step: 8300, Loss: 0.11518653482198715
step: 8400, Loss: 0.11411316692829132
step: 8500, Loss: 0.11356451362371445
step: 8600, Loss: 0.11466683447360992
step: 8700, Loss: 0.12797807157039642
step: 8800, Loss: 0.11774232983589172
step: 8900, Loss: 0.11812641471624374
step: 9000, Loss: 0.1148701161146164
step: 9100, Loss: 0.11447340250015259
step: 9200, Loss: 0.1167554035782814
step: 9300, Loss: 0.11454024165868759
step: 9400, Loss: 0.11643587052822113
step: 9500, Loss: 0.114815354347229
step: 9600, Loss: 0.11768508702516556
step: 9700, Loss: 0.11488230526447296
step: 9800, Loss: 0.11518877744674683
step: 9900, Loss: 0.11349770426750183
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7142857142857143
recall: 0.8333333333333334
F_score: 0.7692307692307692
subject 6 Avgacc: 0.7814583333333334 Avgfscore: 0.7840732265213542 
 Max acc:0.9333333333333333, Max f score:0.9444444444444444
******** mix subject_7 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 28.08331871032715
step: 100, Loss: 1.5900895595550537
step: 200, Loss: 0.14961403608322144
step: 300, Loss: 0.13557551801204681
step: 400, Loss: 0.14011599123477936
step: 500, Loss: 0.1319626271724701
step: 600, Loss: 0.12839962542057037
step: 700, Loss: 0.12185555696487427
step: 800, Loss: 0.12482224404811859
step: 900, Loss: 0.11608989536762238
step: 1000, Loss: 0.12241196632385254
step: 1100, Loss: 0.11829273402690887
step: 1200, Loss: 0.11885880678892136
step: 1300, Loss: 0.11635133624076843
step: 1400, Loss: 0.11673830449581146
step: 1500, Loss: 0.11942067742347717
step: 1600, Loss: 0.1184748113155365
step: 1700, Loss: 0.11553183943033218
step: 1800, Loss: 0.11578826606273651
step: 1900, Loss: 0.11515293270349503
step: 2000, Loss: 0.11558325588703156
step: 2100, Loss: 0.11775070428848267
step: 2200, Loss: 0.11557935923337936
step: 2300, Loss: 0.1158711314201355
step: 2400, Loss: 0.11476568132638931
step: 2500, Loss: 0.11429765820503235
step: 2600, Loss: 0.1159568727016449
step: 2700, Loss: 0.11546940356492996
step: 2800, Loss: 0.11497396975755692
step: 2900, Loss: 0.11344385892152786
step: 3000, Loss: 0.11517748981714249
step: 3100, Loss: 0.1135789081454277
step: 3200, Loss: 0.12166852504014969
step: 3300, Loss: 0.11561622470617294
step: 3400, Loss: 0.1144704818725586
step: 3500, Loss: 0.11799885332584381
step: 3600, Loss: 0.11479191482067108
step: 3700, Loss: 0.11502555757761002
step: 3800, Loss: 0.11571736633777618
step: 3900, Loss: 0.11555164307355881
step: 4000, Loss: 0.11901640892028809
step: 4100, Loss: 0.11477264016866684
step: 4200, Loss: 0.11430365592241287
step: 4300, Loss: 0.11595005542039871
step: 4400, Loss: 0.11452171951532364
step: 4500, Loss: 0.11474961787462234
step: 4600, Loss: 0.1167973130941391
step: 4700, Loss: 0.1136312261223793
step: 4800, Loss: 0.11569559574127197
step: 4900, Loss: 0.11337988823652267
step: 5000, Loss: 0.11991904675960541
step: 5100, Loss: 0.11705854535102844
step: 5200, Loss: 0.11493033170700073
step: 5300, Loss: 0.11451245099306107
step: 5400, Loss: 0.11492839455604553
step: 5500, Loss: 0.11346632242202759
step: 5600, Loss: 0.11634901911020279
step: 5700, Loss: 0.11404343694448471
step: 5800, Loss: 0.11519566178321838
step: 5900, Loss: 0.11405999213457108
step: 6000, Loss: 0.11566624045372009
step: 6100, Loss: 0.11780525743961334
step: 6200, Loss: 0.115970179438591
step: 6300, Loss: 0.11727209389209747
step: 6400, Loss: 0.18788358569145203
step: 6500, Loss: 0.12373892962932587
step: 6600, Loss: 0.1335555911064148
step: 6700, Loss: 0.11905635893344879
step: 6800, Loss: 0.13220584392547607
step: 6900, Loss: 0.11994684487581253
step: 7000, Loss: 0.12349287420511246
step: 7100, Loss: 0.11779864877462387
step: 7200, Loss: 0.1256149709224701
step: 7300, Loss: 0.11721166968345642
step: 7400, Loss: 0.11691068112850189
step: 7500, Loss: 0.11719273030757904
step: 7600, Loss: 0.12239338457584381
step: 7700, Loss: 0.11590321362018585
step: 7800, Loss: 0.12002858519554138
step: 7900, Loss: 0.11593986302614212
step: 8000, Loss: 0.11535695195198059
step: 8100, Loss: 0.1175147071480751
step: 8200, Loss: 0.11754584312438965
step: 8300, Loss: 0.11620810627937317
step: 8400, Loss: 0.11765401810407639
step: 8500, Loss: 0.11575242877006531
step: 8600, Loss: 0.11457887291908264
step: 8700, Loss: 0.11453233659267426
step: 8800, Loss: 0.12012168765068054
step: 8900, Loss: 0.11589944362640381
step: 9000, Loss: 0.11527548730373383
step: 9100, Loss: 0.11583351343870163
step: 9200, Loss: 0.11962417513132095
step: 9300, Loss: 0.1158139631152153
step: 9400, Loss: 0.12337734550237656
step: 9500, Loss: 0.11395429074764252
step: 9600, Loss: 0.11580342054367065
step: 9700, Loss: 0.11459057778120041
step: 9800, Loss: 0.11404381692409515
step: 9900, Loss: 0.11576934158802032
training successfully ended.
validating...
validate data length:32
acc: 0.34375
precision: 0.2857142857142857
recall: 0.5
F_score: 0.36363636363636365
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.12505224347114563
step: 100, Loss: 0.13288389146327972
step: 200, Loss: 0.12340353429317474
step: 300, Loss: 0.11885414272546768
step: 400, Loss: 0.1154155433177948
step: 500, Loss: 0.12001753598451614
step: 600, Loss: 0.11512526124715805
step: 700, Loss: 0.11834301799535751
step: 800, Loss: 0.11462540179491043
step: 900, Loss: 0.11716072261333466
step: 1000, Loss: 0.11524569988250732
step: 1100, Loss: 0.11875038594007492
step: 1200, Loss: 0.1136976107954979
step: 1300, Loss: 0.11584731936454773
step: 1400, Loss: 0.11585720628499985
step: 1500, Loss: 0.11468919366598129
step: 1600, Loss: 0.11546597629785538
step: 1700, Loss: 0.1146746501326561
step: 1800, Loss: 0.11565203964710236
step: 1900, Loss: 0.1155284196138382
step: 2000, Loss: 0.11401829868555069
step: 2100, Loss: 0.11503574252128601
step: 2200, Loss: 0.11499515175819397
step: 2300, Loss: 0.11696270108222961
step: 2400, Loss: 0.11445115506649017
step: 2500, Loss: 0.11483161896467209
step: 2600, Loss: 0.11504819244146347
step: 2700, Loss: 0.11507807672023773
step: 2800, Loss: 0.11794966459274292
step: 2900, Loss: 0.11729541420936584
step: 3000, Loss: 0.1133197695016861
step: 3100, Loss: 0.1151093989610672
step: 3200, Loss: 0.1153845489025116
step: 3300, Loss: 0.11452452838420868
step: 3400, Loss: 0.11472159624099731
step: 3500, Loss: 0.11472287029027939
step: 3600, Loss: 0.11393413692712784
step: 3700, Loss: 0.11640119552612305
step: 3800, Loss: 0.11487768590450287
step: 3900, Loss: 0.11384240537881851
step: 4000, Loss: 0.11725722253322601
step: 4100, Loss: 0.12378579378128052
step: 4200, Loss: 0.116336390376091
step: 4300, Loss: 0.11403848230838776
step: 4400, Loss: 0.11426445096731186
step: 4500, Loss: 0.11474975198507309
step: 4600, Loss: 0.11773863434791565
step: 6400, Loss: 0.11711777001619339
step: 6500, Loss: 0.11991114169359207
step: 6600, Loss: 0.11870924383401871
step: 6700, Loss: 0.11770607531070709
step: 6800, Loss: 0.1188850998878479
step: 6900, Loss: 0.1157359927892685
step: 7000, Loss: 0.11803664267063141
step: 7100, Loss: 0.11656961590051651
step: 7200, Loss: 0.20774388313293457
step: 7300, Loss: 0.11609778553247452
step: 7400, Loss: 0.11714266985654831
step: 7500, Loss: 0.1162761002779007
step: 7600, Loss: 0.11774787306785583
step: 7700, Loss: 0.1189611628651619
step: 7800, Loss: 0.11677524447441101
step: 7900, Loss: 0.11461518704891205
step: 8000, Loss: 0.11773788183927536
step: 8100, Loss: 0.12022213637828827
step: 8200, Loss: 0.11401858925819397
step: 8300, Loss: 0.11473508179187775
step: 8400, Loss: 0.11399593204259872
step: 8500, Loss: 0.11493559926748276
step: 8600, Loss: 0.11692537367343903
step: 8700, Loss: 0.11496207863092422
step: 8800, Loss: 0.1141335666179657
step: 8900, Loss: 0.11526000499725342
step: 9000, Loss: 0.11416560411453247
step: 9100, Loss: 0.19754627346992493
step: 9200, Loss: 0.1150210052728653
step: 9300, Loss: 0.11485359072685242
step: 9400, Loss: 0.11392650008201599
step: 9500, Loss: 0.11349378526210785
step: 9600, Loss: 0.11412161588668823
step: 9700, Loss: 0.1142745390534401
step: 9800, Loss: 0.11382930725812912
step: 9900, Loss: 0.11617325246334076
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9090909090909091
recall: 1.0
F_score: 0.9523809523809523
******fold 5******

Training... train_data length:684
step: 0, Loss: 2.560055732727051
step: 100, Loss: 0.12505662441253662
step: 200, Loss: 0.117770716547966
step: 300, Loss: 0.11636728048324585
step: 400, Loss: 0.11620040237903595
step: 500, Loss: 0.1165008544921875
step: 600, Loss: 0.11664313822984695
step: 700, Loss: 0.11406965553760529
step: 800, Loss: 0.11583679914474487
step: 900, Loss: 0.11922004818916321
step: 1000, Loss: 0.11375759541988373
step: 1100, Loss: 0.11458960175514221
step: 1200, Loss: 0.11541778594255447
step: 1300, Loss: 0.11515375971794128
step: 1400, Loss: 0.11393126100301743
step: 1500, Loss: 0.19753491878509521
step: 1600, Loss: 0.1159314215183258
step: 1700, Loss: 0.11554679274559021
step: 1800, Loss: 0.11360424011945724
step: 1900, Loss: 0.11545857787132263
step: 2000, Loss: 0.1134917214512825
step: 2100, Loss: 0.11305301636457443
step: 2200, Loss: 0.11434513330459595
step: 2300, Loss: 0.11357776075601578
step: 2400, Loss: 0.11368503421545029
step: 2500, Loss: 0.11485765129327774
step: 2600, Loss: 0.1143651157617569
step: 2700, Loss: 0.11442828178405762
step: 2800, Loss: 0.11368906497955322
step: 2900, Loss: 0.11362090706825256
step: 3000, Loss: 0.11330648511648178
step: 3100, Loss: 0.11419323086738586
step: 3200, Loss: 0.11369543522596359
step: 3300, Loss: 0.1143113523721695
step: 3400, Loss: 0.19603660702705383
step: 3500, Loss: 0.11560584604740143
step: 3600, Loss: 0.11446243524551392
step: 3700, Loss: 0.11374303698539734
step: 3800, Loss: 0.11390581727027893
step: 3900, Loss: 0.11444605886936188
step: 4000, Loss: 0.11500504612922668
step: 4100, Loss: 0.11311523616313934
step: 4200, Loss: 0.1135399267077446
step: 4300, Loss: 0.11454613506793976
step: 4400, Loss: 2.4867656230926514
step: 4500, Loss: 0.2156079262495041
step: 4600, Loss: 0.1397143453359604
step: 4700, Loss: 0.13354724645614624
step: 4800, Loss: 0.12874484062194824
step: 4900, Loss: 0.13065382838249207
step: 5000, Loss: 0.12127936631441116
step: 5100, Loss: 0.12681616842746735
step: 5200, Loss: 0.12404370307922363
step: 5300, Loss: 0.2091740071773529
step: 5400, Loss: 0.12063314765691757
step: 5500, Loss: 0.12011565268039703
step: 5600, Loss: 0.11717706173658371
step: 5700, Loss: 0.11802762746810913
step: 5800, Loss: 0.11802849918603897
step: 5900, Loss: 0.11511385440826416
step: 6000, Loss: 0.11718488484621048
step: 6100, Loss: 0.11724116653203964
step: 6200, Loss: 0.11603168398141861
step: 6300, Loss: 0.11709548532962799
step: 6400, Loss: 0.11525734513998032
step: 6500, Loss: 0.11707209795713425
step: 6600, Loss: 0.11568459123373032
step: 6700, Loss: 0.11632823944091797
step: 6800, Loss: 0.11545884609222412
step: 6900, Loss: 0.11668932437896729
step: 7000, Loss: 0.11567782610654831
step: 7100, Loss: 0.11410210281610489
step: 7200, Loss: 0.20117926597595215
step: 7300, Loss: 0.11441782861948013
step: 7400, Loss: 0.11428096890449524
step: 7500, Loss: 0.1152595728635788
step: 7600, Loss: 0.11361619830131531
step: 7700, Loss: 0.11649006605148315
step: 7800, Loss: 0.1148756891489029
step: 7900, Loss: 0.11448002606630325
step: 8000, Loss: 0.11501267552375793
step: 8100, Loss: 0.11451192200183868
step: 8200, Loss: 0.1126175969839096
step: 8300, Loss: 0.1145613044500351
step: 8400, Loss: 0.11458553373813629
step: 8500, Loss: 0.11489586532115936
step: 8600, Loss: 0.11589426547288895
step: 8700, Loss: 0.11371748894453049
step: 8800, Loss: 0.11569983512163162
step: 8900, Loss: 0.11272400617599487
step: 9000, Loss: 0.11318057775497437
step: 9100, Loss: 0.19437889754772186
step: 9200, Loss: 0.11370982974767685
step: 9300, Loss: 0.11295527964830399
step: 9400, Loss: 0.11345969140529633
step: 9500, Loss: 0.11400548368692398
step: 9600, Loss: 0.11318720877170563
step: 9700, Loss: 0.11408059298992157
step: 9800, Loss: 0.1143416240811348
step: 9900, Loss: 0.11299242824316025
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.972972972972973
recall: 1.0
F_score: 0.9863013698630138
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.12306590378284454
step: 100, Loss: 0.11886757612228394
step: 200, Loss: 0.11649704724550247
step: 300, Loss: 0.11553297191858292
step: 400, Loss: 0.11806914955377579
step: 500, Loss: 0.1156829223036766
step: 600, Loss: 0.11352592706680298
step: 700, Loss: 0.11317893117666245
step: 800, Loss: 0.11413587629795074
step: 900, Loss: 0.11447708308696747
step: 1000, Loss: 0.11496459692716599
step: 1100, Loss: 0.11511990427970886
step: 1200, Loss: 0.11380141228437424
step: 1300, Loss: 0.11440806835889816
step: 1400, Loss: 0.11284494400024414
step: 1500, Loss: 0.19220706820487976
step: 1600, Loss: 0.11394594609737396
step: 1700, Loss: 0.11349859833717346
step: 1800, Loss: 0.11360025405883789
step: 1900, Loss: 0.1140684187412262
step: 2000, Loss: 0.11437948048114777
step: 2100, Loss: 0.11381696164608002
step: 2200, Loss: 0.11431078612804413
step: 2300, Loss: 0.1140168309211731
step: 2400, Loss: 0.11302124708890915
step: 2500, Loss: 0.11424162983894348
step: 2600, Loss: 0.11396785080432892
step: 2700, Loss: 0.11341544985771179
step: 2800, Loss: 0.11362549662590027
step: 2900, Loss: 0.11317449808120728
step: 3000, Loss: 0.11357708275318146
step: 3100, Loss: 0.11414899677038193
step: 3200, Loss: 0.11330603063106537
step: 3300, Loss: 0.1144770011305809
step: 3400, Loss: 0.19547247886657715
step: 3500, Loss: 0.11502891033887863
step: 3600, Loss: 0.11456657946109772
step: 3700, Loss: 0.1139104887843132
step: 3800, Loss: 0.11378398537635803
step: 3900, Loss: 0.11285527050495148
step: 4000, Loss: 0.11463658511638641
step: 4100, Loss: 0.11388665437698364
step: 4200, Loss: 0.11305806785821915
step: 4300, Loss: 0.11472367495298386
step: 4400, Loss: 1.2198983430862427
step: 4500, Loss: 0.13332302868366241
step: 4600, Loss: 0.12944474816322327
step: 4700, Loss: 0.12569008767604828
step: 4800, Loss: 0.12584678828716278
step: 4900, Loss: 0.1219889298081398
step: 5000, Loss: 0.12506529688835144
step: 5100, Loss: 0.12138618528842926
step: 5200, Loss: 0.11998187005519867
step: 5300, Loss: 0.2051171362400055
step: 5400, Loss: 0.1156485378742218
step: 5500, Loss: 0.11805153638124466
step: 5600, Loss: 0.11750461161136627
step: 5700, Loss: 0.1171770691871643
step: 5800, Loss: 0.11779923737049103
step: 5900, Loss: 0.11486849933862686
step: 6000, Loss: 0.11688774824142456
step: 6100, Loss: 0.11580012738704681
step: 6200, Loss: 0.11577194929122925
step: 6300, Loss: 0.11613254249095917
step: 6400, Loss: 0.11702166497707367
step: 6500, Loss: 0.11586179584264755
step: 6600, Loss: 0.11633549630641937
step: 6700, Loss: 0.11623409390449524
step: 6800, Loss: 0.11598596721887589
step: 4700, Loss: 0.11526639014482498
step: 4800, Loss: 0.11476834863424301
step: 4900, Loss: 0.11522286385297775
step: 5000, Loss: 0.11393828690052032
step: 5100, Loss: 0.1139153316617012
step: 5200, Loss: 0.1155439019203186
step: 5300, Loss: 0.11717327684164047
step: 5400, Loss: 0.11522918939590454
step: 5500, Loss: 7.91608190536499
step: 5600, Loss: 0.13134539127349854
step: 5700, Loss: 0.12800084054470062
step: 5800, Loss: 0.12191756814718246
step: 5900, Loss: 0.1259067952632904
step: 6000, Loss: 0.12036004662513733
step: 6100, Loss: 0.12403891235589981
step: 6200, Loss: 0.11965624243021011
step: 6300, Loss: 0.1183944046497345
step: 6400, Loss: 0.12012892961502075
step: 6500, Loss: 0.12007549405097961
step: 6600, Loss: 0.11928366124629974
step: 6700, Loss: 0.12077793478965759
step: 6800, Loss: 0.11661748588085175
step: 6900, Loss: 0.11976085603237152
step: 7000, Loss: 0.11835885047912598
step: 7100, Loss: 0.11870710551738739
step: 7200, Loss: 0.11721888184547424
step: 7300, Loss: 0.11675810068845749
step: 7400, Loss: 0.11755797266960144
step: 7500, Loss: 0.11828672140836716
step: 7600, Loss: 0.11717638373374939
step: 7700, Loss: 0.11691991984844208
step: 7800, Loss: 0.11742100119590759
step: 7900, Loss: 0.11553627252578735
step: 8000, Loss: 0.11512687802314758
step: 8100, Loss: 0.11754868924617767
step: 8200, Loss: 0.11567607522010803
step: 8300, Loss: 0.11503177881240845
step: 8400, Loss: 0.11486689746379852
step: 8500, Loss: 0.11429154872894287
step: 8600, Loss: 0.11532456427812576
step: 8700, Loss: 0.11807660013437271
step: 8800, Loss: 0.11704942584037781
step: 8900, Loss: 0.11482984572649002
step: 9000, Loss: 0.11508622020483017
step: 9100, Loss: 0.11526400595903397
step: 9200, Loss: 0.11501586437225342
step: 9300, Loss: 0.11377154290676117
step: 9400, Loss: 0.11561138927936554
step: 9500, Loss: 0.11480703949928284
step: 9600, Loss: 0.11524367332458496
step: 9700, Loss: 0.11325618624687195
step: 9800, Loss: 0.11441002786159515
step: 9900, Loss: 0.11351583898067474
training successfully ended.
validating...
validate data length:32
acc: 0.8125
precision: 0.7777777777777778
recall: 0.875
F_score: 0.823529411764706
******fold 3******

Training... train_data length:281
step: 0, Loss: 4.734076976776123
step: 100, Loss: 0.11855566501617432
step: 200, Loss: 0.11802934110164642
step: 300, Loss: 0.11511193215847015
step: 400, Loss: 0.11731445044279099
step: 500, Loss: 0.1168457418680191
step: 600, Loss: 0.1147085577249527
step: 700, Loss: 0.11477340757846832
step: 800, Loss: 0.11694665998220444
step: 900, Loss: 0.1148531585931778
step: 1000, Loss: 0.1152164414525032
step: 1100, Loss: 0.11673559993505478
step: 1200, Loss: 0.11665856093168259
step: 1300, Loss: 0.11472215503454208
step: 1400, Loss: 0.1162274032831192
step: 1500, Loss: 0.11432419717311859
step: 1600, Loss: 0.114398293197155
step: 1700, Loss: 0.11355987191200256
step: 1800, Loss: 0.1141929030418396
step: 1900, Loss: 0.11439789086580276
step: 2000, Loss: 0.11581742763519287
step: 2100, Loss: 0.1134580448269844
step: 2200, Loss: 0.11488078534603119
step: 2300, Loss: 0.11537820100784302
step: 2400, Loss: 0.11506142467260361
step: 2500, Loss: 0.11418819427490234
step: 2600, Loss: 0.11621315777301788
step: 2700, Loss: 0.11417513340711594
step: 2800, Loss: 0.1151815801858902
step: 2900, Loss: 0.11472589522600174
step: 3000, Loss: 0.11684195697307587
step: 3100, Loss: 0.11483138799667358
step: 3200, Loss: 0.11527092009782791
step: 3300, Loss: 0.11446529626846313
step: 3400, Loss: 0.11388585716485977
step: 3500, Loss: 0.11538339406251907
step: 3600, Loss: 0.11420506983995438
step: 3700, Loss: 0.11568064987659454
step: 3800, Loss: 0.1151539534330368
step: 3900, Loss: 0.11470901221036911
step: 4000, Loss: 0.11672183126211166
step: 4100, Loss: 0.11436200886964798
step: 4200, Loss: 0.11576187610626221
step: 4300, Loss: 0.11463268101215363
step: 4400, Loss: 0.1150219738483429
step: 4500, Loss: 0.1572321653366089
step: 4600, Loss: 0.13856880366802216
step: 4700, Loss: 0.12421707063913345
step: 4800, Loss: 0.12748482823371887
step: 4900, Loss: 0.11951544135808945
step: 5000, Loss: 0.12361691147089005
step: 5100, Loss: 0.1219770759344101
step: 5200, Loss: 0.11956043541431427
step: 5300, Loss: 0.11798073351383209
step: 5400, Loss: 0.11868530511856079
step: 5500, Loss: 0.11808466911315918
step: 5600, Loss: 0.12345273792743683
step: 5700, Loss: 0.11854597926139832
step: 5800, Loss: 0.11888749897480011
step: 5900, Loss: 0.11533427238464355
step: 6000, Loss: 0.11668767780065536
step: 6100, Loss: 0.11692085862159729
step: 6200, Loss: 0.11647270619869232
step: 6300, Loss: 0.11678706109523773
step: 6400, Loss: 0.11984242498874664
step: 6500, Loss: 0.11598709970712662
step: 6600, Loss: 0.11815399676561356
step: 6700, Loss: 0.11423610150814056
step: 6800, Loss: 0.11455242335796356
step: 6900, Loss: 0.11495202034711838
step: 7000, Loss: 0.11494463682174683
step: 7100, Loss: 0.11917190998792648
step: 7200, Loss: 0.11342441290616989
step: 7300, Loss: 0.11377114802598953
step: 7400, Loss: 0.11493334919214249
step: 7500, Loss: 0.11398183554410934
step: 7600, Loss: 0.11556050181388855
step: 7700, Loss: 0.11700816452503204
step: 7800, Loss: 0.11508431285619736
step: 7900, Loss: 0.11533966660499573
step: 8000, Loss: 0.11470908671617508
step: 8100, Loss: 0.1151680126786232
step: 8200, Loss: 0.11643077433109283
step: 8300, Loss: 0.11503772437572479
step: 8400, Loss: 0.11539703607559204
step: 8500, Loss: 0.1180829405784607
step: 8600, Loss: 0.11444662511348724
step: 8700, Loss: 0.11381286382675171
step: 8800, Loss: 0.11555258929729462
step: 8900, Loss: 0.11563604325056076
step: 9000, Loss: 0.11453761905431747
step: 9100, Loss: 0.11412037909030914
step: 9200, Loss: 0.11631257086992264
step: 9300, Loss: 0.11429751664400101
step: 9400, Loss: 0.11450182646512985
step: 9500, Loss: 0.12014757841825485
step: 9600, Loss: 0.11441321671009064
step: 9700, Loss: 0.11338523030281067
step: 9800, Loss: 0.1167735755443573
step: 9900, Loss: 0.1142735704779625
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.1795353889465332
step: 100, Loss: 0.12031849473714828
step: 200, Loss: 0.11541309207677841
step: 300, Loss: 0.11926918476819992
step: 400, Loss: 0.11509093642234802
step: 500, Loss: 0.11363372206687927
step: 600, Loss: 0.11451302468776703
step: 700, Loss: 0.11440083384513855
step: 800, Loss: 0.11605215072631836
step: 900, Loss: 0.11499513685703278
step: 1000, Loss: 0.11545395106077194
step: 1100, Loss: 0.11496531218290329
step: 1200, Loss: 0.11313189566135406
step: 1300, Loss: 0.11533290147781372
step: 1400, Loss: 0.11714526265859604
step: 1500, Loss: 0.11540084332227707
step: 1600, Loss: 0.11401795595884323
step: 1700, Loss: 0.1147746592760086
step: 1800, Loss: 0.11436034739017487
step: 1900, Loss: 0.11491813510656357
step: 2000, Loss: 0.11633738875389099
step: 2100, Loss: 0.1145443469285965
step: 2200, Loss: 0.1145956739783287
step: 2300, Loss: 0.11407476663589478
step: 2400, Loss: 0.1139422357082367
step: 2500, Loss: 0.1149415597319603
step: 2600, Loss: 0.1175939068198204
step: 2700, Loss: 0.1145564392209053
step: 2800, Loss: 0.11474358290433884
step: 2900, Loss: 0.1134372428059578
step: 3000, Loss: 0.11523214727640152
step: 3100, Loss: 0.11360574513673782
step: 3200, Loss: 0.1137678474187851
step: 3300, Loss: 0.11465123295783997
step: 3400, Loss: 0.11451301723718643
step: 3500, Loss: 0.11517299711704254
step: 3600, Loss: 0.11372128129005432
step: 3700, Loss: 0.11443077772855759
step: 3800, Loss: 0.11449174582958221
step: 3900, Loss: 0.11464697867631912
step: 4000, Loss: 0.11559642851352692
step: 4100, Loss: 0.11436740309000015
step: 4200, Loss: 0.11468782275915146
step: 4300, Loss: 0.11533473432064056
step: 4400, Loss: 0.11561772972345352
step: 4500, Loss: 0.11349651217460632
step: 4600, Loss: 0.11592475324869156
step: 4700, Loss: 0.11439472436904907
step: 4800, Loss: 0.11388221383094788
step: 4900, Loss: 0.1147533506155014
step: 5000, Loss: 0.11589796841144562
step: 5100, Loss: 0.11513598263263702
step: 5200, Loss: 0.11502541601657867
step: 6900, Loss: 0.11567609012126923
step: 7000, Loss: 0.11574938893318176
step: 7100, Loss: 0.11480231583118439
step: 7200, Loss: 0.19750958681106567
step: 7300, Loss: 0.11462599039077759
step: 7400, Loss: 0.11416409909725189
step: 7500, Loss: 0.11407861858606339
step: 7600, Loss: 0.11446729302406311
step: 7700, Loss: 0.11580565571784973
step: 7800, Loss: 0.11325856298208237
step: 7900, Loss: 0.1146833598613739
step: 8000, Loss: 0.11571673303842545
step: 8100, Loss: 0.11342766880989075
step: 8200, Loss: 0.11456379294395447
step: 8300, Loss: 0.11526452749967575
step: 8400, Loss: 0.11555017530918121
step: 8500, Loss: 0.1138802096247673
step: 8600, Loss: 0.11565976589918137
step: 8700, Loss: 0.11388389766216278
step: 8800, Loss: 0.11416051536798477
step: 8900, Loss: 0.1133633628487587
step: 9000, Loss: 0.11372310668230057
step: 9100, Loss: 0.19507604837417603
step: 9200, Loss: 0.11507831513881683
step: 9300, Loss: 0.11374437063932419
step: 9400, Loss: 0.11526293307542801
step: 9500, Loss: 0.1145055815577507
step: 9600, Loss: 0.1137666329741478
step: 9700, Loss: 0.11358843743801117
step: 9800, Loss: 0.11459150165319443
step: 9900, Loss: 0.11259628087282181
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9487179487179487
recall: 1.0
F_score: 0.9736842105263158
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11952685564756393
step: 100, Loss: 0.11727606505155563
step: 200, Loss: 0.11591247469186783
step: 300, Loss: 0.1165429949760437
step: 400, Loss: 0.11437636613845825
step: 500, Loss: 0.11380399018526077
step: 600, Loss: 0.11390447616577148
step: 700, Loss: 0.11363835632801056
step: 800, Loss: 0.11495538055896759
step: 900, Loss: 0.11416856944561005
step: 1000, Loss: 0.11382116377353668
step: 1100, Loss: 0.11410944163799286
step: 1200, Loss: 0.11279340088367462
step: 1300, Loss: 0.1140093132853508
step: 1400, Loss: 0.11434970051050186
step: 1500, Loss: 0.19232267141342163
step: 1600, Loss: 0.11275602877140045
step: 1700, Loss: 0.11393025517463684
step: 1800, Loss: 0.1142742782831192
step: 1900, Loss: 0.11470584571361542
step: 2000, Loss: 0.11387015134096146
step: 2100, Loss: 0.11421740800142288
step: 2200, Loss: 0.11357170343399048
step: 2300, Loss: 0.11372022330760956
step: 2400, Loss: 0.11360540241003036
step: 2500, Loss: 0.1137709766626358
step: 2600, Loss: 0.11348294466733932
step: 2700, Loss: 0.11425827443599701
step: 2800, Loss: 0.11325544118881226
step: 2900, Loss: 0.11473778635263443
step: 3000, Loss: 0.11522629112005234
step: 3100, Loss: 0.11667917668819427
step: 3200, Loss: 0.11368013173341751
step: 3300, Loss: 0.11460661888122559
step: 3400, Loss: 0.20418205857276917
step: 3500, Loss: 0.1134549230337143
step: 3600, Loss: 0.11378258466720581
step: 3700, Loss: 0.11558026075363159
step: 3800, Loss: 1.5476679801940918
step: 3900, Loss: 0.19976383447647095
step: 4000, Loss: 0.12370756268501282
step: 4100, Loss: 0.12398050725460052
step: 4200, Loss: 0.12080544233322144
step: 4300, Loss: 0.12001655250787735
step: 4400, Loss: 0.11748628318309784
step: 4500, Loss: 0.11572571098804474
step: 4600, Loss: 0.11666102707386017
step: 4700, Loss: 0.12113536894321442
step: 4800, Loss: 0.11745424568653107
step: 4900, Loss: 0.11567503213882446
step: 5000, Loss: 0.11771705746650696
step: 5100, Loss: 0.11484207212924957
step: 5200, Loss: 0.11484558880329132
step: 5300, Loss: 0.2057371884584427
step: 5400, Loss: 0.1154843121767044
step: 5500, Loss: 0.11564110219478607
step: 5600, Loss: 0.1140250563621521
step: 5700, Loss: 0.11652864515781403
step: 5800, Loss: 0.11734718084335327
step: 5900, Loss: 0.11426540464162827
step: 6000, Loss: 0.11580047011375427
step: 6100, Loss: 0.11498960852622986
step: 6200, Loss: 0.11429482698440552
step: 6300, Loss: 0.1163957342505455
step: 6400, Loss: 0.1151801198720932
step: 6500, Loss: 0.11457325518131256
step: 6600, Loss: 0.11492927372455597
step: 6700, Loss: 0.11509905755519867
step: 6800, Loss: 0.11398263275623322
step: 6900, Loss: 0.11433763056993484
step: 7000, Loss: 0.11381425708532333
step: 7100, Loss: 0.11382627487182617
step: 7200, Loss: 0.20167696475982666
step: 7300, Loss: 0.11309991031885147
step: 7400, Loss: 0.11591491848230362
step: 7500, Loss: 0.11529047787189484
step: 7600, Loss: 0.1147778332233429
step: 7700, Loss: 0.11404735594987869
step: 7800, Loss: 0.11395363509654999
step: 7900, Loss: 0.11379832029342651
step: 8000, Loss: 0.11375761032104492
step: 8100, Loss: 0.11523546278476715
step: 8200, Loss: 0.11388637125492096
step: 8300, Loss: 0.11367635428905487
step: 8400, Loss: 0.11258836090564728
step: 8500, Loss: 0.11625741422176361
step: 8600, Loss: 0.11419927328824997
step: 8700, Loss: 0.11330723762512207
step: 8800, Loss: 0.11387546360492706
step: 8900, Loss: 0.11417431384325027
step: 9000, Loss: 0.11331478506326675
step: 9100, Loss: 0.19421033561229706
step: 9200, Loss: 0.1148023009300232
step: 9300, Loss: 0.11288988590240479
step: 9400, Loss: 0.11395850032567978
step: 9500, Loss: 0.11355192214250565
step: 9600, Loss: 0.11688228696584702
step: 9700, Loss: 0.11365227401256561
step: 9800, Loss: 0.11365655064582825
step: 9900, Loss: 0.11302260309457779
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9696969696969697
recall: 0.9411764705882353
F_score: 0.955223880597015
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11919032782316208
step: 100, Loss: 0.1154138594865799
step: 200, Loss: 0.11677691340446472
step: 300, Loss: 0.11352758854627609
step: 400, Loss: 0.11429233849048615
step: 500, Loss: 0.11420118063688278
step: 600, Loss: 0.11393946409225464
step: 700, Loss: 0.11387628316879272
step: 800, Loss: 0.1139216497540474
step: 900, Loss: 0.11647003889083862
step: 1000, Loss: 0.11418889462947845
step: 1100, Loss: 0.11784588545560837
step: 1200, Loss: 0.11448165029287338
step: 1300, Loss: 0.11250008642673492
step: 1400, Loss: 0.11404883116483688
step: 1500, Loss: 0.19451814889907837
step: 1600, Loss: 0.11399185657501221
step: 1700, Loss: 0.11566388607025146
step: 1800, Loss: 0.11392594873905182
step: 1900, Loss: 0.11371691524982452
step: 2000, Loss: 0.11306971311569214
step: 2100, Loss: 0.11428035795688629
step: 2200, Loss: 0.11371401697397232
step: 2300, Loss: 0.11296018958091736
step: 2400, Loss: 0.11351636052131653
step: 2500, Loss: 0.11467649042606354
step: 2600, Loss: 0.11302370578050613
step: 2700, Loss: 0.11311578005552292
step: 2800, Loss: 0.11507781594991684
step: 2900, Loss: 0.1160394549369812
step: 3000, Loss: 0.11340256035327911
step: 3100, Loss: 0.1142231822013855
step: 3200, Loss: 0.11398714035749435
step: 3300, Loss: 0.11289316415786743
step: 3400, Loss: 0.19778907299041748
step: 3500, Loss: 0.11464589089155197
step: 3600, Loss: 0.11313705146312714
step: 3700, Loss: 0.1141674816608429
step: 3800, Loss: 0.1139296367764473
step: 3900, Loss: 0.11420745402574539
step: 4000, Loss: 0.11411549150943756
step: 4100, Loss: 0.11359379440546036
step: 4200, Loss: 0.11387572437524796
step: 4300, Loss: 0.22223877906799316
step: 4400, Loss: 0.2987178564071655
step: 4500, Loss: 0.13173925876617432
step: 4600, Loss: 0.12358593940734863
step: 4700, Loss: 0.12381342053413391
step: 4800, Loss: 0.12299959361553192
step: 4900, Loss: 0.12021017074584961
step: 5000, Loss: 0.11640304327011108
step: 5100, Loss: 0.1155524030327797
step: 5200, Loss: 0.11841638386249542
step: 5300, Loss: 0.20179742574691772
step: 5400, Loss: 0.12274818122386932
step: 5500, Loss: 0.11686605960130692
step: 5600, Loss: 0.11527376621961594
step: 5700, Loss: 0.1168968677520752
step: 5800, Loss: 0.11724866181612015
step: 5900, Loss: 0.11642255634069443
step: 6000, Loss: 0.11680789291858673
step: 6100, Loss: 0.11616259813308716
step: 6200, Loss: 0.11552426218986511
step: 6300, Loss: 0.11546735465526581
step: 6400, Loss: 0.11659865826368332
step: 6500, Loss: 0.1152164489030838
step: 6600, Loss: 0.11612673848867416
step: 6700, Loss: 0.11663089692592621
step: 6800, Loss: 0.11444824934005737
step: 6900, Loss: 0.11567278951406479
step: 7000, Loss: 0.11370916664600372
step: 7100, Loss: 0.11505342274904251
step: 7200, Loss: 0.19865182042121887
step: 7300, Loss: 0.11437755078077316
step: 5300, Loss: 0.11417951434850693
step: 5400, Loss: 0.1137295588850975
step: 5500, Loss: 0.11442577093839645
step: 5600, Loss: 0.11742625385522842
step: 5700, Loss: 0.11657826602458954
step: 5800, Loss: 0.11676885187625885
step: 5900, Loss: 2.9269332885742188
step: 6000, Loss: 0.13921156525611877
step: 6100, Loss: 0.11819280683994293
step: 6200, Loss: 0.13821221888065338
step: 6300, Loss: 0.12206883728504181
step: 6400, Loss: 0.12476421147584915
step: 6500, Loss: 0.12096565961837769
step: 6600, Loss: 0.12170432507991791
step: 6700, Loss: 0.1177261620759964
step: 6800, Loss: 0.1178971529006958
step: 6900, Loss: 0.14793920516967773
step: 7000, Loss: 0.1177503764629364
step: 7100, Loss: 0.11773590743541718
step: 7200, Loss: 0.12027886509895325
step: 7300, Loss: 0.12308768928050995
step: 7400, Loss: 0.1195250153541565
step: 7500, Loss: 0.11697520315647125
step: 7600, Loss: 0.11817608773708344
step: 7700, Loss: 0.11496677994728088
step: 7800, Loss: 0.11709725111722946
step: 7900, Loss: 0.11510341614484787
step: 8000, Loss: 0.12005721032619476
step: 8100, Loss: 0.11457318812608719
step: 8200, Loss: 0.11707285046577454
step: 8300, Loss: 0.11801573634147644
step: 8400, Loss: 0.11699557304382324
step: 8500, Loss: 0.11493987590074539
step: 8600, Loss: 0.1142377182841301
step: 8700, Loss: 0.11441627889871597
step: 8800, Loss: 0.115679070353508
step: 8900, Loss: 0.11513820290565491
step: 9000, Loss: 0.11361470073461533
step: 9100, Loss: 0.11636842042207718
step: 9200, Loss: 0.11403261125087738
step: 9300, Loss: 0.11406155675649643
step: 9400, Loss: 0.11415163427591324
step: 9500, Loss: 0.1135534718632698
step: 9600, Loss: 0.11504895985126495
step: 9700, Loss: 0.11502605676651001
step: 9800, Loss: 0.11503741145133972
step: 9900, Loss: 0.11390960216522217
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.6875
recall: 0.7333333333333333
F_score: 0.7096774193548386
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.17971599102020264
step: 100, Loss: 0.11612378060817719
step: 200, Loss: 0.11629592627286911
step: 300, Loss: 0.11394787579774857
step: 400, Loss: 0.11514703929424286
step: 500, Loss: 0.11421027779579163
step: 600, Loss: 0.11415877938270569
step: 700, Loss: 0.11413212865591049
step: 800, Loss: 0.11491447687149048
step: 900, Loss: 0.11441467702388763
step: 1000, Loss: 0.11429031938314438
step: 1100, Loss: 0.11524561047554016
step: 1200, Loss: 0.11412177234888077
step: 1300, Loss: 0.11463410407304764
step: 1400, Loss: 0.11295934021472931
step: 1500, Loss: 0.11483298987150192
step: 1600, Loss: 0.11420202255249023
step: 1700, Loss: 0.1131625548005104
step: 1800, Loss: 0.11373715102672577
step: 1900, Loss: 0.11621616035699844
step: 2000, Loss: 0.11993898451328278
step: 2100, Loss: 0.11399918049573898
step: 2200, Loss: 0.11431722342967987
step: 2300, Loss: 0.1171334758400917
step: 2400, Loss: 0.11313360929489136
step: 2500, Loss: 0.1139822006225586
step: 2600, Loss: 0.11447009444236755
step: 2700, Loss: 0.1145341694355011
step: 2800, Loss: 0.11412689089775085
step: 2900, Loss: 0.11603036522865295
step: 3000, Loss: 0.11595091223716736
step: 3100, Loss: 0.11438144743442535
step: 3200, Loss: 0.11691826581954956
step: 3300, Loss: 0.11366873979568481
step: 3400, Loss: 0.11343730241060257
step: 3500, Loss: 0.11508785933256149
step: 3600, Loss: 0.11398427188396454
step: 3700, Loss: 0.1129106655716896
step: 3800, Loss: 0.1174677163362503
step: 3900, Loss: 0.11702434718608856
step: 4000, Loss: 0.11432521790266037
step: 4100, Loss: 0.1145302802324295
step: 4200, Loss: 0.11477255076169968
step: 4300, Loss: 0.11420027911663055
step: 4400, Loss: 0.11432638764381409
step: 4500, Loss: 0.1153421700000763
step: 4600, Loss: 0.11577580869197845
step: 4700, Loss: 0.11532694846391678
step: 4800, Loss: 0.11366190761327744
step: 4900, Loss: 0.11405249685049057
step: 5000, Loss: 0.11529932916164398
step: 5100, Loss: 0.11481080949306488
step: 5200, Loss: 0.11552167683839798
step: 5300, Loss: 0.11437185853719711
step: 5400, Loss: 0.11421835422515869
step: 5500, Loss: 1.227685570716858
step: 5600, Loss: 0.12833534181118011
step: 5700, Loss: 0.12341520935297012
step: 5800, Loss: 0.1252976655960083
step: 5900, Loss: 0.12146663665771484
step: 6000, Loss: 0.12501168251037598
step: 6100, Loss: 0.11640951037406921
step: 6200, Loss: 0.11853837966918945
step: 6300, Loss: 0.12346445024013519
step: 6400, Loss: 0.11823735386133194
step: 6500, Loss: 0.11492945998907089
step: 6600, Loss: 0.12073267251253128
step: 6700, Loss: 0.11863523721694946
step: 6800, Loss: 0.11596433818340302
step: 6900, Loss: 0.11696307361125946
step: 7000, Loss: 0.11815822124481201
step: 7100, Loss: 0.11548548191785812
step: 7200, Loss: 0.11544153839349747
step: 7300, Loss: 0.11557013541460037
step: 7400, Loss: 0.1148148775100708
step: 7500, Loss: 0.11768987774848938
step: 7600, Loss: 0.11829167604446411
step: 7700, Loss: 0.11526992172002792
step: 7800, Loss: 0.11554698646068573
step: 7900, Loss: 0.11466390639543533
step: 8000, Loss: 0.11753737926483154
step: 8100, Loss: 0.11440948396921158
step: 8200, Loss: 0.11428143829107285
step: 8300, Loss: 0.11461152136325836
step: 8400, Loss: 0.11402709037065506
step: 8500, Loss: 0.11677219718694687
step: 8600, Loss: 0.11882640421390533
step: 8700, Loss: 0.11452549695968628
step: 8800, Loss: 0.11506346613168716
step: 8900, Loss: 0.11700426042079926
step: 9000, Loss: 0.11537294089794159
step: 9100, Loss: 0.11250970512628555
step: 9200, Loss: 0.11561769992113113
step: 9300, Loss: 0.1142207533121109
step: 9400, Loss: 0.1157408058643341
step: 9500, Loss: 0.11409436166286469
step: 9600, Loss: 0.11870265007019043
step: 9700, Loss: 0.11321012675762177
step: 9800, Loss: 0.11444036662578583
step: 9900, Loss: 0.11474499106407166
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8888888888888888
recall: 0.8888888888888888
F_score: 0.8888888888888888
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.18643590807914734
step: 100, Loss: 0.11727701127529144
step: 200, Loss: 0.12037355452775955
step: 300, Loss: 0.11669325083494186
step: 400, Loss: 0.1152891218662262
step: 500, Loss: 0.11376191675662994
step: 600, Loss: 0.11376295238733292
step: 700, Loss: 0.11400239169597626
step: 800, Loss: 0.11699876189231873
step: 900, Loss: 0.11364834010601044
step: 1000, Loss: 0.11461769044399261
step: 1100, Loss: 0.11372362077236176
step: 1200, Loss: 0.11498604714870453
step: 1300, Loss: 0.11411052942276001
step: 1400, Loss: 0.1149938702583313
step: 1500, Loss: 0.11287189275026321
step: 1600, Loss: 0.11615832895040512
step: 1700, Loss: 0.11411569267511368
step: 1800, Loss: 0.11415120214223862
step: 1900, Loss: 0.11605338752269745
step: 2000, Loss: 0.1139751598238945
step: 2100, Loss: 0.11316157877445221
step: 2200, Loss: 0.11471520364284515
step: 2300, Loss: 0.11324654519557953
step: 2400, Loss: 0.11311249434947968
step: 2500, Loss: 0.11385183781385422
step: 2600, Loss: 0.11454083025455475
step: 2700, Loss: 0.11665471643209457
step: 2800, Loss: 0.11577565222978592
step: 2900, Loss: 0.11333400011062622
step: 3000, Loss: 0.11450648307800293
step: 3100, Loss: 0.11395006626844406
step: 3200, Loss: 0.11439114063978195
step: 3300, Loss: 0.11407750844955444
step: 3400, Loss: 0.11538741737604141
step: 3500, Loss: 0.11621350795030594
step: 3600, Loss: 0.11491291224956512
step: 3700, Loss: 0.11546055972576141
step: 3800, Loss: 0.11455397307872772
step: 3900, Loss: 0.11446714401245117
step: 4000, Loss: 0.11623767018318176
step: 4100, Loss: 0.11330989748239517
step: 4200, Loss: 0.1139892041683197
step: 4300, Loss: 0.11483711749315262
step: 4400, Loss: 0.11672909557819366
step: 4500, Loss: 0.11568515747785568
step: 4600, Loss: 0.11950299143791199
step: 4700, Loss: 0.11382627487182617
step: 4800, Loss: 0.11546263098716736
step: 4900, Loss: 0.1625441610813141
step: 5000, Loss: 0.12836185097694397
step: 5100, Loss: 0.12487999349832535
step: 5200, Loss: 0.12214154005050659
step: 5300, Loss: 0.11684770882129669
step: 5400, Loss: 0.11690249294042587
step: 5500, Loss: 0.1174367219209671
step: 5600, Loss: 0.11725468933582306
step: 5700, Loss: 0.11581507325172424
step: 7400, Loss: 0.1145535409450531
step: 7500, Loss: 0.11430313438177109
step: 7600, Loss: 0.11636924743652344
step: 7700, Loss: 0.11630609631538391
step: 7800, Loss: 0.11458182334899902
step: 7900, Loss: 0.11444193124771118
step: 8000, Loss: 0.11356249451637268
step: 8100, Loss: 0.11361173540353775
step: 8200, Loss: 0.11323811858892441
step: 8300, Loss: 0.11429685354232788
step: 8400, Loss: 0.11369243264198303
step: 8500, Loss: 0.11359398066997528
step: 8600, Loss: 0.11300850659608841
step: 8700, Loss: 0.11524046212434769
step: 8800, Loss: 0.1138954758644104
step: 8900, Loss: 0.11487516015768051
step: 9000, Loss: 0.11548854410648346
step: 9100, Loss: 0.19585950672626495
step: 9200, Loss: 0.11361478269100189
step: 9300, Loss: 0.11439551413059235
step: 9400, Loss: 0.11315424740314484
step: 9500, Loss: 0.11451482027769089
step: 9600, Loss: 0.11297933757305145
step: 9700, Loss: 0.11333432048559189
step: 9800, Loss: 0.11318174749612808
step: 9900, Loss: 0.11484784632921219
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9696969696969697
recall: 1.0
F_score: 0.9846153846153847
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.12426082789897919
step: 100, Loss: 0.12324386835098267
step: 200, Loss: 0.11483358591794968
step: 300, Loss: 0.11416024714708328
step: 400, Loss: 0.11371426284313202
step: 500, Loss: 0.11412344872951508
step: 600, Loss: 0.11453011631965637
step: 700, Loss: 0.1133681908249855
step: 800, Loss: 0.11310867965221405
step: 900, Loss: 0.11474436521530151
step: 1000, Loss: 0.11326360702514648
step: 1100, Loss: 0.11489196866750717
step: 1200, Loss: 0.11350944638252258
step: 1300, Loss: 0.11420072615146637
step: 1400, Loss: 0.11351662129163742
step: 1500, Loss: 0.19396230578422546
step: 1600, Loss: 0.11396954953670502
step: 1700, Loss: 0.11374615132808685
step: 1800, Loss: 0.11382155865430832
step: 1900, Loss: 0.11301574856042862
step: 2000, Loss: 0.11394043266773224
step: 2100, Loss: 0.11294103413820267
step: 2200, Loss: 0.11491692066192627
step: 2300, Loss: 0.11323709040880203
step: 2400, Loss: 0.11298834532499313
step: 2500, Loss: 0.11384701728820801
step: 2600, Loss: 0.1132274866104126
step: 2700, Loss: 0.11337688565254211
step: 2800, Loss: 0.11278023570775986
step: 2900, Loss: 0.11370404064655304
step: 3000, Loss: 0.11364763230085373
step: 3100, Loss: 0.11272892355918884
step: 3200, Loss: 0.11351378262042999
step: 3300, Loss: 0.11463562399148941
step: 3400, Loss: 0.19203263521194458
step: 3500, Loss: 0.11346069723367691
step: 3600, Loss: 0.11365354806184769
step: 3700, Loss: 0.11299999803304672
step: 3800, Loss: 0.11226077377796173
step: 3900, Loss: 0.11304610967636108
step: 4000, Loss: 0.11554437130689621
step: 4100, Loss: 0.11383961886167526
step: 4200, Loss: 0.11344029754400253
step: 4300, Loss: 0.11410173773765564
step: 4400, Loss: 2.3361117839813232
step: 4500, Loss: 0.14189094305038452
step: 4600, Loss: 0.14494559168815613
step: 4700, Loss: 0.12555626034736633
step: 4800, Loss: 0.12398926913738251
step: 4900, Loss: 0.12227246165275574
step: 5000, Loss: 0.12354935705661774
step: 5100, Loss: 0.12169356644153595
step: 5200, Loss: 0.11665050685405731
step: 5300, Loss: 0.21106000244617462
step: 5400, Loss: 0.11946547031402588
step: 5500, Loss: 0.11707329005002975
step: 5600, Loss: 0.115167036652565
step: 5700, Loss: 0.11570258438587189
step: 5800, Loss: 0.11536365002393723
step: 5900, Loss: 0.11904957890510559
step: 6000, Loss: 0.11601655185222626
step: 6100, Loss: 0.11549776792526245
step: 6200, Loss: 0.11681067943572998
step: 6300, Loss: 0.11466842889785767
step: 6400, Loss: 0.11384831368923187
step: 6500, Loss: 0.11552594602108002
step: 6600, Loss: 0.11724157631397247
step: 6700, Loss: 0.11522862315177917
step: 6800, Loss: 0.11389543116092682
step: 6900, Loss: 0.11418451368808746
step: 7000, Loss: 0.11412376165390015
step: 7100, Loss: 0.11589600145816803
step: 7200, Loss: 0.19839702546596527
step: 7300, Loss: 0.11459541320800781
step: 7400, Loss: 0.11551941186189651
step: 7500, Loss: 0.11415460705757141
step: 7600, Loss: 0.11299765855073929
step: 7700, Loss: 0.11606716364622116
step: 7800, Loss: 0.11425431072711945
step: 7900, Loss: 0.11459637433290482
step: 8000, Loss: 0.11610843986272812
step: 8100, Loss: 0.11609968543052673
step: 8200, Loss: 0.11379753053188324
step: 8300, Loss: 0.11284007877111435
step: 8400, Loss: 0.112466961145401
step: 8500, Loss: 0.11470552533864975
step: 8600, Loss: 0.11462284624576569
step: 8700, Loss: 0.11352213472127914
step: 8800, Loss: 0.11351335048675537
step: 8900, Loss: 0.1149817705154419
step: 9000, Loss: 0.11349310725927353
step: 9100, Loss: 0.19133925437927246
step: 9200, Loss: 0.11447153985500336
step: 9300, Loss: 0.1151193231344223
step: 9400, Loss: 0.11356966197490692
step: 9500, Loss: 0.11288876086473465
step: 9600, Loss: 0.11351698637008667
step: 9700, Loss: 0.11472474038600922
step: 9800, Loss: 0.11337072402238846
step: 9900, Loss: 0.11570259183645248
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.975609756097561
F_score: 0.9876543209876543
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.1230812519788742
step: 100, Loss: 0.1196569949388504
step: 200, Loss: 0.11525974422693253
step: 300, Loss: 0.1154920756816864
step: 400, Loss: 0.1141955778002739
step: 500, Loss: 0.11557988822460175
step: 600, Loss: 0.11324229836463928
step: 700, Loss: 0.11397982388734818
step: 800, Loss: 0.11627797782421112
step: 900, Loss: 0.11320044100284576
step: 1000, Loss: 0.11402887105941772
step: 1100, Loss: 0.114207923412323
step: 1200, Loss: 0.11392823606729507
step: 1300, Loss: 0.11366540193557739
step: 1400, Loss: 0.11443045735359192
step: 1500, Loss: 0.19153864681720734
step: 1600, Loss: 0.11291161179542542
step: 1700, Loss: 0.11475743353366852
step: 1800, Loss: 0.1146787628531456
step: 1900, Loss: 0.11383233964443207
step: 2000, Loss: 0.11438260972499847
step: 2100, Loss: 0.11340521275997162
step: 2200, Loss: 0.11542047560214996
step: 2300, Loss: 0.11378553509712219
step: 2400, Loss: 0.11516907811164856
step: 2500, Loss: 0.11480118334293365
step: 2600, Loss: 0.11432033777236938
step: 2700, Loss: 0.1127830222249031
step: 2800, Loss: 0.11289803683757782
step: 2900, Loss: 0.11343486607074738
step: 3000, Loss: 0.11425833404064178
step: 3100, Loss: 0.11292217671871185
step: 3200, Loss: 0.11319267004728317
step: 3300, Loss: 0.11314165592193604
step: 3400, Loss: 0.19275064766407013
step: 3500, Loss: 0.1141340583562851
step: 3600, Loss: 0.11351627111434937
step: 3700, Loss: 0.11297755688428879
step: 3800, Loss: 0.1143253818154335
step: 3900, Loss: 0.1126561388373375
step: 4000, Loss: 0.11491629481315613
step: 4100, Loss: 0.1137310266494751
step: 4200, Loss: 0.11309792101383209
step: 4300, Loss: 0.11389903724193573
step: 4400, Loss: 0.11285550892353058
step: 4500, Loss: 0.11356938630342484
step: 4600, Loss: 0.11385927349328995
step: 4700, Loss: 0.11416882276535034
step: 4800, Loss: 0.11444831639528275
step: 4900, Loss: 0.11269119381904602
step: 5000, Loss: 6.555912017822266
step: 5100, Loss: 0.6251568794250488
step: 5200, Loss: 0.23991288244724274
step: 5300, Loss: 0.22883647680282593
step: 5400, Loss: 0.1288074404001236
step: 5500, Loss: 0.12498834729194641
step: 5600, Loss: 0.12042977660894394
step: 5700, Loss: 0.12172285467386246
step: 5800, Loss: 0.12079033255577087
step: 5900, Loss: 0.11608748137950897
step: 6000, Loss: 0.11855228245258331
step: 6100, Loss: 0.11854385584592819
step: 6200, Loss: 0.11678779125213623
step: 6300, Loss: 0.1140405684709549
step: 6400, Loss: 0.11661375313997269
step: 6500, Loss: 0.11657548695802689
step: 6600, Loss: 0.11827664077281952
step: 6700, Loss: 0.11642640829086304
step: 6800, Loss: 0.11881585419178009
step: 6900, Loss: 0.11702907085418701
step: 7000, Loss: 0.11490027606487274
step: 7100, Loss: 0.11876630783081055
step: 7200, Loss: 0.20218883454799652
step: 7300, Loss: 0.11593426018953323
step: 7400, Loss: 0.11427497118711472
step: 7500, Loss: 0.11445678770542145
step: 7600, Loss: 0.11691073328256607
step: 7700, Loss: 0.11649763584136963
step: 7800, Loss: 0.11462055146694183
step: 5800, Loss: 0.11636121571063995
step: 5900, Loss: 0.11601072549819946
step: 6000, Loss: 0.11367659270763397
step: 6100, Loss: 0.11550644785165787
step: 6200, Loss: 0.11710748821496964
step: 6300, Loss: 0.11606857180595398
step: 6400, Loss: 0.1185571551322937
step: 6500, Loss: 0.11633533239364624
step: 6600, Loss: 0.11813169717788696
step: 6700, Loss: 0.11424674838781357
step: 6800, Loss: 0.11860238015651703
step: 6900, Loss: 0.11546719074249268
step: 7000, Loss: 0.11676738411188126
step: 7100, Loss: 0.12023849785327911
step: 7200, Loss: 0.11581753194332123
step: 7300, Loss: 0.1144934892654419
step: 7400, Loss: 0.11401992291212082
step: 7500, Loss: 0.11547645181417465
step: 7600, Loss: 0.12012771517038345
step: 7700, Loss: 0.11370225250720978
step: 7800, Loss: 0.11469113826751709
step: 7900, Loss: 0.11558276414871216
step: 8000, Loss: 0.11863075941801071
step: 8100, Loss: 0.11494465172290802
step: 8200, Loss: 0.11759687215089798
step: 8300, Loss: 0.11456087231636047
step: 8400, Loss: 0.11455077677965164
step: 8500, Loss: 0.11444075405597687
step: 8600, Loss: 0.1160537451505661
step: 8700, Loss: 0.11397203058004379
step: 8800, Loss: 0.11499425023794174
step: 8900, Loss: 0.1152762845158577
step: 9000, Loss: 0.11583927273750305
step: 9100, Loss: 0.11442457139492035
step: 9200, Loss: 0.11477801948785782
step: 9300, Loss: 0.11400604993104935
step: 9400, Loss: 0.11446977406740189
step: 9500, Loss: 0.11408846825361252
step: 9600, Loss: 0.11809121072292328
step: 9700, Loss: 0.11466215550899506
step: 9800, Loss: 0.1140841394662857
step: 9900, Loss: 0.11397578567266464
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.9333333333333333
recall: 0.7777777777777778
F_score: 0.8484848484848485
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.19653105735778809
step: 100, Loss: 0.11664829403162003
step: 200, Loss: 0.11757020652294159
step: 300, Loss: 0.11679670214653015
step: 400, Loss: 0.11591001600027084
step: 500, Loss: 0.1140555888414383
step: 600, Loss: 0.11500975489616394
step: 700, Loss: 0.1165308877825737
step: 800, Loss: 0.11676846444606781
step: 900, Loss: 0.11441535502672195
step: 1000, Loss: 0.11455279588699341
step: 1100, Loss: 0.11487163603305817
step: 1200, Loss: 0.11463183164596558
step: 1300, Loss: 0.11693882942199707
step: 1400, Loss: 0.11530450731515884
step: 1500, Loss: 0.1150507926940918
step: 1600, Loss: 0.1138949990272522
step: 1700, Loss: 0.11473175883293152
step: 1800, Loss: 0.11506006866693497
step: 1900, Loss: 0.11347164213657379
step: 2000, Loss: 0.11501558870077133
step: 2100, Loss: 0.11503031104803085
step: 2200, Loss: 0.11352509260177612
step: 2300, Loss: 0.11413826048374176
step: 2400, Loss: 0.11744662374258041
step: 2500, Loss: 0.11435167491436005
step: 2600, Loss: 0.11355432868003845
step: 2700, Loss: 0.11438930034637451
step: 2800, Loss: 0.11467789113521576
step: 2900, Loss: 0.11443997919559479
step: 3000, Loss: 0.11402081698179245
step: 3100, Loss: 0.11359239369630814
step: 3200, Loss: 0.11576071381568909
step: 3300, Loss: 0.113563172519207
step: 3400, Loss: 0.11723820865154266
step: 3500, Loss: 0.1140884980559349
step: 3600, Loss: 0.11451012641191483
step: 3700, Loss: 0.11504054814577103
step: 3800, Loss: 0.11364142596721649
step: 3900, Loss: 0.11471407860517502
step: 4000, Loss: 0.11503860354423523
step: 4100, Loss: 0.11389660835266113
step: 4200, Loss: 0.11587509512901306
step: 4300, Loss: 0.1147746592760086
step: 4400, Loss: 0.11410779505968094
step: 4500, Loss: 0.11405399441719055
step: 4600, Loss: 0.11397955566644669
step: 4700, Loss: 2.1469507217407227
step: 4800, Loss: 0.1470707654953003
step: 4900, Loss: 0.13977624475955963
step: 5000, Loss: 0.12696650624275208
step: 5100, Loss: 0.12456534057855606
step: 5200, Loss: 0.1224265769124031
step: 5300, Loss: 0.12578076124191284
step: 5400, Loss: 0.11938947439193726
step: 5500, Loss: 0.1202530711889267
step: 5600, Loss: 0.11763010174036026
step: 5700, Loss: 0.11773607134819031
step: 5800, Loss: 0.11749240010976791
step: 5900, Loss: 0.12212400138378143
step: 6000, Loss: 0.11720118671655655
step: 6100, Loss: 0.1190592423081398
step: 6200, Loss: 0.11562692373991013
step: 6300, Loss: 0.11596567183732986
step: 6400, Loss: 0.11695459485054016
step: 6500, Loss: 0.11987964808940887
step: 6600, Loss: 0.11730984598398209
step: 6700, Loss: 0.11605747789144516
step: 6800, Loss: 0.11811640858650208
step: 6900, Loss: 0.11679694056510925
step: 7000, Loss: 0.11533739417791367
step: 7100, Loss: 0.115571029484272
step: 7200, Loss: 0.11423517763614655
step: 7300, Loss: 0.11723741888999939
step: 7400, Loss: 0.11570050567388535
step: 7500, Loss: 0.114190012216568
step: 7600, Loss: 0.12104463577270508
step: 7700, Loss: 0.11558207869529724
step: 7800, Loss: 0.11444806307554245
step: 7900, Loss: 0.11443597078323364
step: 8000, Loss: 0.1164800375699997
step: 8100, Loss: 0.11482980847358704
step: 8200, Loss: 0.1204066053032875
step: 8300, Loss: 0.11412197351455688
step: 8400, Loss: 0.11452871561050415
step: 8500, Loss: 0.11627501249313354
step: 8600, Loss: 0.11959157884120941
step: 8700, Loss: 0.11550222337245941
step: 8800, Loss: 0.11362504214048386
step: 8900, Loss: 0.11442840844392776
step: 9000, Loss: 0.11516638100147247
step: 9100, Loss: 0.11416323482990265
step: 9200, Loss: 0.11382964253425598
step: 9300, Loss: 0.11470399051904678
step: 9400, Loss: 0.11405996233224869
step: 9500, Loss: 0.1161675751209259
step: 9600, Loss: 0.11466450244188309
step: 9700, Loss: 0.11438095569610596
step: 9800, Loss: 0.11399402469396591
step: 9900, Loss: 0.11459633708000183
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.75
recall: 0.8823529411764706
F_score: 0.8108108108108107
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.19512450695037842
step: 100, Loss: 0.118155837059021
step: 200, Loss: 0.11919354647397995
step: 300, Loss: 0.11378670483827591
step: 400, Loss: 0.11468562483787537
step: 500, Loss: 0.11520206928253174
step: 600, Loss: 0.11656773835420609
step: 700, Loss: 0.1150144711136818
step: 800, Loss: 0.11534835398197174
step: 900, Loss: 0.11398189514875412
step: 1000, Loss: 0.11448366940021515
step: 1100, Loss: 0.1148938313126564
step: 1200, Loss: 0.11438639461994171
step: 1300, Loss: 0.11458270251750946
step: 1400, Loss: 0.11517795920372009
step: 1500, Loss: 0.114478200674057
step: 1600, Loss: 0.11558123677968979
step: 1700, Loss: 0.11485880613327026
step: 1800, Loss: 0.11434932798147202
step: 1900, Loss: 0.1141677126288414
step: 2000, Loss: 0.11516022682189941
step: 2100, Loss: 0.1139867901802063
step: 2200, Loss: 0.11663878709077835
step: 2300, Loss: 0.11373081803321838
step: 2400, Loss: 0.11663654446601868
step: 2500, Loss: 0.11504343897104263
step: 2600, Loss: 0.11412371695041656
step: 2700, Loss: 0.11563514918088913
step: 2800, Loss: 0.11497162282466888
step: 2900, Loss: 0.11514386534690857
step: 3000, Loss: 0.11582523584365845
step: 3100, Loss: 0.11509399116039276
step: 3200, Loss: 0.11425532400608063
step: 3300, Loss: 0.11392239481210709
step: 3400, Loss: 0.11473081260919571
step: 3500, Loss: 0.1151181310415268
step: 3600, Loss: 0.11535070091485977
step: 3700, Loss: 0.11412878334522247
step: 3800, Loss: 0.11361624300479889
step: 3900, Loss: 0.11342859268188477
step: 4000, Loss: 0.11625571548938751
step: 4100, Loss: 0.11453555524349213
step: 4200, Loss: 0.12697456777095795
step: 4300, Loss: 0.117186538875103
step: 4400, Loss: 0.11504462361335754
step: 4500, Loss: 0.11531095206737518
step: 4600, Loss: 0.11397936940193176
step: 4700, Loss: 0.11457429826259613
step: 4800, Loss: 0.11413079500198364
step: 4900, Loss: 0.11356829106807709
step: 5000, Loss: 0.11514262109994888
step: 5100, Loss: 0.11428151279687881
step: 5200, Loss: 12.011533737182617
step: 5300, Loss: 0.12686502933502197
step: 5400, Loss: 0.12806396186351776
step: 5500, Loss: 0.12187134474515915
step: 5600, Loss: 0.1266188770532608
step: 5700, Loss: 0.12007342278957367
step: 5800, Loss: 0.12372655421495438
step: 5900, Loss: 0.12292681634426117
step: 6000, Loss: 0.12165254354476929
step: 6100, Loss: 0.1269790083169937
step: 6200, Loss: 0.12812253832817078
step: 7900, Loss: 0.11674253642559052
step: 8000, Loss: 0.11393779516220093
step: 8100, Loss: 0.11453261971473694
step: 8200, Loss: 0.1140953078866005
step: 8300, Loss: 0.11333101987838745
step: 8400, Loss: 0.11422845721244812
step: 8500, Loss: 0.1141679584980011
step: 8600, Loss: 0.11322955787181854
step: 8700, Loss: 0.11482574790716171
step: 8800, Loss: 0.11625019460916519
step: 8900, Loss: 0.11472232639789581
step: 9000, Loss: 0.11470375955104828
step: 9100, Loss: 0.19519343972206116
step: 9200, Loss: 0.11476275324821472
step: 9300, Loss: 0.11336866021156311
step: 9400, Loss: 0.1135256364941597
step: 9500, Loss: 0.1136469766497612
step: 9600, Loss: 0.11461307853460312
step: 9700, Loss: 0.1164475679397583
step: 9800, Loss: 0.11374901980161667
step: 9900, Loss: 0.11270506680011749
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 7 Avgacc: 0.9486111111111111 Avgfscore: 0.9499638696557131 
 Max acc:1.0, Max f score:1.0
******** mix subject_8 ********

[361, 399]
******fold 1******

Training... train_data length:684
step: 0, Loss: 44.92323303222656
step: 100, Loss: 4.886035442352295
step: 200, Loss: 1.756402850151062
step: 300, Loss: 0.36894628405570984
step: 400, Loss: 1.0699479579925537
step: 500, Loss: 0.18528971076011658
step: 600, Loss: 0.1859106570482254
step: 700, Loss: 0.16613462567329407
step: 800, Loss: 0.15887200832366943
step: 900, Loss: 0.1593983918428421
step: 1000, Loss: 0.15858528017997742
step: 1100, Loss: 0.15034477412700653
step: 1200, Loss: 0.14685381948947906
step: 1300, Loss: 0.13549494743347168
step: 1400, Loss: 0.1523178517818451
step: 1500, Loss: 0.23634980618953705
step: 1600, Loss: 0.15202724933624268
step: 1700, Loss: 0.1283785104751587
step: 1800, Loss: 0.13746033608913422
step: 1900, Loss: 0.14191538095474243
step: 2000, Loss: 0.14102816581726074
step: 2100, Loss: 0.1398419439792633
step: 2200, Loss: 0.13325603306293488
step: 2300, Loss: 0.13314245641231537
step: 2400, Loss: 0.1417332887649536
step: 2500, Loss: 0.12768496572971344
step: 2600, Loss: 0.13234128057956696
step: 2700, Loss: 0.12793509662151337
step: 2800, Loss: 0.12531116604804993
step: 2900, Loss: 0.13501504063606262
step: 3000, Loss: 0.12517228722572327
step: 3100, Loss: 0.12878501415252686
step: 3200, Loss: 0.12219227850437164
step: 3300, Loss: 0.1227164939045906
step: 3400, Loss: 0.212709441781044
step: 3500, Loss: 0.12729749083518982
step: 3600, Loss: 0.1192890852689743
step: 3700, Loss: 0.12361092865467072
step: 3800, Loss: 0.12163543701171875
step: 3900, Loss: 0.12061306834220886
step: 4000, Loss: 0.12377719581127167
step: 4100, Loss: 0.12083423137664795
step: 4200, Loss: 0.12289561331272125
step: 4300, Loss: 0.12380491197109222
step: 4400, Loss: 0.11946512013673782
step: 4500, Loss: 0.12228989601135254
step: 4600, Loss: 0.11878587305545807
step: 4700, Loss: 0.11691389977931976
step: 4800, Loss: 0.1175861656665802
step: 4900, Loss: 0.11866744607686996
step: 5000, Loss: 0.12346506118774414
step: 5100, Loss: 0.11639207601547241
step: 5200, Loss: 0.11824938654899597
step: 5300, Loss: 0.20124636590480804
step: 5400, Loss: 0.11590886861085892
step: 5500, Loss: 0.11645899713039398
step: 5600, Loss: 0.11700431257486343
step: 5700, Loss: 0.11512267589569092
step: 5800, Loss: 0.11682362109422684
step: 5900, Loss: 0.1177857369184494
step: 6000, Loss: 7.489444732666016
step: 6100, Loss: 0.29482874274253845
step: 6200, Loss: 0.5392267107963562
step: 6300, Loss: 0.21560287475585938
step: 6400, Loss: 0.15297316014766693
step: 6500, Loss: 0.14087489247322083
step: 6600, Loss: 0.14576838910579681
step: 6700, Loss: 0.1296428143978119
step: 6800, Loss: 0.1292102038860321
step: 6900, Loss: 0.14263790845870972
step: 7000, Loss: 0.13216237723827362
step: 7100, Loss: 0.1362336426973343
step: 7200, Loss: 0.21981438994407654
step: 7300, Loss: 0.12859542667865753
step: 7400, Loss: 0.12563595175743103
step: 7500, Loss: 0.1249999925494194
step: 7600, Loss: 0.1268875151872635
step: 7700, Loss: 0.12398318946361542
step: 7800, Loss: 0.12182524800300598
step: 7900, Loss: 0.1269189715385437
step: 8000, Loss: 0.12114808708429337
step: 8100, Loss: 0.1214948445558548
step: 8200, Loss: 0.1268874704837799
step: 8300, Loss: 0.12052935361862183
step: 8400, Loss: 0.12092626094818115
step: 8500, Loss: 0.12538063526153564
step: 8600, Loss: 0.11932189762592316
step: 8700, Loss: 0.12076246738433838
step: 8800, Loss: 0.11991634964942932
step: 8900, Loss: 0.11842664331197739
step: 9000, Loss: 0.12308415025472641
step: 9100, Loss: 0.20397095382213593
step: 9200, Loss: 0.11730605363845825
step: 9300, Loss: 0.1164880022406578
step: 9400, Loss: 0.11873263120651245
step: 9500, Loss: 0.11573103070259094
step: 9600, Loss: 0.11737652868032455
step: 9700, Loss: 0.11575248837471008
step: 9800, Loss: 0.11875800788402557
step: 9900, Loss: 0.11458654701709747
training successfully ended.
validating...
validate data length:76
acc: 0.8055555555555556
precision: 0.7560975609756098
recall: 0.8857142857142857
F_score: 0.8157894736842105
******fold 2******

Training... train_data length:684
step: 0, Loss: 3.845127582550049
step: 100, Loss: 0.16836240887641907
step: 200, Loss: 0.15924738347530365
step: 300, Loss: 0.12685039639472961
step: 400, Loss: 0.12765558063983917
step: 500, Loss: 0.12534648180007935
step: 600, Loss: 0.12464968115091324
step: 700, Loss: 0.1190398558974266
step: 800, Loss: 0.12497701495885849
step: 900, Loss: 0.11926083266735077
step: 1000, Loss: 0.1203916147351265
step: 1100, Loss: 0.12097042798995972
step: 1200, Loss: 0.11750029027462006
step: 1300, Loss: 0.11782561242580414
step: 1400, Loss: 0.1181178092956543
step: 1500, Loss: 0.20635986328125
step: 1600, Loss: 0.11652656644582748
step: 1700, Loss: 8.661882400512695
step: 1800, Loss: 0.17579109966754913
step: 1900, Loss: 0.13780072331428528
step: 2000, Loss: 0.14945998787879944
step: 2100, Loss: 0.13489583134651184
step: 2200, Loss: 0.12414763122797012
step: 2300, Loss: 0.13985230028629303
step: 2400, Loss: 0.1242823600769043
step: 2500, Loss: 0.12943650782108307
step: 2600, Loss: 0.1302727460861206
step: 2700, Loss: 0.1247151643037796
step: 2800, Loss: 0.12172510474920273
step: 2900, Loss: 0.1264227032661438
step: 3000, Loss: 0.12381383031606674
step: 3100, Loss: 0.12311998754739761
step: 3200, Loss: 0.12355220317840576
step: 3300, Loss: 0.11901559680700302
step: 3400, Loss: 0.20920708775520325
step: 3500, Loss: 0.11905360221862793
step: 3600, Loss: 0.12116667628288269
step: 3700, Loss: 0.1171000599861145
step: 3800, Loss: 0.11872734129428864
step: 3900, Loss: 0.11926475167274475
step: 4000, Loss: 0.12119589745998383
step: 4100, Loss: 0.11779238283634186
step: 4200, Loss: 0.12057355791330338
step: 4300, Loss: 0.11859433352947235
step: 4400, Loss: 0.1182488352060318
step: 4500, Loss: 0.1189817562699318
step: 4600, Loss: 0.11888078600168228
step: 4700, Loss: 0.11640077829360962
step: 4800, Loss: 0.11817605048418045
step: 4900, Loss: 0.11802797019481659
step: 5000, Loss: 0.12192559987306595
step: 5100, Loss: 0.11749369651079178
step: 5200, Loss: 0.11567437648773193
step: 5300, Loss: 0.2036360651254654
step: 5400, Loss: 0.11473414301872253
step: 5500, Loss: 0.11532959342002869
step: 5600, Loss: 0.11711471527814865
step: 5700, Loss: 0.11674489080905914
step: 5800, Loss: 0.11635783314704895
step: 5900, Loss: 0.11470478773117065
step: 6000, Loss: 0.11557411402463913
step: 6100, Loss: 0.11541339010000229
step: 6200, Loss: 0.11473404616117477
step: 6300, Loss: 0.11406297981739044
step: 6400, Loss: 0.11761799454689026
step: 6500, Loss: 0.11570687592029572
step: 6600, Loss: 0.11602050811052322
step: 6700, Loss: 0.1156749427318573
step: 6800, Loss: 0.1175173670053482
step: 6900, Loss: 0.115931436419487
step: 7000, Loss: 0.1148713231086731
step: 7100, Loss: 0.11561735719442368
step: 7200, Loss: 0.19917601346969604
step: 7300, Loss: 0.11395762115716934
step: 7400, Loss: 0.12051290273666382
step: 7500, Loss: 0.11469767242670059
step: 7600, Loss: 0.11558828502893448
step: 7700, Loss: 0.11524900794029236
step: 7800, Loss: 0.11424452066421509
step: 7900, Loss: 3.3524789810180664
step: 8000, Loss: 0.15035471320152283
step: 8100, Loss: 0.13682956993579865
step: 6300, Loss: 0.11899228394031525
step: 6400, Loss: 0.11618131399154663
step: 6500, Loss: 0.11840054392814636
step: 6600, Loss: 0.12147589027881622
step: 6700, Loss: 0.11925756931304932
step: 6800, Loss: 0.11690423637628555
step: 6900, Loss: 0.11699599027633667
step: 7000, Loss: 0.118885338306427
step: 7100, Loss: 0.1172678992152214
step: 7200, Loss: 0.11513739079236984
step: 7300, Loss: 0.11571001261472702
step: 7400, Loss: 0.11856891214847565
step: 7500, Loss: 0.11597602814435959
step: 7600, Loss: 0.1147422194480896
step: 7700, Loss: 0.11913607269525528
step: 7800, Loss: 0.11590701341629028
step: 7900, Loss: 0.11572825908660889
step: 8000, Loss: 0.11548031121492386
step: 8100, Loss: 0.1302318274974823
step: 8200, Loss: 0.11595365405082703
step: 8300, Loss: 0.11518317461013794
step: 8400, Loss: 0.11543085426092148
step: 8500, Loss: 0.11581258475780487
step: 8600, Loss: 0.11414740234613419
step: 8700, Loss: 0.1162337213754654
step: 8800, Loss: 0.11588258296251297
step: 8900, Loss: 0.11655395478010178
step: 9000, Loss: 0.11547848582267761
step: 9100, Loss: 0.1150565817952156
step: 9200, Loss: 0.1157134473323822
step: 9300, Loss: 0.11425746977329254
step: 9400, Loss: 0.11421378701925278
step: 9500, Loss: 0.11447492241859436
step: 9600, Loss: 0.1146930605173111
step: 9700, Loss: 0.11689016968011856
step: 9800, Loss: 0.11496222019195557
step: 9900, Loss: 0.11491568386554718
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.25685393810272217
step: 100, Loss: 0.11957873404026031
step: 200, Loss: 0.11670298874378204
step: 300, Loss: 0.11313952505588531
step: 400, Loss: 0.11385461688041687
step: 500, Loss: 0.11472424864768982
step: 600, Loss: 0.11441982537508011
step: 700, Loss: 0.11404608190059662
step: 800, Loss: 0.11483004689216614
step: 900, Loss: 0.11526551097631454
step: 1000, Loss: 0.11450043320655823
step: 1100, Loss: 0.11368581652641296
step: 1200, Loss: 0.1164751872420311
step: 1300, Loss: 0.11414024233818054
step: 1400, Loss: 0.11462679505348206
step: 1500, Loss: 0.11431039869785309
step: 1600, Loss: 0.11619695276021957
step: 1700, Loss: 0.11497395485639572
step: 1800, Loss: 0.11477746814489365
step: 1900, Loss: 0.11388655751943588
step: 2000, Loss: 0.11373892426490784
step: 2100, Loss: 0.11530502140522003
step: 2200, Loss: 0.11356168240308762
step: 2300, Loss: 0.11323036253452301
step: 2400, Loss: 0.11430037021636963
step: 2500, Loss: 0.11287150532007217
step: 2600, Loss: 0.11450738459825516
step: 2700, Loss: 0.1151566207408905
step: 2800, Loss: 0.11381655186414719
step: 2900, Loss: 0.11396215856075287
step: 3000, Loss: 0.11691763997077942
step: 3100, Loss: 0.11450705677270889
step: 3200, Loss: 0.11531940847635269
step: 3300, Loss: 0.11495600640773773
step: 3400, Loss: 0.11424338817596436
step: 3500, Loss: 0.11507859826087952
step: 3600, Loss: 0.11354944109916687
step: 3700, Loss: 0.1145378053188324
step: 3800, Loss: 0.11748430877923965
step: 3900, Loss: 0.11567100882530212
step: 4000, Loss: 0.11584210395812988
step: 4100, Loss: 0.1151527538895607
step: 4200, Loss: 0.11430390179157257
step: 4300, Loss: 0.1142265647649765
step: 4400, Loss: 0.11784718930721283
step: 4500, Loss: 0.1156257838010788
step: 4600, Loss: 0.11598188430070877
step: 4700, Loss: 0.11479900777339935
step: 4800, Loss: 0.1148313581943512
step: 4900, Loss: 0.1162865161895752
step: 5000, Loss: 0.1145339235663414
step: 5100, Loss: 0.11440578103065491
step: 5200, Loss: 0.11438663303852081
step: 5300, Loss: 0.11388910561800003
step: 5400, Loss: 0.11247533559799194
step: 5500, Loss: 0.11489278078079224
step: 5600, Loss: 0.11467568576335907
step: 5700, Loss: 0.14446710050106049
step: 5800, Loss: 0.13014978170394897
step: 5900, Loss: 0.12844550609588623
step: 6000, Loss: 0.12310098111629486
step: 6100, Loss: 0.12314118444919586
step: 6200, Loss: 0.11986984312534332
step: 6300, Loss: 0.11924062669277191
step: 6400, Loss: 0.1171887218952179
step: 6500, Loss: 0.11591250449419022
step: 6600, Loss: 0.11739635467529297
step: 6700, Loss: 0.11543168127536774
step: 6800, Loss: 0.11925021559000015
step: 6900, Loss: 0.11800733208656311
step: 7000, Loss: 0.11446288228034973
step: 7100, Loss: 0.11608397215604782
step: 7200, Loss: 0.12229388952255249
step: 7300, Loss: 0.11429516971111298
step: 7400, Loss: 0.11394958198070526
step: 7500, Loss: 0.11491745710372925
step: 7600, Loss: 0.11596108973026276
step: 7700, Loss: 0.11449432373046875
step: 7800, Loss: 0.11745266616344452
step: 7900, Loss: 0.11507532000541687
step: 8000, Loss: 0.11505931615829468
step: 8100, Loss: 0.11565491557121277
step: 8200, Loss: 0.11469323933124542
step: 8300, Loss: 0.11358469724655151
step: 8400, Loss: 0.11569150537252426
step: 8500, Loss: 0.11524979025125504
step: 8600, Loss: 0.11419066786766052
step: 8700, Loss: 0.11482036113739014
step: 8800, Loss: 0.1146135926246643
step: 8900, Loss: 0.1151670515537262
step: 9000, Loss: 0.11483368277549744
step: 9100, Loss: 0.112870953977108
step: 9200, Loss: 0.11829166114330292
step: 9300, Loss: 0.11450657248497009
step: 9400, Loss: 0.11573050916194916
step: 9500, Loss: 0.11604644358158112
step: 9600, Loss: 0.11554627120494843
step: 9700, Loss: 0.11465740203857422
step: 9800, Loss: 0.11417121440172195
step: 9900, Loss: 0.11571326851844788
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.75
recall: 0.7058823529411765
F_score: 0.7272727272727272
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.1977633237838745
step: 100, Loss: 0.11790059506893158
step: 200, Loss: 0.11659760773181915
step: 300, Loss: 0.11753837764263153
step: 400, Loss: 0.1139376163482666
step: 500, Loss: 0.11345775425434113
step: 600, Loss: 0.11520624160766602
step: 700, Loss: 0.11570829153060913
step: 800, Loss: 0.11909160017967224
step: 900, Loss: 0.11585371196269989
step: 1000, Loss: 0.11513223499059677
step: 1100, Loss: 0.11416793614625931
step: 1200, Loss: 0.11603434383869171
step: 1300, Loss: 0.1127670407295227
step: 1400, Loss: 0.11434175074100494
step: 1500, Loss: 0.11451534926891327
step: 1600, Loss: 0.11585834622383118
step: 1700, Loss: 0.11424459517002106
step: 1800, Loss: 0.11555369943380356
step: 1900, Loss: 0.11410805583000183
step: 2000, Loss: 0.11307450383901596
step: 2100, Loss: 0.11365784704685211
step: 2200, Loss: 0.11452320218086243
step: 2300, Loss: 0.11500169336795807
step: 2400, Loss: 0.1145370677113533
step: 2500, Loss: 0.11407476663589478
step: 2600, Loss: 0.11490175873041153
step: 2700, Loss: 0.11474090069532394
step: 2800, Loss: 0.11589173227548599
step: 2900, Loss: 0.11482919752597809
step: 3000, Loss: 0.11371408402919769
step: 3100, Loss: 0.11444913595914841
step: 3200, Loss: 0.11371444165706635
step: 3300, Loss: 0.11354374885559082
step: 3400, Loss: 0.11498142778873444
step: 3500, Loss: 0.11511065065860748
step: 3600, Loss: 0.11529681086540222
step: 3700, Loss: 0.11410747468471527
step: 3800, Loss: 0.11816136538982391
step: 3900, Loss: 0.11407304555177689
step: 4000, Loss: 0.11355675011873245
step: 4100, Loss: 0.11436465382575989
step: 4200, Loss: 0.11598049849271774
step: 4300, Loss: 0.11421725153923035
step: 4400, Loss: 0.11469347774982452
step: 4500, Loss: 0.11568522453308105
step: 4600, Loss: 0.11531507968902588
step: 4700, Loss: 0.1158425584435463
step: 4800, Loss: 0.11469198018312454
step: 4900, Loss: 0.11520405858755112
step: 5000, Loss: 0.11710374057292938
step: 5100, Loss: 0.11266446113586426
step: 5200, Loss: 0.11410888284444809
step: 5300, Loss: 0.11582830548286438
step: 5400, Loss: 0.6357008814811707
step: 5500, Loss: 0.1736690104007721
step: 5600, Loss: 0.12513092160224915
step: 5700, Loss: 0.1310771107673645
step: 5800, Loss: 0.12332668900489807
step: 5900, Loss: 0.1183168888092041
step: 6000, Loss: 0.11919920891523361
step: 6100, Loss: 0.1214107796549797
step: 6200, Loss: 0.12020229548215866
step: 6300, Loss: 0.11919580399990082
step: 6400, Loss: 0.11725243180990219
step: 6500, Loss: 0.11723611503839493
step: 6600, Loss: 0.11925730109214783
step: 6700, Loss: 0.11578620225191116
step: 6800, Loss: 0.11962295323610306
step: 8200, Loss: 0.13609640300273895
step: 8300, Loss: 0.12903589010238647
step: 8400, Loss: 0.12924541532993317
step: 8500, Loss: 0.12894049286842346
step: 8600, Loss: 0.12597154080867767
step: 8700, Loss: 0.12439426779747009
step: 8800, Loss: 0.11959018558263779
step: 8900, Loss: 0.12477602809667587
step: 9000, Loss: 0.12512525916099548
step: 9100, Loss: 0.21009841561317444
step: 9200, Loss: 0.12531936168670654
step: 9300, Loss: 0.1252511888742447
step: 9400, Loss: 0.12112197279930115
step: 9500, Loss: 0.1197299063205719
step: 9600, Loss: 0.11784670501947403
step: 9700, Loss: 0.12291643023490906
step: 9800, Loss: 0.12267294526100159
step: 9900, Loss: 0.1166272908449173
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9696969696969697
recall: 0.9411764705882353
F_score: 0.955223880597015
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.1220991387963295
step: 100, Loss: 0.12914061546325684
step: 200, Loss: 0.12854284048080444
step: 300, Loss: 0.1234172135591507
step: 400, Loss: 0.11635083705186844
step: 500, Loss: 0.11809831857681274
step: 600, Loss: 0.11793854832649231
step: 700, Loss: 0.11637541651725769
step: 800, Loss: 0.11672519892454147
step: 900, Loss: 0.11986985057592392
step: 1000, Loss: 0.11625578999519348
step: 1100, Loss: 0.11510560661554337
step: 1200, Loss: 0.11659012734889984
step: 1300, Loss: 0.11710351705551147
step: 1400, Loss: 0.11727076768875122
step: 1500, Loss: 0.19656531512737274
step: 1600, Loss: 0.11648262292146683
step: 1700, Loss: 0.11404497921466827
step: 1800, Loss: 0.11433611810207367
step: 1900, Loss: 0.1186145469546318
step: 2000, Loss: 0.11994867026805878
step: 2100, Loss: 0.1143018826842308
step: 2200, Loss: 0.11588418483734131
step: 2300, Loss: 0.11488282680511475
step: 2400, Loss: 0.11528486013412476
step: 2500, Loss: 0.11411948502063751
step: 2600, Loss: 0.11326276510953903
step: 2700, Loss: 0.11485527455806732
step: 2800, Loss: 2.799532175064087
step: 2900, Loss: 0.17356270551681519
step: 3000, Loss: 0.1426609307527542
step: 3100, Loss: 0.13965623080730438
step: 3200, Loss: 0.12471994012594223
step: 3300, Loss: 0.1283450722694397
step: 3400, Loss: 0.21077609062194824
step: 3500, Loss: 0.1210739016532898
step: 3600, Loss: 0.12394330650568008
step: 3700, Loss: 0.12424113601446152
step: 3800, Loss: 0.12062054872512817
step: 3900, Loss: 0.1370992362499237
step: 4000, Loss: 0.11853788048028946
step: 4100, Loss: 0.11950789391994476
step: 4200, Loss: 0.12166392803192139
step: 4300, Loss: 0.11879017949104309
step: 4400, Loss: 0.1256282925605774
step: 4500, Loss: 0.12096653133630753
step: 4600, Loss: 0.11673051118850708
step: 4700, Loss: 0.12530502676963806
step: 4800, Loss: 0.11746075004339218
step: 4900, Loss: 0.11724986135959625
step: 5000, Loss: 0.1183154359459877
step: 5100, Loss: 0.11681082844734192
step: 5200, Loss: 0.11621827632188797
step: 5300, Loss: 0.20161564648151398
step: 5400, Loss: 0.11701656132936478
step: 5500, Loss: 0.11806187033653259
step: 5600, Loss: 0.11537051200866699
step: 5700, Loss: 0.11629703640937805
step: 5800, Loss: 0.11436709761619568
step: 5900, Loss: 0.11498019099235535
step: 6000, Loss: 0.11455047130584717
step: 6100, Loss: 0.11687552183866501
step: 6200, Loss: 0.11488942801952362
step: 6300, Loss: 0.11913834512233734
step: 6400, Loss: 0.11592546105384827
step: 6500, Loss: 0.1155104711651802
step: 6600, Loss: 0.11680721491575241
step: 6700, Loss: 0.11470023542642593
step: 6800, Loss: 0.11417162418365479
step: 6900, Loss: 0.11379489302635193
step: 7000, Loss: 0.11640588939189911
step: 7100, Loss: 0.11406117677688599
step: 7200, Loss: 0.20026415586471558
step: 7300, Loss: 0.11286608874797821
step: 7400, Loss: 0.11438451707363129
step: 7500, Loss: 0.11471650004386902
step: 7600, Loss: 0.11341185122728348
step: 7700, Loss: 0.11486642062664032
step: 7800, Loss: 0.11419215798377991
step: 7900, Loss: 0.11370597779750824
step: 8000, Loss: 0.11463560163974762
step: 8100, Loss: 0.1136355921626091
step: 8200, Loss: 0.11728736758232117
step: 8300, Loss: 0.11527354270219803
step: 8400, Loss: 0.11354877054691315
step: 8500, Loss: 0.1150253564119339
step: 8600, Loss: 0.11472467333078384
step: 8700, Loss: 0.11488604545593262
step: 8800, Loss: 0.11518596112728119
step: 8900, Loss: 0.11332923918962479
step: 9000, Loss: 0.11571963876485825
step: 9100, Loss: 0.1930629014968872
step: 9200, Loss: 0.11323511600494385
step: 9300, Loss: 0.1138731837272644
step: 9400, Loss: 0.11390716582536697
step: 9500, Loss: 0.1137312576174736
step: 9600, Loss: 0.11433076858520508
step: 9700, Loss: 0.11365555971860886
step: 9800, Loss: 0.11435258388519287
step: 9900, Loss: 0.11475050449371338
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.972972972972973
F_score: 0.9863013698630138
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.1239459365606308
step: 100, Loss: 0.12691372632980347
step: 200, Loss: 0.11428193747997284
step: 300, Loss: 0.11448272317647934
step: 400, Loss: 0.11480124294757843
step: 500, Loss: 0.11459210515022278
step: 600, Loss: 0.11487967520952225
step: 700, Loss: 0.11382859200239182
step: 800, Loss: 0.11337171494960785
step: 900, Loss: 0.1142461821436882
step: 1000, Loss: 0.11384417861700058
step: 1100, Loss: 0.11697901040315628
step: 1200, Loss: 0.11491633951663971
step: 1300, Loss: 0.11599265038967133
step: 1400, Loss: 0.11438439041376114
step: 1500, Loss: 0.19377878308296204
step: 1600, Loss: 0.11450085043907166
step: 1700, Loss: 0.113980233669281
step: 1800, Loss: 0.11409503221511841
step: 1900, Loss: 0.11404645442962646
step: 2000, Loss: 0.11400550603866577
step: 2100, Loss: 0.113629050552845
step: 2200, Loss: 0.11392342299222946
step: 2300, Loss: 0.11291678249835968
step: 2400, Loss: 0.11483287811279297
step: 2500, Loss: 0.11383435130119324
step: 2600, Loss: 0.11366279423236847
step: 2700, Loss: 0.11422356963157654
step: 2800, Loss: 0.11324584484100342
step: 2900, Loss: 0.11457005143165588
step: 3000, Loss: 0.11412899196147919
step: 3100, Loss: 0.11393312364816666
step: 3200, Loss: 0.11651085317134857
step: 3300, Loss: 0.11331785470247269
step: 3400, Loss: 0.19503825902938843
step: 3500, Loss: 0.11538869142532349
step: 3600, Loss: 0.11350932717323303
step: 3700, Loss: 0.11387620866298676
step: 3800, Loss: 0.1136290580034256
step: 3900, Loss: 0.11376632750034332
step: 4000, Loss: 0.11302568018436432
step: 4100, Loss: 0.11423684656620026
step: 4200, Loss: 0.11423762887716293
step: 4300, Loss: 0.11536640673875809
step: 4400, Loss: 0.11568810045719147
step: 4500, Loss: 1.1310042142868042
step: 4600, Loss: 0.18099147081375122
step: 4700, Loss: 0.21500679850578308
step: 4800, Loss: 0.12999211251735687
step: 4900, Loss: 0.1310952603816986
step: 5000, Loss: 0.13568998873233795
step: 5100, Loss: 0.1296406388282776
step: 5200, Loss: 0.12867294251918793
step: 5300, Loss: 0.20993921160697937
step: 5400, Loss: 0.12357214838266373
step: 5500, Loss: 0.11851443350315094
step: 5600, Loss: 0.11930689215660095
step: 5700, Loss: 0.12036295980215073
step: 5800, Loss: 0.1212407648563385
step: 5900, Loss: 0.12001343816518784
step: 6000, Loss: 0.12148552387952805
step: 6100, Loss: 0.12000527232885361
step: 6200, Loss: 0.1182403415441513
step: 6300, Loss: 0.1196938157081604
step: 6400, Loss: 0.11997074633836746
step: 6500, Loss: 0.11631336063146591
step: 6600, Loss: 0.12689542770385742
step: 6700, Loss: 0.11728553473949432
step: 6800, Loss: 0.1165817603468895
step: 6900, Loss: 0.11840377748012543
step: 7000, Loss: 0.11649417877197266
step: 7100, Loss: 0.11740371584892273
step: 7200, Loss: 0.2000432014465332
step: 7300, Loss: 0.11587179452180862
step: 7400, Loss: 0.11432567983865738
step: 7500, Loss: 0.11594153940677643
step: 7600, Loss: 0.11467961221933365
step: 7700, Loss: 0.11603300273418427
step: 7800, Loss: 0.11567916721105576
step: 7900, Loss: 0.11659757792949677
step: 8000, Loss: 0.11575295031070709
step: 8100, Loss: 0.11583060771226883
step: 8200, Loss: 0.11517363786697388
step: 8300, Loss: 0.11568175256252289
step: 8400, Loss: 0.11558528244495392
step: 8500, Loss: 0.11762982606887817
step: 8600, Loss: 0.11510640382766724
step: 6900, Loss: 0.11563577502965927
step: 7000, Loss: 0.11676996946334839
step: 7100, Loss: 0.11525830626487732
step: 7200, Loss: 0.11804468929767609
step: 7300, Loss: 0.11406940966844559
step: 7400, Loss: 0.1153903603553772
step: 7500, Loss: 0.1155046671628952
step: 7600, Loss: 0.11837594211101532
step: 7700, Loss: 0.11626523733139038
step: 7800, Loss: 0.11519967019557953
step: 7900, Loss: 0.11425462365150452
step: 8000, Loss: 0.1172940731048584
step: 8100, Loss: 0.11919133365154266
step: 8200, Loss: 0.11368842422962189
step: 8300, Loss: 0.11384323239326477
step: 8400, Loss: 0.11856354027986526
step: 8500, Loss: 0.11382954567670822
step: 8600, Loss: 0.11671213805675507
step: 8700, Loss: 0.11422955244779587
step: 8800, Loss: 0.11519072204828262
step: 8900, Loss: 0.11571822315454483
step: 9000, Loss: 0.11619135737419128
step: 9100, Loss: 0.11582852900028229
step: 9200, Loss: 0.11370104551315308
step: 9300, Loss: 0.11447734385728836
step: 9400, Loss: 0.11420108377933502
step: 9500, Loss: 0.11322422325611115
step: 9600, Loss: 0.1148240864276886
step: 9700, Loss: 0.11448893696069717
step: 9800, Loss: 0.11362645775079727
step: 9900, Loss: 0.11415375769138336
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.6
recall: 0.75
F_score: 0.6666666666666665
subject 7 Avgacc: 0.7456250000000001 Avgfscore: 0.7580902620750818 
 Max acc:0.9, Max f score:0.9032258064516129
******** mix subject_8 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 30.95296859741211
step: 100, Loss: 0.5116583108901978
step: 200, Loss: 0.1456979662179947
step: 300, Loss: 0.12710438668727875
step: 400, Loss: 0.1279820203781128
step: 500, Loss: 0.11894097924232483
step: 600, Loss: 0.1278274953365326
step: 700, Loss: 0.12166941165924072
step: 800, Loss: 0.11938710510730743
step: 900, Loss: 0.11957135051488876
step: 1000, Loss: 0.11625555157661438
step: 1100, Loss: 0.11840474605560303
step: 1200, Loss: 0.11837247014045715
step: 1300, Loss: 0.11740350723266602
step: 1400, Loss: 0.11834202706813812
step: 1500, Loss: 0.11539424955844879
step: 1600, Loss: 0.11694242805242538
step: 1700, Loss: 0.11406032741069794
step: 1800, Loss: 0.11710846424102783
step: 1900, Loss: 0.11903735250234604
step: 2000, Loss: 0.11574310064315796
step: 2100, Loss: 0.11400745809078217
step: 2200, Loss: 0.11351413279771805
step: 2300, Loss: 0.11613963544368744
step: 2400, Loss: 0.11347541958093643
step: 2500, Loss: 0.11729303002357483
step: 2600, Loss: 0.1151345819234848
step: 2700, Loss: 0.1193234920501709
step: 2800, Loss: 0.11580467224121094
step: 2900, Loss: 0.115937739610672
step: 3000, Loss: 0.11674083769321442
step: 3100, Loss: 0.12031841278076172
step: 3200, Loss: 0.11650799959897995
step: 3300, Loss: 0.1168828159570694
step: 3400, Loss: 0.11595848202705383
step: 3500, Loss: 0.11521712690591812
step: 3600, Loss: 0.11527607589960098
step: 3700, Loss: 0.1151556670665741
step: 3800, Loss: 0.11633734405040741
step: 3900, Loss: 0.11403249949216843
step: 4000, Loss: 0.11476150155067444
step: 4100, Loss: 0.11525817215442657
step: 4200, Loss: 0.1156824380159378
step: 4300, Loss: 0.11520978808403015
step: 4400, Loss: 0.11640898883342743
step: 4500, Loss: 0.11906294524669647
step: 4600, Loss: 0.11400704830884933
step: 4700, Loss: 0.11416444182395935
step: 4800, Loss: 0.11341802030801773
step: 4900, Loss: 0.11606910824775696
step: 5000, Loss: 0.11506088078022003
step: 5100, Loss: 0.11440708488225937
step: 5200, Loss: 0.11544342339038849
step: 5300, Loss: 0.11727278679609299
step: 5400, Loss: 0.11442017555236816
step: 5500, Loss: 0.11500082165002823
step: 5600, Loss: 0.12017664313316345
step: 5700, Loss: 0.117762990295887
step: 5800, Loss: 0.11785683035850525
step: 5900, Loss: 0.11669743061065674
step: 6000, Loss: 0.11369797587394714
step: 6100, Loss: 0.11387237161397934
step: 6200, Loss: 0.11449465155601501
step: 6300, Loss: 0.11595173925161362
step: 6400, Loss: 0.11515716463327408
step: 6500, Loss: 0.11776198446750641
step: 6600, Loss: 0.11458531767129898
step: 6700, Loss: 1.1334832906723022
step: 6800, Loss: 0.14384610950946808
step: 6900, Loss: 0.15117309987545013
step: 7000, Loss: 0.13050517439842224
step: 7100, Loss: 0.1353074610233307
step: 7200, Loss: 0.12997928261756897
step: 7300, Loss: 0.13169929385185242
step: 7400, Loss: 0.1270623803138733
step: 7500, Loss: 0.122530996799469
step: 7600, Loss: 0.12134760618209839
step: 7700, Loss: 0.1255417764186859
step: 7800, Loss: 0.11955015361309052
step: 7900, Loss: 0.12041828781366348
step: 8000, Loss: 0.11736515909433365
step: 8100, Loss: 0.12142632156610489
step: 8200, Loss: 0.11669332534074783
step: 8300, Loss: 0.11891055107116699
step: 8400, Loss: 0.1172274500131607
step: 8500, Loss: 0.1228543370962143
step: 8600, Loss: 0.11955609172582626
step: 8700, Loss: 0.11759628355503082
step: 8800, Loss: 0.11771036684513092
step: 8900, Loss: 0.11673027276992798
step: 9000, Loss: 0.11578088998794556
step: 9100, Loss: 0.11593557894229889
step: 9200, Loss: 0.11531262844800949
step: 9300, Loss: 0.11584052443504333
step: 9400, Loss: 0.11584047973155975
step: 9500, Loss: 0.11644266545772552
step: 9600, Loss: 0.11662803590297699
step: 9700, Loss: 0.1150282695889473
step: 9800, Loss: 0.1148647889494896
step: 9900, Loss: 0.11619190871715546
training successfully ended.
validating...
validate data length:32
acc: 0.59375
precision: 0.4666666666666667
recall: 0.5833333333333334
F_score: 0.5185185185185186
******fold 2******

Training... train_data length:280
step: 0, Loss: 1.1081757545471191
step: 100, Loss: 0.1423492729663849
step: 200, Loss: 0.12325191497802734
step: 300, Loss: 0.1580730825662613
step: 400, Loss: 0.13140103220939636
step: 500, Loss: 0.11969113349914551
step: 600, Loss: 0.11966612935066223
step: 700, Loss: 0.1143636554479599
step: 800, Loss: 0.11863158643245697
step: 900, Loss: 0.1216936782002449
step: 1000, Loss: 0.11503512412309647
step: 1100, Loss: 0.1159883588552475
step: 1200, Loss: 0.11336897313594818
step: 1300, Loss: 0.11697482317686081
step: 1400, Loss: 0.11398562043905258
step: 1500, Loss: 0.11556611955165863
step: 1600, Loss: 0.11519923061132431
step: 1700, Loss: 0.11506875604391098
step: 1800, Loss: 0.11644619703292847
step: 1900, Loss: 0.11478204280138016
step: 2000, Loss: 0.11428653448820114
step: 2100, Loss: 0.11539024114608765
step: 2200, Loss: 0.11450610309839249
step: 2300, Loss: 0.11525879800319672
step: 2400, Loss: 0.11538802087306976
step: 2500, Loss: 0.1163775697350502
step: 2600, Loss: 0.11478692293167114
step: 2700, Loss: 0.11708639562129974
step: 2800, Loss: 0.11629213392734528
step: 2900, Loss: 0.11507150530815125
step: 3000, Loss: 0.11529458314180374
step: 3100, Loss: 0.11516909301280975
step: 3200, Loss: 0.11512982845306396
step: 3300, Loss: 0.11739137768745422
step: 3400, Loss: 0.1141386553645134
step: 3500, Loss: 0.11594485491514206
step: 3600, Loss: 0.11521579325199127
step: 3700, Loss: 0.1217845231294632
step: 3800, Loss: 0.11316370964050293
step: 3900, Loss: 0.11514538526535034
step: 4000, Loss: 0.11381251364946365
step: 4100, Loss: 0.1135755330324173
step: 4200, Loss: 0.11546674370765686
step: 4300, Loss: 0.11528480798006058
step: 4400, Loss: 0.11459636688232422
step: 4500, Loss: 0.11483292281627655
step: 4600, Loss: 0.11460933089256287
step: 4700, Loss: 0.11630900949239731
step: 4800, Loss: 0.11370937526226044
step: 4900, Loss: 0.11670038104057312
step: 5000, Loss: 0.11509647220373154
step: 5100, Loss: 0.11469749361276627
step: 5200, Loss: 0.11520954966545105
step: 5300, Loss: 0.11418859660625458
step: 5400, Loss: 0.11509044468402863
step: 5500, Loss: 0.11535213887691498
step: 5600, Loss: 0.11915604025125504
step: 5700, Loss: 0.1176440492272377
step: 5800, Loss: 0.11471904814243317
step: 5900, Loss: 0.1142333447933197
step: 6000, Loss: 0.11745723336935043
step: 6100, Loss: 0.11415287852287292
step: 6200, Loss: 0.11520200222730637
step: 6300, Loss: 0.1153508871793747
step: 6400, Loss: 0.11448406428098679
step: 6500, Loss: 0.11450265347957611
step: 6600, Loss: 0.11409328877925873
step: 6700, Loss: 0.3225504457950592
step: 6800, Loss: 0.17035703361034393
step: 6900, Loss: 0.13624975085258484
step: 7000, Loss: 0.14048977196216583
step: 8700, Loss: 0.1149829626083374
step: 8800, Loss: 0.11587043851613998
step: 8900, Loss: 0.11375357210636139
step: 9000, Loss: 0.1138092651963234
step: 9100, Loss: 0.198269784450531
step: 9200, Loss: 0.11484403163194656
step: 9300, Loss: 0.11470188200473785
step: 9400, Loss: 0.11437004059553146
step: 9500, Loss: 0.11345165967941284
step: 9600, Loss: 0.11309826374053955
step: 9700, Loss: 0.11402381956577301
step: 9800, Loss: 0.117618627846241
step: 9900, Loss: 0.11339430510997772
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9459459459459459
recall: 1.0
F_score: 0.9722222222222222
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.12692151963710785
step: 100, Loss: 0.12003925442695618
step: 200, Loss: 0.11461137235164642
step: 300, Loss: 0.11740025132894516
step: 400, Loss: 0.11434978991746902
step: 500, Loss: 0.11448105424642563
step: 600, Loss: 0.1150003969669342
step: 700, Loss: 0.11680403351783752
step: 800, Loss: 0.11361494660377502
step: 900, Loss: 0.11492116004228592
step: 1000, Loss: 0.1153760701417923
step: 1100, Loss: 0.11540660262107849
step: 1200, Loss: 0.1137521043419838
step: 1300, Loss: 0.11429382860660553
step: 1400, Loss: 0.11509081721305847
step: 1500, Loss: 0.19257019460201263
step: 1600, Loss: 0.11304187029600143
step: 1700, Loss: 0.11425795406103134
step: 1800, Loss: 0.11508932709693909
step: 1900, Loss: 0.11304713785648346
step: 2000, Loss: 0.11328732967376709
step: 2100, Loss: 0.11412560194730759
step: 2200, Loss: 0.11378837376832962
step: 2300, Loss: 0.11411923915147781
step: 2400, Loss: 0.11415421217679977
step: 2500, Loss: 0.1144711822271347
step: 2600, Loss: 0.11387773603200912
step: 2700, Loss: 0.11296534538269043
step: 2800, Loss: 0.11458712816238403
step: 2900, Loss: 0.11393813043832779
step: 3000, Loss: 0.11424130201339722
step: 3100, Loss: 0.11338876932859421
step: 3200, Loss: 0.11411821842193604
step: 3300, Loss: 0.11470365524291992
step: 3400, Loss: 0.1933857947587967
step: 3500, Loss: 0.11401281505823135
step: 3600, Loss: 0.11568716168403625
step: 3700, Loss: 0.11527636647224426
step: 3800, Loss: 0.11471791565418243
step: 3900, Loss: 0.5579538345336914
step: 4000, Loss: 0.16862818598747253
step: 4100, Loss: 0.15112462639808655
step: 4200, Loss: 0.13234242796897888
step: 4300, Loss: 0.13134953379631042
step: 4400, Loss: 0.12615492939949036
step: 4500, Loss: 0.12027253210544586
step: 4600, Loss: 0.12288279086351395
step: 4700, Loss: 0.1250506192445755
step: 4800, Loss: 0.12551507353782654
step: 4900, Loss: 0.12337169051170349
step: 5000, Loss: 0.12029266357421875
step: 5100, Loss: 0.11430790275335312
step: 5200, Loss: 0.11830171942710876
step: 5300, Loss: 0.20732367038726807
step: 5400, Loss: 0.11808660626411438
step: 5500, Loss: 0.11704041063785553
step: 5600, Loss: 0.11786004155874252
step: 5700, Loss: 0.11595973372459412
step: 5800, Loss: 0.12001221626996994
step: 5900, Loss: 0.11498910188674927
step: 6000, Loss: 0.11565778404474258
step: 6100, Loss: 0.11701378226280212
step: 6200, Loss: 0.11661693453788757
step: 6300, Loss: 0.11451497673988342
step: 6400, Loss: 0.1168598160147667
step: 6500, Loss: 0.11520784348249435
step: 6600, Loss: 0.11673572659492493
step: 6700, Loss: 0.11601230502128601
step: 6800, Loss: 0.11736620217561722
step: 6900, Loss: 0.11546540260314941
step: 7000, Loss: 0.11416009068489075
step: 7100, Loss: 0.11629144847393036
step: 7200, Loss: 0.19523116946220398
step: 7300, Loss: 0.11469460278749466
step: 7400, Loss: 0.11476021260023117
step: 7500, Loss: 0.11470505595207214
step: 7600, Loss: 0.11591330170631409
step: 7700, Loss: 0.1143355518579483
step: 7800, Loss: 0.11422700434923172
step: 7900, Loss: 0.11584249138832092
step: 8000, Loss: 0.11441648006439209
step: 8100, Loss: 0.11316843330860138
step: 8200, Loss: 0.11464054137468338
step: 8300, Loss: 0.1136593222618103
step: 8400, Loss: 0.11349911987781525
step: 8500, Loss: 0.11424709111452103
step: 8600, Loss: 0.11494843661785126
step: 8700, Loss: 0.11415387690067291
step: 8800, Loss: 0.11348797380924225
step: 8900, Loss: 0.11460713297128677
step: 9000, Loss: 0.11414504051208496
step: 9100, Loss: 0.19242197275161743
step: 9200, Loss: 0.11357131600379944
step: 9300, Loss: 0.11358105391263962
step: 9400, Loss: 0.1156764030456543
step: 9500, Loss: 0.11335030943155289
step: 9600, Loss: 0.1138235479593277
step: 9700, Loss: 0.11388996243476868
step: 9800, Loss: 0.11374612152576447
step: 9900, Loss: 0.11488452553749084
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9696969696969697
recall: 0.9696969696969697
F_score: 0.9696969696969697
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.12478967756032944
step: 100, Loss: 0.12215940654277802
step: 200, Loss: 0.11431404948234558
step: 300, Loss: 0.11619023978710175
step: 400, Loss: 0.11400198936462402
step: 500, Loss: 0.11520753055810928
step: 600, Loss: 0.11455239355564117
step: 700, Loss: 0.11505234241485596
step: 800, Loss: 0.11488746851682663
step: 900, Loss: 0.11701344698667526
step: 1000, Loss: 0.11599528789520264
step: 1100, Loss: 0.11302919685840607
step: 1200, Loss: 0.11441591382026672
step: 1300, Loss: 0.11310208588838577
step: 1400, Loss: 0.11362189799547195
step: 1500, Loss: 0.19377459585666656
step: 1600, Loss: 0.11438432335853577
step: 1700, Loss: 0.11441382020711899
step: 1800, Loss: 0.11417220532894135
step: 1900, Loss: 0.11424572765827179
step: 2000, Loss: 0.11471007764339447
step: 2100, Loss: 0.1132381483912468
step: 2200, Loss: 0.11377185583114624
step: 2300, Loss: 0.11331536620855331
step: 2400, Loss: 0.11390675604343414
step: 2500, Loss: 0.11423644423484802
step: 2600, Loss: 0.11450368911027908
step: 2700, Loss: 0.11347559094429016
step: 2800, Loss: 0.11371767520904541
step: 2900, Loss: 0.11525662243366241
step: 3000, Loss: 0.11350014805793762
step: 3100, Loss: 0.1140914037823677
step: 3200, Loss: 0.1144387423992157
step: 3300, Loss: 0.1142883375287056
step: 3400, Loss: 0.19829046726226807
step: 3500, Loss: 0.11494958400726318
step: 3600, Loss: 0.11308617144823074
step: 3700, Loss: 0.11434903740882874
step: 3800, Loss: 0.11370289325714111
step: 3900, Loss: 0.1131100207567215
step: 4000, Loss: 0.11391282081604004
step: 4100, Loss: 0.1141107976436615
step: 4200, Loss: 0.11323840171098709
step: 4300, Loss: 0.1143980473279953
step: 4400, Loss: 2.9595048427581787
step: 4500, Loss: 0.18096031248569489
step: 4600, Loss: 0.12966929376125336
step: 4700, Loss: 0.1283080130815506
step: 4800, Loss: 0.12821325659751892
step: 4900, Loss: 0.1267825812101364
step: 5000, Loss: 0.11794214695692062
step: 5100, Loss: 0.12475864589214325
step: 5200, Loss: 0.11856996268033981
step: 5300, Loss: 0.20880357921123505
step: 5400, Loss: 0.1191895455121994
step: 5500, Loss: 0.1193915382027626
step: 5600, Loss: 0.12202619016170502
step: 5700, Loss: 0.1183675080537796
step: 5800, Loss: 0.11844827234745026
step: 5900, Loss: 0.11897128075361252
step: 6000, Loss: 0.1190146654844284
step: 6100, Loss: 0.11706864088773727
step: 6200, Loss: 0.11641892790794373
step: 6300, Loss: 0.11567243188619614
step: 6400, Loss: 0.11750389635562897
step: 6500, Loss: 0.11770488321781158
step: 6600, Loss: 0.11827127635478973
step: 6700, Loss: 0.11571034789085388
step: 6800, Loss: 0.11595628410577774
step: 6900, Loss: 0.11637340486049652
step: 7000, Loss: 0.11656037718057632
step: 7100, Loss: 0.1163550540804863
step: 7200, Loss: 0.2025679349899292
step: 7300, Loss: 0.11533866077661514
step: 7400, Loss: 0.11299946904182434
step: 7500, Loss: 0.11489421129226685
step: 7600, Loss: 0.11619226634502411
step: 7700, Loss: 0.11411380022764206
step: 7800, Loss: 0.11961585283279419
step: 7900, Loss: 0.11514267325401306
step: 8000, Loss: 0.1137779951095581
step: 8100, Loss: 0.11531057953834534
step: 8200, Loss: 0.11582358181476593
step: 8300, Loss: 0.11595673859119415
step: 8400, Loss: 0.11426780372858047
step: 8500, Loss: 0.11568260937929153
step: 8600, Loss: 0.11381692439317703
step: 8700, Loss: 0.11548399180173874
step: 8800, Loss: 0.1141631007194519
step: 8900, Loss: 0.11499720066785812
step: 9000, Loss: 0.11394835263490677
step: 9100, Loss: 0.19704987108707428
step: 7100, Loss: 0.1278449296951294
step: 7200, Loss: 0.13081319630146027
step: 7300, Loss: 0.12584371864795685
step: 7400, Loss: 0.12909376621246338
step: 7500, Loss: 0.12323614209890366
step: 7600, Loss: 0.12183421850204468
step: 7700, Loss: 0.11684946715831757
step: 7800, Loss: 0.121345654129982
step: 7900, Loss: 0.11873950064182281
step: 8000, Loss: 0.1178257018327713
step: 8100, Loss: 0.1199461966753006
step: 8200, Loss: 0.11921294033527374
step: 8300, Loss: 0.11811519414186478
step: 8400, Loss: 0.1155116930603981
step: 8500, Loss: 0.11891428381204605
step: 8600, Loss: 0.1156955286860466
step: 8700, Loss: 0.11912143230438232
step: 8800, Loss: 0.1152569055557251
step: 8900, Loss: 0.11865311861038208
step: 9000, Loss: 0.11514787375926971
step: 9100, Loss: 0.1158958449959755
step: 9200, Loss: 0.12056337296962738
step: 9300, Loss: 0.11784926801919937
step: 9400, Loss: 0.11529853940010071
step: 9500, Loss: 0.11519749462604523
step: 9600, Loss: 0.11667241901159286
step: 9700, Loss: 0.11892759799957275
step: 9800, Loss: 0.11766551434993744
step: 9900, Loss: 0.11472851783037186
training successfully ended.
validating...
validate data length:32
acc: 0.78125
precision: 0.7368421052631579
recall: 0.875
F_score: 0.7999999999999999
******fold 3******

Training... train_data length:281
step: 0, Loss: 3.1897034645080566
step: 100, Loss: 0.12108968198299408
step: 200, Loss: 0.11675988137722015
step: 300, Loss: 0.11570613086223602
step: 400, Loss: 0.1159777119755745
step: 500, Loss: 0.11548951268196106
step: 600, Loss: 0.11864440888166428
step: 700, Loss: 0.1166335716843605
step: 800, Loss: 0.11508236825466156
step: 900, Loss: 0.11347047984600067
step: 1000, Loss: 0.11765936762094498
step: 1100, Loss: 0.11362645030021667
step: 1200, Loss: 0.11867129802703857
step: 1300, Loss: 0.11428151279687881
step: 1400, Loss: 0.11645256727933884
step: 1500, Loss: 0.11577539891004562
step: 1600, Loss: 0.11507688462734222
step: 1700, Loss: 0.11528843641281128
step: 1800, Loss: 0.11535519361495972
step: 1900, Loss: 0.1134130135178566
step: 2000, Loss: 0.11463510990142822
step: 2100, Loss: 0.11387113481760025
step: 2200, Loss: 0.11856015026569366
step: 2300, Loss: 0.11655877530574799
step: 2400, Loss: 0.11404477059841156
step: 2500, Loss: 0.11523930728435516
step: 2600, Loss: 0.11462923139333725
step: 2700, Loss: 0.11545655131340027
step: 2800, Loss: 0.11433586478233337
step: 2900, Loss: 0.11455836147069931
step: 3000, Loss: 0.11400724947452545
step: 3100, Loss: 0.1140640452504158
step: 3200, Loss: 0.11387939751148224
step: 3300, Loss: 0.11328849196434021
step: 3400, Loss: 0.11443779617547989
step: 3500, Loss: 0.11452466249465942
step: 3600, Loss: 0.11412220448255539
step: 3700, Loss: 0.11465182900428772
step: 3800, Loss: 0.11409514397382736
step: 3900, Loss: 0.11254973709583282
step: 4000, Loss: 0.1137014850974083
step: 4100, Loss: 0.11390919238328934
step: 4200, Loss: 0.11482956260442734
step: 4300, Loss: 0.11462227255105972
step: 4400, Loss: 0.12363177537918091
step: 4500, Loss: 0.11364143341779709
step: 4600, Loss: 0.11558770388364792
step: 4700, Loss: 0.11519728600978851
step: 4800, Loss: 0.1138925775885582
step: 4900, Loss: 0.11476726830005646
step: 5000, Loss: 0.11350603401660919
step: 5100, Loss: 0.11598387360572815
step: 5200, Loss: 0.11510299891233444
step: 5300, Loss: 0.11564872413873672
step: 5400, Loss: 0.11418814957141876
step: 5500, Loss: 0.1132194995880127
step: 5600, Loss: 0.1179899051785469
step: 5700, Loss: 0.11318296194076538
step: 5800, Loss: 0.1138337180018425
step: 5900, Loss: 0.1143437922000885
step: 6000, Loss: 0.11422477662563324
step: 6100, Loss: 0.1161658763885498
step: 6200, Loss: 0.11420044302940369
step: 6300, Loss: 0.11526310443878174
step: 6400, Loss: 0.11464390903711319
step: 6500, Loss: 0.7933924198150635
step: 6600, Loss: 0.1337132751941681
step: 6700, Loss: 0.13203591108322144
step: 6800, Loss: 0.12190761417150497
step: 6900, Loss: 0.13688905537128448
step: 7000, Loss: 0.13601282238960266
step: 7100, Loss: 0.1221189945936203
step: 7200, Loss: 0.12202956527471542
step: 7300, Loss: 0.11826111376285553
step: 7400, Loss: 0.12116751074790955
step: 7500, Loss: 0.11602728068828583
step: 7600, Loss: 0.12645213305950165
step: 7700, Loss: 0.11510060727596283
step: 7800, Loss: 0.11596950143575668
step: 7900, Loss: 0.11590690910816193
step: 8000, Loss: 0.12187372148036957
step: 8100, Loss: 0.1146979033946991
step: 8200, Loss: 0.12096099555492401
step: 8300, Loss: 0.11539851874113083
step: 8400, Loss: 0.11742132902145386
step: 8500, Loss: 0.11863535642623901
step: 8600, Loss: 0.11480831354856491
step: 8700, Loss: 0.11652355641126633
step: 8800, Loss: 0.11581341922283173
step: 8900, Loss: 0.1177746057510376
step: 9000, Loss: 0.13092809915542603
step: 9100, Loss: 0.11522656679153442
step: 9200, Loss: 0.1158241331577301
step: 9300, Loss: 0.11704495549201965
step: 9400, Loss: 0.11732248961925507
step: 9500, Loss: 0.11639055609703064
step: 9600, Loss: 0.11697732657194138
step: 9700, Loss: 0.1129027009010315
step: 9800, Loss: 0.113924041390419
step: 9900, Loss: 0.1143839955329895
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7
recall: 0.9333333333333333
F_score: 0.8
******fold 4******

Training... train_data length:281
step: 0, Loss: 2.0427324771881104
step: 100, Loss: 0.116176538169384
step: 200, Loss: 0.11860325932502747
step: 300, Loss: 0.11552317440509796
step: 400, Loss: 0.11598661541938782
step: 500, Loss: 0.11432691663503647
step: 600, Loss: 0.11465366929769516
step: 700, Loss: 0.1133827194571495
step: 800, Loss: 0.11478003114461899
step: 900, Loss: 0.11621837317943573
step: 1000, Loss: 0.11561472713947296
step: 1100, Loss: 0.11492174118757248
step: 1200, Loss: 0.11360669136047363
step: 1300, Loss: 0.11444883793592453
step: 1400, Loss: 0.11733058094978333
step: 1500, Loss: 0.11417901515960693
step: 1600, Loss: 0.11482167989015579
step: 1700, Loss: 0.1161436215043068
step: 1800, Loss: 0.11601615697145462
step: 1900, Loss: 0.11542714387178421
step: 2000, Loss: 0.11668521910905838
step: 2100, Loss: 0.11441463232040405
step: 2200, Loss: 0.11548258364200592
step: 2300, Loss: 0.11414501070976257
step: 2400, Loss: 0.11559393256902695
step: 2500, Loss: 0.11559951305389404
step: 2600, Loss: 0.11418774724006653
step: 2700, Loss: 0.11507883667945862
step: 2800, Loss: 0.11641502380371094
step: 2900, Loss: 0.11483270674943924
step: 3000, Loss: 0.11406173557043076
step: 3100, Loss: 0.11542799323797226
step: 3200, Loss: 0.11505922675132751
step: 3300, Loss: 0.11378541588783264
step: 3400, Loss: 0.11594285070896149
step: 3500, Loss: 0.11531469225883484
step: 3600, Loss: 0.11578691005706787
step: 3700, Loss: 0.11326713860034943
step: 3800, Loss: 0.11753111332654953
step: 3900, Loss: 0.11419349163770676
step: 4000, Loss: 0.11477386951446533
step: 4100, Loss: 0.11433716118335724
step: 4200, Loss: 0.11331579834222794
step: 4300, Loss: 0.11301525682210922
step: 4400, Loss: 0.11415691673755646
step: 4500, Loss: 0.11516440659761429
step: 4600, Loss: 0.11571318656206131
step: 4700, Loss: 0.11415906995534897
step: 4800, Loss: 0.11470552533864975
step: 4900, Loss: 0.11575692147016525
step: 5000, Loss: 0.1145140752196312
step: 5100, Loss: 0.11407754570245743
step: 5200, Loss: 0.1154651790857315
step: 5300, Loss: 0.1161251962184906
step: 5400, Loss: 0.11391527205705643
step: 5500, Loss: 0.11526132375001907
step: 5600, Loss: 0.1138749048113823
step: 5700, Loss: 0.11544056236743927
step: 5800, Loss: 0.1143362894654274
step: 5900, Loss: 0.11489531397819519
step: 6000, Loss: 0.11571764945983887
step: 6100, Loss: 0.11597218364477158
step: 6200, Loss: 0.11560307443141937
step: 6300, Loss: 0.11553952842950821
step: 6400, Loss: 0.1578572690486908
step: 6500, Loss: 0.14909480512142181
step: 6600, Loss: 0.13214071094989777
step: 6700, Loss: 0.1310289204120636
step: 6800, Loss: 0.12174831330776215
step: 6900, Loss: 0.124155193567276
step: 7000, Loss: 0.12055285274982452
step: 7100, Loss: 0.12328529357910156
step: 7200, Loss: 0.12135647237300873
step: 7300, Loss: 0.11739884316921234
step: 7400, Loss: 0.11667363345623016
step: 7500, Loss: 0.11847952008247375
step: 7600, Loss: 0.11567085236310959
step: 9200, Loss: 0.11351227760314941
step: 9300, Loss: 0.11550121754407883
step: 9400, Loss: 0.11315758526325226
step: 9500, Loss: 0.11355897784233093
step: 9600, Loss: 0.11411216109991074
step: 9700, Loss: 0.11296091228723526
step: 9800, Loss: 0.11326958239078522
step: 9900, Loss: 0.11413761973381042
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.967741935483871
recall: 1.0
F_score: 0.9836065573770492
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.1200115978717804
step: 100, Loss: 0.12180699408054352
step: 200, Loss: 0.11754323542118073
step: 300, Loss: 0.11608214676380157
step: 400, Loss: 0.11440824717283249
step: 500, Loss: 0.11671385914087296
step: 600, Loss: 0.11417778581380844
step: 700, Loss: 0.11502177268266678
step: 800, Loss: 0.11419402807950974
step: 900, Loss: 0.11520589888095856
step: 1000, Loss: 0.11630890518426895
step: 1100, Loss: 0.11597712337970734
step: 1200, Loss: 0.11397051066160202
step: 1300, Loss: 0.11435138434171677
step: 1400, Loss: 0.11361571401357651
step: 1500, Loss: 0.19228114187717438
step: 1600, Loss: 0.11464785039424896
step: 1700, Loss: 0.11378920078277588
step: 1800, Loss: 0.114604152739048
step: 1900, Loss: 0.11394388228654861
step: 2000, Loss: 0.11399435997009277
step: 2100, Loss: 0.11303812265396118
step: 2200, Loss: 0.11361102759838104
step: 2300, Loss: 0.11423929035663605
step: 2400, Loss: 0.11403797566890717
step: 2500, Loss: 0.11354485154151917
step: 2600, Loss: 0.11531150341033936
step: 2700, Loss: 0.11410382390022278
step: 2800, Loss: 0.11321478337049484
step: 2900, Loss: 0.11414546519517899
step: 3000, Loss: 0.11361335963010788
step: 3100, Loss: 0.11363354325294495
step: 3200, Loss: 0.1133878231048584
step: 3300, Loss: 0.11369332671165466
step: 3400, Loss: 2.41093111038208
step: 3500, Loss: 0.3685012757778168
step: 3600, Loss: 0.14552056789398193
step: 3700, Loss: 0.13015449047088623
step: 3800, Loss: 0.13446909189224243
step: 3900, Loss: 0.12974828481674194
step: 4000, Loss: 0.12535984814167023
step: 4100, Loss: 0.1213386058807373
step: 4200, Loss: 0.12201815098524094
step: 4300, Loss: 0.1267731934785843
step: 4400, Loss: 0.12625721096992493
step: 4500, Loss: 0.12197577208280563
step: 4600, Loss: 0.1180112212896347
step: 4700, Loss: 0.11978332698345184
step: 4800, Loss: 0.1198367178440094
step: 4900, Loss: 0.11921052634716034
step: 5000, Loss: 0.119595006108284
step: 5100, Loss: 0.11983881145715714
step: 5200, Loss: 0.11751332879066467
step: 5300, Loss: 0.2107589691877365
step: 5400, Loss: 0.11691942811012268
step: 5500, Loss: 0.11500129103660583
step: 5600, Loss: 0.11646191775798798
step: 5700, Loss: 0.11534471064805984
step: 5800, Loss: 0.11722508072853088
step: 5900, Loss: 0.115571528673172
step: 6000, Loss: 0.11453740298748016
step: 6100, Loss: 0.11531241238117218
step: 6200, Loss: 0.11746957153081894
step: 6300, Loss: 0.11604419350624084
step: 6400, Loss: 0.1156286969780922
step: 6500, Loss: 0.11434975266456604
step: 6600, Loss: 0.11686014384031296
step: 6700, Loss: 0.1150740459561348
step: 6800, Loss: 0.11442923545837402
step: 6900, Loss: 0.11548392474651337
step: 7000, Loss: 0.1135292649269104
step: 7100, Loss: 0.11462078988552094
step: 7200, Loss: 0.20187589526176453
step: 7300, Loss: 0.11469588428735733
step: 7400, Loss: 0.1140388697385788
step: 7500, Loss: 0.11403419077396393
step: 7600, Loss: 0.11465667188167572
step: 7700, Loss: 0.11512234061956406
step: 7800, Loss: 0.11398107558488846
step: 7900, Loss: 0.11401402950286865
step: 8000, Loss: 0.11381089687347412
step: 8100, Loss: 0.1127411276102066
step: 8200, Loss: 0.11503621935844421
step: 8300, Loss: 0.1139918714761734
step: 8400, Loss: 0.11380637437105179
step: 8500, Loss: 0.1159670501947403
step: 8600, Loss: 0.11302695423364639
step: 8700, Loss: 0.11361522227525711
step: 8800, Loss: 0.11396524310112
step: 8900, Loss: 0.1139570027589798
step: 9000, Loss: 0.11415122449398041
step: 9100, Loss: 0.19155754148960114
step: 9200, Loss: 0.1133461743593216
step: 9300, Loss: 0.11352148652076721
step: 9400, Loss: 0.114447683095932
step: 9500, Loss: 0.11346593499183655
step: 9600, Loss: 0.1128961369395256
step: 9700, Loss: 0.11413867771625519
step: 9800, Loss: 0.11413391679525375
step: 9900, Loss: 0.11289805918931961
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.96875
F_score: 0.9841269841269841
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11724662780761719
step: 100, Loss: 0.12316052615642548
step: 200, Loss: 0.11488067358732224
step: 300, Loss: 0.11540751904249191
step: 400, Loss: 0.11566200852394104
step: 500, Loss: 0.11584390699863434
step: 600, Loss: 0.11435891687870026
step: 700, Loss: 0.11513633280992508
step: 800, Loss: 0.1157669723033905
step: 900, Loss: 0.11684884876012802
step: 1000, Loss: 0.11520466208457947
step: 1100, Loss: 0.11407475173473358
step: 1200, Loss: 0.11474229395389557
step: 1300, Loss: 0.11347818374633789
step: 1400, Loss: 0.11521286517381668
step: 1500, Loss: 0.1930239349603653
step: 1600, Loss: 0.11311712861061096
step: 1700, Loss: 0.11435374617576599
step: 1800, Loss: 0.11390762776136398
step: 1900, Loss: 0.11513614654541016
step: 2000, Loss: 0.11398415267467499
step: 2100, Loss: 0.11370351165533066
step: 2200, Loss: 0.11266491562128067
step: 2300, Loss: 0.11309316754341125
step: 2400, Loss: 0.11304986476898193
step: 2500, Loss: 0.11482730507850647
step: 2600, Loss: 0.1139521598815918
step: 2700, Loss: 0.11361618340015411
step: 2800, Loss: 0.11377772688865662
step: 2900, Loss: 0.114361971616745
step: 3000, Loss: 0.11554831266403198
step: 3100, Loss: 0.11310078203678131
step: 3200, Loss: 0.11351115256547928
step: 3300, Loss: 0.11309050023555756
step: 3400, Loss: 0.19559013843536377
step: 3500, Loss: 0.11372178047895432
step: 3600, Loss: 1.337790608406067
step: 3700, Loss: 0.19524216651916504
step: 3800, Loss: 0.13573606312274933
step: 3900, Loss: 0.12326501309871674
step: 4000, Loss: 0.12112356722354889
step: 4100, Loss: 0.12755164504051208
step: 4200, Loss: 0.11966690421104431
step: 4300, Loss: 0.12064492702484131
step: 4400, Loss: 0.12215565145015717
step: 4500, Loss: 0.12335430085659027
step: 4600, Loss: 0.12028329819440842
step: 4700, Loss: 0.11718656122684479
step: 4800, Loss: 0.11775058507919312
step: 4900, Loss: 0.12606121599674225
step: 5000, Loss: 0.11619336158037186
step: 5100, Loss: 0.11597779393196106
step: 5200, Loss: 0.11675502359867096
step: 5300, Loss: 0.1999531388282776
step: 5400, Loss: 0.1162940263748169
step: 5500, Loss: 0.1255262792110443
step: 5600, Loss: 0.11570080369710922
step: 5700, Loss: 0.11616776883602142
step: 5800, Loss: 0.11990857124328613
step: 5900, Loss: 0.11636854708194733
step: 6000, Loss: 0.11628297716379166
step: 6100, Loss: 0.11524836719036102
step: 6200, Loss: 0.11608195304870605
step: 6300, Loss: 0.11664523929357529
step: 6400, Loss: 0.11588985472917557
step: 6500, Loss: 0.11462204158306122
step: 6600, Loss: 0.11959844827651978
step: 6700, Loss: 0.11406643688678741
step: 6800, Loss: 0.11448302119970322
step: 6900, Loss: 0.11447560787200928
step: 7000, Loss: 0.11441923677921295
step: 7100, Loss: 0.11723189800977707
step: 7200, Loss: 0.19576451182365417
step: 7300, Loss: 0.11527620255947113
step: 7400, Loss: 0.1141272708773613
step: 7500, Loss: 0.11585691571235657
step: 7600, Loss: 0.11480940878391266
step: 7700, Loss: 0.11539079248905182
step: 7800, Loss: 0.11478756368160248
step: 7900, Loss: 0.11502089351415634
step: 8000, Loss: 0.1150716096162796
step: 8100, Loss: 0.11475890129804611
step: 8200, Loss: 0.1136983335018158
step: 8300, Loss: 0.1136949360370636
step: 8400, Loss: 0.11365026235580444
step: 8500, Loss: 0.11449460685253143
step: 8600, Loss: 0.11471235007047653
step: 8700, Loss: 0.11547396332025528
step: 8800, Loss: 0.1151561513543129
step: 8900, Loss: 0.11411179602146149
step: 9000, Loss: 0.11350651830434799
step: 9100, Loss: 0.19309119880199432
step: 9200, Loss: 0.11326641589403152
step: 9300, Loss: 0.11328798532485962
step: 9400, Loss: 0.11522307246923447
step: 9500, Loss: 0.11601569503545761
step: 9600, Loss: 0.11352614313364029
step: 9700, Loss: 0.11484391242265701
step: 7700, Loss: 0.11953088641166687
step: 7800, Loss: 0.11852074414491653
step: 7900, Loss: 0.1175551488995552
step: 8000, Loss: 0.1177043616771698
step: 8100, Loss: 0.11814886331558228
step: 8200, Loss: 0.12193775177001953
step: 8300, Loss: 0.11604537814855576
step: 8400, Loss: 0.11937562376260757
step: 8500, Loss: 0.11633146554231644
step: 8600, Loss: 0.11824726313352585
step: 8700, Loss: 0.115324005484581
step: 8800, Loss: 0.11609752476215363
step: 8900, Loss: 0.11793680489063263
step: 9000, Loss: 0.11688003689050674
step: 9100, Loss: 0.11496859788894653
step: 9200, Loss: 0.11545217037200928
step: 9300, Loss: 0.11924249678850174
step: 9400, Loss: 0.11534377187490463
step: 9500, Loss: 0.1152350977063179
step: 9600, Loss: 0.11444979906082153
step: 9700, Loss: 0.11483146995306015
step: 9800, Loss: 0.11656811088323593
step: 9900, Loss: 0.1138586774468422
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7857142857142857
recall: 0.7333333333333333
F_score: 0.7586206896551724
******fold 5******

Training... train_data length:281
step: 0, Loss: 1.8137931823730469
step: 100, Loss: 0.1195225641131401
step: 200, Loss: 0.1523512601852417
step: 300, Loss: 0.11964152753353119
step: 400, Loss: 0.1200537234544754
step: 500, Loss: 0.11683785915374756
step: 600, Loss: 0.1180962398648262
step: 700, Loss: 0.11541694402694702
step: 800, Loss: 0.11932521313428879
step: 900, Loss: 0.11593908071517944
step: 1000, Loss: 0.11540903151035309
step: 1100, Loss: 0.1140604242682457
step: 1200, Loss: 0.11604378372430801
step: 1300, Loss: 0.11648896336555481
step: 1400, Loss: 0.11441919207572937
step: 1500, Loss: 0.11527001112699509
step: 1600, Loss: 0.11577770113945007
step: 1700, Loss: 0.1148717999458313
step: 1800, Loss: 0.11663056164979935
step: 1900, Loss: 0.11379152536392212
step: 2000, Loss: 0.11413591355085373
step: 2100, Loss: 0.11530936509370804
step: 2200, Loss: 0.11681792140007019
step: 2300, Loss: 0.1172441765666008
step: 2400, Loss: 0.1172667071223259
step: 2500, Loss: 0.1148553192615509
step: 2600, Loss: 0.11642783880233765
step: 2700, Loss: 0.11447884887456894
step: 2800, Loss: 0.11367493122816086
step: 2900, Loss: 0.11634986847639084
step: 3000, Loss: 0.11572615802288055
step: 3100, Loss: 0.1141037866473198
step: 3200, Loss: 0.11465142667293549
step: 3300, Loss: 0.11412013322114944
step: 3400, Loss: 0.11441212892532349
step: 3500, Loss: 0.11394207924604416
step: 3600, Loss: 0.11450827121734619
step: 3700, Loss: 0.11503076553344727
step: 3800, Loss: 0.11537634581327438
step: 3900, Loss: 0.11545723676681519
step: 4000, Loss: 0.11490639299154282
step: 4100, Loss: 0.11723002046346664
step: 4200, Loss: 0.1167970597743988
step: 4300, Loss: 0.11431624740362167
step: 4400, Loss: 0.11362019181251526
step: 4500, Loss: 0.11561368405818939
step: 4600, Loss: 0.11464579403400421
step: 4700, Loss: 0.11489713191986084
step: 4800, Loss: 0.11505302041769028
step: 4900, Loss: 0.11354637891054153
step: 5000, Loss: 0.11653906106948853
step: 5100, Loss: 0.11488217860460281
step: 5200, Loss: 0.11333296447992325
step: 5300, Loss: 0.11471951752901077
step: 5400, Loss: 0.11355988681316376
step: 5500, Loss: 0.1134185865521431
step: 5600, Loss: 0.11628314852714539
step: 5700, Loss: 0.11452668160200119
step: 5800, Loss: 0.11309666931629181
step: 5900, Loss: 0.20466367900371552
step: 6000, Loss: 0.14239537715911865
step: 6100, Loss: 0.1328330934047699
step: 6200, Loss: 0.1233230009675026
step: 6300, Loss: 0.1259457916021347
step: 6400, Loss: 0.12009279429912567
step: 6500, Loss: 0.11820060759782791
step: 6600, Loss: 0.12204062193632126
step: 6700, Loss: 0.11936190724372864
step: 6800, Loss: 0.11990857869386673
step: 6900, Loss: 0.11445172131061554
step: 7000, Loss: 0.11854027211666107
step: 7100, Loss: 0.1159740537405014
step: 7200, Loss: 0.11622034758329391
step: 7300, Loss: 0.11515715718269348
step: 7400, Loss: 0.11636118590831757
step: 7500, Loss: 0.12549930810928345
step: 7600, Loss: 0.11691978573799133
step: 7700, Loss: 0.11701647192239761
step: 7800, Loss: 0.11575720459222794
step: 7900, Loss: 0.11368446797132492
step: 8000, Loss: 0.1197294145822525
step: 8100, Loss: 0.11480484902858734
step: 8200, Loss: 0.11950323730707169
step: 8300, Loss: 0.11539668589830399
step: 8400, Loss: 0.11724518984556198
step: 8500, Loss: 0.11718827486038208
step: 8600, Loss: 0.11590467393398285
step: 8700, Loss: 0.11463410407304764
step: 8800, Loss: 0.11558216065168381
step: 8900, Loss: 0.11452766507863998
step: 9000, Loss: 0.1163957417011261
step: 9100, Loss: 0.11320507526397705
step: 9200, Loss: 0.1240968182682991
step: 9300, Loss: 0.11406965553760529
step: 9400, Loss: 0.11528530716896057
step: 9500, Loss: 0.11598538607358932
step: 9600, Loss: 0.11624705791473389
step: 9700, Loss: 0.11397957801818848
step: 9800, Loss: 0.1151609942317009
step: 9900, Loss: 0.11701976507902145
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.8571428571428571
recall: 0.6666666666666666
F_score: 0.75
******fold 6******

Training... train_data length:281
step: 0, Loss: 1.8025223016738892
step: 100, Loss: 0.11869531869888306
step: 200, Loss: 0.12063616514205933
step: 300, Loss: 0.11838610470294952
step: 400, Loss: 0.11767661571502686
step: 500, Loss: 0.1156134083867073
step: 600, Loss: 0.11725179851055145
step: 700, Loss: 0.11730757355690002
step: 800, Loss: 0.11853238940238953
step: 900, Loss: 0.11675561964511871
step: 1000, Loss: 0.11765415221452713
step: 1100, Loss: 0.11694448441267014
step: 1200, Loss: 0.11510776728391647
step: 1300, Loss: 0.11478395015001297
step: 1400, Loss: 0.11425166577100754
step: 1500, Loss: 0.11609739065170288
step: 1600, Loss: 0.1141766607761383
step: 1700, Loss: 0.11407919973134995
step: 1800, Loss: 0.12377125769853592
step: 1900, Loss: 0.11441376805305481
step: 2000, Loss: 0.11787644028663635
step: 2100, Loss: 0.11400999873876572
step: 2200, Loss: 0.11395208537578583
step: 2300, Loss: 0.11529874801635742
step: 2400, Loss: 0.11369484663009644
step: 2500, Loss: 0.11421439051628113
step: 2600, Loss: 0.11494728177785873
step: 2700, Loss: 0.11371683329343796
step: 2800, Loss: 0.11389026045799255
step: 2900, Loss: 0.1150883287191391
step: 3000, Loss: 0.11648119240999222
step: 3100, Loss: 0.11442256718873978
step: 3200, Loss: 0.11615979671478271
step: 3300, Loss: 0.11438776552677155
step: 3400, Loss: 0.115633025765419
step: 3500, Loss: 0.11453061550855637
step: 3600, Loss: 0.11475535482168198
step: 3700, Loss: 0.11286651343107224
step: 3800, Loss: 0.11457608640193939
step: 3900, Loss: 0.11393144726753235
step: 4000, Loss: 0.11679219454526901
step: 4100, Loss: 0.1159457117319107
step: 4200, Loss: 0.11834865063428879
step: 4300, Loss: 0.11490266025066376
step: 4400, Loss: 0.1141672432422638
step: 4500, Loss: 0.11423753201961517
step: 4600, Loss: 0.11660341918468475
step: 4700, Loss: 0.11679952591657639
step: 4800, Loss: 0.11492603272199631
step: 4900, Loss: 0.1147083193063736
step: 5000, Loss: 0.1154228001832962
step: 5100, Loss: 0.11558401584625244
step: 5200, Loss: 0.11311507225036621
step: 5300, Loss: 0.11356811225414276
step: 5400, Loss: 0.11566240340471268
step: 5500, Loss: 0.11477680504322052
step: 5600, Loss: 0.1131620779633522
step: 5700, Loss: 0.1161881759762764
step: 5800, Loss: 0.11803144961595535
step: 5900, Loss: 0.11647423356771469
step: 6000, Loss: 0.11535997688770294
step: 6100, Loss: 0.11414692550897598
step: 6200, Loss: 0.2703940272331238
step: 6300, Loss: 0.17832723259925842
step: 6400, Loss: 0.1486700177192688
step: 6500, Loss: 0.13102418184280396
step: 6600, Loss: 0.12579457461833954
step: 6700, Loss: 0.1235961839556694
step: 6800, Loss: 0.12541423738002777
step: 6900, Loss: 0.12417526543140411
step: 7000, Loss: 0.12554126977920532
step: 7100, Loss: 0.12348290532827377
step: 7200, Loss: 0.1286332756280899
step: 7300, Loss: 0.12010038644075394
step: 7400, Loss: 0.12024208903312683
step: 7500, Loss: 0.11646554619073868
step: 7600, Loss: 0.12070568650960922
step: 7700, Loss: 0.11981216073036194
step: 7800, Loss: 0.11889521777629852
step: 7900, Loss: 0.1208074763417244
step: 8000, Loss: 0.12405996769666672
step: 8100, Loss: 0.11722570657730103
step: 9800, Loss: 0.11333159357309341
step: 9900, Loss: 0.11385490000247955
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9393939393939394
recall: 1.0
F_score: 0.96875
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.12379424273967743
step: 100, Loss: 0.1161847934126854
step: 200, Loss: 0.11639384925365448
step: 300, Loss: 0.11585308611392975
step: 400, Loss: 0.11381351202726364
step: 500, Loss: 0.11710869520902634
step: 600, Loss: 0.11511264741420746
step: 700, Loss: 0.11477817595005035
step: 800, Loss: 0.11436481028795242
step: 900, Loss: 0.11378547549247742
step: 1000, Loss: 0.11457152664661407
step: 1100, Loss: 0.11688251793384552
step: 1200, Loss: 0.11333538591861725
step: 1300, Loss: 0.11360004544258118
step: 1400, Loss: 0.11467130482196808
step: 1500, Loss: 0.19467633962631226
step: 1600, Loss: 0.11351772397756577
step: 1700, Loss: 0.11354406177997589
step: 1800, Loss: 0.1133059561252594
step: 1900, Loss: 0.11589737236499786
step: 2000, Loss: 0.11443831771612167
step: 2100, Loss: 0.11398735642433167
step: 2200, Loss: 0.11327039450407028
step: 2300, Loss: 0.11257890611886978
step: 2400, Loss: 0.11259075254201889
step: 2500, Loss: 0.11358219385147095
step: 2600, Loss: 0.11355719715356827
step: 2700, Loss: 0.1142202764749527
step: 2800, Loss: 0.114857017993927
step: 2900, Loss: 0.11481837928295135
step: 3000, Loss: 0.1140616238117218
step: 3100, Loss: 0.1155087873339653
step: 3200, Loss: 0.11405983567237854
step: 3300, Loss: 0.11392464488744736
step: 3400, Loss: 0.20078632235527039
step: 3500, Loss: 0.11374573409557343
step: 3600, Loss: 0.11411377787590027
step: 3700, Loss: 0.11454817652702332
step: 3800, Loss: 0.11353274434804916
step: 3900, Loss: 0.11397812515497208
step: 4000, Loss: 0.1136406883597374
step: 4100, Loss: 0.11507828533649445
step: 4200, Loss: 0.11514715105295181
step: 4300, Loss: 10.72596263885498
step: 4400, Loss: 2.1916449069976807
step: 4500, Loss: 0.14393824338912964
step: 4600, Loss: 0.1279049515724182
step: 4700, Loss: 0.12350878119468689
step: 4800, Loss: 0.1272808462381363
step: 4900, Loss: 0.12526699900627136
step: 5000, Loss: 0.12291628867387772
step: 5100, Loss: 0.12114705890417099
step: 5200, Loss: 0.12297225743532181
step: 5300, Loss: 0.20247414708137512
step: 5400, Loss: 0.12208317965269089
step: 5500, Loss: 0.11793231964111328
step: 5600, Loss: 0.12068255245685577
step: 5700, Loss: 0.12087274342775345
step: 5800, Loss: 0.11621668934822083
step: 5900, Loss: 0.11718717217445374
step: 6000, Loss: 0.11684629321098328
step: 6100, Loss: 0.11668401211500168
step: 6200, Loss: 0.11877325922250748
step: 6300, Loss: 0.11729738116264343
step: 6400, Loss: 0.11928039789199829
step: 6500, Loss: 0.11640635132789612
step: 6600, Loss: 0.1178237795829773
step: 6700, Loss: 0.1186952292919159
step: 6800, Loss: 0.11585773527622223
step: 6900, Loss: 0.11709140241146088
step: 7000, Loss: 0.11538221687078476
step: 7100, Loss: 0.1159762591123581
step: 7200, Loss: 0.1964382529258728
step: 7300, Loss: 0.11432743817567825
step: 7400, Loss: 0.11388346552848816
step: 7500, Loss: 0.1166049912571907
step: 7600, Loss: 0.11697294563055038
step: 7700, Loss: 0.11478593200445175
step: 7800, Loss: 0.11687041074037552
step: 7900, Loss: 0.1147012785077095
step: 8000, Loss: 0.1136820912361145
step: 8100, Loss: 0.1156679317355156
step: 8200, Loss: 0.1170976385474205
step: 8300, Loss: 0.11527764797210693
step: 8400, Loss: 0.11489756405353546
step: 8500, Loss: 0.1152394711971283
step: 8600, Loss: 0.11778971552848816
step: 8700, Loss: 0.11509186774492264
step: 8800, Loss: 0.11507414281368256
step: 8900, Loss: 0.1135171577334404
step: 9000, Loss: 0.11440415680408478
step: 9100, Loss: 0.1948740929365158
step: 9200, Loss: 0.11453213542699814
step: 9300, Loss: 0.11434737592935562
step: 9400, Loss: 0.11435621231794357
step: 9500, Loss: 0.11303219199180603
step: 9600, Loss: 0.114564910531044
step: 9700, Loss: 0.11374077945947647
step: 9800, Loss: 0.11451582610607147
step: 9900, Loss: 0.11441955715417862
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.95
recall: 1.0
F_score: 0.9743589743589743
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11836203932762146
step: 100, Loss: 0.11888039112091064
step: 200, Loss: 0.1167074665427208
step: 300, Loss: 0.11557082086801529
step: 400, Loss: 0.11553467065095901
step: 500, Loss: 0.11422595381736755
step: 600, Loss: 0.11491572856903076
step: 700, Loss: 0.11478730291128159
step: 800, Loss: 0.11940088868141174
step: 900, Loss: 0.11407563090324402
step: 1000, Loss: 0.11537119746208191
step: 1100, Loss: 0.11346688866615295
step: 1200, Loss: 0.11474418640136719
step: 1300, Loss: 0.11304886639118195
step: 1400, Loss: 0.11397891491651535
step: 1500, Loss: 0.19262845814228058
step: 1600, Loss: 0.11504640430212021
step: 1700, Loss: 0.11505234241485596
step: 1800, Loss: 0.11351317167282104
step: 1900, Loss: 0.114456407725811
step: 2000, Loss: 0.1147337406873703
step: 2100, Loss: 0.11363022029399872
step: 2200, Loss: 0.11363989114761353
step: 2300, Loss: 0.11294335126876831
step: 2400, Loss: 0.11437103152275085
step: 2500, Loss: 0.11352560669183731
step: 2600, Loss: 0.11387185007333755
step: 2700, Loss: 0.11456819623708725
step: 2800, Loss: 0.11370411515235901
step: 2900, Loss: 0.11582838743925095
step: 3000, Loss: 0.11419611424207687
step: 3100, Loss: 0.1138887032866478
step: 3200, Loss: 0.11620593070983887
step: 3300, Loss: 0.11372464895248413
step: 3400, Loss: 0.1944400668144226
step: 3500, Loss: 0.11441325396299362
step: 3600, Loss: 0.11456073820590973
step: 3700, Loss: 0.11540057510137558
step: 3800, Loss: 0.11369702219963074
step: 3900, Loss: 0.11342624574899673
step: 4000, Loss: 0.11448308080434799
step: 4100, Loss: 0.11399264633655548
step: 4200, Loss: 0.11648134887218475
step: 4300, Loss: 0.1129772737622261
step: 4400, Loss: 3.5229434967041016
step: 4500, Loss: 0.30688348412513733
step: 4600, Loss: 0.130903959274292
step: 4700, Loss: 0.13798007369041443
step: 4800, Loss: 0.12175711989402771
step: 4900, Loss: 0.12958751618862152
step: 5000, Loss: 0.12906894087791443
step: 5100, Loss: 0.1223883107304573
step: 5200, Loss: 0.1303972750902176
step: 5300, Loss: 0.20725546777248383
step: 5400, Loss: 0.1180410087108612
step: 5500, Loss: 0.11903320997953415
step: 5600, Loss: 0.11848214268684387
step: 5700, Loss: 0.11773604154586792
step: 5800, Loss: 0.11904562264680862
step: 5900, Loss: 0.11778941750526428
step: 6000, Loss: 0.11852079629898071
step: 6100, Loss: 0.1185004860162735
step: 6200, Loss: 0.11764977872371674
step: 6300, Loss: 0.11857695877552032
step: 6400, Loss: 0.11702904850244522
step: 6500, Loss: 0.1154499351978302
step: 6600, Loss: 0.1160520538687706
step: 6700, Loss: 0.11614993214607239
step: 6800, Loss: 0.11769585311412811
step: 6900, Loss: 0.11552530527114868
step: 7000, Loss: 0.11564090102910995
step: 7100, Loss: 0.11722216755151749
step: 7200, Loss: 0.19867843389511108
step: 7300, Loss: 0.11667964607477188
step: 7400, Loss: 0.11480355262756348
step: 7500, Loss: 0.11738105863332748
step: 7600, Loss: 0.11427835375070572
step: 7700, Loss: 0.11507433652877808
step: 7800, Loss: 0.11417479813098907
step: 7900, Loss: 0.1158534586429596
step: 8000, Loss: 0.11482759565114975
step: 8100, Loss: 0.11649920046329498
step: 8200, Loss: 0.11581408977508545
step: 8300, Loss: 0.1143016666173935
step: 8400, Loss: 0.11575202643871307
step: 8500, Loss: 0.11363935470581055
step: 8600, Loss: 0.11488226801156998
step: 8700, Loss: 0.11392435431480408
step: 8800, Loss: 0.11358518153429031
step: 8900, Loss: 0.11443167924880981
step: 9000, Loss: 0.11389429122209549
step: 9100, Loss: 0.1948070228099823
step: 9200, Loss: 0.1132802814245224
step: 9300, Loss: 0.11393905431032181
step: 9400, Loss: 0.11521193385124207
step: 9500, Loss: 0.1150440201163292
step: 9600, Loss: 0.11344189941883087
step: 9700, Loss: 0.11375148594379425
step: 9800, Loss: 0.11429362744092941
step: 9900, Loss: 0.11317272484302521
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.967741935483871
recall: 1.0
F_score: 0.9836065573770492
step: 8200, Loss: 0.11871647089719772
step: 8300, Loss: 0.11464297026395798
step: 8400, Loss: 0.11772062629461288
step: 8500, Loss: 0.11580494791269302
step: 8600, Loss: 0.11548058688640594
step: 8700, Loss: 0.11363373696804047
step: 8800, Loss: 0.1175042986869812
step: 8900, Loss: 0.11536549776792526
step: 9000, Loss: 0.1142750084400177
step: 9100, Loss: 0.11459244787693024
step: 9200, Loss: 0.11891649663448334
step: 9300, Loss: 0.11359600722789764
step: 9400, Loss: 0.11913520097732544
step: 9500, Loss: 0.11424384266138077
step: 9600, Loss: 0.11691160500049591
step: 9700, Loss: 0.11483855545520782
step: 9800, Loss: 0.1270875632762909
step: 9900, Loss: 0.11407405138015747
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8095238095238095
recall: 0.9444444444444444
F_score: 0.8717948717948718
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.12266720831394196
step: 100, Loss: 0.11811073124408722
step: 200, Loss: 0.11692677438259125
step: 300, Loss: 0.11510435491800308
step: 400, Loss: 0.11921241879463196
step: 500, Loss: 0.11546662449836731
step: 600, Loss: 0.1143442839384079
step: 700, Loss: 0.11530227959156036
step: 800, Loss: 0.11624914407730103
step: 900, Loss: 0.11541742831468582
step: 1000, Loss: 0.11547708511352539
step: 1100, Loss: 0.11849010735750198
step: 1200, Loss: 0.11634699255228043
step: 1300, Loss: 0.11450868099927902
step: 1400, Loss: 0.11405797302722931
step: 1500, Loss: 0.11390559375286102
step: 1600, Loss: 0.11742109805345535
step: 1700, Loss: 0.1146281287074089
step: 1800, Loss: 0.11527484655380249
step: 1900, Loss: 0.11493369191884995
step: 2000, Loss: 0.11481691896915436
step: 2100, Loss: 0.11672703921794891
step: 2200, Loss: 0.11619634926319122
step: 2300, Loss: 0.1147562637925148
step: 2400, Loss: 0.11555822193622589
step: 2500, Loss: 0.11576872318983078
step: 2600, Loss: 0.11837050318717957
step: 2700, Loss: 0.1130727082490921
step: 2800, Loss: 0.11398555338382721
step: 2900, Loss: 0.11406055837869644
step: 3000, Loss: 0.1128111332654953
step: 3100, Loss: 0.1152801588177681
step: 3200, Loss: 0.11489766836166382
step: 3300, Loss: 0.11421229690313339
step: 3400, Loss: 0.11584778130054474
step: 3500, Loss: 0.11390349268913269
step: 3600, Loss: 0.11763536930084229
step: 3700, Loss: 0.11581642925739288
step: 3800, Loss: 0.11597926914691925
step: 3900, Loss: 0.1153605729341507
step: 4000, Loss: 0.1156173050403595
step: 4100, Loss: 0.11441119015216827
step: 4200, Loss: 0.11509937047958374
step: 4300, Loss: 0.11475089192390442
step: 4400, Loss: 0.11431679874658585
step: 4500, Loss: 0.11378192156553268
step: 4600, Loss: 0.11323823034763336
step: 4700, Loss: 0.11519129574298859
step: 4800, Loss: 0.11545539647340775
step: 4900, Loss: 0.11671540886163712
step: 5000, Loss: 0.11527380347251892
step: 5100, Loss: 0.11515104025602341
step: 5200, Loss: 0.1170601025223732
step: 5300, Loss: 1.356114387512207
step: 5400, Loss: 0.14156121015548706
step: 5500, Loss: 0.12645021080970764
step: 5600, Loss: 0.12415078282356262
step: 5700, Loss: 0.11743276566267014
step: 5800, Loss: 0.1233757957816124
step: 5900, Loss: 0.11657150834798813
step: 6000, Loss: 0.11735807359218597
step: 6100, Loss: 0.11542822420597076
step: 6200, Loss: 0.11788345873355865
step: 6300, Loss: 0.11875954270362854
step: 6400, Loss: 0.1148708388209343
step: 6500, Loss: 0.11571837216615677
step: 6600, Loss: 0.11444934457540512
step: 6700, Loss: 0.11539222300052643
step: 6800, Loss: 0.11603762954473495
step: 6900, Loss: 0.11659082770347595
step: 7000, Loss: 0.1164369285106659
step: 7100, Loss: 0.1152769923210144
step: 7200, Loss: 0.11567439883947372
step: 7300, Loss: 0.11642023175954819
step: 7400, Loss: 0.1162487268447876
step: 7500, Loss: 0.1181284636259079
step: 7600, Loss: 0.11772651970386505
step: 7700, Loss: 0.1169394701719284
step: 7800, Loss: 0.11549204587936401
step: 7900, Loss: 0.11419127136468887
step: 8000, Loss: 0.1172415092587471
step: 8100, Loss: 0.11471940577030182
step: 8200, Loss: 0.11431992053985596
step: 8300, Loss: 0.11426761746406555
step: 8400, Loss: 0.1161709651350975
step: 8500, Loss: 0.1128905862569809
step: 8600, Loss: 0.11732123792171478
step: 8700, Loss: 0.11558304727077484
step: 8800, Loss: 0.11496258527040482
step: 8900, Loss: 0.11437021195888519
step: 9000, Loss: 0.1159633994102478
step: 9100, Loss: 0.11396245658397675
step: 9200, Loss: 0.11599141359329224
step: 9300, Loss: 0.11481326818466187
step: 9400, Loss: 0.11483065783977509
step: 9500, Loss: 0.11573181301355362
step: 9600, Loss: 0.11444614082574844
step: 9700, Loss: 0.1145719587802887
step: 9800, Loss: 0.11811722815036774
step: 9900, Loss: 0.11431884765625
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9333333333333333
recall: 0.8235294117647058
F_score: 0.8749999999999999
******fold 8******

Training... train_data length:281
step: 0, Loss: 3.482926845550537
step: 100, Loss: 0.11561156809329987
step: 200, Loss: 0.1152227520942688
step: 300, Loss: 0.11546225845813751
step: 400, Loss: 0.11486706137657166
step: 500, Loss: 0.11450189352035522
step: 600, Loss: 0.11597362160682678
step: 700, Loss: 0.11524242162704468
step: 800, Loss: 0.11569486558437347
step: 900, Loss: 0.11415599286556244
step: 1000, Loss: 0.11320727318525314
step: 1100, Loss: 0.11445159465074539
step: 1200, Loss: 0.11450272798538208
step: 1300, Loss: 0.11435628682374954
step: 1400, Loss: 0.11540583521127701
step: 1500, Loss: 0.11466819792985916
step: 1600, Loss: 0.11369232088327408
step: 1700, Loss: 0.11371461302042007
step: 1800, Loss: 0.11607514321804047
step: 1900, Loss: 0.11395672708749771
step: 2000, Loss: 0.11368060111999512
step: 2100, Loss: 0.11414484679698944
step: 2200, Loss: 0.11561775207519531
step: 2300, Loss: 0.11379380524158478
step: 2400, Loss: 0.11368180811405182
step: 2500, Loss: 0.11593780666589737
step: 2600, Loss: 0.11470632255077362
step: 2700, Loss: 0.1154136136174202
step: 2800, Loss: 0.11382579803466797
step: 2900, Loss: 0.11335183680057526
step: 3000, Loss: 0.1157342866063118
step: 3100, Loss: 0.11322294175624847
step: 3200, Loss: 0.11792408674955368
step: 3300, Loss: 0.11912261694669724
step: 3400, Loss: 0.11390082538127899
step: 3500, Loss: 0.11398951709270477
step: 3600, Loss: 0.11531583964824677
step: 3700, Loss: 0.11402129381895065
step: 3800, Loss: 0.11525893211364746
step: 3900, Loss: 0.11297650635242462
step: 4000, Loss: 0.11812374740839005
step: 4100, Loss: 0.11573795229196548
step: 4200, Loss: 0.11419191211462021
step: 4300, Loss: 0.1150669977068901
step: 4400, Loss: 0.1137440875172615
step: 4500, Loss: 0.11451602727174759
step: 4600, Loss: 0.11360691487789154
step: 4700, Loss: 0.11266172677278519
step: 4800, Loss: 0.11560265719890594
step: 4900, Loss: 0.11398737132549286
step: 5000, Loss: 0.11447399109601974
step: 5100, Loss: 0.11357824504375458
step: 5200, Loss: 0.1140589565038681
step: 5300, Loss: 0.11411376297473907
step: 5400, Loss: 0.11555942893028259
step: 5500, Loss: 0.11529332399368286
step: 5600, Loss: 0.11340275406837463
step: 5700, Loss: 0.11675537377595901
step: 5800, Loss: 0.11340407282114029
step: 5900, Loss: 0.11586374044418335
step: 6000, Loss: 3.3116180896759033
step: 6100, Loss: 0.13702617585659027
step: 6200, Loss: 0.130630224943161
step: 6300, Loss: 0.1275668740272522
step: 6400, Loss: 0.1306363046169281
step: 6500, Loss: 0.12542854249477386
step: 6600, Loss: 0.11643287539482117
step: 6700, Loss: 0.12201808393001556
step: 6800, Loss: 0.11622738093137741
step: 6900, Loss: 0.11781233549118042
step: 7000, Loss: 0.11773919314146042
step: 7100, Loss: 0.1176019087433815
step: 7200, Loss: 0.11547893285751343
step: 7300, Loss: 0.1168440580368042
step: 7400, Loss: 0.11611094325780869
step: 7500, Loss: 0.11733942478895187
step: 7600, Loss: 0.1161460131406784
step: 7700, Loss: 0.116002157330513
step: 7800, Loss: 0.11758418381214142
step: 7900, Loss: 0.1174832433462143
step: 8000, Loss: 0.11539123952388763
step: 8100, Loss: 0.11744627356529236
step: 8200, Loss: 0.11380041390657425
step: 8300, Loss: 0.11645966023206711
step: 8400, Loss: 0.11453849822282791
step: 8500, Loss: 0.11434300243854523
step: 8600, Loss: 0.11718538403511047
subject 8 Avgacc: 0.9597222222222221 Avgfscore: 0.9593682989303488 
 Max acc:0.9861111111111112, Max f score:0.9863013698630138
******** mix subject_9 ********

[266, 494]
******fold 1******

Training... train_data length:889
step: 0, Loss: 48.17172622680664
step: 100, Loss: 3.3833255767822266
step: 200, Loss: 0.24666759371757507
step: 300, Loss: 0.26835525035858154
step: 400, Loss: 0.17871922254562378
step: 500, Loss: 0.17584800720214844
step: 600, Loss: 0.17779792845249176
step: 700, Loss: 0.16007398068904877
step: 800, Loss: 0.14635412395000458
step: 900, Loss: 0.1711033135652542
step: 1000, Loss: 0.15597021579742432
step: 1100, Loss: 0.15328066051006317
step: 1200, Loss: 0.1497008204460144
step: 1300, Loss: 0.14420200884342194
step: 1400, Loss: 0.13825097680091858
step: 1500, Loss: 0.15648692846298218
step: 1600, Loss: 0.1433112621307373
step: 1700, Loss: 0.1372685581445694
step: 1800, Loss: 0.13550956547260284
step: 1900, Loss: 0.13212206959724426
step: 2000, Loss: 0.12891845405101776
step: 2100, Loss: 0.14020369946956635
step: 2200, Loss: 0.1413118839263916
step: 2300, Loss: 0.13256633281707764
step: 2400, Loss: 0.12946796417236328
step: 2500, Loss: 0.12133683264255524
step: 2600, Loss: 0.12775465846061707
step: 2700, Loss: 0.13274839520454407
step: 2800, Loss: 0.1328887641429901
step: 2900, Loss: 0.12436781823635101
step: 3000, Loss: 0.12373882532119751
step: 3100, Loss: 0.11826922744512558
step: 3200, Loss: 0.12038837373256683
step: 3300, Loss: 0.12368935346603394
step: 3400, Loss: 0.1252027153968811
step: 3500, Loss: 0.12122146785259247
step: 3600, Loss: 0.11868368834257126
step: 3700, Loss: 0.11482390016317368
step: 3800, Loss: 0.11890658736228943
step: 3900, Loss: 0.12115740031003952
step: 4000, Loss: 0.1208055317401886
step: 4100, Loss: 0.11700978130102158
step: 4200, Loss: 0.11510032415390015
step: 4300, Loss: 0.11606424301862717
step: 4400, Loss: 0.12026774138212204
step: 4500, Loss: 0.11816218495368958
step: 4600, Loss: 5.590446472167969
step: 4700, Loss: 0.19472883641719818
step: 4800, Loss: 0.14096355438232422
step: 4900, Loss: 0.139484241604805
step: 5000, Loss: 0.14429530501365662
step: 5100, Loss: 0.15960268676280975
step: 5200, Loss: 0.13932691514492035
step: 5300, Loss: 0.12618756294250488
step: 5400, Loss: 0.13206559419631958
step: 5500, Loss: 0.12427031248807907
step: 5600, Loss: 0.1269637644290924
step: 5700, Loss: 0.14279480278491974
step: 5800, Loss: 0.13220471143722534
step: 5900, Loss: 0.12516656517982483
step: 6000, Loss: 0.12240494042634964
step: 6100, Loss: 0.120685875415802
step: 6200, Loss: 0.12310308963060379
step: 6300, Loss: 0.1374940127134323
step: 6400, Loss: 0.12594011425971985
step: 6500, Loss: 0.12156976759433746
step: 6600, Loss: 0.11581972241401672
step: 6700, Loss: 0.11934522539377213
step: 6800, Loss: 0.12159749120473862
step: 6900, Loss: 0.13466691970825195
step: 7000, Loss: 0.12862640619277954
step: 7100, Loss: 0.1202736422419548
step: 7200, Loss: 0.1147950068116188
step: 7300, Loss: 0.11813968420028687
step: 7400, Loss: 0.12102848291397095
step: 7500, Loss: 0.12337380647659302
step: 7600, Loss: 0.11814890056848526
step: 7700, Loss: 0.11434637755155563
step: 7800, Loss: 0.11773938685655594
step: 7900, Loss: 0.11714396625757217
step: 8000, Loss: 0.11940816044807434
step: 8100, Loss: 0.12449982762336731
step: 8200, Loss: 0.11641047149896622
step: 8300, Loss: 0.11531107127666473
step: 8400, Loss: 0.11523181200027466
step: 8500, Loss: 0.11532068997621536
step: 8600, Loss: 0.11625640094280243
step: 8700, Loss: 0.11574147641658783
step: 8800, Loss: 0.11612411588430405
step: 8900, Loss: 0.11421667784452438
step: 9000, Loss: 0.11405980587005615
step: 9100, Loss: 0.11746172606945038
step: 9200, Loss: 0.1145535558462143
step: 9300, Loss: 0.11568836867809296
step: 9400, Loss: 0.11565650254487991
step: 9500, Loss: 0.11351653933525085
step: 9600, Loss: 0.11478514224290848
step: 9700, Loss: 0.13548651337623596
step: 9800, Loss: 0.11452003568410873
step: 9900, Loss: 0.11623338609933853
training successfully ended.
validating...
validate data length:99
acc: 0.9166666666666666
precision: 0.8787878787878788
recall: 1.0
F_score: 0.9354838709677419
******fold 2******

Training... train_data length:889
step: 0, Loss: 0.15106293559074402
step: 100, Loss: 0.14461253583431244
step: 200, Loss: 0.11938299983739853
step: 300, Loss: 0.12249798327684402
step: 400, Loss: 0.1197231188416481
step: 500, Loss: 0.11744274944067001
step: 600, Loss: 0.11824016273021698
step: 700, Loss: 0.12236960232257843
step: 800, Loss: 0.11728758364915848
step: 900, Loss: 0.11409296840429306
step: 1000, Loss: 0.11657781153917313
step: 1100, Loss: 0.11487869918346405
step: 1200, Loss: 0.11488374322652817
step: 1300, Loss: 0.11815504729747772
step: 1400, Loss: 0.11455444991588593
step: 1500, Loss: 0.11501079797744751
step: 1600, Loss: 0.11387252807617188
step: 1700, Loss: 0.11484819650650024
step: 1800, Loss: 0.11392927169799805
step: 1900, Loss: 0.11630485951900482
step: 2000, Loss: 0.11502405256032944
step: 2100, Loss: 0.11562670767307281
step: 2200, Loss: 0.11417441070079803
step: 2300, Loss: 0.11468735337257385
step: 2400, Loss: 0.11417192220687866
step: 2500, Loss: 0.11395905911922455
step: 2600, Loss: 0.11483640968799591
step: 2700, Loss: 0.11703560501337051
step: 2800, Loss: 0.11394503712654114
step: 2900, Loss: 0.11344670504331589
step: 3000, Loss: 0.11312472820281982
step: 3100, Loss: 0.11313150823116302
step: 3200, Loss: 0.11349079757928848
step: 3300, Loss: 0.11448000371456146
step: 3400, Loss: 0.11328739672899246
step: 3500, Loss: 0.11240213364362717
step: 3600, Loss: 0.11553812772035599
step: 3700, Loss: 0.11359534412622452
step: 3800, Loss: 0.11379555612802505
step: 3900, Loss: 0.11428222060203552
step: 4000, Loss: 0.11441829055547714
step: 4100, Loss: 0.11339689791202545
step: 4200, Loss: 0.11285772919654846
step: 4300, Loss: 0.11272674798965454
step: 4400, Loss: 0.11360248178243637
step: 4500, Loss: 0.11385226249694824
step: 4600, Loss: 0.11361219733953476
step: 4700, Loss: 0.11366148293018341
step: 4800, Loss: 0.1146421954035759
step: 4900, Loss: 0.11492348462343216
step: 5000, Loss: 0.11490156501531601
step: 5100, Loss: 0.11686132848262787
step: 5200, Loss: 0.1161041185259819
step: 5300, Loss: 0.11532735824584961
step: 5400, Loss: 0.11456218361854553
step: 5500, Loss: 0.11424997448921204
step: 5600, Loss: 0.11505401134490967
step: 5700, Loss: 1.6017532348632812
step: 5800, Loss: 0.14674749970436096
step: 5900, Loss: 0.14291827380657196
step: 6000, Loss: 0.1289398968219757
step: 6100, Loss: 0.12875854969024658
step: 6200, Loss: 0.1377180814743042
step: 6300, Loss: 0.12786702811717987
step: 6400, Loss: 0.1261383295059204
step: 6500, Loss: 0.12562313675880432
step: 6600, Loss: 0.11898361146450043
step: 6700, Loss: 0.12485333532094955
step: 6800, Loss: 0.1263461709022522
step: 6900, Loss: 0.11848421394824982
step: 7000, Loss: 0.11917530745267868
step: 7100, Loss: 0.1174095943570137
step: 7200, Loss: 0.11892185360193253
step: 7300, Loss: 0.11838561296463013
step: 7400, Loss: 0.11858166009187698
step: 7500, Loss: 0.11902064085006714
step: 7600, Loss: 0.11613812297582626
step: 7700, Loss: 0.11627461016178131
step: 7800, Loss: 0.11678946018218994
step: 7900, Loss: 0.11527758091688156
step: 8000, Loss: 0.11805567890405655
step: 8100, Loss: 0.11803850531578064
step: 8200, Loss: 0.11588946729898453
step: 8300, Loss: 0.11559589952230453
step: 8400, Loss: 0.11465815454721451
step: 8500, Loss: 0.11625096946954727
step: 8600, Loss: 0.1155705377459526
step: 8700, Loss: 0.11493462324142456
step: 8800, Loss: 0.11614076048135757
step: 8900, Loss: 0.11498837172985077
step: 9000, Loss: 0.11397163569927216
step: 9100, Loss: 0.11520395427942276
step: 9200, Loss: 0.11492662131786346
step: 9300, Loss: 0.11697699129581451
step: 9400, Loss: 0.11467009037733078
step: 9500, Loss: 0.1134641095995903
step: 9600, Loss: 0.11364170163869858
step: 9700, Loss: 0.11471351981163025
step: 9800, Loss: 0.11406495422124863
step: 9900, Loss: 0.11447884887456894
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.9347826086956522
recall: 0.9772727272727273
F_score: 0.9555555555555557
******fold 3******

step: 8700, Loss: 0.11632776260375977
step: 8800, Loss: 0.1142074391245842
step: 8900, Loss: 0.1143760085105896
step: 9000, Loss: 0.11454594135284424
step: 9100, Loss: 0.11484289914369583
step: 9200, Loss: 0.11528757959604263
step: 9300, Loss: 0.11524511873722076
step: 9400, Loss: 0.11436574161052704
step: 9500, Loss: 0.11448846012353897
step: 9600, Loss: 0.11480404436588287
step: 9700, Loss: 0.11501196026802063
step: 9800, Loss: 0.11404117196798325
step: 9900, Loss: 0.1137007474899292
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.75
recall: 0.8
F_score: 0.7741935483870969
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.2559038996696472
step: 100, Loss: 0.12504859268665314
step: 200, Loss: 0.11703668534755707
step: 300, Loss: 0.11750500649213791
step: 400, Loss: 0.11547460407018661
step: 500, Loss: 0.114421546459198
step: 600, Loss: 0.11524024605751038
step: 700, Loss: 0.11617935448884964
step: 800, Loss: 0.11382147669792175
step: 900, Loss: 0.11359324306249619
step: 1000, Loss: 0.11401014029979706
step: 1100, Loss: 0.1156892254948616
step: 1200, Loss: 0.11475085467100143
step: 1300, Loss: 0.11333508789539337
step: 1400, Loss: 0.11485913395881653
step: 1500, Loss: 0.1140688806772232
step: 1600, Loss: 0.1141253188252449
step: 1700, Loss: 0.11528593301773071
step: 1800, Loss: 0.11356417834758759
step: 1900, Loss: 0.11330732703208923
step: 2000, Loss: 0.11299745738506317
step: 2100, Loss: 0.11504406481981277
step: 2200, Loss: 0.11454059928655624
step: 2300, Loss: 0.1153193786740303
step: 2400, Loss: 0.11480141431093216
step: 2500, Loss: 0.11408418416976929
step: 2600, Loss: 0.11346293985843658
step: 2700, Loss: 0.11654800176620483
step: 2800, Loss: 0.11408604681491852
step: 2900, Loss: 0.11766207963228226
step: 3000, Loss: 0.11308184266090393
step: 3100, Loss: 0.11457212269306183
step: 3200, Loss: 0.11437145620584488
step: 3300, Loss: 0.11723463982343674
step: 3400, Loss: 0.11806681007146835
step: 3500, Loss: 0.1154080405831337
step: 3600, Loss: 0.11367660760879517
step: 3700, Loss: 0.11358039826154709
step: 3800, Loss: 0.11424072086811066
step: 3900, Loss: 0.11405031383037567
step: 4000, Loss: 0.11427724361419678
step: 4100, Loss: 0.11396871507167816
step: 4200, Loss: 0.11587291210889816
step: 4300, Loss: 0.12008452415466309
step: 4400, Loss: 0.1156226098537445
step: 4500, Loss: 0.11467919498682022
step: 4600, Loss: 0.11563742160797119
step: 4700, Loss: 0.11319980025291443
step: 4800, Loss: 0.11403161287307739
step: 4900, Loss: 0.11343713104724884
step: 5000, Loss: 0.11471256613731384
step: 5100, Loss: 0.11741309612989426
step: 5200, Loss: 0.11452734470367432
step: 5300, Loss: 0.11903419345617294
step: 5400, Loss: 0.11375364661216736
step: 5500, Loss: 0.11295372247695923
step: 5600, Loss: 0.11349117755889893
step: 5700, Loss: 0.1159619688987732
step: 5800, Loss: 0.11394394189119339
step: 5900, Loss: 0.11529510468244553
step: 6000, Loss: 0.11539576947689056
step: 6100, Loss: 0.1148214042186737
step: 6200, Loss: 0.11585208028554916
step: 6300, Loss: 0.11541104316711426
step: 6400, Loss: 0.11372264474630356
step: 6500, Loss: 0.11580953001976013
step: 6600, Loss: 0.2551276683807373
step: 6700, Loss: 0.1513412743806839
step: 6800, Loss: 0.13363999128341675
step: 6900, Loss: 0.13355451822280884
step: 7000, Loss: 0.1277318149805069
step: 7100, Loss: 0.1283780336380005
step: 7200, Loss: 0.12498974055051804
step: 7300, Loss: 0.12596535682678223
step: 7400, Loss: 0.12304975092411041
step: 7500, Loss: 0.12094834446907043
step: 7600, Loss: 0.12324044108390808
step: 7700, Loss: 0.11934266984462738
step: 7800, Loss: 0.12230676412582397
step: 7900, Loss: 0.1187620535492897
step: 8000, Loss: 0.11951836943626404
step: 8100, Loss: 0.11839158833026886
step: 8200, Loss: 0.115897998213768
step: 8300, Loss: 0.11900262534618378
step: 8400, Loss: 0.11718868464231491
step: 8500, Loss: 0.1154455617070198
step: 8600, Loss: 0.11775172501802444
step: 8700, Loss: 0.11835014820098877
step: 8800, Loss: 0.11706283688545227
step: 8900, Loss: 0.11537907272577286
step: 9000, Loss: 0.11540493369102478
step: 9100, Loss: 0.11554431170225143
step: 9200, Loss: 0.11537386476993561
step: 9300, Loss: 0.11413253098726273
step: 9400, Loss: 0.11362455040216446
step: 9500, Loss: 0.11678817868232727
step: 9600, Loss: 0.11570340394973755
step: 9700, Loss: 0.11349961906671524
step: 9800, Loss: 0.11434648931026459
step: 9900, Loss: 0.11604981869459152
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8888888888888888
recall: 0.9411764705882353
F_score: 0.9142857142857143
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.11629632860422134
step: 100, Loss: 0.12287172675132751
step: 200, Loss: 0.11546436697244644
step: 300, Loss: 0.11585068702697754
step: 400, Loss: 0.11543979495763779
step: 500, Loss: 0.11359997093677521
step: 600, Loss: 0.11561598628759384
step: 700, Loss: 0.11481258273124695
step: 800, Loss: 0.11596427112817764
step: 900, Loss: 0.11444072425365448
step: 1000, Loss: 0.11352932453155518
step: 1100, Loss: 0.1151343435049057
step: 1200, Loss: 0.11521416157484055
step: 1300, Loss: 0.11398308724164963
step: 1400, Loss: 0.11467788368463516
step: 1500, Loss: 0.1142657995223999
step: 1600, Loss: 0.1145159900188446
step: 1700, Loss: 0.11426129192113876
step: 1800, Loss: 0.1138315424323082
step: 1900, Loss: 0.1146416962146759
step: 2000, Loss: 0.1145266443490982
step: 2100, Loss: 0.11377868801355362
step: 2200, Loss: 0.1135626882314682
step: 2300, Loss: 0.1142411008477211
step: 2400, Loss: 0.11426092684268951
step: 2500, Loss: 0.11373837292194366
step: 2600, Loss: 0.11339764297008514
step: 2700, Loss: 0.11500157415866852
step: 2800, Loss: 0.11591820418834686
step: 2900, Loss: 0.11410494893789291
step: 3000, Loss: 0.11524144560098648
step: 3100, Loss: 0.1167304515838623
step: 3200, Loss: 0.11721018701791763
step: 3300, Loss: 0.11892852187156677
step: 3400, Loss: 0.11393409967422485
step: 3500, Loss: 0.11801063269376755
step: 3600, Loss: 0.11388157308101654
step: 3700, Loss: 0.11393558979034424
step: 3800, Loss: 0.11460529267787933
step: 3900, Loss: 0.11375647783279419
step: 4000, Loss: 0.11357815563678741
step: 4100, Loss: 0.11445285379886627
step: 4200, Loss: 0.11501488089561462
step: 4300, Loss: 0.11659125983715057
step: 4400, Loss: 0.11454146355390549
step: 4500, Loss: 0.11416885256767273
step: 4600, Loss: 0.11367277801036835
step: 4700, Loss: 0.11484849452972412
step: 4800, Loss: 0.11398760229349136
step: 4900, Loss: 0.11468742042779922
step: 5000, Loss: 0.11441995948553085
step: 5100, Loss: 0.11496292054653168
step: 5200, Loss: 0.11518591642379761
step: 5300, Loss: 0.11328749358654022
step: 5400, Loss: 0.11322884261608124
step: 5500, Loss: 0.11453979462385178
step: 5600, Loss: 0.11416321992874146
step: 5700, Loss: 0.1144912987947464
step: 5800, Loss: 0.11406133323907852
step: 5900, Loss: 0.11283529549837112
step: 6000, Loss: 0.1146332323551178
step: 6100, Loss: 0.116889089345932
step: 6200, Loss: 0.11402314156293869
step: 6300, Loss: 0.11355777084827423
step: 6400, Loss: 0.11303495615720749
step: 6500, Loss: 0.11515873670578003
step: 6600, Loss: 0.11526844650506973
step: 6700, Loss: 0.11326273530721664
step: 6800, Loss: 0.11444420367479324
step: 6900, Loss: 0.11537157744169235
step: 7000, Loss: 0.11332458257675171
step: 7100, Loss: 0.11381706595420837
step: 7200, Loss: 0.1152372807264328
step: 7300, Loss: 0.11369799077510834
step: 7400, Loss: 0.1157616376876831
step: 7500, Loss: 0.1143706887960434
step: 7600, Loss: 0.11366802453994751
step: 7700, Loss: 0.11930285394191742
step: 7800, Loss: 0.28304824233055115
step: 7900, Loss: 0.14150772988796234
step: 8000, Loss: 0.12752512097358704
step: 8100, Loss: 0.12621724605560303
step: 8200, Loss: 0.12260414659976959
step: 8300, Loss: 0.11922779679298401
step: 8400, Loss: 0.11934081465005875
step: 8500, Loss: 0.12017650902271271
step: 8600, Loss: 0.11986649036407471
step: 8700, Loss: 0.11781013011932373
step: 8800, Loss: 0.11504969745874405
step: 8900, Loss: 0.12260972708463669
step: 9000, Loss: 0.11557205021381378
step: 9100, Loss: 0.1173352524638176
step: 9200, Loss: 0.11838080734014511
Training... train_data length:889
step: 0, Loss: 0.12723013758659363
step: 100, Loss: 0.11799398064613342
step: 200, Loss: 0.11762829124927521
step: 300, Loss: 0.11603035032749176
step: 400, Loss: 0.11484771966934204
step: 500, Loss: 0.11475886404514313
step: 600, Loss: 0.11371798068284988
step: 700, Loss: 0.1141728013753891
step: 800, Loss: 0.11484578996896744
step: 900, Loss: 0.11436084657907486
step: 1000, Loss: 0.11367259919643402
step: 1100, Loss: 0.11448909342288971
step: 1200, Loss: 0.11323803663253784
step: 1300, Loss: 0.11527399718761444
step: 1400, Loss: 0.11396507918834686
step: 1500, Loss: 0.1134321391582489
step: 1600, Loss: 0.11417766660451889
step: 1700, Loss: 0.11485320329666138
step: 1800, Loss: 0.11399148404598236
step: 1900, Loss: 0.11373082548379898
step: 2000, Loss: 0.11263561248779297
step: 2100, Loss: 0.11448513716459274
step: 2200, Loss: 0.11363540589809418
step: 2300, Loss: 0.11444232612848282
step: 2400, Loss: 0.11339204013347626
step: 2500, Loss: 0.1139930933713913
step: 2600, Loss: 0.11338678747415543
step: 2700, Loss: 0.11586537957191467
step: 2800, Loss: 0.11369817703962326
step: 2900, Loss: 0.11472897976636887
step: 3000, Loss: 0.11590488255023956
step: 3100, Loss: 0.11416631191968918
step: 3200, Loss: 0.11492408812046051
step: 3300, Loss: 0.11337868124246597
step: 3400, Loss: 0.11560463160276413
step: 3500, Loss: 0.1126495972275734
step: 3600, Loss: 0.11393749713897705
step: 3700, Loss: 0.11355651915073395
step: 3800, Loss: 1.4986927509307861
step: 3900, Loss: 0.15354882180690765
step: 4000, Loss: 0.1594008505344391
step: 4100, Loss: 0.13266374170780182
step: 4200, Loss: 0.1282142698764801
step: 4300, Loss: 0.1306806355714798
step: 4400, Loss: 0.12634101510047913
step: 4500, Loss: 0.12028288096189499
step: 4600, Loss: 0.1251916140317917
step: 4700, Loss: 0.12244369089603424
step: 4800, Loss: 0.12037545442581177
step: 4900, Loss: 0.11988736689090729
step: 5000, Loss: 0.1205209344625473
step: 5100, Loss: 0.12080542743206024
step: 5200, Loss: 0.11725166440010071
step: 5300, Loss: 0.1193024069070816
step: 5400, Loss: 0.11794750392436981
step: 5500, Loss: 0.12160409986972809
step: 5600, Loss: 0.11481045186519623
step: 5700, Loss: 0.11645729839801788
step: 5800, Loss: 0.11796784400939941
step: 5900, Loss: 0.11736546456813812
step: 6000, Loss: 0.11661916971206665
step: 6100, Loss: 0.11548157036304474
step: 6200, Loss: 0.11549259722232819
step: 6300, Loss: 0.1163112074136734
step: 6400, Loss: 0.11789403110742569
step: 6500, Loss: 0.11567311733961105
step: 6600, Loss: 0.1180305927991867
step: 6700, Loss: 0.11630646139383316
step: 6800, Loss: 0.11580465734004974
step: 6900, Loss: 0.11565852165222168
step: 7000, Loss: 0.11534515768289566
step: 7100, Loss: 0.11417540162801743
step: 7200, Loss: 0.11643385887145996
step: 7300, Loss: 0.11482115834951401
step: 7400, Loss: 0.1140189841389656
step: 7500, Loss: 0.11760876327753067
step: 7600, Loss: 0.11486552655696869
step: 7700, Loss: 0.11460383236408234
step: 7800, Loss: 0.11338049173355103
step: 7900, Loss: 0.11388066411018372
step: 8000, Loss: 0.11413392424583435
step: 8100, Loss: 0.11358501017093658
step: 8200, Loss: 0.11494611948728561
step: 8300, Loss: 0.11289487034082413
step: 8400, Loss: 0.11306736618280411
step: 8500, Loss: 0.11469012498855591
step: 8600, Loss: 0.11393801867961884
step: 8700, Loss: 0.11465269327163696
step: 8800, Loss: 0.11419655382633209
step: 8900, Loss: 0.11411651968955994
step: 9000, Loss: 0.11351130157709122
step: 9100, Loss: 0.11393897235393524
step: 9200, Loss: 0.11375896632671356
step: 9300, Loss: 0.11361129581928253
step: 9400, Loss: 0.11408815532922745
step: 9500, Loss: 0.11480577290058136
step: 9600, Loss: 0.1134888157248497
step: 9700, Loss: 0.11523379385471344
step: 9800, Loss: 0.1134147047996521
step: 9900, Loss: 0.11433970928192139
training successfully ended.
validating...
validate data length:99
acc: 0.96875
precision: 0.9795918367346939
recall: 0.96
F_score: 0.9696969696969697
******fold 4******

Training... train_data length:889
step: 0, Loss: 0.11517620086669922
step: 100, Loss: 0.1163131594657898
step: 200, Loss: 0.11549148708581924
step: 300, Loss: 0.117385134100914
step: 400, Loss: 0.11819657683372498
step: 500, Loss: 0.1147366389632225
step: 600, Loss: 0.11313339322805405
step: 700, Loss: 0.11476437747478485
step: 800, Loss: 0.1136905699968338
step: 900, Loss: 0.11435940861701965
step: 1000, Loss: 0.11395157873630524
step: 1100, Loss: 0.11354637891054153
step: 1200, Loss: 0.11424903571605682
step: 1300, Loss: 0.11442336440086365
step: 1400, Loss: 0.11314351856708527
step: 1500, Loss: 0.1129300519824028
step: 1600, Loss: 0.11434513330459595
step: 1700, Loss: 0.11350198835134506
step: 1800, Loss: 0.11337485909461975
step: 1900, Loss: 0.1139216274023056
step: 2000, Loss: 0.11334540694952011
step: 2100, Loss: 0.1148587167263031
step: 2200, Loss: 0.11459287256002426
step: 2300, Loss: 0.11351317167282104
step: 2400, Loss: 0.11379027366638184
step: 2500, Loss: 0.11474449932575226
step: 2600, Loss: 0.11349184066057205
step: 2700, Loss: 0.11386436223983765
step: 2800, Loss: 0.11495155096054077
step: 2900, Loss: 0.11295785009860992
step: 3000, Loss: 0.11321522295475006
step: 3100, Loss: 0.11478772014379501
step: 3200, Loss: 0.11286561191082001
step: 3300, Loss: 0.11323656141757965
step: 3400, Loss: 0.11362826824188232
step: 3500, Loss: 0.11275723576545715
step: 3600, Loss: 0.11406958103179932
step: 3700, Loss: 0.11303544044494629
step: 3800, Loss: 0.1136155053973198
step: 3900, Loss: 0.11428384482860565
step: 4000, Loss: 0.11277827620506287
step: 4100, Loss: 0.11376895010471344
step: 4200, Loss: 0.11336341500282288
step: 4300, Loss: 0.11487776041030884
step: 4400, Loss: 0.11369502544403076
step: 4500, Loss: 0.11418135464191437
step: 4600, Loss: 0.11397071182727814
step: 4700, Loss: 0.11304251849651337
step: 4800, Loss: 0.11394362151622772
step: 4900, Loss: 0.1151651069521904
step: 5000, Loss: 0.1135610044002533
step: 5100, Loss: 0.11324597150087357
step: 5200, Loss: 0.11344566196203232
step: 5300, Loss: 3.582906484603882
step: 5400, Loss: 0.19576041400432587
step: 5500, Loss: 0.14219731092453003
step: 5600, Loss: 0.14161893725395203
step: 5700, Loss: 0.12230265140533447
step: 5800, Loss: 0.12702840566635132
step: 5900, Loss: 0.12970708310604095
step: 6000, Loss: 0.11970681697130203
step: 6100, Loss: 0.12071860581636429
step: 6200, Loss: 0.12048755586147308
step: 6300, Loss: 0.11785031855106354
step: 6400, Loss: 0.1180826872587204
step: 6500, Loss: 0.12071513384580612
step: 6600, Loss: 0.1169087290763855
step: 6700, Loss: 0.11752952635288239
step: 6800, Loss: 0.11606063693761826
step: 6900, Loss: 0.11504608392715454
step: 7000, Loss: 0.11821066588163376
step: 7100, Loss: 0.11642161756753922
step: 7200, Loss: 0.11516593396663666
step: 7300, Loss: 0.11601772904396057
step: 7400, Loss: 0.11536619067192078
step: 7500, Loss: 0.11482365429401398
step: 7600, Loss: 0.1166452169418335
step: 7700, Loss: 0.11506667733192444
step: 7800, Loss: 0.11477583646774292
step: 7900, Loss: 0.11533363163471222
step: 8000, Loss: 0.11470100283622742
step: 8100, Loss: 0.11538487672805786
step: 8200, Loss: 0.11744697391986847
step: 8300, Loss: 0.11514653265476227
step: 8400, Loss: 0.11475695669651031
step: 8500, Loss: 0.11624278128147125
step: 8600, Loss: 0.11323649436235428
step: 8700, Loss: 0.11403059959411621
step: 8800, Loss: 0.11491185426712036
step: 8900, Loss: 0.11374355107545853
step: 9000, Loss: 0.11437363922595978
step: 9100, Loss: 0.11456014215946198
step: 9200, Loss: 0.11472485214471817
step: 9300, Loss: 0.11519212275743484
step: 9400, Loss: 0.11454823613166809
step: 9500, Loss: 0.11303851753473282
step: 9600, Loss: 0.1140027716755867
step: 9700, Loss: 0.11319120973348618
step: 9800, Loss: 0.11534655094146729
step: 9900, Loss: 0.11424662172794342
training successfully ended.
validating...
validate data length:99
acc: 0.9895833333333334
precision: 1.0
recall: 0.9803921568627451
F_score: 0.99009900990099
******fold 5******

Training... train_data length:889
step: 0, Loss: 0.11451660841703415
step: 100, Loss: 0.11523076891899109
step: 200, Loss: 0.11476870626211166
step: 300, Loss: 0.11598452925682068
step: 400, Loss: 0.1165386214852333
step: 9300, Loss: 0.11741581559181213
step: 9400, Loss: 0.11580798029899597
step: 9500, Loss: 0.11669597774744034
step: 9600, Loss: 0.11741702258586884
step: 9700, Loss: 0.1150735393166542
step: 9800, Loss: 0.11365212500095367
step: 9900, Loss: 0.11343082785606384
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7333333333333333
recall: 0.9166666666666666
F_score: 0.8148148148148148
subject 8 Avgacc: 0.7841666666666667 Avgfscore: 0.7877228157456189 
 Max acc:0.9, Max f score:0.9142857142857143
******** mix subject_9 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 33.861820220947266
step: 100, Loss: 3.9132492542266846
step: 200, Loss: 0.1371799260377884
step: 300, Loss: 0.15325483679771423
step: 400, Loss: 0.14067405462265015
step: 500, Loss: 0.13038569688796997
step: 600, Loss: 0.11762658506631851
step: 700, Loss: 0.1250854879617691
step: 800, Loss: 0.12148703634738922
step: 900, Loss: 0.11674794554710388
step: 1000, Loss: 0.11808542907238007
step: 1100, Loss: 0.12076104432344437
step: 1200, Loss: 0.11935774981975555
step: 1300, Loss: 0.12173471599817276
step: 1400, Loss: 0.11890996247529984
step: 1500, Loss: 0.12135538458824158
step: 1600, Loss: 0.11935767531394958
step: 1700, Loss: 5.134417533874512
step: 1800, Loss: 0.13560357689857483
step: 1900, Loss: 0.17248888313770294
step: 2000, Loss: 0.12522995471954346
step: 2100, Loss: 0.1408199816942215
step: 2200, Loss: 0.12619534134864807
step: 2300, Loss: 0.1338527500629425
step: 2400, Loss: 0.13695311546325684
step: 2500, Loss: 0.1362176239490509
step: 2600, Loss: 0.11740407347679138
step: 2700, Loss: 0.13113045692443848
step: 2800, Loss: 0.12094362080097198
step: 2900, Loss: 0.12630845606327057
step: 3000, Loss: 0.12070021778345108
step: 3100, Loss: 0.12574699521064758
step: 3200, Loss: 0.12086495757102966
step: 3300, Loss: 0.12555712461471558
step: 3400, Loss: 0.12647974491119385
step: 3500, Loss: 0.12128753960132599
step: 3600, Loss: 0.12412609159946442
step: 3700, Loss: 0.11898742616176605
step: 3800, Loss: 0.11656462401151657
step: 3900, Loss: 0.12086241692304611
step: 4000, Loss: 0.11835164576768875
step: 4100, Loss: 0.1203620657324791
step: 4200, Loss: 0.12013193964958191
step: 4300, Loss: 0.12407820671796799
step: 4400, Loss: 0.11638457328081131
step: 4500, Loss: 0.11457903683185577
step: 4600, Loss: 0.11869662255048752
step: 4700, Loss: 0.1254097819328308
step: 4800, Loss: 0.12029821425676346
step: 4900, Loss: 0.1201210469007492
step: 5000, Loss: 0.11772163212299347
step: 5100, Loss: 0.11575046181678772
step: 5200, Loss: 0.11905239522457123
step: 5300, Loss: 0.11609554290771484
step: 5400, Loss: 0.1188461109995842
step: 5500, Loss: 0.11972878873348236
step: 5600, Loss: 0.11825177073478699
step: 5700, Loss: 0.13286486268043518
step: 5800, Loss: 0.11934907734394073
step: 5900, Loss: 0.12167838215827942
step: 6000, Loss: 0.11864374577999115
step: 6100, Loss: 0.12004588544368744
step: 6200, Loss: 0.11426304280757904
step: 6300, Loss: 0.1163816899061203
step: 6400, Loss: 0.11727919429540634
step: 6500, Loss: 0.11742624640464783
step: 6600, Loss: 0.11638907343149185
step: 6700, Loss: 0.1211802214384079
step: 6800, Loss: 0.11761849373579025
step: 6900, Loss: 0.11858879029750824
step: 7000, Loss: 0.11569333076477051
step: 7100, Loss: 0.11644711345434189
step: 7200, Loss: 0.11689980328083038
step: 7300, Loss: 0.12332206964492798
step: 7400, Loss: 0.11464501172304153
step: 7500, Loss: 0.11648421734571457
step: 7600, Loss: 0.11708412319421768
step: 7700, Loss: 0.11885349452495575
step: 7800, Loss: 5.2461137771606445
step: 7900, Loss: 0.15736649930477142
step: 8000, Loss: 0.14293117821216583
step: 8100, Loss: 0.13602255284786224
step: 8200, Loss: 0.13139937818050385
step: 8300, Loss: 0.1224890947341919
step: 8400, Loss: 0.1288294494152069
step: 8500, Loss: 0.12302467226982117
step: 8600, Loss: 0.12433339655399323
step: 8700, Loss: 0.12328039854764938
step: 8800, Loss: 0.12310705333948135
step: 8900, Loss: 0.11829942464828491
step: 9000, Loss: 0.11869675666093826
step: 9100, Loss: 0.12090888619422913
step: 9200, Loss: 0.11925975233316422
step: 9300, Loss: 0.11731699109077454
step: 9400, Loss: 0.12066104263067245
step: 9500, Loss: 0.1206967830657959
step: 9600, Loss: 0.11871922016143799
step: 9700, Loss: 0.11717797815799713
step: 9800, Loss: 0.11892697215080261
step: 9900, Loss: 0.11555568873882294
training successfully ended.
validating...
validate data length:32
acc: 0.40625
precision: 0.34782608695652173
recall: 0.6666666666666666
F_score: 0.4571428571428571
******fold 2******

Training... train_data length:280
step: 0, Loss: 2.7668771743774414
step: 100, Loss: 0.13760213553905487
step: 200, Loss: 0.12615884840488434
step: 300, Loss: 0.12131121009588242
step: 400, Loss: 0.11896885931491852
step: 500, Loss: 0.11796452105045319
step: 600, Loss: 0.11857648938894272
step: 700, Loss: 0.11846593767404556
step: 800, Loss: 0.11818214505910873
step: 900, Loss: 0.11708717793226242
step: 1000, Loss: 0.1162283718585968
step: 1100, Loss: 0.11550597846508026
step: 1200, Loss: 0.11768994480371475
step: 1300, Loss: 0.11725408583879471
step: 1400, Loss: 0.11451904475688934
step: 1500, Loss: 0.11452232301235199
step: 1600, Loss: 0.11776581406593323
step: 1700, Loss: 0.11547904461622238
step: 1800, Loss: 0.11634539812803268
step: 1900, Loss: 0.11539460718631744
step: 2000, Loss: 0.11572835594415665
step: 2100, Loss: 0.11623737215995789
step: 2200, Loss: 0.11827340722084045
step: 2300, Loss: 0.11646702885627747
step: 2400, Loss: 0.11563660204410553
step: 2500, Loss: 0.1171039566397667
step: 2600, Loss: 0.11982537060976028
step: 2700, Loss: 0.11588180065155029
step: 2800, Loss: 0.11666960269212723
step: 2900, Loss: 0.1142067015171051
step: 3000, Loss: 0.114793561398983
step: 3100, Loss: 0.11689183115959167
step: 3200, Loss: 0.11512460559606552
step: 3300, Loss: 0.11327292770147324
step: 3400, Loss: 0.11446716636419296
step: 3500, Loss: 0.11700668185949326
step: 3600, Loss: 0.11550900340080261
step: 3700, Loss: 0.11845184117555618
step: 3800, Loss: 0.11570913344621658
step: 3900, Loss: 0.11445831507444382
step: 4000, Loss: 0.1166728287935257
step: 4100, Loss: 0.11369860172271729
step: 4200, Loss: 0.11506588757038116
step: 4300, Loss: 0.11483719944953918
step: 4400, Loss: 0.11458392441272736
step: 4500, Loss: 0.11399872601032257
step: 4600, Loss: 0.11864978075027466
step: 4700, Loss: 0.11439065635204315
step: 4800, Loss: 0.11535625159740448
step: 4900, Loss: 0.11636357754468918
step: 5000, Loss: 0.11612897366285324
step: 5100, Loss: 0.11560022830963135
step: 5200, Loss: 0.11512192338705063
step: 5300, Loss: 0.11583414673805237
step: 5400, Loss: 0.14296212792396545
step: 5500, Loss: 0.12893125414848328
step: 5600, Loss: 0.13062922656536102
step: 5700, Loss: 0.1273522973060608
step: 5800, Loss: 0.12483660876750946
step: 5900, Loss: 0.12206801772117615
step: 6000, Loss: 0.12308259308338165
step: 6100, Loss: 0.12112054973840714
step: 6200, Loss: 0.12245745956897736
step: 6300, Loss: 0.11668391525745392
step: 6400, Loss: 0.12002579867839813
step: 6500, Loss: 0.11761821806430817
step: 6600, Loss: 0.12047024816274643
step: 6700, Loss: 0.11737829446792603
step: 6800, Loss: 0.12101636081933975
step: 6900, Loss: 0.11500490456819534
step: 7000, Loss: 0.11854895949363708
step: 7100, Loss: 0.11449625343084335
step: 7200, Loss: 0.11441731452941895
step: 7300, Loss: 0.11838582903146744
step: 7400, Loss: 0.11746417731046677
step: 7500, Loss: 0.11583685874938965
step: 7600, Loss: 0.11542536318302155
step: 7700, Loss: 0.11906187236309052
step: 7800, Loss: 0.11629266291856766
step: 7900, Loss: 0.11745820939540863
step: 8000, Loss: 0.11646222323179245
step: 8100, Loss: 0.11819450557231903
step: 8200, Loss: 0.11505427956581116
step: 8300, Loss: 0.11688059568405151
step: 8400, Loss: 0.11678975075483322
step: 8500, Loss: 0.11454986780881882
step: 8600, Loss: 0.11600695550441742
step: 8700, Loss: 0.11592075228691101
step: 8800, Loss: 0.11426345258951187
step: 8900, Loss: 0.1139766275882721
step: 9000, Loss: 0.11357143521308899
step: 9100, Loss: 0.11400444805622101
step: 9200, Loss: 0.11457856744527817
step: 9300, Loss: 0.11456061899662018
step: 500, Loss: 0.11430378258228302
step: 600, Loss: 0.11478129029273987
step: 700, Loss: 0.11458748579025269
step: 800, Loss: 0.11421073973178864
step: 900, Loss: 0.11815939843654633
step: 1000, Loss: 0.1163783147931099
step: 1100, Loss: 0.11375793069601059
step: 1200, Loss: 0.11379413306713104
step: 1300, Loss: 0.11409464478492737
step: 1400, Loss: 0.11257701367139816
step: 1500, Loss: 0.114304319024086
step: 1600, Loss: 0.11357756704092026
step: 1700, Loss: 0.11292621493339539
step: 1800, Loss: 0.11338808387517929
step: 1900, Loss: 0.11423734575510025
step: 2000, Loss: 0.11437790840864182
step: 2100, Loss: 0.11340194940567017
step: 2200, Loss: 0.11298128962516785
step: 2300, Loss: 0.1144760400056839
step: 2400, Loss: 0.11376488953828812
step: 2500, Loss: 0.11459428071975708
step: 2600, Loss: 0.11470861732959747
step: 2700, Loss: 0.1137012392282486
step: 2800, Loss: 0.11389978229999542
step: 2900, Loss: 0.112668476998806
step: 3000, Loss: 0.11294117569923401
step: 3100, Loss: 0.114216148853302
step: 3200, Loss: 0.11373589187860489
step: 3300, Loss: 0.11324802041053772
step: 3400, Loss: 0.11421451717615128
step: 3500, Loss: 0.1124582290649414
step: 3600, Loss: 0.11368264257907867
step: 3700, Loss: 0.11415783315896988
step: 3800, Loss: 0.1141633540391922
step: 3900, Loss: 0.11564560979604721
step: 4000, Loss: 0.11502005904912949
step: 4100, Loss: 0.11436137557029724
step: 4200, Loss: 0.1136249229311943
step: 4300, Loss: 0.11389917135238647
step: 4400, Loss: 0.11466845870018005
step: 4500, Loss: 0.1133626401424408
step: 4600, Loss: 0.11316151916980743
step: 4700, Loss: 0.1133926510810852
step: 4800, Loss: 0.11423318833112717
step: 4900, Loss: 0.11310461163520813
step: 5000, Loss: 0.11287946254014969
step: 5100, Loss: 0.11399634927511215
step: 5200, Loss: 0.6481508016586304
step: 5300, Loss: 0.6078301072120667
step: 5400, Loss: 0.13840527832508087
step: 5500, Loss: 0.13565272092819214
step: 5600, Loss: 0.13376715779304504
step: 5700, Loss: 0.12244660407304764
step: 5800, Loss: 0.12485609948635101
step: 5900, Loss: 0.12134908139705658
step: 6000, Loss: 0.12004382908344269
step: 6100, Loss: 0.12324041873216629
step: 6200, Loss: 0.11734985560178757
step: 6300, Loss: 0.11736071854829788
step: 6400, Loss: 0.11869576573371887
step: 6500, Loss: 0.11785096675157547
step: 6600, Loss: 0.11552727222442627
step: 6700, Loss: 0.11850540339946747
step: 6800, Loss: 0.11624398082494736
step: 6900, Loss: 0.1160668432712555
step: 7000, Loss: 0.11549580842256546
step: 7100, Loss: 0.11805383116006851
step: 7200, Loss: 0.11391400545835495
step: 7300, Loss: 0.11772821098566055
step: 7400, Loss: 0.11525292694568634
step: 7500, Loss: 0.11568297445774078
step: 7600, Loss: 0.11565177887678146
step: 7700, Loss: 0.11745688319206238
step: 7800, Loss: 0.11502505838871002
step: 7900, Loss: 0.11497007310390472
step: 8000, Loss: 0.11671018600463867
step: 8100, Loss: 0.11491451412439346
step: 8200, Loss: 0.11549284309148788
step: 8300, Loss: 0.1132371574640274
step: 8400, Loss: 0.1141899973154068
step: 8500, Loss: 0.11456728726625443
step: 8600, Loss: 0.11329881846904755
step: 8700, Loss: 0.11522222310304642
step: 8800, Loss: 0.11305221170186996
step: 8900, Loss: 0.11573527753353119
step: 9000, Loss: 0.11376747488975525
step: 9100, Loss: 0.11685266345739365
step: 9200, Loss: 0.1145227700471878
step: 9300, Loss: 0.11331015825271606
step: 9400, Loss: 0.11432339996099472
step: 9500, Loss: 0.11426079273223877
step: 9600, Loss: 0.11397449672222137
step: 9700, Loss: 0.11693531274795532
step: 9800, Loss: 0.11354002356529236
step: 9900, Loss: 0.11416418105363846
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

Training... train_data length:889
step: 0, Loss: 0.11357757449150085
step: 100, Loss: 0.11477016657590866
step: 200, Loss: 0.11421453952789307
step: 300, Loss: 0.1137346625328064
step: 400, Loss: 0.11357156932353973
step: 500, Loss: 0.11342031508684158
step: 600, Loss: 0.11380217224359512
step: 700, Loss: 0.11584069579839706
step: 800, Loss: 0.11332444846630096
step: 900, Loss: 0.11287394165992737
step: 1000, Loss: 0.11402396857738495
step: 1100, Loss: 0.1136619821190834
step: 1200, Loss: 0.11539028584957123
step: 1300, Loss: 0.11318209767341614
step: 1400, Loss: 0.11309412121772766
step: 1500, Loss: 0.1135535016655922
step: 1600, Loss: 0.11320538818836212
step: 1700, Loss: 0.11369186639785767
step: 1800, Loss: 0.11424437910318375
step: 1900, Loss: 0.11359160393476486
step: 2000, Loss: 0.11272899806499481
step: 2100, Loss: 0.11462672799825668
step: 2200, Loss: 0.11360704153776169
step: 2300, Loss: 0.11479154229164124
step: 2400, Loss: 0.11372388154268265
step: 2500, Loss: 0.11301244795322418
step: 2600, Loss: 0.11411294341087341
step: 2700, Loss: 0.11418698728084564
step: 2800, Loss: 0.11349159479141235
step: 2900, Loss: 0.11357175558805466
step: 3000, Loss: 0.11361417174339294
step: 3100, Loss: 0.11459961533546448
step: 3200, Loss: 0.11390530318021774
step: 3300, Loss: 0.11314113438129425
step: 3400, Loss: 0.11355873942375183
step: 3500, Loss: 2.2694671154022217
step: 3600, Loss: 0.15962007641792297
step: 3700, Loss: 0.1375962346792221
step: 3800, Loss: 0.1320772022008896
step: 3900, Loss: 0.13262078166007996
step: 4000, Loss: 0.1221972405910492
step: 4100, Loss: 0.12377051264047623
step: 4200, Loss: 0.12647798657417297
step: 4300, Loss: 0.11823032051324844
step: 4400, Loss: 0.12158474326133728
step: 4500, Loss: 0.12269675731658936
step: 4600, Loss: 0.11652965843677521
step: 4700, Loss: 0.11847669631242752
step: 4800, Loss: 0.1153566986322403
step: 4900, Loss: 0.12673158943653107
step: 5000, Loss: 0.11727368831634521
step: 5100, Loss: 0.11564590036869049
step: 5200, Loss: 0.11583234369754791
step: 5300, Loss: 0.11687958985567093
step: 5400, Loss: 0.11532595753669739
step: 5500, Loss: 0.11649288982152939
step: 5600, Loss: 0.11543320119380951
step: 5700, Loss: 0.11635544896125793
step: 5800, Loss: 0.1165500059723854
step: 5900, Loss: 0.11504704505205154
step: 6000, Loss: 0.11732599139213562
step: 6100, Loss: 0.1155390664935112
step: 6200, Loss: 0.1154455840587616
step: 6300, Loss: 0.11450853943824768
step: 6400, Loss: 0.11380190402269363
step: 6500, Loss: 0.11638593673706055
step: 6600, Loss: 0.11495155096054077
step: 6700, Loss: 0.1160534992814064
step: 6800, Loss: 0.11427335441112518
step: 6900, Loss: 0.11397362500429153
step: 7000, Loss: 0.1140565276145935
step: 7100, Loss: 0.11412122845649719
step: 7200, Loss: 0.1149115264415741
step: 7300, Loss: 0.11386766284704208
step: 7400, Loss: 0.1144665852189064
step: 7500, Loss: 0.11517525464296341
step: 7600, Loss: 0.11478430032730103
step: 7700, Loss: 0.11386799812316895
step: 7800, Loss: 0.11536294966936111
step: 7900, Loss: 0.11468549072742462
step: 8000, Loss: 0.11410821229219437
step: 8100, Loss: 0.11403506994247437
step: 8200, Loss: 0.11335352808237076
step: 8300, Loss: 0.11438451707363129
step: 8400, Loss: 0.1140342503786087
step: 8500, Loss: 0.11478938162326813
step: 8600, Loss: 0.11348804086446762
step: 8700, Loss: 0.11336489766836166
step: 8800, Loss: 0.11411324143409729
step: 8900, Loss: 0.11377841234207153
step: 9000, Loss: 0.11309673637151718
step: 9100, Loss: 0.1146477460861206
step: 9200, Loss: 0.11412550508975983
step: 9300, Loss: 0.11399810016155243
step: 9400, Loss: 0.11334603279829025
step: 9500, Loss: 0.11332149803638458
step: 9600, Loss: 0.1138666570186615
step: 9700, Loss: 0.11439833790063858
step: 9800, Loss: 0.1131763607263565
step: 9900, Loss: 0.11364332586526871
training successfully ended.
validating...
validate data length:99
acc: 0.9895833333333334
precision: 1.0
recall: 0.9782608695652174
F_score: 0.989010989010989
******fold 7******

Training... train_data length:889
step: 0, Loss: 0.11572588980197906
step: 100, Loss: 0.11578493565320969
step: 200, Loss: 0.11488865315914154
step: 300, Loss: 0.11551665514707565
step: 400, Loss: 0.11538425087928772
step: 500, Loss: 0.11411205679178238
step: 600, Loss: 0.11457084864377975
step: 700, Loss: 0.11412610858678818
step: 800, Loss: 0.11391876637935638
step: 900, Loss: 0.11506395041942596
step: 1000, Loss: 0.11301577091217041
step: 1100, Loss: 0.11347935348749161
step: 9400, Loss: 0.11389298737049103
step: 9500, Loss: 0.11559546738862991
step: 9600, Loss: 0.11632876843214035
step: 9700, Loss: 0.11352638155221939
step: 9800, Loss: 0.11505062133073807
step: 9900, Loss: 0.11459074914455414
training successfully ended.
validating...
validate data length:32
acc: 0.75
precision: 0.7222222222222222
recall: 0.8125
F_score: 0.7647058823529411
******fold 3******

Training... train_data length:281
step: 0, Loss: 1.4423701763153076
step: 100, Loss: 0.1352829933166504
step: 200, Loss: 0.12099768221378326
step: 300, Loss: 0.11550547927618027
step: 400, Loss: 0.1182527020573616
step: 500, Loss: 0.11374960839748383
step: 600, Loss: 0.11709579080343246
step: 700, Loss: 0.11481447517871857
step: 800, Loss: 0.11379359662532806
step: 900, Loss: 0.11456280201673508
step: 1000, Loss: 0.11455433070659637
step: 1100, Loss: 0.11558026820421219
step: 1200, Loss: 0.11515329778194427
step: 1300, Loss: 0.1155107244849205
step: 1400, Loss: 0.11531829088926315
step: 1500, Loss: 0.11557089537382126
step: 1600, Loss: 0.11343725025653839
step: 1700, Loss: 0.11442142724990845
step: 1800, Loss: 0.11485344171524048
step: 1900, Loss: 0.11447302997112274
step: 2000, Loss: 0.11542119085788727
step: 2100, Loss: 0.11515045911073685
step: 2200, Loss: 0.11424809694290161
step: 2300, Loss: 0.11267384886741638
step: 2400, Loss: 0.11993761360645294
step: 2500, Loss: 0.11604899168014526
step: 2600, Loss: 0.11575135588645935
step: 2700, Loss: 0.11423328518867493
step: 2800, Loss: 0.11440245807170868
step: 2900, Loss: 0.11401843279600143
step: 3000, Loss: 0.1147853210568428
step: 3100, Loss: 0.11436278373003006
step: 3200, Loss: 0.11649810522794724
step: 3300, Loss: 0.11378855258226395
step: 3400, Loss: 0.11659231781959534
step: 3500, Loss: 0.1149912104010582
step: 3600, Loss: 0.11490561813116074
step: 3700, Loss: 0.11429231613874435
step: 3800, Loss: 0.11384682357311249
step: 3900, Loss: 0.11631286889314651
step: 4000, Loss: 0.1155182346701622
step: 4100, Loss: 0.11636440455913544
step: 4200, Loss: 0.1165204793214798
step: 4300, Loss: 0.1140805184841156
step: 4400, Loss: 0.11342674493789673
step: 4500, Loss: 0.11451864242553711
step: 4600, Loss: 0.11377564817667007
step: 4700, Loss: 0.1135474443435669
step: 4800, Loss: 0.11453241109848022
step: 4900, Loss: 0.11517985910177231
step: 5000, Loss: 0.11551865935325623
step: 5100, Loss: 0.11899270862340927
step: 5200, Loss: 0.11393186450004578
step: 5300, Loss: 0.11559843271970749
step: 5400, Loss: 0.11493131518363953
step: 5500, Loss: 0.1152537539601326
step: 5600, Loss: 0.11612211912870407
step: 5700, Loss: 0.11421678215265274
step: 5800, Loss: 0.11431489139795303
step: 5900, Loss: 0.11629548668861389
step: 6000, Loss: 0.11477672308683395
step: 6100, Loss: 0.1168806329369545
step: 6200, Loss: 0.1603412628173828
step: 6300, Loss: 0.1283736228942871
step: 6400, Loss: 0.13122521340847015
step: 6500, Loss: 0.12848877906799316
step: 6600, Loss: 0.12404682487249374
step: 6700, Loss: 0.11899777501821518
step: 6800, Loss: 0.11737632006406784
step: 6900, Loss: 0.12168651819229126
step: 7000, Loss: 0.1173587366938591
step: 7100, Loss: 0.11820785701274872
step: 7200, Loss: 0.11760726571083069
step: 7300, Loss: 0.12087319791316986
step: 7400, Loss: 0.12031886726617813
step: 7500, Loss: 0.1183602437376976
step: 7600, Loss: 0.1149984747171402
step: 7700, Loss: 0.11930431425571442
step: 7800, Loss: 0.11944560706615448
step: 7900, Loss: 0.11583978682756424
step: 8000, Loss: 0.11595924198627472
step: 8100, Loss: 0.11836099624633789
step: 8200, Loss: 0.1159011572599411
step: 8300, Loss: 0.11855873465538025
step: 8400, Loss: 0.11454523354768753
step: 8500, Loss: 0.11501393467187881
step: 8600, Loss: 0.11500653624534607
step: 8700, Loss: 0.1160416230559349
step: 8800, Loss: 0.11672817915678024
step: 8900, Loss: 0.11591242998838425
step: 9000, Loss: 0.11356384307146072
step: 9100, Loss: 0.11674269288778305
step: 9200, Loss: 0.1137719675898552
step: 9300, Loss: 0.11318773031234741
step: 9400, Loss: 0.11425695568323135
step: 9500, Loss: 0.11408494412899017
step: 9600, Loss: 0.11404064297676086
step: 9700, Loss: 0.11514459550380707
step: 9800, Loss: 0.11444242298603058
step: 9900, Loss: 0.11610051244497299
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.13554343581199646
step: 100, Loss: 0.11521665751934052
step: 200, Loss: 0.11894284188747406
step: 300, Loss: 0.11695844680070877
step: 400, Loss: 0.115248903632164
step: 500, Loss: 0.11766131967306137
step: 600, Loss: 0.11543319374322891
step: 700, Loss: 0.11406037211418152
step: 800, Loss: 0.11411108821630478
step: 900, Loss: 0.11359080672264099
step: 1000, Loss: 0.11378707736730576
step: 1100, Loss: 0.11829708516597748
step: 1200, Loss: 0.1146511361002922
step: 1300, Loss: 0.11763232201337814
step: 1400, Loss: 0.11450763791799545
step: 1500, Loss: 0.1150871068239212
step: 1600, Loss: 0.1169888898730278
step: 1700, Loss: 0.11521098762750626
step: 1800, Loss: 0.11417671293020248
step: 1900, Loss: 0.11419115960597992
step: 2000, Loss: 0.11416100710630417
step: 2100, Loss: 0.11523449420928955
step: 2200, Loss: 0.11368630826473236
step: 2300, Loss: 0.11289829015731812
step: 2400, Loss: 0.11663934588432312
step: 2500, Loss: 0.11492345482110977
step: 2600, Loss: 0.11627267301082611
step: 2700, Loss: 0.11406965553760529
step: 2800, Loss: 0.11576008796691895
step: 2900, Loss: 0.1140802800655365
step: 3000, Loss: 0.11424950510263443
step: 3100, Loss: 0.11396513134241104
step: 3200, Loss: 0.1150338277220726
step: 3300, Loss: 0.11593480408191681
step: 3400, Loss: 0.11503882706165314
step: 3500, Loss: 0.11385206878185272
step: 3600, Loss: 0.11457112431526184
step: 3700, Loss: 0.11430349200963974
step: 3800, Loss: 0.11470365524291992
step: 3900, Loss: 0.11415429413318634
step: 4000, Loss: 0.11432872712612152
step: 4100, Loss: 0.11499577760696411
step: 4200, Loss: 0.11421631276607513
step: 4300, Loss: 0.11425961554050446
step: 4400, Loss: 0.11557800322771072
step: 4500, Loss: 0.11338958144187927
step: 4600, Loss: 0.11466819047927856
step: 4700, Loss: 0.11390594393014908
step: 4800, Loss: 0.11395953595638275
step: 4900, Loss: 0.11398446559906006
step: 5000, Loss: 1.8737092018127441
step: 5100, Loss: 0.16670453548431396
step: 5200, Loss: 0.13514140248298645
step: 5300, Loss: 0.11949774622917175
step: 5400, Loss: 0.12453027069568634
step: 5500, Loss: 0.12285428494215012
step: 5600, Loss: 0.12297692894935608
step: 5700, Loss: 0.12797655165195465
step: 5800, Loss: 0.12037350237369537
step: 5900, Loss: 0.11878494918346405
step: 6000, Loss: 0.12031876295804977
step: 6100, Loss: 0.11570543795824051
step: 6200, Loss: 0.12081723660230637
step: 6300, Loss: 0.12162689119577408
step: 6400, Loss: 0.11779007315635681
step: 6500, Loss: 0.11634302139282227
step: 6600, Loss: 0.11617368459701538
step: 6700, Loss: 0.11766166985034943
step: 6800, Loss: 0.11765650659799576
step: 6900, Loss: 0.11784288287162781
step: 7000, Loss: 0.1173110157251358
step: 7100, Loss: 0.11877523362636566
step: 7200, Loss: 0.11692465841770172
step: 7300, Loss: 0.11395062506198883
step: 7400, Loss: 0.11493413895368576
step: 7500, Loss: 0.11812664568424225
step: 7600, Loss: 0.11468401551246643
step: 7700, Loss: 0.11665791273117065
step: 7800, Loss: 0.11616228520870209
step: 7900, Loss: 0.11417172104120255
step: 8000, Loss: 0.116329126060009
step: 8100, Loss: 0.11440484970808029
step: 8200, Loss: 0.1182941198348999
step: 8300, Loss: 0.11588174104690552
step: 8400, Loss: 0.11671102792024612
step: 8500, Loss: 0.1173294335603714
step: 8600, Loss: 0.11575465649366379
step: 8700, Loss: 0.11483095586299896
step: 8800, Loss: 0.11625726521015167
step: 8900, Loss: 0.11477158963680267
step: 9000, Loss: 0.11513921618461609
step: 9100, Loss: 0.11594519019126892
step: 9200, Loss: 0.11642294377088547
step: 9300, Loss: 0.11477537453174591
step: 9400, Loss: 0.11430752277374268
step: 9500, Loss: 0.11341571807861328
step: 9600, Loss: 0.114396832883358
step: 9700, Loss: 0.11415796726942062
step: 9800, Loss: 0.11435931921005249
step: 9900, Loss: 0.11464033275842667
step: 1200, Loss: 0.11453886330127716
step: 1300, Loss: 0.11610937118530273
step: 1400, Loss: 0.11382710933685303
step: 1500, Loss: 0.11355535686016083
step: 1600, Loss: 0.11417236179113388
step: 1700, Loss: 0.11571097373962402
step: 1800, Loss: 0.11469907313585281
step: 1900, Loss: 0.11350912600755692
step: 2000, Loss: 0.11415348201990128
step: 2100, Loss: 0.11358527094125748
step: 2200, Loss: 0.11428900808095932
step: 2300, Loss: 0.11331839859485626
step: 2400, Loss: 0.11369448900222778
step: 2500, Loss: 0.11339052021503448
step: 2600, Loss: 0.11282142996788025
step: 2700, Loss: 0.11338299512863159
step: 2800, Loss: 0.11393984407186508
step: 2900, Loss: 0.11455092579126358
step: 3000, Loss: 0.11424165219068527
step: 3100, Loss: 0.11321626603603363
step: 3200, Loss: 0.1917293816804886
step: 3300, Loss: 0.4183664619922638
step: 3400, Loss: 0.14334768056869507
step: 3500, Loss: 0.11915617436170578
step: 3600, Loss: 0.11771398037672043
step: 3700, Loss: 0.11856978386640549
step: 3800, Loss: 0.12299351394176483
step: 3900, Loss: 0.11785104125738144
step: 4000, Loss: 0.12087109684944153
step: 4100, Loss: 0.11567680537700653
step: 4200, Loss: 0.11727496981620789
step: 4300, Loss: 0.11617759615182877
step: 4400, Loss: 0.1153796911239624
step: 4500, Loss: 0.11564869433641434
step: 4600, Loss: 0.11537998169660568
step: 4700, Loss: 0.1165764257311821
step: 4800, Loss: 0.11581273376941681
step: 4900, Loss: 0.11439166963100433
step: 5000, Loss: 0.11931069940328598
step: 5100, Loss: 0.11648976802825928
step: 5200, Loss: 0.11668605357408524
step: 5300, Loss: 0.1154782772064209
step: 5400, Loss: 0.11474160850048065
step: 5500, Loss: 0.11599404364824295
step: 5600, Loss: 0.11576998978853226
step: 5700, Loss: 0.11384870111942291
step: 5800, Loss: 0.11592079699039459
step: 5900, Loss: 0.11473819613456726
step: 6000, Loss: 0.11384215950965881
step: 6100, Loss: 0.11476683616638184
step: 6200, Loss: 0.11408554017543793
step: 6300, Loss: 0.11416332423686981
step: 6400, Loss: 0.11482176184654236
step: 6500, Loss: 0.1140533909201622
step: 6600, Loss: 0.11400681734085083
step: 6700, Loss: 0.11352653056383133
step: 6800, Loss: 0.11459680646657944
step: 6900, Loss: 0.11471395939588547
step: 7000, Loss: 0.11401927471160889
step: 7100, Loss: 0.11326553672552109
step: 7200, Loss: 0.11495136469602585
step: 7300, Loss: 0.11333741247653961
step: 7400, Loss: 0.11330674588680267
step: 7500, Loss: 0.11480402946472168
step: 7600, Loss: 0.11292716860771179
step: 7700, Loss: 0.11495623737573624
step: 7800, Loss: 0.11272285878658295
step: 7900, Loss: 0.11402325332164764
step: 8000, Loss: 0.1136617437005043
step: 8100, Loss: 0.11346897482872009
step: 8200, Loss: 0.11401066184043884
step: 8300, Loss: 0.11277671158313751
step: 8400, Loss: 0.11378349363803864
step: 8500, Loss: 0.11435798555612564
step: 8600, Loss: 0.1128239706158638
step: 8700, Loss: 0.1164999008178711
step: 8800, Loss: 0.11341693252325058
step: 8900, Loss: 0.1134699359536171
step: 9000, Loss: 0.11463094502687454
step: 9100, Loss: 0.11569204181432724
step: 9200, Loss: 0.11349646002054214
step: 9300, Loss: 0.11572293937206268
step: 9400, Loss: 0.11257947981357574
step: 9500, Loss: 0.11404312402009964
step: 9600, Loss: 0.11406642943620682
step: 9700, Loss: 0.11365047097206116
step: 9800, Loss: 0.11443241685628891
step: 9900, Loss: 0.11340124160051346
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.9565217391304348
recall: 1.0
F_score: 0.9777777777777777
******fold 8******

Training... train_data length:889
step: 0, Loss: 0.1131971925497055
step: 100, Loss: 0.1415901780128479
step: 200, Loss: 0.11451951414346695
step: 300, Loss: 0.11521332710981369
step: 400, Loss: 0.11420805007219315
step: 500, Loss: 0.1136198416352272
step: 600, Loss: 0.11255798488855362
step: 700, Loss: 0.11425140500068665
step: 800, Loss: 0.11290666460990906
step: 900, Loss: 0.1146988645195961
step: 1000, Loss: 0.11356116086244583
step: 1100, Loss: 0.11450079083442688
step: 1200, Loss: 0.11391401290893555
step: 1300, Loss: 0.11444151401519775
step: 1400, Loss: 0.11376631259918213
step: 1500, Loss: 0.11419123411178589
step: 1600, Loss: 0.11343159526586533
step: 1700, Loss: 0.11376746743917465
step: 1800, Loss: 0.11450743675231934
step: 1900, Loss: 0.11314993351697922
step: 2000, Loss: 0.11270144581794739
step: 2100, Loss: 0.11357225477695465
step: 2200, Loss: 0.11280019581317902
step: 2300, Loss: 0.11323387175798416
step: 2400, Loss: 0.11411625891923904
step: 2500, Loss: 0.11412558704614639
step: 2600, Loss: 0.11396105587482452
step: 2700, Loss: 0.1128314658999443
step: 2800, Loss: 0.11244077235460281
step: 2900, Loss: 0.11397358030080795
step: 3000, Loss: 0.11440049111843109
step: 3100, Loss: 0.11376678943634033
step: 3200, Loss: 0.11280783265829086
step: 3300, Loss: 0.11328531801700592
step: 3400, Loss: 0.11330138146877289
step: 3500, Loss: 0.1134994700551033
step: 3600, Loss: 0.11358065903186798
step: 3700, Loss: 0.11477883905172348
step: 3800, Loss: 0.11371184140443802
step: 3900, Loss: 0.11336323618888855
step: 4000, Loss: 0.11354020982980728
step: 4100, Loss: 0.11294294893741608
step: 4200, Loss: 0.11260300874710083
step: 4300, Loss: 0.11446955054998398
step: 4400, Loss: 0.11348868906497955
step: 4500, Loss: 0.11379537731409073
step: 4600, Loss: 0.11361774802207947
step: 4700, Loss: 0.11365053802728653
step: 4800, Loss: 0.11427035927772522
step: 4900, Loss: 0.11420118808746338
step: 5000, Loss: 0.11392590403556824
step: 5100, Loss: 0.11481505632400513
step: 5200, Loss: 0.11533594131469727
step: 5300, Loss: 0.11229179799556732
step: 5400, Loss: 0.11248685419559479
step: 5500, Loss: 0.11337910592556
step: 5600, Loss: 0.11336152255535126
step: 5700, Loss: 0.11219777166843414
step: 5800, Loss: 0.1810746043920517
step: 5900, Loss: 0.14450208842754364
step: 6000, Loss: 0.12849482893943787
step: 6100, Loss: 0.12358840554952621
step: 6200, Loss: 0.1485620141029358
step: 6300, Loss: 0.12800708413124084
step: 6400, Loss: 0.1198221743106842
step: 6500, Loss: 0.11860384792089462
step: 6600, Loss: 0.12199040502309799
step: 6700, Loss: 0.1171070784330368
step: 6800, Loss: 0.11781863868236542
step: 6900, Loss: 0.11956141144037247
step: 7000, Loss: 0.11515234410762787
step: 7100, Loss: 0.1193821132183075
step: 7200, Loss: 0.11582961678504944
step: 7300, Loss: 0.11672056466341019
step: 7400, Loss: 0.12137556821107864
step: 7500, Loss: 0.1154714822769165
step: 7600, Loss: 0.11598695814609528
step: 7700, Loss: 0.11393216997385025
step: 7800, Loss: 0.11481417715549469
step: 7900, Loss: 0.11425245553255081
step: 8000, Loss: 0.11608780920505524
step: 8100, Loss: 0.11540985107421875
step: 8200, Loss: 0.1140332892537117
step: 8300, Loss: 0.11569738388061523
step: 8400, Loss: 0.11588210612535477
step: 8500, Loss: 0.11509191244840622
step: 8600, Loss: 0.11459073424339294
step: 8700, Loss: 0.11377362906932831
step: 8800, Loss: 0.11427965015172958
step: 8900, Loss: 0.11374820023775101
step: 9000, Loss: 0.11521553993225098
step: 9100, Loss: 0.11393854022026062
step: 9200, Loss: 0.11515894532203674
step: 9300, Loss: 0.1125882938504219
step: 9400, Loss: 0.11544203758239746
step: 9500, Loss: 0.11302734911441803
step: 9600, Loss: 0.1131991371512413
step: 9700, Loss: 0.11337094753980637
step: 9800, Loss: 0.11501730978488922
step: 9900, Loss: 0.11300938576459885
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.9545454545454546
recall: 0.9545454545454546
F_score: 0.9545454545454546
******fold 9******

Training... train_data length:890
step: 0, Loss: 0.1137014701962471
step: 100, Loss: 0.11895368993282318
step: 200, Loss: 0.12325156480073929
step: 300, Loss: 0.11904558539390564
step: 400, Loss: 0.11583243310451508
step: 500, Loss: 0.11664262413978577
step: 600, Loss: 0.11669360101222992
step: 700, Loss: 0.11647515743970871
step: 800, Loss: 0.1149216964840889
step: 900, Loss: 0.11887078732252121
step: 1000, Loss: 0.1139814481139183
step: 1100, Loss: 0.11520850658416748
step: 1200, Loss: 0.11620555073022842
step: 1300, Loss: 0.11528246104717255
step: 1400, Loss: 0.1144481673836708
step: 1500, Loss: 0.11445513367652893
step: 1600, Loss: 0.11302602291107178
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7857142857142857
recall: 0.7333333333333333
F_score: 0.7586206896551724
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.13263091444969177
step: 100, Loss: 0.12188571691513062
step: 200, Loss: 0.1165899857878685
step: 300, Loss: 0.11654737591743469
step: 400, Loss: 0.11473387479782104
step: 500, Loss: 0.11410257965326309
step: 600, Loss: 0.11358238756656647
step: 700, Loss: 0.11344797909259796
step: 800, Loss: 0.11520728468894958
step: 900, Loss: 0.11496737599372864
step: 1000, Loss: 0.11645545065402985
step: 1100, Loss: 0.11486274749040604
step: 1200, Loss: 0.11448829621076584
step: 1300, Loss: 0.11433114111423492
step: 1400, Loss: 0.11489396542310715
step: 1500, Loss: 0.11385654658079147
step: 1600, Loss: 0.11270303279161453
step: 1700, Loss: 0.11488647013902664
step: 1800, Loss: 0.11456623673439026
step: 1900, Loss: 0.11652868986129761
step: 2000, Loss: 0.11547203361988068
step: 2100, Loss: 0.11422418057918549
step: 2200, Loss: 0.11464838683605194
step: 2300, Loss: 0.11384458839893341
step: 2400, Loss: 0.11551991105079651
step: 2500, Loss: 0.1145392581820488
step: 2600, Loss: 0.11438418179750443
step: 2700, Loss: 0.11909462511539459
step: 2800, Loss: 0.11313183605670929
step: 2900, Loss: 0.1148541197180748
step: 3000, Loss: 0.11308933794498444
step: 3100, Loss: 0.11525063961744308
step: 3200, Loss: 0.11627067625522614
step: 3300, Loss: 0.11544181406497955
step: 3400, Loss: 0.11421766132116318
step: 3500, Loss: 0.11579954624176025
step: 3600, Loss: 0.11399850249290466
step: 3700, Loss: 0.11620564758777618
step: 3800, Loss: 0.11487442255020142
step: 3900, Loss: 0.1171957403421402
step: 4000, Loss: 0.11401922255754471
step: 4100, Loss: 0.11438945680856705
step: 4200, Loss: 0.1128501147031784
step: 4300, Loss: 0.11455705761909485
step: 4400, Loss: 0.11381957679986954
step: 4500, Loss: 0.11334308981895447
step: 4600, Loss: 0.11391746997833252
step: 4700, Loss: 0.11586224287748337
step: 4800, Loss: 0.11476890742778778
step: 4900, Loss: 0.11605656147003174
step: 5000, Loss: 0.11409568786621094
step: 5100, Loss: 0.11500291526317596
step: 5200, Loss: 0.16586685180664062
step: 5300, Loss: 0.14582963287830353
step: 5400, Loss: 0.14215779304504395
step: 5500, Loss: 0.12914809584617615
step: 5600, Loss: 0.1276247203350067
step: 5700, Loss: 0.13456961512565613
step: 5800, Loss: 0.1246691569685936
step: 5900, Loss: 0.12278016656637192
step: 6000, Loss: 0.13892745971679688
step: 6100, Loss: 0.11752750724554062
step: 6200, Loss: 0.11708712577819824
step: 6300, Loss: 0.11923730373382568
step: 6400, Loss: 0.11711077392101288
step: 6500, Loss: 0.11973617970943451
step: 6600, Loss: 0.11522171646356583
step: 6700, Loss: 0.11825080960988998
step: 6800, Loss: 0.12300466001033783
step: 6900, Loss: 0.1152515560388565
step: 7000, Loss: 0.11577055603265762
step: 7100, Loss: 0.11824198067188263
step: 7200, Loss: 0.11902093142271042
step: 7300, Loss: 0.11612202972173691
step: 7400, Loss: 0.11598915606737137
step: 7500, Loss: 0.11501798778772354
step: 7600, Loss: 0.11860799789428711
step: 7700, Loss: 0.11587566882371902
step: 7800, Loss: 0.1162739247083664
step: 7900, Loss: 0.11708205938339233
step: 8000, Loss: 0.11532662063837051
step: 8100, Loss: 0.11978863924741745
step: 8200, Loss: 0.11439686268568039
step: 8300, Loss: 0.11702615022659302
step: 8400, Loss: 0.11445385962724686
step: 8500, Loss: 0.11409179121255875
step: 8600, Loss: 0.114810049533844
step: 8700, Loss: 0.11764094233512878
step: 8800, Loss: 0.11395691335201263
step: 8900, Loss: 0.11351203918457031
step: 9000, Loss: 0.11409929394721985
step: 9100, Loss: 0.11605746299028397
step: 9200, Loss: 0.11571475863456726
step: 9300, Loss: 0.11514072865247726
step: 9400, Loss: 0.11434310674667358
step: 9500, Loss: 0.11661852896213531
step: 9600, Loss: 0.11437384784221649
step: 9700, Loss: 0.11334508657455444
step: 9800, Loss: 0.11566458642482758
step: 9900, Loss: 0.11595578491687775
training successfully ended.
validating...
validate data length:31
acc: 0.6333333333333333
precision: 0.6666666666666666
recall: 0.7777777777777778
F_score: 0.717948717948718
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.12311573326587677
step: 100, Loss: 0.11982826888561249
step: 200, Loss: 0.11652230471372604
step: 300, Loss: 0.11696378141641617
step: 400, Loss: 0.11500125378370285
step: 500, Loss: 0.11593914777040482
step: 600, Loss: 0.11598685383796692
step: 700, Loss: 0.1141824871301651
step: 800, Loss: 0.11421950161457062
step: 900, Loss: 0.1152665838599205
step: 1000, Loss: 0.11514696478843689
step: 1100, Loss: 0.11414296925067902
step: 1200, Loss: 0.11376094073057175
step: 1300, Loss: 0.11465877294540405
step: 1400, Loss: 0.1151190847158432
step: 1500, Loss: 0.11540652066469193
step: 1600, Loss: 0.11572006344795227
step: 1700, Loss: 0.11813691258430481
step: 1800, Loss: 0.11392749100923538
step: 1900, Loss: 0.1141752302646637
step: 2000, Loss: 0.11355791240930557
step: 2100, Loss: 0.1134883239865303
step: 2200, Loss: 0.11525314301252365
step: 2300, Loss: 0.113792285323143
step: 2400, Loss: 0.11510246992111206
step: 2500, Loss: 0.1203436404466629
step: 2600, Loss: 0.1144864484667778
step: 2700, Loss: 0.11529959738254547
step: 2800, Loss: 0.11511800438165665
step: 2900, Loss: 0.11695954203605652
step: 3000, Loss: 0.11498049646615982
step: 3100, Loss: 0.11448940634727478
step: 3200, Loss: 0.1140734925866127
step: 3300, Loss: 0.11482920497655869
step: 3400, Loss: 0.11642180383205414
step: 3500, Loss: 0.11475662142038345
step: 3600, Loss: 0.11556977778673172
step: 3700, Loss: 0.11566675454378128
step: 3800, Loss: 0.11413629353046417
step: 3900, Loss: 0.11366996169090271
step: 4000, Loss: 0.11625087261199951
step: 4100, Loss: 0.11454130709171295
step: 4200, Loss: 0.11564771085977554
step: 4300, Loss: 0.1141483262181282
step: 4400, Loss: 0.11397305130958557
step: 4500, Loss: 0.11441083252429962
step: 4600, Loss: 0.11377415060997009
step: 4700, Loss: 0.11507225036621094
step: 4800, Loss: 0.11307675391435623
step: 4900, Loss: 0.11368313431739807
step: 5000, Loss: 0.11795095354318619
step: 5100, Loss: 0.11348965764045715
step: 5200, Loss: 0.11457738280296326
step: 5300, Loss: 0.11432106792926788
step: 5400, Loss: 0.11569779366254807
step: 5500, Loss: 0.11391940712928772
step: 5600, Loss: 0.11519306898117065
step: 5700, Loss: 0.9766250848770142
step: 5800, Loss: 0.16525554656982422
step: 5900, Loss: 0.13115009665489197
step: 6000, Loss: 0.14237560331821442
step: 6100, Loss: 0.1272357851266861
step: 6200, Loss: 0.1334567815065384
step: 6300, Loss: 0.12075929343700409
step: 6400, Loss: 0.1448618769645691
step: 6500, Loss: 0.12185066193342209
step: 6600, Loss: 0.1228761151432991
step: 6700, Loss: 0.11925996840000153
step: 6800, Loss: 0.11810410767793655
step: 6900, Loss: 0.12143312394618988
step: 7000, Loss: 0.11961071938276291
step: 7100, Loss: 0.12122151255607605
step: 7200, Loss: 0.11925305426120758
step: 7300, Loss: 0.1169915646314621
step: 7400, Loss: 0.12465975433588028
step: 7500, Loss: 0.1182042583823204
step: 7600, Loss: 0.11859018355607986
step: 7700, Loss: 0.12066923081874847
step: 7800, Loss: 0.11971575021743774
step: 7900, Loss: 0.1165313795208931
step: 8000, Loss: 0.11718476563692093
step: 8100, Loss: 0.12363620102405548
step: 8200, Loss: 0.11682575941085815
step: 8300, Loss: 0.12008336931467056
step: 8400, Loss: 0.11624404042959213
step: 8500, Loss: 0.11555950343608856
step: 8600, Loss: 0.11768670380115509
step: 8700, Loss: 0.1170276403427124
step: 8800, Loss: 0.1171140968799591
step: 8900, Loss: 0.11543388664722443
step: 9000, Loss: 0.11612619459629059
step: 9100, Loss: 0.11623363196849823
step: 9200, Loss: 0.1165185198187828
step: 9300, Loss: 0.11824242770671844
step: 9400, Loss: 0.11453375965356827
step: 9500, Loss: 0.11514487117528915
step: 9600, Loss: 0.11558422446250916
step: 9700, Loss: 0.11608148366212845
step: 9800, Loss: 0.11935724318027496
step: 9900, Loss: 0.11545609682798386
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8095238095238095
recall: 0.9444444444444444
F_score: 0.8717948717948718
step: 1700, Loss: 0.11570437997579575
step: 1800, Loss: 0.11388735473155975
step: 1900, Loss: 0.11373341083526611
step: 2000, Loss: 0.11444157361984253
step: 2100, Loss: 0.11498430371284485
step: 2200, Loss: 0.11424607783555984
step: 2300, Loss: 0.11303866654634476
step: 2400, Loss: 0.11386813968420029
step: 2500, Loss: 0.1133398488163948
step: 2600, Loss: 0.11373994499444962
step: 2700, Loss: 0.11340377479791641
step: 2800, Loss: 0.11358209699392319
step: 2900, Loss: 0.1143595427274704
step: 3000, Loss: 0.11421032249927521
step: 3100, Loss: 0.11541137099266052
step: 3200, Loss: 0.11382114887237549
step: 3300, Loss: 0.1149381771683693
step: 3400, Loss: 0.11477993428707123
step: 3500, Loss: 0.11357609182596207
step: 3600, Loss: 0.1144910603761673
step: 3700, Loss: 0.1137038841843605
step: 3800, Loss: 0.1135215163230896
step: 3900, Loss: 0.11466288566589355
step: 4000, Loss: 0.1144096776843071
step: 4100, Loss: 0.11334124207496643
step: 4200, Loss: 0.11766603589057922
step: 4300, Loss: 0.33299046754837036
step: 4400, Loss: 1.4593050479888916
step: 4500, Loss: 0.13664831221103668
step: 4600, Loss: 0.12574821710586548
step: 4700, Loss: 0.12476026266813278
step: 4800, Loss: 0.1247626394033432
step: 4900, Loss: 0.1233961433172226
step: 5000, Loss: 0.12076171487569809
step: 5100, Loss: 0.1196691021323204
step: 5200, Loss: 0.12222844362258911
step: 5300, Loss: 0.11682340502738953
step: 5400, Loss: 0.11699619144201279
step: 5500, Loss: 0.11788903176784515
step: 5600, Loss: 0.11781388521194458
step: 5700, Loss: 0.11641303449869156
step: 5800, Loss: 0.11606302112340927
step: 5900, Loss: 0.11731258779764175
step: 6000, Loss: 0.11743059754371643
step: 6100, Loss: 0.11731941998004913
step: 6200, Loss: 0.11646197736263275
step: 6300, Loss: 0.11544433236122131
step: 6400, Loss: 0.11535882949829102
step: 6500, Loss: 0.1164904236793518
step: 6600, Loss: 0.11584845185279846
step: 6700, Loss: 0.11641323566436768
step: 6800, Loss: 0.11715856939554214
step: 6900, Loss: 0.11553633958101273
step: 7000, Loss: 0.11436387896537781
step: 7100, Loss: 0.11552784591913223
step: 7200, Loss: 0.11527524143457413
step: 7300, Loss: 0.11586113274097443
step: 7400, Loss: 0.11442691832780838
step: 7500, Loss: 0.11516512930393219
step: 7600, Loss: 0.11566144973039627
step: 7700, Loss: 0.11514230817556381
step: 7800, Loss: 0.11384548246860504
step: 7900, Loss: 0.11543215066194534
step: 8000, Loss: 0.1140531525015831
step: 8100, Loss: 0.11442390084266663
step: 8200, Loss: 0.1141727864742279
step: 8300, Loss: 0.11432826519012451
step: 8400, Loss: 0.11383669823408127
step: 8500, Loss: 0.1162620484828949
step: 8600, Loss: 0.11356699466705322
step: 8700, Loss: 0.11453918367624283
step: 8800, Loss: 0.11445952206850052
step: 8900, Loss: 0.1149083599448204
step: 9000, Loss: 0.1136859580874443
step: 9100, Loss: 0.11521735787391663
step: 9200, Loss: 0.11368968337774277
step: 9300, Loss: 0.11332608014345169
step: 9400, Loss: 0.11269091069698334
step: 9500, Loss: 0.1140362024307251
step: 9600, Loss: 0.11356854438781738
step: 9700, Loss: 0.11560441553592682
step: 9800, Loss: 0.1161671057343483
step: 9900, Loss: 0.11292947083711624
training successfully ended.
validating...
validate data length:98
acc: 0.9895833333333334
precision: 0.9807692307692307
recall: 1.0
F_score: 0.9902912621359222
******fold 10******

Training... train_data length:890
step: 0, Loss: 0.11367423832416534
step: 100, Loss: 0.11522562801837921
step: 200, Loss: 0.11634427309036255
step: 300, Loss: 0.11613284051418304
step: 400, Loss: 0.11484992504119873
step: 500, Loss: 0.11416026204824448
step: 600, Loss: 0.11548064649105072
step: 700, Loss: 0.11637880653142929
step: 800, Loss: 0.11455639451742172
step: 900, Loss: 0.11502091586589813
step: 1000, Loss: 0.11328303068876266
step: 1100, Loss: 0.11490623652935028
step: 1200, Loss: 0.11341774463653564
step: 1300, Loss: 0.11386928707361221
step: 1400, Loss: 0.11453575640916824
step: 1500, Loss: 0.11566661298274994
step: 1600, Loss: 0.11392415314912796
step: 1700, Loss: 0.113278329372406
step: 1800, Loss: 0.1161947026848793
step: 1900, Loss: 0.11431767046451569
step: 2000, Loss: 0.11300156265497208
step: 2100, Loss: 0.11429449915885925
step: 2200, Loss: 0.11498510092496872
step: 2300, Loss: 0.11467019468545914
step: 2400, Loss: 0.11357185244560242
step: 2500, Loss: 0.11808803677558899
step: 2600, Loss: 0.11345268785953522
step: 2700, Loss: 0.11429040879011154
step: 2800, Loss: 0.1135689914226532
step: 2900, Loss: 0.1143648624420166
step: 3000, Loss: 0.11393143236637115
step: 3100, Loss: 0.11534172296524048
step: 3200, Loss: 0.11397312581539154
step: 3300, Loss: 0.11446168273687363
step: 3400, Loss: 0.11410210281610489
step: 3500, Loss: 0.11388795077800751
step: 3600, Loss: 0.11523549258708954
step: 3700, Loss: 0.1142943874001503
step: 3800, Loss: 0.11432584375143051
step: 3900, Loss: 0.11296147108078003
step: 4000, Loss: 0.11568884551525116
step: 4100, Loss: 0.11371511220932007
step: 4200, Loss: 0.1151968315243721
step: 4300, Loss: 0.11380891501903534
step: 4400, Loss: 0.11503896117210388
step: 4500, Loss: 0.11424346268177032
step: 4600, Loss: 0.11958812922239304
step: 4700, Loss: 0.11437243223190308
step: 4800, Loss: 0.31122663617134094
step: 4900, Loss: 0.13479650020599365
step: 5000, Loss: 0.12773139774799347
step: 5100, Loss: 0.127017080783844
step: 5200, Loss: 0.12245522439479828
step: 5300, Loss: 0.12567304074764252
step: 5400, Loss: 0.12095551192760468
step: 5500, Loss: 0.11976417899131775
step: 5600, Loss: 0.11902938038110733
step: 5700, Loss: 0.11809946596622467
step: 5800, Loss: 0.11723122745752335
step: 5900, Loss: 0.11654408276081085
step: 6000, Loss: 0.11610312759876251
step: 6100, Loss: 0.11612462997436523
step: 6200, Loss: 0.11837077140808105
step: 6300, Loss: 0.11707023531198502
step: 6400, Loss: 0.11539750546216965
step: 6500, Loss: 0.11765843629837036
step: 6600, Loss: 0.1158098354935646
step: 6700, Loss: 0.11620467901229858
step: 6800, Loss: 0.1153472512960434
step: 6900, Loss: 0.11605226993560791
step: 7000, Loss: 0.114865742623806
step: 7100, Loss: 0.1149856448173523
step: 7200, Loss: 0.11362393200397491
step: 7300, Loss: 0.11501345783472061
step: 7400, Loss: 0.11433905363082886
step: 7500, Loss: 0.11532777547836304
step: 7600, Loss: 0.11541318148374557
step: 7700, Loss: 0.1149916872382164
step: 7800, Loss: 0.11500871181488037
step: 7900, Loss: 0.11389867961406708
step: 8000, Loss: 0.11463412642478943
step: 8100, Loss: 0.11544650793075562
step: 8200, Loss: 0.11595932394266129
step: 8300, Loss: 0.11421098560094833
step: 8400, Loss: 0.11498220264911652
step: 8500, Loss: 0.11519575119018555
step: 8600, Loss: 0.11441223323345184
step: 8700, Loss: 0.11456508934497833
step: 8800, Loss: 0.11478756368160248
step: 8900, Loss: 0.11564803123474121
step: 9000, Loss: 0.1133442223072052
step: 9100, Loss: 0.11319727450609207
step: 9200, Loss: 0.11491379141807556
step: 9300, Loss: 0.1151275783777237
step: 9400, Loss: 0.1140601858496666
step: 9500, Loss: 0.11365446448326111
step: 9600, Loss: 0.1141320988535881
step: 9700, Loss: 0.11314565688371658
step: 9800, Loss: 0.11382297426462173
step: 9900, Loss: 0.11360139399766922
training successfully ended.
validating...
validate data length:98
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 9 Avgacc: 0.975 Avgfscore: 0.9762460889591402 
 Max acc:1.0, Max f score:1.0
******** mix subject_10 ********

[399, 361]
******fold 1******

Training... train_data length:684
step: 0, Loss: 55.080081939697266
step: 100, Loss: 3.0296695232391357
step: 200, Loss: 0.16090266406536102
step: 300, Loss: 0.13904213905334473
step: 400, Loss: 0.13717660307884216
step: 500, Loss: 0.16149261593818665
step: 600, Loss: 0.1324724406003952
step: 700, Loss: 0.13110622763633728
step: 800, Loss: 0.12837448716163635
step: 900, Loss: 0.1233978271484375
step: 1000, Loss: 0.1311967968940735
step: 1100, Loss: 0.1345868557691574
step: 1200, Loss: 0.1277543008327484
step: 1300, Loss: 0.12782296538352966
step: 1400, Loss: 0.12044523656368256
step: 1500, Loss: 0.19900338351726532
step: 1600, Loss: 0.11901973187923431
step: 1700, Loss: 0.1264009177684784
step: 1800, Loss: 0.12292977422475815
step: 1900, Loss: 0.11810699850320816
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.1242208331823349
step: 100, Loss: 0.1195945143699646
step: 200, Loss: 0.11643917858600616
step: 300, Loss: 0.11952802538871765
step: 400, Loss: 0.11408958584070206
step: 500, Loss: 0.11956837773323059
step: 600, Loss: 0.1145939975976944
step: 700, Loss: 0.11466757208108902
step: 800, Loss: 0.11429329216480255
step: 900, Loss: 0.1168162077665329
step: 1000, Loss: 0.11330249905586243
step: 1100, Loss: 0.11483027040958405
step: 1200, Loss: 0.11418027430772781
step: 1300, Loss: 0.11503305286169052
step: 1400, Loss: 0.11387301236391068
step: 1500, Loss: 0.11577345430850983
step: 1600, Loss: 0.11543676257133484
step: 1700, Loss: 0.1144266426563263
step: 1800, Loss: 0.11478681862354279
step: 1900, Loss: 0.11491654813289642
step: 2000, Loss: 0.1139441505074501
step: 2100, Loss: 0.11595690250396729
step: 2200, Loss: 0.11490672081708908
step: 2300, Loss: 0.11697804927825928
step: 2400, Loss: 0.11454615741968155
step: 2500, Loss: 0.1150173768401146
step: 2600, Loss: 0.11375647038221359
step: 2700, Loss: 0.11510487645864487
step: 2800, Loss: 0.11393214762210846
step: 2900, Loss: 0.11512649804353714
step: 3000, Loss: 0.11490694433450699
step: 3100, Loss: 0.11546720564365387
step: 3200, Loss: 0.11330253630876541
step: 3300, Loss: 0.11562436819076538
step: 3400, Loss: 0.11470898240804672
step: 3500, Loss: 0.11624982208013535
step: 3600, Loss: 0.114658884704113
step: 3700, Loss: 0.1150999665260315
step: 3800, Loss: 0.1148492693901062
step: 3900, Loss: 0.11448575556278229
step: 4000, Loss: 0.1144726574420929
step: 4100, Loss: 0.11474639177322388
step: 4200, Loss: 0.11763408035039902
step: 4300, Loss: 0.11845173686742783
step: 4400, Loss: 0.11427450180053711
step: 4500, Loss: 0.1138986125588417
step: 4600, Loss: 0.11544355750083923
step: 4700, Loss: 0.11499463021755219
step: 4800, Loss: 0.11553800106048584
step: 4900, Loss: 0.11556772142648697
step: 5000, Loss: 0.11486804485321045
step: 5100, Loss: 1.4253766536712646
step: 5200, Loss: 0.13216885924339294
step: 5300, Loss: 0.12279307842254639
step: 5400, Loss: 0.12343228608369827
step: 5500, Loss: 0.12576179206371307
step: 5600, Loss: 0.11838233470916748
step: 5700, Loss: 0.12149713188409805
step: 5800, Loss: 0.12105992436408997
step: 5900, Loss: 0.11996712535619736
step: 6000, Loss: 0.12078295648097992
step: 6100, Loss: 0.12107869237661362
step: 6200, Loss: 0.11850575357675552
step: 6300, Loss: 0.11561480909585953
step: 6400, Loss: 0.1157309040427208
step: 6500, Loss: 0.1167827695608139
step: 6600, Loss: 0.12011173367500305
step: 6700, Loss: 0.1165570393204689
step: 6800, Loss: 0.11576274037361145
step: 6900, Loss: 0.1200246810913086
step: 7000, Loss: 0.12103129923343658
step: 7100, Loss: 0.11740031838417053
step: 7200, Loss: 0.11653320491313934
step: 7300, Loss: 0.11913405358791351
step: 7400, Loss: 0.11562758684158325
step: 7500, Loss: 0.12815068662166595
step: 7600, Loss: 0.11601598560810089
step: 7700, Loss: 0.11833019554615021
step: 7800, Loss: 0.12265324592590332
step: 7900, Loss: 0.11927732825279236
step: 8000, Loss: 0.11467108875513077
step: 8100, Loss: 0.11757122725248337
step: 8200, Loss: 0.11476198583841324
step: 8300, Loss: 0.11609750986099243
step: 8400, Loss: 0.11484229564666748
step: 8500, Loss: 0.11842775344848633
step: 8600, Loss: 0.1133175939321518
step: 8700, Loss: 0.1166553795337677
step: 8800, Loss: 0.11500679701566696
step: 8900, Loss: 0.11554428189992905
step: 9000, Loss: 0.11447538435459137
step: 9100, Loss: 0.11755182594060898
step: 9200, Loss: 0.11324983835220337
step: 9300, Loss: 0.11726240813732147
step: 9400, Loss: 0.11354465037584305
step: 9500, Loss: 0.11636057496070862
step: 9600, Loss: 0.11537754535675049
step: 9700, Loss: 0.1204436719417572
step: 9800, Loss: 0.11437256634235382
step: 9900, Loss: 0.11429300904273987
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8
recall: 0.9411764705882353
F_score: 0.8648648648648648
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.12473741173744202
step: 100, Loss: 0.11578106135129929
step: 200, Loss: 0.11637435108423233
step: 300, Loss: 0.11645254492759705
step: 400, Loss: 0.1146756038069725
step: 500, Loss: 0.11327315866947174
step: 600, Loss: 0.11411483585834503
step: 700, Loss: 0.11690405011177063
step: 800, Loss: 0.11401477456092834
step: 900, Loss: 0.11521324515342712
step: 1000, Loss: 0.11491797864437103
step: 1100, Loss: 0.11471535265445709
step: 1200, Loss: 0.11443275213241577
step: 1300, Loss: 0.1159190982580185
step: 1400, Loss: 0.11460696905851364
step: 1500, Loss: 0.11493130773305893
step: 1600, Loss: 0.11385148763656616
step: 1700, Loss: 0.1147710382938385
step: 1800, Loss: 0.11500764638185501
step: 1900, Loss: 0.11580532789230347
step: 2000, Loss: 0.11621150374412537
step: 2100, Loss: 0.11555330455303192
step: 2200, Loss: 0.11499875038862228
step: 2300, Loss: 0.11936479061841965
step: 2400, Loss: 0.1148645430803299
step: 2500, Loss: 0.11503492295742035
step: 2600, Loss: 0.11447057127952576
step: 2700, Loss: 0.11389482766389847
step: 2800, Loss: 0.11786291003227234
step: 2900, Loss: 0.11451834440231323
step: 3000, Loss: 0.11975238472223282
step: 3100, Loss: 0.11357481777667999
step: 3200, Loss: 0.11551424860954285
step: 3300, Loss: 0.11533792316913605
step: 3400, Loss: 0.11497127264738083
step: 3500, Loss: 0.11438744515180588
step: 3600, Loss: 0.11379720270633698
step: 3700, Loss: 0.11705106496810913
step: 3800, Loss: 0.11567317694425583
step: 3900, Loss: 0.11653175950050354
step: 4000, Loss: 0.11622317135334015
step: 4100, Loss: 0.11532515287399292
step: 4200, Loss: 0.1166120320558548
step: 4300, Loss: 0.11477917432785034
step: 4400, Loss: 0.11398367583751678
step: 4500, Loss: 0.11375536769628525
step: 4600, Loss: 0.1148945614695549
step: 4700, Loss: 0.11553715169429779
step: 4800, Loss: 0.11449341475963593
step: 4900, Loss: 0.11488794535398483
step: 5000, Loss: 0.11394993960857391
step: 5100, Loss: 0.1154663935303688
step: 5200, Loss: 0.1159699410200119
step: 5300, Loss: 0.11465145647525787
step: 5400, Loss: 0.11538655310869217
step: 5500, Loss: 0.1156737208366394
step: 5600, Loss: 0.11533670127391815
step: 5700, Loss: 0.11659681797027588
step: 5800, Loss: 0.7064193487167358
step: 5900, Loss: 0.13721367716789246
step: 6000, Loss: 0.13472972810268402
step: 6100, Loss: 0.12684375047683716
step: 6200, Loss: 0.12447433173656464
step: 6300, Loss: 0.12396794557571411
step: 6400, Loss: 0.12093295902013779
step: 6500, Loss: 0.11679033935070038
step: 6600, Loss: 0.12037599086761475
step: 6700, Loss: 0.11944368481636047
step: 6800, Loss: 0.11917167901992798
step: 6900, Loss: 0.12371139228343964
step: 7000, Loss: 0.11941857635974884
step: 7100, Loss: 0.1197454184293747
step: 7200, Loss: 0.1198064312338829
step: 7300, Loss: 0.11815232783555984
step: 7400, Loss: 0.11607760190963745
step: 7500, Loss: 0.11723364889621735
step: 7600, Loss: 0.11795532703399658
step: 7700, Loss: 0.1186707466840744
step: 7800, Loss: 0.11922097951173782
step: 7900, Loss: 0.11683772504329681
step: 8000, Loss: 0.11612030118703842
step: 8100, Loss: 0.11456918716430664
step: 8200, Loss: 0.1157757043838501
step: 8300, Loss: 0.11647719889879227
step: 8400, Loss: 0.11547651886940002
step: 8500, Loss: 0.11592653393745422
step: 8600, Loss: 0.1152186319231987
step: 8700, Loss: 0.11514761298894882
step: 8800, Loss: 0.11639051139354706
step: 8900, Loss: 0.11542531102895737
step: 9000, Loss: 0.1177457869052887
step: 9100, Loss: 0.11641581356525421
step: 9200, Loss: 0.11520138382911682
step: 9300, Loss: 0.11801498383283615
step: 9400, Loss: 0.11555194854736328
step: 9500, Loss: 0.11518501490354538
step: 9600, Loss: 0.11498282849788666
step: 9700, Loss: 0.11704139411449432
step: 9800, Loss: 0.11531142890453339
step: 9900, Loss: 0.11554327607154846
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7222222222222222
recall: 0.8666666666666667
F_score: 0.7878787878787877
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.12736335396766663
step: 100, Loss: 0.1169145330786705
step: 200, Loss: 0.11769745498895645
step: 300, Loss: 0.11691722273826599
step: 2000, Loss: 0.12062957882881165
step: 2100, Loss: 0.11659646779298782
step: 2200, Loss: 0.11791691184043884
step: 2300, Loss: 0.1171903908252716
step: 2400, Loss: 0.11562255769968033
step: 2500, Loss: 0.11844899505376816
step: 2600, Loss: 0.11393088847398758
step: 2700, Loss: 0.11599690467119217
step: 2800, Loss: 0.11495892703533173
step: 2900, Loss: 0.11430680751800537
step: 3000, Loss: 0.11559443920850754
step: 3100, Loss: 0.1156507357954979
step: 3200, Loss: 0.11630089581012726
step: 3300, Loss: 0.11506585776805878
step: 3400, Loss: 0.19706960022449493
step: 3500, Loss: 0.11484090238809586
step: 3600, Loss: 0.1148771122097969
step: 3700, Loss: 0.11594779789447784
step: 3800, Loss: 0.11716824769973755
step: 3900, Loss: 0.11551272124052048
step: 4000, Loss: 0.11407998204231262
step: 4100, Loss: 0.11550816893577576
step: 4200, Loss: 0.11448900401592255
step: 4300, Loss: 0.1154240071773529
step: 4400, Loss: 0.11405184119939804
step: 4500, Loss: 0.11585699021816254
step: 4600, Loss: 0.1165471151471138
step: 4700, Loss: 0.11522534489631653
step: 4800, Loss: 0.11431071162223816
step: 4900, Loss: 0.11597560346126556
step: 5000, Loss: 0.11471124738454819
step: 5100, Loss: 0.11424911022186279
step: 5200, Loss: 0.1143074482679367
step: 5300, Loss: 0.19669583439826965
step: 5400, Loss: 0.11353504657745361
step: 5500, Loss: 0.11414199322462082
step: 5600, Loss: 0.11425136029720306
step: 5700, Loss: 0.11413859575986862
step: 5800, Loss: 0.11365489661693573
step: 5900, Loss: 0.11273746192455292
step: 6000, Loss: 0.11311127245426178
step: 6100, Loss: 0.11396419256925583
step: 6200, Loss: 0.11559103429317474
step: 6300, Loss: 0.11406272649765015
step: 6400, Loss: 0.1133471354842186
step: 6500, Loss: 0.11462757736444473
step: 6600, Loss: 0.11402498930692673
step: 6700, Loss: 0.11478470265865326
step: 6800, Loss: 1.4571752548217773
step: 6900, Loss: 0.15648426115512848
step: 7000, Loss: 0.13493473827838898
step: 7100, Loss: 0.1319386512041092
step: 7200, Loss: 0.22792312502861023
step: 7300, Loss: 0.1211649626493454
step: 7400, Loss: 0.11918757855892181
step: 7500, Loss: 0.12331116199493408
step: 7600, Loss: 0.11977238208055496
step: 7700, Loss: 0.1334535777568817
step: 7800, Loss: 0.1207180991768837
step: 7900, Loss: 0.12163162231445312
step: 8000, Loss: 0.12249346822500229
step: 8100, Loss: 0.12317602336406708
step: 8200, Loss: 0.118251733481884
step: 8300, Loss: 0.12036188691854477
step: 8400, Loss: 0.11748193949460983
step: 8500, Loss: 0.11452766507863998
step: 8600, Loss: 0.12028983980417252
step: 8700, Loss: 0.11608406901359558
step: 8800, Loss: 0.11434774100780487
step: 8900, Loss: 0.11707928031682968
step: 9000, Loss: 0.12107090651988983
step: 9100, Loss: 0.19955049455165863
step: 9200, Loss: 0.1147998496890068
step: 9300, Loss: 0.11637068539857864
step: 9400, Loss: 0.11747243255376816
step: 9500, Loss: 0.11576637625694275
step: 9600, Loss: 0.11715038120746613
step: 9700, Loss: 0.11574201285839081
step: 9800, Loss: 0.11762269586324692
step: 9900, Loss: 0.1150171086192131
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.9459459459459459
recall: 0.9210526315789473
F_score: 0.9333333333333332
******fold 2******

Training... train_data length:684
step: 0, Loss: 0.8157463073730469
step: 100, Loss: 0.1222478598356247
step: 200, Loss: 0.11751314252614975
step: 300, Loss: 0.11542802304029465
step: 400, Loss: 0.1176854819059372
step: 500, Loss: 0.11449933052062988
step: 600, Loss: 0.11595307290554047
step: 700, Loss: 0.11400608718395233
step: 800, Loss: 0.11559353023767471
step: 900, Loss: 0.11392340064048767
step: 1000, Loss: 0.11624472588300705
step: 1100, Loss: 0.11439468711614609
step: 1200, Loss: 0.11488509178161621
step: 1300, Loss: 0.11520054191350937
step: 1400, Loss: 0.11345285922288895
step: 1500, Loss: 0.1950932741165161
step: 1600, Loss: 0.11469694972038269
step: 1700, Loss: 0.11511845141649246
step: 1800, Loss: 0.11456826329231262
step: 1900, Loss: 0.11314118653535843
step: 2000, Loss: 0.11418337374925613
step: 2100, Loss: 0.11396431177854538
step: 2200, Loss: 0.11299903690814972
step: 2300, Loss: 0.11440249532461166
step: 2400, Loss: 0.11408823728561401
step: 2500, Loss: 0.11412438005208969
step: 2600, Loss: 0.1142982617020607
step: 2700, Loss: 0.11400245130062103
step: 2800, Loss: 0.1142873466014862
step: 2900, Loss: 0.1130967065691948
step: 3000, Loss: 0.11268990486860275
step: 3100, Loss: 0.11409731209278107
step: 3200, Loss: 0.11446061730384827
step: 3300, Loss: 0.11400943249464035
step: 3400, Loss: 0.19245891273021698
step: 3500, Loss: 0.11375057697296143
step: 3600, Loss: 0.12136681377887726
step: 3700, Loss: 0.11428912729024887
step: 3800, Loss: 0.11443427950143814
step: 3900, Loss: 0.11343852430582047
step: 4000, Loss: 0.11394070833921432
step: 4100, Loss: 0.11355556547641754
step: 4200, Loss: 0.11284316331148148
step: 4300, Loss: 0.11347176134586334
step: 4400, Loss: 0.11326979100704193
step: 4500, Loss: 0.11378200352191925
step: 4600, Loss: 0.11392492055892944
step: 4700, Loss: 0.11409754306077957
step: 4800, Loss: 0.11557159572839737
step: 4900, Loss: 0.11322702467441559
step: 5000, Loss: 0.11362824589014053
step: 5100, Loss: 0.11590012907981873
step: 5200, Loss: 0.11451438814401627
step: 5300, Loss: 0.19773612916469574
step: 5400, Loss: 0.11498194932937622
step: 5500, Loss: 0.11565732210874557
step: 5600, Loss: 0.11368215829133987
step: 5700, Loss: 0.11650723963975906
step: 5800, Loss: 0.5308369398117065
step: 5900, Loss: 0.13861988484859467
step: 6000, Loss: 0.13241709768772125
step: 6100, Loss: 0.12673844397068024
step: 6200, Loss: 0.12631379067897797
step: 6300, Loss: 0.12264098227024078
step: 6400, Loss: 0.12077973037958145
step: 6500, Loss: 0.12022220343351364
step: 6600, Loss: 0.12125785648822784
step: 6700, Loss: 0.11762263625860214
step: 6800, Loss: 0.1196981742978096
step: 6900, Loss: 0.1176961213350296
step: 7000, Loss: 0.12056559324264526
step: 7100, Loss: 0.11581495404243469
step: 7200, Loss: 0.21309436857700348
step: 7300, Loss: 0.116121806204319
step: 7400, Loss: 0.1174488291144371
step: 7500, Loss: 0.11913905292749405
step: 7600, Loss: 0.1178993284702301
step: 7700, Loss: 0.11724036931991577
step: 7800, Loss: 0.11531753838062286
step: 7900, Loss: 0.11679477989673615
step: 8000, Loss: 0.11414966732263565
step: 8100, Loss: 0.117288738489151
step: 8200, Loss: 0.11639759689569473
step: 8300, Loss: 0.1179429143667221
step: 8400, Loss: 0.11481277644634247
step: 8500, Loss: 0.1171259954571724
step: 8600, Loss: 0.11282911151647568
step: 8700, Loss: 0.11439630389213562
step: 8800, Loss: 0.11470425128936768
step: 8900, Loss: 0.11636482179164886
step: 9000, Loss: 0.11503135412931442
step: 9100, Loss: 0.19876143336296082
step: 9200, Loss: 0.11372215300798416
step: 9300, Loss: 0.11403925716876984
step: 9400, Loss: 0.11636243760585785
step: 9500, Loss: 0.11411985754966736
step: 9600, Loss: 0.11443455517292023
step: 9700, Loss: 0.1128334254026413
step: 9800, Loss: 0.11464568227529526
step: 9900, Loss: 0.11489345133304596
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9705882352941176
recall: 0.9705882352941176
F_score: 0.9705882352941176
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.11558584868907928
step: 100, Loss: 0.11641529202461243
step: 200, Loss: 0.11850053071975708
step: 300, Loss: 0.1170695424079895
step: 400, Loss: 0.1151716411113739
step: 500, Loss: 0.11422189325094223
step: 600, Loss: 0.11475516110658646
step: 700, Loss: 0.11586838960647583
step: 800, Loss: 0.11349941790103912
step: 900, Loss: 0.11534538120031357
step: 1000, Loss: 0.11379223316907883
step: 1100, Loss: 0.11340191215276718
step: 1200, Loss: 0.11447947472333908
step: 1300, Loss: 0.11343727260828018
step: 1400, Loss: 0.1141912117600441
step: 1500, Loss: 0.19596385955810547
step: 1600, Loss: 0.11422351002693176
step: 1700, Loss: 0.11432860791683197
step: 1800, Loss: 0.11420529335737228
step: 1900, Loss: 0.11305572837591171
step: 2000, Loss: 0.11284792423248291
step: 2100, Loss: 0.11367035657167435
step: 2200, Loss: 0.11407393217086792
step: 2300, Loss: 0.11438025534152985
step: 2400, Loss: 0.11396674811840057
step: 400, Loss: 0.11937201023101807
step: 500, Loss: 0.11600649356842041
step: 600, Loss: 0.11580082774162292
step: 700, Loss: 0.11505669355392456
step: 800, Loss: 0.1163424700498581
step: 900, Loss: 0.11488857120275497
step: 1000, Loss: 0.11487819254398346
step: 1100, Loss: 0.11408168822526932
step: 1200, Loss: 0.11443865299224854
step: 1300, Loss: 0.11393722146749496
step: 1400, Loss: 0.11462067812681198
step: 1500, Loss: 0.11415793001651764
step: 1600, Loss: 0.11444968730211258
step: 1700, Loss: 0.11475978791713715
step: 1800, Loss: 0.11535418778657913
step: 1900, Loss: 0.11342720687389374
step: 2000, Loss: 0.11487908661365509
step: 2100, Loss: 0.11593679338693619
step: 2200, Loss: 0.11455903947353363
step: 2300, Loss: 0.11436617374420166
step: 2400, Loss: 0.11467765271663666
step: 2500, Loss: 0.11330229043960571
step: 2600, Loss: 0.114407017827034
step: 2700, Loss: 0.11402230709791183
step: 2800, Loss: 0.1138157993555069
step: 2900, Loss: 0.11362876743078232
step: 3000, Loss: 0.11454042792320251
step: 3100, Loss: 0.11540552228689194
step: 3200, Loss: 0.11436395347118378
step: 3300, Loss: 0.11400368809700012
step: 3400, Loss: 0.11506889760494232
step: 3500, Loss: 0.11378394812345505
step: 3600, Loss: 0.11654500663280487
step: 3700, Loss: 0.11505960673093796
step: 3800, Loss: 0.11503923684358597
step: 3900, Loss: 0.11563711613416672
step: 4000, Loss: 0.11365105956792831
step: 4100, Loss: 0.11431147158145905
step: 4200, Loss: 0.1133551150560379
step: 4300, Loss: 0.11438056826591492
step: 4400, Loss: 0.11448676139116287
step: 4500, Loss: 0.11564745754003525
step: 4600, Loss: 0.1143672913312912
step: 4700, Loss: 0.1157020851969719
step: 4800, Loss: 0.11549874395132065
step: 4900, Loss: 0.2510131597518921
step: 5000, Loss: 0.15615829825401306
step: 5100, Loss: 0.14055925607681274
step: 5200, Loss: 0.13342629373073578
step: 5300, Loss: 0.13450337946414948
step: 5400, Loss: 0.1261824369430542
step: 5500, Loss: 0.12500758469104767
step: 5600, Loss: 0.12140828371047974
step: 5700, Loss: 0.12256719917058945
step: 5800, Loss: 0.12269221246242523
step: 5900, Loss: 0.1202492043375969
step: 6000, Loss: 0.12001734972000122
step: 6100, Loss: 0.11700454354286194
step: 6200, Loss: 0.118406742811203
step: 6300, Loss: 0.11925828456878662
step: 6400, Loss: 0.12067701667547226
step: 6500, Loss: 0.11790341883897781
step: 6600, Loss: 0.11756174266338348
step: 6700, Loss: 0.1195928156375885
step: 6800, Loss: 0.11582287400960922
step: 6900, Loss: 0.1182963103055954
step: 7000, Loss: 0.11623924970626831
step: 7100, Loss: 0.1162794828414917
step: 7200, Loss: 0.115720234811306
step: 7300, Loss: 0.11538206785917282
step: 7400, Loss: 0.11480781435966492
step: 7500, Loss: 0.11656152456998825
step: 7600, Loss: 0.11486402153968811
step: 7700, Loss: 0.12094385176897049
step: 7800, Loss: 0.11713346838951111
step: 7900, Loss: 0.11629078537225723
step: 8000, Loss: 0.11725099384784698
step: 8100, Loss: 0.11520902812480927
step: 8200, Loss: 0.11691922694444656
step: 8300, Loss: 0.11798007041215897
step: 8400, Loss: 0.11634330451488495
step: 8500, Loss: 0.11522045731544495
step: 8600, Loss: 0.1158667504787445
step: 8700, Loss: 0.11559232324361801
step: 8800, Loss: 0.11466647684574127
step: 8900, Loss: 0.11618437618017197
step: 9000, Loss: 0.11522650718688965
step: 9100, Loss: 0.11661399900913239
step: 9200, Loss: 0.11425501853227615
step: 9300, Loss: 0.11420588940382004
step: 9400, Loss: 0.1152014285326004
step: 9500, Loss: 0.11497867852449417
step: 9600, Loss: 0.11413893103599548
step: 9700, Loss: 0.11447127908468246
step: 9800, Loss: 0.11483393609523773
step: 9900, Loss: 0.11470122635364532
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.9375
recall: 0.8823529411764706
F_score: 0.9090909090909091
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.12168644368648529
step: 100, Loss: 0.12554901838302612
step: 200, Loss: 0.11522596329450607
step: 300, Loss: 0.11590142548084259
step: 400, Loss: 0.11473590135574341
step: 500, Loss: 0.11513815820217133
step: 600, Loss: 0.11625263839960098
step: 700, Loss: 0.11278073489665985
step: 800, Loss: 0.1149340346455574
step: 900, Loss: 0.11374574899673462
step: 1000, Loss: 0.11407040059566498
step: 1100, Loss: 0.11425904929637909
step: 1200, Loss: 0.11531201750040054
step: 1300, Loss: 0.11555340141057968
step: 1400, Loss: 0.11290344595909119
step: 1500, Loss: 0.11421822011470795
step: 1600, Loss: 0.11365312337875366
step: 1700, Loss: 0.11520855128765106
step: 1800, Loss: 0.11393491178750992
step: 1900, Loss: 0.11675387620925903
step: 2000, Loss: 0.11596591025590897
step: 2100, Loss: 0.11664006114006042
step: 2200, Loss: 0.11368821561336517
step: 2300, Loss: 0.11433343589305878
step: 2400, Loss: 0.11394259333610535
step: 2500, Loss: 0.11394407600164413
step: 2600, Loss: 0.11406577378511429
step: 2700, Loss: 0.11384640634059906
step: 2800, Loss: 0.11320202797651291
step: 2900, Loss: 0.11633135378360748
step: 3000, Loss: 0.1151769608259201
step: 3100, Loss: 0.11551549285650253
step: 3200, Loss: 0.11903133988380432
step: 3300, Loss: 0.11366309970617294
step: 3400, Loss: 0.11617757380008698
step: 3500, Loss: 0.11502411961555481
step: 3600, Loss: 0.11448653787374496
step: 3700, Loss: 0.11566998809576035
step: 3800, Loss: 0.11305215954780579
step: 3900, Loss: 0.11351969838142395
step: 4000, Loss: 0.11487335711717606
step: 4100, Loss: 0.11398130655288696
step: 4200, Loss: 0.11505694687366486
step: 4300, Loss: 0.11451967060565948
step: 4400, Loss: 0.11657442152500153
step: 4500, Loss: 0.11514226347208023
step: 4600, Loss: 0.11425958573818207
step: 4700, Loss: 0.1151861846446991
step: 4800, Loss: 0.1146683543920517
step: 4900, Loss: 0.11516927927732468
step: 5000, Loss: 0.11403031647205353
step: 5100, Loss: 0.1140013337135315
step: 5200, Loss: 0.11558713018894196
step: 5300, Loss: 0.1166263222694397
step: 5400, Loss: 0.11629550158977509
step: 5500, Loss: 0.11418330669403076
step: 5600, Loss: 0.11551867425441742
step: 5700, Loss: 0.11601798236370087
step: 5800, Loss: 0.11532772332429886
step: 5900, Loss: 0.13769498467445374
step: 6000, Loss: 0.14005392789840698
step: 6100, Loss: 0.12444356083869934
step: 6200, Loss: 0.1213994026184082
step: 6300, Loss: 0.12729571759700775
step: 6400, Loss: 0.1220112219452858
step: 6500, Loss: 0.1382668912410736
step: 6600, Loss: 0.11827389150857925
step: 6700, Loss: 0.11886032670736313
step: 6800, Loss: 0.11973760277032852
step: 6900, Loss: 0.11790580302476883
step: 7000, Loss: 0.11709454655647278
step: 7100, Loss: 0.11636511236429214
step: 7200, Loss: 0.11702609807252884
step: 7300, Loss: 0.11697616428136826
step: 7400, Loss: 0.11707394570112228
step: 7500, Loss: 0.11472457647323608
step: 7600, Loss: 0.11930690705776215
step: 7700, Loss: 0.11809864640235901
step: 7800, Loss: 0.11651147156953812
step: 7900, Loss: 0.11383660137653351
step: 8000, Loss: 0.11802343279123306
step: 8100, Loss: 0.11504072695970535
step: 8200, Loss: 0.11389903724193573
step: 8300, Loss: 0.11552195250988007
step: 8400, Loss: 0.11503475904464722
step: 8500, Loss: 0.11675562709569931
step: 8600, Loss: 0.11383038759231567
step: 8700, Loss: 0.12017473578453064
step: 8800, Loss: 0.11658214777708054
step: 8900, Loss: 0.11355394870042801
step: 9000, Loss: 0.11460131406784058
step: 9100, Loss: 0.11375370621681213
step: 9200, Loss: 0.12019576132297516
step: 9300, Loss: 0.11853012442588806
step: 9400, Loss: 0.11445415765047073
step: 9500, Loss: 0.11394722014665604
step: 9600, Loss: 0.11559183150529861
step: 9700, Loss: 0.11859056353569031
step: 9800, Loss: 0.11562133580446243
step: 9900, Loss: 0.11496339738368988
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.6666666666666666
recall: 0.8333333333333334
F_score: 0.7407407407407408
subject 9 Avgacc: 0.7556250000000001 Avgfscore: 0.7776014127921476 
 Max acc:0.9, Max f score:0.9090909090909091
******** mix subject_10 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 41.710933685302734
step: 100, Loss: 9.095788955688477
step: 200, Loss: 5.253477096557617
step: 300, Loss: 5.2880048751831055
step: 400, Loss: 6.728062629699707
step: 2500, Loss: 0.11347313225269318
step: 2600, Loss: 0.11302676051855087
step: 2700, Loss: 0.1137373149394989
step: 2800, Loss: 0.11345651745796204
step: 2900, Loss: 0.11428304016590118
step: 3000, Loss: 0.1126614362001419
step: 3100, Loss: 0.11312548816204071
step: 3200, Loss: 0.11475975066423416
step: 3300, Loss: 0.11318108439445496
step: 3400, Loss: 0.19921845197677612
step: 3500, Loss: 0.11322867125272751
step: 3600, Loss: 0.11413294076919556
step: 3700, Loss: 0.11369681358337402
step: 3800, Loss: 0.11323709040880203
step: 3900, Loss: 0.11308272182941437
step: 4000, Loss: 0.11517101526260376
step: 4100, Loss: 0.1142616719007492
step: 4200, Loss: 0.1139601469039917
step: 4300, Loss: 0.11295124143362045
step: 4400, Loss: 0.1133318543434143
step: 4500, Loss: 0.11361771076917648
step: 4600, Loss: 0.11468273401260376
step: 4700, Loss: 0.12621931731700897
step: 4800, Loss: 0.16364124417304993
step: 4900, Loss: 0.12252227216959
step: 5000, Loss: 0.12014394253492355
step: 5100, Loss: 0.1217232272028923
step: 5200, Loss: 0.11992930620908737
step: 5300, Loss: 0.22143593430519104
step: 5400, Loss: 0.1177443265914917
step: 5500, Loss: 0.12322838604450226
step: 5600, Loss: 0.11963528394699097
step: 5700, Loss: 0.11719362437725067
step: 5800, Loss: 0.11611757427453995
step: 5900, Loss: 0.11801397800445557
step: 6000, Loss: 0.11668023467063904
step: 6100, Loss: 0.11489494889974594
step: 6200, Loss: 0.11572109162807465
step: 6300, Loss: 0.11493827402591705
step: 6400, Loss: 0.1162615418434143
step: 6500, Loss: 0.11590512841939926
step: 6600, Loss: 0.11483161896467209
step: 6700, Loss: 0.11417848616838455
step: 6800, Loss: 0.11377628147602081
step: 6900, Loss: 0.11493068188428879
step: 7000, Loss: 0.11690902709960938
step: 7100, Loss: 0.1157621443271637
step: 7200, Loss: 0.19989550113677979
step: 7300, Loss: 0.11464231461286545
step: 7400, Loss: 0.11548435688018799
step: 7500, Loss: 0.11448365449905396
step: 7600, Loss: 0.11542699486017227
step: 7700, Loss: 0.11453825980424881
step: 7800, Loss: 0.11458850651979446
step: 7900, Loss: 0.113839291036129
step: 8000, Loss: 0.11447077989578247
step: 8100, Loss: 0.11489475518465042
step: 8200, Loss: 0.11390463262796402
step: 8300, Loss: 0.11498910933732986
step: 8400, Loss: 0.11708216369152069
step: 8500, Loss: 0.11492501199245453
step: 8600, Loss: 0.11475464701652527
step: 8700, Loss: 0.11438067257404327
step: 8800, Loss: 0.1139858290553093
step: 8900, Loss: 0.11390763521194458
step: 9000, Loss: 0.11455365270376205
step: 9100, Loss: 0.1944713592529297
step: 9200, Loss: 0.11299863457679749
step: 9300, Loss: 0.11392557621002197
step: 9400, Loss: 0.11439806967973709
step: 9500, Loss: 0.11330542713403702
step: 9600, Loss: 0.11414051800966263
step: 9700, Loss: 0.11391913890838623
step: 9800, Loss: 0.1134880781173706
step: 9900, Loss: 0.11445117741823196
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.96875
F_score: 0.9841269841269841
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.11507151275873184
step: 100, Loss: 0.11463504284620285
step: 200, Loss: 0.11478632688522339
step: 300, Loss: 0.11383175849914551
step: 400, Loss: 0.11537426710128784
step: 500, Loss: 0.11374274641275406
step: 600, Loss: 0.1143268495798111
step: 700, Loss: 0.11542350053787231
step: 800, Loss: 0.11399511992931366
step: 900, Loss: 0.11453205347061157
step: 1000, Loss: 0.11499951034784317
step: 1100, Loss: 0.11272718757390976
step: 1200, Loss: 0.11339733004570007
step: 1300, Loss: 0.11418591439723969
step: 1400, Loss: 0.11367975920438766
step: 1500, Loss: 0.19760653376579285
step: 1600, Loss: 0.11335252225399017
step: 1700, Loss: 0.11424954980611801
step: 1800, Loss: 0.11300010979175568
step: 1900, Loss: 0.11339578032493591
step: 2000, Loss: 0.11497840285301208
step: 2100, Loss: 0.11428417265415192
step: 2200, Loss: 0.11301084607839584
step: 2300, Loss: 0.11342453956604004
step: 2400, Loss: 0.11340431123971939
step: 2500, Loss: 0.11366517841815948
step: 2600, Loss: 0.11366337537765503
step: 2700, Loss: 0.1133054718375206
step: 2800, Loss: 0.11340314149856567
step: 2900, Loss: 0.11429423838853836
step: 3000, Loss: 0.11423011124134064
step: 3100, Loss: 0.11322557181119919
step: 3200, Loss: 0.11468080431222916
step: 3300, Loss: 0.11308380961418152
step: 3400, Loss: 0.19872991740703583
step: 3500, Loss: 0.11345872282981873
step: 3600, Loss: 0.11432178318500519
step: 3700, Loss: 0.11255383491516113
step: 3800, Loss: 0.11343257874250412
step: 3900, Loss: 0.11337794363498688
step: 4000, Loss: 0.11365874111652374
step: 4100, Loss: 0.17714974284172058
step: 4200, Loss: 0.13337628543376923
step: 4300, Loss: 0.12391722202301025
step: 4400, Loss: 0.12264242023229599
step: 4500, Loss: 0.12302254140377045
step: 4600, Loss: 0.11887446790933609
step: 4700, Loss: 0.11916650831699371
step: 4800, Loss: 0.11814839392900467
step: 4900, Loss: 0.11801046133041382
step: 5000, Loss: 0.11571289598941803
step: 5100, Loss: 0.11620438098907471
step: 5200, Loss: 0.11654068529605865
step: 5300, Loss: 0.20537209510803223
step: 5400, Loss: 0.1205558329820633
step: 5500, Loss: 0.11431283503770828
step: 5600, Loss: 0.1155644878745079
step: 5700, Loss: 0.11590972542762756
step: 5800, Loss: 0.11639298498630524
step: 5900, Loss: 0.11493594944477081
step: 6000, Loss: 0.1149146780371666
step: 6100, Loss: 0.11343662440776825
step: 6200, Loss: 0.11672168225049973
step: 6300, Loss: 0.1162983849644661
step: 6400, Loss: 0.11549166589975357
step: 6500, Loss: 0.11706516146659851
step: 6600, Loss: 0.11442739516496658
step: 6700, Loss: 0.11454247683286667
step: 6800, Loss: 0.11413882672786713
step: 6900, Loss: 0.11447753012180328
step: 7000, Loss: 0.11431605368852615
step: 7100, Loss: 0.11604759097099304
step: 7200, Loss: 0.19888615608215332
step: 7300, Loss: 0.11600546538829803
step: 7400, Loss: 0.11414696276187897
step: 7500, Loss: 0.11462003737688065
step: 7600, Loss: 0.11501471698284149
step: 7700, Loss: 0.11341777443885803
step: 7800, Loss: 0.11385269463062286
step: 7900, Loss: 0.11425384879112244
step: 8000, Loss: 0.11418036371469498
step: 8100, Loss: 0.11429046839475632
step: 8200, Loss: 0.11375096440315247
step: 8300, Loss: 0.11460691690444946
step: 8400, Loss: 0.11452630907297134
step: 8500, Loss: 0.1132841408252716
step: 8600, Loss: 0.11382203549146652
step: 8700, Loss: 0.1138041689991951
step: 8800, Loss: 0.11409536004066467
step: 8900, Loss: 0.11359462887048721
step: 9000, Loss: 0.11344993114471436
step: 9100, Loss: 0.1939503401517868
step: 9200, Loss: 0.11487536132335663
step: 9300, Loss: 0.11333798617124557
step: 9400, Loss: 0.11404359340667725
step: 9500, Loss: 0.11341896653175354
step: 9600, Loss: 0.11413328349590302
step: 9700, Loss: 0.11375352740287781
step: 9800, Loss: 0.11447852104902267
step: 9900, Loss: 0.11606062948703766
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.11982627958059311
step: 100, Loss: 0.11394652724266052
step: 200, Loss: 0.1141124814748764
step: 300, Loss: 0.11315440386533737
step: 400, Loss: 0.11406973004341125
step: 500, Loss: 0.11390721797943115
step: 600, Loss: 0.11454561352729797
step: 700, Loss: 0.11406775563955307
step: 800, Loss: 0.11371240764856339
step: 900, Loss: 0.2738109529018402
step: 1000, Loss: 0.12096822261810303
step: 1100, Loss: 0.12120650708675385
step: 1200, Loss: 0.11833839118480682
step: 1300, Loss: 0.1168147623538971
step: 1400, Loss: 0.11506416648626328
step: 1500, Loss: 0.20716217160224915
step: 1600, Loss: 0.11719641089439392
step: 1700, Loss: 0.1211484968662262
step: 1800, Loss: 0.11549151688814163
step: 1900, Loss: 0.11530296504497528
step: 2000, Loss: 0.11556343734264374
step: 2100, Loss: 0.11981501430273056
step: 2200, Loss: 0.11401233077049255
step: 2300, Loss: 0.11478414386510849
step: 2400, Loss: 0.11382047832012177
step: 2500, Loss: 0.11810041218996048
step: 2600, Loss: 0.1147724986076355
step: 2700, Loss: 0.11467958986759186
step: 2800, Loss: 0.11498293280601501
step: 2900, Loss: 0.11512310802936554
step: 3000, Loss: 0.11446547508239746
step: 3100, Loss: 0.11415141075849533
step: 500, Loss: 4.568455696105957
step: 600, Loss: 4.3674163818359375
step: 700, Loss: 4.357595920562744
step: 800, Loss: 4.885603904724121
step: 900, Loss: 3.5684731006622314
step: 1000, Loss: 3.661773204803467
step: 1100, Loss: 1.4778422117233276
step: 1200, Loss: 2.5346477031707764
step: 1300, Loss: 1.7031327486038208
step: 1400, Loss: 1.5864267349243164
step: 1500, Loss: 1.0116029977798462
step: 1600, Loss: 1.674341082572937
step: 1700, Loss: 0.9826421737670898
step: 1800, Loss: 1.5584443807601929
step: 1900, Loss: 0.41641131043434143
step: 2000, Loss: 0.7810460329055786
step: 2100, Loss: 0.5657727718353271
step: 2200, Loss: 0.45739006996154785
step: 2300, Loss: 0.7378395199775696
step: 2400, Loss: 0.49919503927230835
step: 2500, Loss: 0.31988900899887085
step: 2600, Loss: 0.509219765663147
step: 2700, Loss: 0.2992858290672302
step: 2800, Loss: 0.40529394149780273
step: 2900, Loss: 0.2560286521911621
step: 3000, Loss: 0.2605307400226593
step: 3100, Loss: 0.24774593114852905
step: 3200, Loss: 0.2768663763999939
step: 3300, Loss: 0.20310071110725403
step: 3400, Loss: 0.4415661096572876
step: 3500, Loss: 0.24534741044044495
step: 3600, Loss: 0.2524906396865845
step: 3700, Loss: 0.6834644079208374
step: 3800, Loss: 0.26946794986724854
step: 3900, Loss: 0.22873535752296448
step: 4000, Loss: 0.2199094295501709
step: 4100, Loss: 0.17729394137859344
step: 4200, Loss: 0.21610160171985626
step: 4300, Loss: 0.18408502638339996
step: 4400, Loss: 0.25597822666168213
step: 4500, Loss: 0.17424583435058594
step: 4600, Loss: 0.19660082459449768
step: 4700, Loss: 0.861003041267395
step: 4800, Loss: 0.5497113466262817
step: 4900, Loss: 0.18137282133102417
step: 5000, Loss: 0.261149138212204
step: 5100, Loss: 0.18215589225292206
step: 5200, Loss: 0.20411504805088043
step: 5300, Loss: 0.16837438941001892
step: 5400, Loss: 0.17160429060459137
step: 5500, Loss: 0.1581186056137085
step: 5600, Loss: 0.1659286618232727
step: 5700, Loss: 0.15619276463985443
step: 5800, Loss: 0.16363951563835144
step: 5900, Loss: 0.1427113115787506
step: 6000, Loss: 0.1631489098072052
step: 6100, Loss: 1.3594272136688232
step: 6200, Loss: 1.5192054510116577
step: 6300, Loss: 0.2791626453399658
step: 6400, Loss: 0.2029707431793213
step: 6500, Loss: 0.17084965109825134
step: 6600, Loss: 0.19252978265285492
step: 6700, Loss: 0.16064679622650146
step: 6800, Loss: 0.18014617264270782
step: 6900, Loss: 0.16935287415981293
step: 7000, Loss: 0.17834283411502838
step: 7100, Loss: 0.15229876339435577
step: 7200, Loss: 0.16627123951911926
step: 7300, Loss: 0.15515291690826416
step: 7400, Loss: 0.16160592436790466
step: 7500, Loss: 0.1477780044078827
step: 7600, Loss: 0.16710692644119263
step: 7700, Loss: 0.1510189175605774
step: 7800, Loss: 0.15486198663711548
step: 7900, Loss: 1.365281343460083
step: 8000, Loss: 0.48501986265182495
step: 8100, Loss: 0.19299599528312683
step: 8200, Loss: 0.2504752278327942
step: 8300, Loss: 0.1740795224905014
step: 8400, Loss: 0.17597410082817078
step: 8500, Loss: 0.1666802614927292
step: 8600, Loss: 0.173353374004364
step: 8700, Loss: 0.15612289309501648
step: 8800, Loss: 0.16748635470867157
step: 8900, Loss: 0.1498870849609375
step: 9000, Loss: 0.1588614135980606
step: 9100, Loss: 0.1356455385684967
step: 9200, Loss: 0.1977156549692154
step: 9300, Loss: 0.5849503874778748
step: 9400, Loss: 0.3472098708152771
step: 9500, Loss: 0.20889969170093536
step: 9600, Loss: 0.1803446114063263
step: 9700, Loss: 0.16580437123775482
step: 9800, Loss: 0.15692546963691711
step: 9900, Loss: 0.16581906378269196
training successfully ended.
validating...
validate data length:32
acc: 0.9375
precision: 0.8571428571428571
recall: 1.0
F_score: 0.923076923076923
******fold 2******

Training... train_data length:280
step: 0, Loss: 1.8964979648590088
step: 100, Loss: 0.2730286121368408
step: 200, Loss: 0.20278051495552063
step: 300, Loss: 0.20759811997413635
step: 400, Loss: 0.6449839472770691
step: 500, Loss: 0.22134239971637726
step: 600, Loss: 0.23109641671180725
step: 700, Loss: 0.19384656846523285
step: 800, Loss: 0.32646286487579346
step: 900, Loss: 0.17646896839141846
step: 1000, Loss: 0.1866557002067566
step: 1100, Loss: 0.16409407556056976
step: 1200, Loss: 1.2816534042358398
step: 1300, Loss: 0.19887906312942505
step: 1400, Loss: 0.19659624993801117
step: 1500, Loss: 0.1823437362909317
step: 1600, Loss: 0.17387881875038147
step: 1700, Loss: 0.1648966372013092
step: 1800, Loss: 0.1649329960346222
step: 1900, Loss: 0.15670669078826904
step: 2000, Loss: 0.1761697232723236
step: 2100, Loss: 0.1551315188407898
step: 2200, Loss: 0.16957829892635345
step: 2300, Loss: 0.14455664157867432
step: 2400, Loss: 0.15415550768375397
step: 2500, Loss: 0.14778855443000793
step: 2600, Loss: 0.15748897194862366
step: 2700, Loss: 0.14367641508579254
step: 2800, Loss: 0.15200158953666687
step: 2900, Loss: 0.1342000514268875
step: 3000, Loss: 0.15651437640190125
step: 3100, Loss: 0.14008791744709015
step: 3200, Loss: 0.14313878118991852
step: 3300, Loss: 0.1362801343202591
step: 3400, Loss: 0.1642860323190689
step: 3500, Loss: 0.141454815864563
step: 3600, Loss: 0.1468053162097931
step: 3700, Loss: 1.9613924026489258
step: 3800, Loss: 2.1766555309295654
step: 3900, Loss: 0.21524843573570251
step: 4000, Loss: 0.20153452455997467
step: 4100, Loss: 0.18477194011211395
step: 4200, Loss: 0.1745014488697052
step: 4300, Loss: 0.17623528838157654
step: 4400, Loss: 0.16024591028690338
step: 4500, Loss: 0.16264215111732483
step: 4600, Loss: 0.1527879238128662
step: 4700, Loss: 0.14694517850875854
step: 4800, Loss: 0.15378156304359436
step: 4900, Loss: 0.17384718358516693
step: 5000, Loss: 0.15615975856781006
step: 5100, Loss: 0.15725702047348022
step: 5200, Loss: 0.1424730271100998
step: 5300, Loss: 0.1481461375951767
step: 5400, Loss: 0.1479695439338684
step: 5500, Loss: 0.14495299756526947
step: 5600, Loss: 0.16213680803775787
step: 5700, Loss: 0.12919320166110992
step: 5800, Loss: 0.15382258594036102
step: 5900, Loss: 0.1425780951976776
step: 6000, Loss: 1.273781180381775
step: 6100, Loss: 0.4354424476623535
step: 6200, Loss: 0.2553711533546448
step: 6300, Loss: 0.19962725043296814
step: 6400, Loss: 0.19722622632980347
step: 6500, Loss: 0.20322246849536896
step: 6600, Loss: 0.21173898875713348
step: 6700, Loss: 0.20343944430351257
step: 6800, Loss: 0.17756414413452148
step: 6900, Loss: 0.16465647518634796
step: 7000, Loss: 0.2297481894493103
step: 7100, Loss: 0.15729594230651855
step: 7200, Loss: 0.1655743420124054
step: 7300, Loss: 0.1543777585029602
step: 7400, Loss: 0.16171902418136597
step: 7500, Loss: 0.1557052880525589
step: 7600, Loss: 0.17590802907943726
step: 7700, Loss: 0.1486825942993164
step: 7800, Loss: 0.1488337218761444
step: 7900, Loss: 0.15381866693496704
step: 8000, Loss: 0.13891810178756714
step: 8100, Loss: 0.13727226853370667
step: 8200, Loss: 0.13732901215553284
step: 8300, Loss: 0.14105647802352905
step: 8400, Loss: 0.15251758694648743
step: 8500, Loss: 0.13475549221038818
step: 8600, Loss: 0.15130063891410828
step: 8700, Loss: 0.13666976988315582
step: 8800, Loss: 0.13279913365840912
step: 8900, Loss: 0.13515762984752655
step: 9000, Loss: 0.1544913649559021
step: 9100, Loss: 0.13622009754180908
step: 9200, Loss: 0.6125718951225281
step: 9300, Loss: 0.22444960474967957
step: 9400, Loss: 0.3093535304069519
step: 9500, Loss: 0.21018828451633453
step: 9600, Loss: 0.17660844326019287
step: 9700, Loss: 0.17534440755844116
step: 9800, Loss: 0.21052345633506775
step: 9900, Loss: 0.1806803196668625
training successfully ended.
validating...
validate data length:32
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.17569120228290558
step: 100, Loss: 0.1572733223438263
step: 200, Loss: 0.2547481656074524
step: 300, Loss: 0.1454596221446991
step: 400, Loss: 0.19784314930438995
step: 500, Loss: 0.1422543227672577
step: 600, Loss: 0.16261470317840576
step: 700, Loss: 0.13956212997436523
step: 800, Loss: 0.15961657464504242
step: 900, Loss: 0.13545309007167816
step: 1000, Loss: 0.18196141719818115
step: 1100, Loss: 0.14200301468372345
step: 1200, Loss: 0.14858616888523102
step: 1300, Loss: 0.14173436164855957
step: 3200, Loss: 0.11522652208805084
step: 3300, Loss: 0.11611031740903854
step: 3400, Loss: 0.20204146206378937
step: 3500, Loss: 0.11534490436315536
step: 3600, Loss: 0.11375890672206879
step: 3700, Loss: 0.11388018727302551
step: 3800, Loss: 0.11363957077264786
step: 3900, Loss: 0.11444792151451111
step: 4000, Loss: 0.11389181762933731
step: 4100, Loss: 0.11453805863857269
step: 4200, Loss: 0.11358550190925598
step: 4300, Loss: 0.11417701095342636
step: 4400, Loss: 0.11316676437854767
step: 4500, Loss: 0.1136271208524704
step: 4600, Loss: 0.11405327916145325
step: 4700, Loss: 0.11627397686243057
step: 4800, Loss: 0.11378897726535797
step: 4900, Loss: 0.1131407767534256
step: 5000, Loss: 0.11498825252056122
step: 5100, Loss: 0.11407080292701721
step: 5200, Loss: 0.11297847330570221
step: 5300, Loss: 0.1926872879266739
step: 5400, Loss: 0.11472512036561966
step: 5500, Loss: 0.11353709548711777
step: 5600, Loss: 0.11352546513080597
step: 5700, Loss: 0.11326295137405396
step: 5800, Loss: 0.11392523348331451
step: 5900, Loss: 0.11302337050437927
step: 6000, Loss: 0.11314578354358673
step: 6100, Loss: 0.11435301601886749
step: 6200, Loss: 0.11342460662126541
step: 6300, Loss: 0.11377047747373581
step: 6400, Loss: 0.11327861249446869
step: 6500, Loss: 0.1132354661822319
step: 6600, Loss: 0.1134386658668518
step: 6700, Loss: 0.11425377428531647
step: 6800, Loss: 0.11307170242071152
step: 6900, Loss: 0.11313368380069733
step: 7000, Loss: 0.11400854587554932
step: 7100, Loss: 0.11366668343544006
step: 7200, Loss: 0.19304721057415009
step: 7300, Loss: 0.11262333393096924
step: 7400, Loss: 0.11322572827339172
step: 7500, Loss: 0.11267619580030441
step: 7600, Loss: 0.1135421097278595
step: 7700, Loss: 0.11408258229494095
step: 7800, Loss: 0.1133561059832573
step: 7900, Loss: 0.11365914344787598
step: 8000, Loss: 0.11367729306221008
step: 8100, Loss: 0.11344298720359802
step: 8200, Loss: 0.11300846189260483
step: 8300, Loss: 0.1131252720952034
step: 8400, Loss: 0.11315950751304626
step: 8500, Loss: 0.11320023238658905
step: 8600, Loss: 0.11326145380735397
step: 8700, Loss: 0.11424127966165543
step: 8800, Loss: 0.11393885314464569
step: 8900, Loss: 0.11456718295812607
step: 9000, Loss: 0.11388906836509705
step: 9100, Loss: 0.1967061311006546
step: 9200, Loss: 0.1146998256444931
step: 9300, Loss: 0.11348047852516174
step: 9400, Loss: 0.11382387578487396
step: 9500, Loss: 0.11606330424547195
step: 9600, Loss: 0.14745481312274933
step: 9700, Loss: 0.12728378176689148
step: 9800, Loss: 0.13360907137393951
step: 9900, Loss: 0.12306945770978928
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.975609756097561
recall: 1.0
F_score: 0.9876543209876543
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.12166259437799454
step: 100, Loss: 0.11388415098190308
step: 200, Loss: 0.1140635684132576
step: 300, Loss: 0.11377546936273575
step: 400, Loss: 0.11342952400445938
step: 500, Loss: 0.11473572254180908
step: 600, Loss: 0.11343608051538467
step: 700, Loss: 0.11355999857187271
step: 800, Loss: 0.11399790644645691
step: 900, Loss: 0.11382368206977844
step: 1000, Loss: 0.11422133445739746
step: 1100, Loss: 0.11430422961711884
step: 1200, Loss: 0.11344505101442337
step: 1300, Loss: 0.11280875653028488
step: 1400, Loss: 0.11369932442903519
step: 1500, Loss: 0.19498686492443085
step: 1600, Loss: 0.1140146255493164
step: 1700, Loss: 0.11424734443426132
step: 1800, Loss: 0.1135132759809494
step: 1900, Loss: 0.11347294598817825
step: 2000, Loss: 0.11300519853830338
step: 2100, Loss: 0.1789521723985672
step: 2200, Loss: 0.14599137008190155
step: 2300, Loss: 0.1267869770526886
step: 2400, Loss: 0.12718769907951355
step: 2500, Loss: 0.1272737830877304
step: 2600, Loss: 0.11803168803453445
step: 2700, Loss: 0.11642895638942719
step: 2800, Loss: 0.11779267340898514
step: 2900, Loss: 0.11527635157108307
step: 3000, Loss: 0.11730227619409561
step: 3100, Loss: 0.11438111215829849
step: 3200, Loss: 0.11523684859275818
step: 3300, Loss: 0.11604809761047363
step: 3400, Loss: 0.20741431415081024
step: 3500, Loss: 0.11444618552923203
step: 3600, Loss: 0.11378689855337143
step: 3700, Loss: 0.11721661686897278
step: 3800, Loss: 0.1156587302684784
step: 3900, Loss: 0.11461425572633743
step: 4000, Loss: 0.11493309587240219
step: 4100, Loss: 0.11523426324129105
step: 4200, Loss: 0.11489307880401611
step: 4300, Loss: 0.1168963611125946
step: 4400, Loss: 0.11391765624284744
step: 4500, Loss: 0.11411561071872711
step: 4600, Loss: 0.1148688867688179
step: 4700, Loss: 0.11484462022781372
step: 4800, Loss: 0.11467809975147247
step: 4900, Loss: 0.1181747168302536
step: 5000, Loss: 0.11555643379688263
step: 5100, Loss: 0.11398745328187943
step: 5200, Loss: 0.114535853266716
step: 5300, Loss: 0.19896766543388367
step: 5400, Loss: 0.11348335444927216
step: 5500, Loss: 0.11363562196493149
step: 5600, Loss: 0.11571443825960159
step: 5700, Loss: 0.11419671028852463
step: 5800, Loss: 0.1143670380115509
step: 5900, Loss: 0.11460955440998077
step: 6000, Loss: 0.113634392619133
step: 6100, Loss: 0.11244672536849976
step: 6200, Loss: 0.11502453684806824
step: 6300, Loss: 0.11308984458446503
step: 6400, Loss: 0.1134515255689621
step: 6500, Loss: 0.11587706953287125
step: 6600, Loss: 0.1138375997543335
step: 6700, Loss: 0.11446492373943329
step: 6800, Loss: 0.1140129566192627
step: 6900, Loss: 0.11386654525995255
step: 7000, Loss: 0.11413124203681946
step: 7100, Loss: 0.11447575688362122
step: 7200, Loss: 0.1951937973499298
step: 7300, Loss: 0.11396977305412292
step: 7400, Loss: 0.11370672285556793
step: 7500, Loss: 0.11328937858343124
step: 7600, Loss: 0.11424687504768372
step: 7700, Loss: 0.11313319951295853
step: 7800, Loss: 0.11460850387811661
step: 7900, Loss: 0.11411337554454803
step: 8000, Loss: 0.11250770092010498
step: 8100, Loss: 0.11311109364032745
step: 8200, Loss: 0.11354643851518631
step: 8300, Loss: 0.11388975381851196
step: 8400, Loss: 0.11508599668741226
step: 8500, Loss: 0.11228308081626892
step: 8600, Loss: 0.11388671398162842
step: 8700, Loss: 0.11298318952322006
step: 8800, Loss: 0.11303813755512238
step: 8900, Loss: 0.11412692815065384
step: 9000, Loss: 0.11251954734325409
step: 9100, Loss: 0.19484218955039978
step: 9200, Loss: 0.1164623349905014
step: 9300, Loss: 0.11348629742860794
step: 9400, Loss: 0.11347424983978271
step: 9500, Loss: 0.11454351991415024
step: 9600, Loss: 0.113944411277771
step: 9700, Loss: 0.11408958584070206
step: 9800, Loss: 0.1136409193277359
step: 9900, Loss: 0.11348484456539154
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.12868361175060272
step: 100, Loss: 0.11353692412376404
step: 200, Loss: 0.11540309339761734
step: 300, Loss: 0.11381858587265015
step: 400, Loss: 0.11490145325660706
step: 500, Loss: 0.11382448673248291
step: 600, Loss: 0.11317887157201767
step: 700, Loss: 0.11427150666713715
step: 800, Loss: 0.1132921427488327
step: 900, Loss: 0.11306075751781464
step: 1000, Loss: 0.11346235871315002
step: 1100, Loss: 0.6012598276138306
step: 1200, Loss: 0.12977451086044312
step: 1300, Loss: 0.11852029711008072
step: 1400, Loss: 0.12465614080429077
step: 1500, Loss: 0.21186688542366028
step: 1600, Loss: 0.11793012917041779
step: 1700, Loss: 0.11567432433366776
step: 1800, Loss: 0.11704251170158386
step: 1900, Loss: 0.11519096791744232
step: 2000, Loss: 0.11776179820299149
step: 2100, Loss: 0.1164999008178711
step: 2200, Loss: 0.11438803374767303
step: 2300, Loss: 0.11638196557760239
step: 2400, Loss: 0.11786758899688721
step: 2500, Loss: 0.11694123595952988
step: 2600, Loss: 0.11547078937292099
step: 2700, Loss: 0.11700168997049332
step: 2800, Loss: 0.11429375410079956
step: 2900, Loss: 0.11464282870292664
step: 3000, Loss: 0.11380548030138016
step: 3100, Loss: 0.11401881277561188
step: 3200, Loss: 0.11365886777639389
step: 3300, Loss: 0.1152980625629425
step: 3400, Loss: 0.20291881263256073
step: 3500, Loss: 0.12010405212640762
step: 3600, Loss: 0.113347128033638
step: 3700, Loss: 0.1141786128282547
step: 3800, Loss: 0.11374440044164658
step: 1400, Loss: 0.16161295771598816
step: 1500, Loss: 0.13534922897815704
step: 1600, Loss: 0.13778753578662872
step: 1700, Loss: 0.13138990104198456
step: 1800, Loss: 0.14623790979385376
step: 1900, Loss: 0.1374988853931427
step: 2000, Loss: 0.1733311116695404
step: 2100, Loss: 0.13093876838684082
step: 2200, Loss: 5.9335103034973145
step: 2300, Loss: 0.23067891597747803
step: 2400, Loss: 0.1945260465145111
step: 2500, Loss: 0.1913461834192276
step: 2600, Loss: 0.1695677936077118
step: 2700, Loss: 0.15421637892723083
step: 2800, Loss: 0.17402419447898865
step: 2900, Loss: 0.14236004650592804
step: 3000, Loss: 0.17610867321491241
step: 3100, Loss: 0.1401364803314209
step: 3200, Loss: 0.14908051490783691
step: 3300, Loss: 0.14130699634552002
step: 3400, Loss: 0.17214974761009216
step: 3500, Loss: 0.13978976011276245
step: 3600, Loss: 0.13841025531291962
step: 3700, Loss: 0.13943393528461456
step: 3800, Loss: 0.13957209885120392
step: 3900, Loss: 0.1371753066778183
step: 4000, Loss: 0.13631953299045563
step: 4100, Loss: 0.13696984946727753
step: 4200, Loss: 0.1333630383014679
step: 4300, Loss: 0.13017170131206512
step: 4400, Loss: 0.1304558515548706
step: 4500, Loss: 0.13158656656742096
step: 4600, Loss: 0.13400889933109283
step: 4700, Loss: 0.13610588014125824
step: 4800, Loss: 0.1433085948228836
step: 4900, Loss: 0.13410250842571259
step: 5000, Loss: 0.1601572334766388
step: 5100, Loss: 1.6662956476211548
step: 5200, Loss: 2.947972297668457
step: 5300, Loss: 0.17238785326480865
step: 5400, Loss: 0.17353300750255585
step: 5500, Loss: 0.13554804027080536
step: 5600, Loss: 0.15486758947372437
step: 5700, Loss: 0.1370828002691269
step: 5800, Loss: 0.1366095244884491
step: 5900, Loss: 0.13144013285636902
step: 6000, Loss: 0.15464279055595398
step: 6100, Loss: 0.13438257575035095
step: 6200, Loss: 0.14620985090732574
step: 6300, Loss: 0.13450364768505096
step: 6400, Loss: 0.14475077390670776
step: 6500, Loss: 0.13130202889442444
step: 6600, Loss: 0.13705246150493622
step: 6700, Loss: 0.12677747011184692
step: 6800, Loss: 0.14478033781051636
step: 6900, Loss: 0.12733066082000732
step: 7000, Loss: 0.14016005396842957
step: 7100, Loss: 0.12781570851802826
step: 7200, Loss: 0.1608000248670578
step: 7300, Loss: 0.12749475240707397
step: 7400, Loss: 0.13166911900043488
step: 7500, Loss: 0.12609770894050598
step: 7600, Loss: 0.13699652254581451
step: 7700, Loss: 0.12551426887512207
step: 7800, Loss: 0.1344943642616272
step: 7900, Loss: 0.1356397122144699
step: 8000, Loss: 0.13485658168792725
step: 8100, Loss: 0.12630504369735718
step: 8200, Loss: 0.13686354458332062
step: 8300, Loss: 0.12960469722747803
step: 8400, Loss: 0.13262000679969788
step: 8500, Loss: 0.12784399092197418
step: 8600, Loss: 0.130484938621521
step: 8700, Loss: 0.1397225707769394
step: 8800, Loss: 0.12928321957588196
step: 8900, Loss: 0.1272948831319809
step: 9000, Loss: 0.13329187035560608
step: 9100, Loss: 0.12281247973442078
step: 9200, Loss: 0.15895769000053406
step: 9300, Loss: 1.3586890697479248
step: 9400, Loss: 0.2647494673728943
step: 9500, Loss: 0.17983710765838623
step: 9600, Loss: 0.1712503731250763
step: 9700, Loss: 0.1657346487045288
step: 9800, Loss: 0.20696555078029633
step: 9900, Loss: 0.14823777973651886
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 1.0
recall: 0.8666666666666667
F_score: 0.9285714285714286
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.17856289446353912
step: 100, Loss: 0.15816475450992584
step: 200, Loss: 0.15587076544761658
step: 300, Loss: 0.14316603541374207
step: 400, Loss: 0.1490560621023178
step: 500, Loss: 0.14481747150421143
step: 600, Loss: 0.1663946807384491
step: 700, Loss: 0.14324378967285156
step: 800, Loss: 0.142083078622818
step: 900, Loss: 3.415971040725708
step: 1000, Loss: 0.20327121019363403
step: 1100, Loss: 0.15583398938179016
step: 1200, Loss: 0.17055675387382507
step: 1300, Loss: 0.14405310153961182
step: 1400, Loss: 0.15225842595100403
step: 1500, Loss: 0.1462932676076889
step: 1600, Loss: 0.14375701546669006
step: 1700, Loss: 0.1433020681142807
step: 1800, Loss: 0.13942837715148926
step: 1900, Loss: 0.1306004673242569
step: 2000, Loss: 0.13268433511257172
step: 2100, Loss: 0.14142809808254242
step: 2200, Loss: 0.14125721156597137
step: 2300, Loss: 0.13877516984939575
step: 2400, Loss: 0.13901959359645844
step: 2500, Loss: 0.1251150518655777
step: 2600, Loss: 0.14116398990154266
step: 2700, Loss: 0.12767578661441803
step: 2800, Loss: 0.13648036122322083
step: 2900, Loss: 0.13436293601989746
step: 3000, Loss: 0.13551443815231323
step: 3100, Loss: 0.13756681978702545
step: 3200, Loss: 0.1293705403804779
step: 3300, Loss: 0.126665860414505
step: 3400, Loss: 0.12627488374710083
step: 3500, Loss: 0.12636882066726685
step: 3600, Loss: 2.4960789680480957
step: 3700, Loss: 0.20919840037822723
step: 3800, Loss: 0.2425650656223297
step: 3900, Loss: 0.15309318900108337
step: 4000, Loss: 0.17754290997982025
step: 4100, Loss: 0.14049403369426727
step: 4200, Loss: 0.1503477394580841
step: 4300, Loss: 0.13269342482089996
step: 4400, Loss: 0.14115267992019653
step: 4500, Loss: 0.13356603682041168
step: 4600, Loss: 0.14568540453910828
step: 4700, Loss: 0.13279776275157928
step: 4800, Loss: 0.14903369545936584
step: 4900, Loss: 0.12770584225654602
step: 5000, Loss: 0.1413026601076126
step: 5100, Loss: 0.13189519941806793
step: 5200, Loss: 0.13749824464321136
step: 5300, Loss: 0.12729506194591522
step: 5400, Loss: 0.13205038011074066
step: 5500, Loss: 0.13187356293201447
step: 5600, Loss: 0.13361087441444397
step: 5700, Loss: 0.1277698129415512
step: 5800, Loss: 0.13659857213497162
step: 5900, Loss: 0.12827010452747345
step: 6000, Loss: 0.14391107857227325
step: 6100, Loss: 0.1266985535621643
step: 6200, Loss: 0.13625073432922363
step: 6300, Loss: 0.12466838210821152
step: 6400, Loss: 0.1321093738079071
step: 6500, Loss: 0.253089040517807
step: 6600, Loss: 0.1999407261610031
step: 6700, Loss: 0.15958620607852936
step: 6800, Loss: 0.20137478411197662
step: 6900, Loss: 0.17297250032424927
step: 7000, Loss: 0.16058076918125153
step: 7100, Loss: 0.14051073789596558
step: 7200, Loss: 0.16361059248447418
step: 7300, Loss: 0.1330542266368866
step: 7400, Loss: 0.14019306004047394
step: 7500, Loss: 0.1307862550020218
step: 7600, Loss: 0.1358811855316162
step: 7700, Loss: 0.12756606936454773
step: 7800, Loss: 0.1394425928592682
step: 7900, Loss: 0.1283949464559555
step: 8000, Loss: 0.1301020085811615
step: 8100, Loss: 0.12495816498994827
step: 8200, Loss: 0.14116013050079346
step: 8300, Loss: 0.13304315507411957
step: 8400, Loss: 0.13955122232437134
step: 8500, Loss: 0.12255885452032089
step: 8600, Loss: 0.13110606372356415
step: 8700, Loss: 0.12255734950304031
step: 8800, Loss: 0.12878720462322235
step: 8900, Loss: 0.12895119190216064
step: 9000, Loss: 0.16517682373523712
step: 9100, Loss: 0.1325550675392151
step: 9200, Loss: 0.1313839852809906
step: 9300, Loss: 0.12396140396595001
step: 9400, Loss: 0.14650119841098785
step: 9500, Loss: 0.12566034495830536
step: 9600, Loss: 4.439111232757568
step: 9700, Loss: 0.2024126648902893
step: 9800, Loss: 0.17058181762695312
step: 9900, Loss: 0.14698006212711334
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.1652015596628189
step: 100, Loss: 0.16039539873600006
step: 200, Loss: 0.15532773733139038
step: 300, Loss: 0.1342596411705017
step: 400, Loss: 0.14598268270492554
step: 500, Loss: 0.13141237199306488
step: 600, Loss: 0.17169822752475739
step: 700, Loss: 0.1251104325056076
step: 800, Loss: 0.15982724726200104
step: 900, Loss: 0.12615366280078888
step: 1000, Loss: 0.6315860152244568
step: 1100, Loss: 0.18192127346992493
step: 1200, Loss: 0.18970987200737
step: 1300, Loss: 0.14583773910999298
step: 1400, Loss: 0.16631624102592468
step: 1500, Loss: 0.13923558592796326
step: 1600, Loss: 0.13720305263996124
step: 1700, Loss: 0.1352260410785675
step: 1800, Loss: 0.16497744619846344
step: 1900, Loss: 0.13535483181476593
step: 2000, Loss: 0.14032724499702454
step: 3900, Loss: 0.11423066258430481
step: 4000, Loss: 0.114520363509655
step: 4100, Loss: 0.11450187116861343
step: 4200, Loss: 0.11433398723602295
step: 4300, Loss: 0.11403525620698929
step: 4400, Loss: 0.11312443017959595
step: 4500, Loss: 0.1131640374660492
step: 4600, Loss: 0.11438199132680893
step: 4700, Loss: 0.11421240866184235
step: 4800, Loss: 0.11363779753446579
step: 4900, Loss: 0.11351142823696136
step: 5000, Loss: 0.11291434615850449
step: 5100, Loss: 0.11371687799692154
step: 5200, Loss: 0.11494433879852295
step: 5300, Loss: 0.19261890649795532
step: 5400, Loss: 0.11557454615831375
step: 5500, Loss: 0.11484715342521667
step: 5600, Loss: 0.11441903561353683
step: 5700, Loss: 0.11346489936113358
step: 5800, Loss: 0.11512257158756256
step: 5900, Loss: 0.1140015572309494
step: 6000, Loss: 0.11339451372623444
step: 6100, Loss: 0.11361120641231537
step: 6200, Loss: 0.11243772506713867
step: 6300, Loss: 0.11424072086811066
step: 6400, Loss: 0.11385679990053177
step: 6500, Loss: 0.11503295600414276
step: 6600, Loss: 0.1133551374077797
step: 6700, Loss: 0.11443153023719788
step: 6800, Loss: 0.11303183436393738
step: 6900, Loss: 0.11347349733114243
step: 7000, Loss: 0.1134181022644043
step: 7100, Loss: 0.11280953884124756
step: 7200, Loss: 0.19375072419643402
step: 7300, Loss: 0.11291009187698364
step: 7400, Loss: 0.1140260398387909
step: 7500, Loss: 0.1140972375869751
step: 7600, Loss: 0.11334636062383652
step: 7700, Loss: 0.11330069601535797
step: 7800, Loss: 0.11370576918125153
step: 7900, Loss: 0.11527294665575027
step: 8000, Loss: 0.1132960319519043
step: 8100, Loss: 0.11364784091711044
step: 8200, Loss: 0.11259568482637405
step: 8300, Loss: 0.1146412044763565
step: 8400, Loss: 0.11375852674245834
step: 8500, Loss: 0.11342570185661316
step: 8600, Loss: 0.11407561600208282
step: 8700, Loss: 0.15906083583831787
step: 8800, Loss: 0.1316307634115219
step: 8900, Loss: 0.12075236439704895
step: 9000, Loss: 0.12829682230949402
step: 9100, Loss: 0.21955211460590363
step: 9200, Loss: 0.12395942211151123
step: 9300, Loss: 0.12158940732479095
step: 9400, Loss: 0.11546255648136139
step: 9500, Loss: 0.11962386965751648
step: 9600, Loss: 0.11951187252998352
step: 9700, Loss: 0.11938396841287613
step: 9800, Loss: 0.11734795570373535
step: 9900, Loss: 0.11664185672998428
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9767441860465116
recall: 0.9767441860465116
F_score: 0.9767441860465116
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.12306716293096542
step: 100, Loss: 0.11435875296592712
step: 200, Loss: 0.11462549865245819
step: 300, Loss: 0.11328155547380447
step: 400, Loss: 0.11401021480560303
step: 500, Loss: 0.11373523622751236
step: 600, Loss: 0.11326326429843903
step: 700, Loss: 0.114495649933815
step: 800, Loss: 0.11403243243694305
step: 900, Loss: 0.11317702382802963
step: 1000, Loss: 0.11367397010326385
step: 1100, Loss: 0.11368025839328766
step: 1200, Loss: 0.1132378801703453
step: 1300, Loss: 0.11331907659769058
step: 1400, Loss: 0.11345533281564713
step: 1500, Loss: 0.1948869675397873
step: 1600, Loss: 0.11318685114383698
step: 1700, Loss: 0.1130012646317482
step: 1800, Loss: 0.48695218563079834
step: 1900, Loss: 0.13480448722839355
step: 2000, Loss: 0.1312989741563797
step: 2100, Loss: 0.120215505361557
step: 2200, Loss: 0.12464771419763565
step: 2300, Loss: 0.12069538980722427
step: 2400, Loss: 0.11688119173049927
step: 2500, Loss: 0.119184210896492
step: 2600, Loss: 0.11609215289354324
step: 2700, Loss: 0.12053991109132767
step: 2800, Loss: 0.12121511995792389
step: 2900, Loss: 0.1169276237487793
step: 3000, Loss: 0.11428631097078323
step: 3100, Loss: 0.113249771296978
step: 3200, Loss: 0.12105612456798553
step: 3300, Loss: 0.12003806978464127
step: 3400, Loss: 0.2070675641298294
step: 3500, Loss: 0.12324889004230499
step: 3600, Loss: 0.11443174630403519
step: 3700, Loss: 0.11774503439664841
step: 3800, Loss: 0.11514590680599213
step: 3900, Loss: 0.11540167033672333
step: 4000, Loss: 0.11776621639728546
step: 4100, Loss: 0.11611351370811462
step: 4200, Loss: 0.11559892445802689
step: 4300, Loss: 0.11408959329128265
step: 4400, Loss: 0.11622142046689987
step: 4500, Loss: 0.11305571347475052
step: 4600, Loss: 0.11463083326816559
step: 4700, Loss: 0.11598483473062515
step: 4800, Loss: 0.11776445806026459
step: 4900, Loss: 0.11401551216840744
step: 5000, Loss: 0.11462214589118958
step: 5100, Loss: 0.11311120539903641
step: 5200, Loss: 0.11712565273046494
step: 5300, Loss: 0.19815388321876526
step: 5400, Loss: 0.11383822560310364
step: 5500, Loss: 0.11367735266685486
step: 5600, Loss: 0.11461672186851501
step: 5700, Loss: 0.11401296406984329
step: 5800, Loss: 0.1157759428024292
step: 5900, Loss: 0.11468787491321564
step: 6000, Loss: 0.11311323195695877
step: 6100, Loss: 0.11388842016458511
step: 6200, Loss: 0.11595273017883301
step: 6300, Loss: 0.11314913630485535
step: 6400, Loss: 0.11508362740278244
step: 6500, Loss: 0.11483819037675858
step: 6600, Loss: 0.11364065855741501
step: 6700, Loss: 0.11391416192054749
step: 6800, Loss: 0.11499926447868347
step: 6900, Loss: 0.11378640681505203
step: 7000, Loss: 0.11418630182743073
step: 7100, Loss: 0.11260096728801727
step: 7200, Loss: 0.19192513823509216
step: 7300, Loss: 0.1129676103591919
step: 7400, Loss: 0.11572849750518799
step: 7500, Loss: 0.11395875364542007
step: 7600, Loss: 0.11307767778635025
step: 7700, Loss: 0.11290495842695236
step: 7800, Loss: 0.11260287463665009
step: 7900, Loss: 0.11394862830638885
step: 8000, Loss: 0.11345568299293518
step: 8100, Loss: 0.11316028982400894
step: 8200, Loss: 0.1142389178276062
step: 8300, Loss: 0.1137889176607132
step: 8400, Loss: 0.11375118046998978
step: 8500, Loss: 0.11321664601564407
step: 8600, Loss: 0.11342160403728485
step: 8700, Loss: 0.11335044354200363
step: 8800, Loss: 0.11337757110595703
step: 8900, Loss: 0.11298620700836182
step: 9000, Loss: 0.11441579461097717
step: 9100, Loss: 0.19305555522441864
step: 9200, Loss: 0.11313918232917786
step: 9300, Loss: 0.11393004655838013
step: 9400, Loss: 0.11390603333711624
step: 9500, Loss: 0.11327062547206879
step: 9600, Loss: 0.11322645097970963
step: 9700, Loss: 0.11399324983358383
step: 9800, Loss: 0.11283017694950104
step: 9900, Loss: 0.1137717217206955
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11872328072786331
step: 100, Loss: 0.11480793356895447
step: 200, Loss: 0.11381556838750839
step: 300, Loss: 0.11446187645196915
step: 400, Loss: 0.11403921991586685
step: 500, Loss: 0.11310853809118271
step: 600, Loss: 0.11379142850637436
step: 700, Loss: 0.11481811106204987
step: 800, Loss: 0.11347449570894241
step: 900, Loss: 0.11330978572368622
step: 1000, Loss: 0.11424984782934189
step: 1100, Loss: 0.1185033768415451
step: 1200, Loss: 0.11480602622032166
step: 1300, Loss: 0.11488509923219681
step: 1400, Loss: 0.11316460371017456
step: 1500, Loss: 0.1935492604970932
step: 1600, Loss: 0.11504404246807098
step: 1700, Loss: 0.11350282281637192
step: 1800, Loss: 4.128513336181641
step: 1900, Loss: 0.1431349515914917
step: 2000, Loss: 0.1256709098815918
step: 2100, Loss: 0.12237218022346497
step: 2200, Loss: 0.12236372381448746
step: 2300, Loss: 0.11850720643997192
step: 2400, Loss: 0.11838185787200928
step: 2500, Loss: 0.116798534989357
step: 2600, Loss: 0.12217982113361359
step: 2700, Loss: 0.11598842591047287
step: 2800, Loss: 0.11694701015949249
step: 2900, Loss: 0.11954717338085175
step: 3000, Loss: 0.11372923851013184
step: 3100, Loss: 0.1157010942697525
step: 3200, Loss: 0.11657454818487167
step: 3300, Loss: 0.11894336342811584
step: 3400, Loss: 0.20599453151226044
step: 3500, Loss: 0.114473856985569
step: 3600, Loss: 0.11540889739990234
step: 3700, Loss: 0.1170826405286789
step: 3800, Loss: 0.11448545753955841
step: 3900, Loss: 0.1141190156340599
step: 4000, Loss: 0.11576685309410095
step: 4100, Loss: 0.11507688462734222
step: 4200, Loss: 0.11512857675552368
step: 4300, Loss: 0.11466162651777267
step: 4400, Loss: 0.11354649066925049
step: 2100, Loss: 0.1239212229847908
step: 2200, Loss: 0.14169439673423767
step: 2300, Loss: 0.12359188497066498
step: 2400, Loss: 0.13755938410758972
step: 2500, Loss: 0.12884029746055603
step: 2600, Loss: 0.13158109784126282
step: 2700, Loss: 0.1270052194595337
step: 2800, Loss: 0.12716028094291687
step: 2900, Loss: 0.12727399170398712
step: 3000, Loss: 0.1359441578388214
step: 3100, Loss: 0.12183986604213715
step: 3200, Loss: 0.13677114248275757
step: 3300, Loss: 0.12175619602203369
step: 3400, Loss: 0.1333680897951126
step: 3500, Loss: 0.12405591458082199
step: 3600, Loss: 0.12707802653312683
step: 3700, Loss: 0.1270160973072052
step: 3800, Loss: 0.1347743421792984
step: 3900, Loss: 0.12027490884065628
step: 4000, Loss: 2.1260359287261963
step: 4100, Loss: 0.20839576423168182
step: 4200, Loss: 0.18538755178451538
step: 4300, Loss: 0.16582947969436646
step: 4400, Loss: 0.17056936025619507
step: 4500, Loss: 0.13879255950450897
step: 4600, Loss: 0.13036876916885376
step: 4700, Loss: 0.13180501759052277
step: 4800, Loss: 0.13755622506141663
step: 4900, Loss: 0.12606951594352722
step: 5000, Loss: 0.1420036107301712
step: 5100, Loss: 0.12886612117290497
step: 5200, Loss: 0.14577282965183258
step: 5300, Loss: 0.12689025700092316
step: 5400, Loss: 0.13525420427322388
step: 5500, Loss: 0.1340775340795517
step: 5600, Loss: 0.13146169483661652
step: 5700, Loss: 0.12552128732204437
step: 5800, Loss: 0.13465411961078644
step: 5900, Loss: 0.12051165103912354
step: 6000, Loss: 0.1288585066795349
step: 6100, Loss: 0.12476184964179993
step: 6200, Loss: 0.1292344480752945
step: 6300, Loss: 0.12523584067821503
step: 6400, Loss: 0.12634484469890594
step: 6500, Loss: 0.12243252992630005
step: 6600, Loss: 0.13168658316135406
step: 6700, Loss: 0.11972934007644653
step: 6800, Loss: 0.14115315675735474
step: 6900, Loss: 0.12361215054988861
step: 7000, Loss: 0.13544315099716187
step: 7100, Loss: 0.1259845793247223
step: 7200, Loss: 0.12642714381217957
step: 7300, Loss: 0.12782235443592072
step: 7400, Loss: 0.12610536813735962
step: 7500, Loss: 0.11989026516675949
step: 7600, Loss: 0.12941083312034607
step: 7700, Loss: 0.12708809971809387
step: 7800, Loss: 0.1261482834815979
step: 7900, Loss: 0.12762503325939178
step: 8000, Loss: 0.12686997652053833
step: 8100, Loss: 0.16157644987106323
step: 8200, Loss: 0.7083912491798401
step: 8300, Loss: 0.15978723764419556
step: 8400, Loss: 0.14920487999916077
step: 8500, Loss: 0.1341160088777542
step: 8600, Loss: 0.14639972150325775
step: 8700, Loss: 0.13236847519874573
step: 8800, Loss: 0.13909804821014404
step: 8900, Loss: 0.1332109421491623
step: 9000, Loss: 0.13959774374961853
step: 9100, Loss: 0.12871715426445007
step: 9200, Loss: 0.129799947142601
step: 9300, Loss: 0.1245555579662323
step: 9400, Loss: 0.12757353484630585
step: 9500, Loss: 0.12397896498441696
step: 9600, Loss: 0.13055264949798584
step: 9700, Loss: 0.1230677142739296
step: 9800, Loss: 0.12791363894939423
step: 9900, Loss: 0.12426122277975082
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8421052631578947
recall: 0.8888888888888888
F_score: 0.8648648648648649
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.1784193068742752
step: 100, Loss: 0.16405172646045685
step: 200, Loss: 0.18663935363292694
step: 300, Loss: 0.1554468423128128
step: 400, Loss: 0.1470651477575302
step: 500, Loss: 0.1444912701845169
step: 600, Loss: 0.15319541096687317
step: 700, Loss: 0.134874165058136
step: 800, Loss: 0.14089234173297882
step: 900, Loss: 0.13970737159252167
step: 1000, Loss: 0.14760743081569672
step: 1100, Loss: 2.9095752239227295
step: 1200, Loss: 0.23118139803409576
step: 1300, Loss: 0.23118561506271362
step: 1400, Loss: 0.21190151572227478
step: 1500, Loss: 0.17483046650886536
step: 1600, Loss: 0.1624407172203064
step: 1700, Loss: 0.19251251220703125
step: 1800, Loss: 0.16664493083953857
step: 1900, Loss: 0.17712482810020447
step: 2000, Loss: 0.15436089038848877
step: 2100, Loss: 0.14628934860229492
step: 2200, Loss: 0.14411912858486176
step: 2300, Loss: 0.13762959837913513
step: 2400, Loss: 0.1398102343082428
step: 2500, Loss: 0.12992538511753082
step: 2600, Loss: 0.15174958109855652
step: 2700, Loss: 0.13514231145381927
step: 2800, Loss: 0.13429871201515198
step: 2900, Loss: 0.13527284562587738
step: 3000, Loss: 0.13193339109420776
step: 3100, Loss: 0.13537828624248505
step: 3200, Loss: 0.144522562623024
step: 3300, Loss: 0.1401178240776062
step: 3400, Loss: 0.1384589970111847
step: 3500, Loss: 0.13419879972934723
step: 3600, Loss: 0.1332714706659317
step: 3700, Loss: 0.1311647742986679
step: 3800, Loss: 0.13546015322208405
step: 3900, Loss: 1.7377341985702515
step: 4000, Loss: 0.3454708456993103
step: 4100, Loss: 0.1893225610256195
step: 4200, Loss: 0.16072680056095123
step: 4300, Loss: 0.17123395204544067
step: 4400, Loss: 0.17014195024967194
step: 4500, Loss: 0.14855721592903137
step: 4600, Loss: 0.17579428851604462
step: 4700, Loss: 0.1510656327009201
step: 4800, Loss: 0.15353445708751678
step: 4900, Loss: 0.14017802476882935
step: 5000, Loss: 0.149942085146904
step: 5100, Loss: 0.1332293152809143
step: 5200, Loss: 0.1370743066072464
step: 5300, Loss: 0.13401013612747192
step: 5400, Loss: 0.15051957964897156
step: 5500, Loss: 0.12368601560592651
step: 5600, Loss: 0.13879594206809998
step: 5700, Loss: 0.12922334671020508
step: 5800, Loss: 0.14516067504882812
step: 5900, Loss: 0.12337517738342285
step: 6000, Loss: 0.13190725445747375
step: 6100, Loss: 0.13251838088035583
step: 6200, Loss: 0.1308651566505432
step: 6300, Loss: 0.12131530046463013
step: 6400, Loss: 0.1440681368112564
step: 6500, Loss: 0.12731142342090607
step: 6600, Loss: 0.13322240114212036
step: 6700, Loss: 0.13132072985172272
step: 6800, Loss: 0.1347545087337494
step: 6900, Loss: 0.12364572286605835
step: 7000, Loss: 0.12889902293682098
step: 7100, Loss: 0.12069177627563477
step: 7200, Loss: 0.9783577919006348
step: 7300, Loss: 0.15954677760601044
step: 7400, Loss: 0.15572524070739746
step: 7500, Loss: 0.1548749804496765
step: 7600, Loss: 0.14535070955753326
step: 7700, Loss: 0.1323641985654831
step: 7800, Loss: 0.13182707130908966
step: 7900, Loss: 0.12875981628894806
step: 8000, Loss: 0.13614103198051453
step: 8100, Loss: 0.12585680186748505
step: 8200, Loss: 0.14199861884117126
step: 8300, Loss: 0.12374797463417053
step: 8400, Loss: 0.12797236442565918
step: 8500, Loss: 0.1197742223739624
step: 8600, Loss: 0.12836281955242157
step: 8700, Loss: 0.12338139861822128
step: 8800, Loss: 0.13036169111728668
step: 8900, Loss: 0.12359482049942017
step: 9000, Loss: 0.1295647919178009
step: 9100, Loss: 0.12381435185670853
step: 9200, Loss: 0.1313416063785553
step: 9300, Loss: 0.12389054149389267
step: 9400, Loss: 0.13168965280056
step: 9500, Loss: 0.12363377213478088
step: 9600, Loss: 0.12862764298915863
step: 9700, Loss: 0.121219702064991
step: 9800, Loss: 0.12923353910446167
step: 9900, Loss: 0.12562407553195953
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.9
recall: 1.0
F_score: 0.9473684210526316
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.1981479376554489
step: 100, Loss: 0.1413687765598297
step: 200, Loss: 0.24268537759780884
step: 300, Loss: 0.13698162138462067
step: 400, Loss: 0.1504380702972412
step: 500, Loss: 0.1262521594762802
step: 600, Loss: 0.14656859636306763
step: 700, Loss: 0.1313619166612625
step: 800, Loss: 0.1806270182132721
step: 900, Loss: 0.2721521854400635
step: 1000, Loss: 0.226256862282753
step: 1100, Loss: 0.14725705981254578
step: 1200, Loss: 0.16606022417545319
step: 1300, Loss: 0.13168683648109436
step: 1400, Loss: 0.1602914035320282
step: 1500, Loss: 0.12937228381633759
step: 1600, Loss: 0.1500425636768341
step: 1700, Loss: 0.12625351548194885
step: 1800, Loss: 0.1540626883506775
step: 1900, Loss: 0.1234680712223053
step: 2000, Loss: 0.13252827525138855
step: 2100, Loss: 0.1259230673313141
step: 2200, Loss: 0.13723695278167725
step: 2300, Loss: 0.12559136748313904
step: 2400, Loss: 0.1313977688550949
step: 2500, Loss: 0.12316425889730453
step: 2600, Loss: 0.13510164618492126
step: 4500, Loss: 0.11601261794567108
step: 4600, Loss: 0.11632011085748672
step: 4700, Loss: 0.11424335837364197
step: 4800, Loss: 0.11422073096036911
step: 4900, Loss: 0.11374035477638245
step: 5000, Loss: 0.11773034930229187
step: 5100, Loss: 0.11356751620769501
step: 5200, Loss: 0.11348564922809601
step: 5300, Loss: 0.1990593820810318
step: 5400, Loss: 0.11385757476091385
step: 5500, Loss: 0.11436989903450012
step: 5600, Loss: 0.11422227323055267
step: 5700, Loss: 0.11474255472421646
step: 5800, Loss: 0.11382214725017548
step: 5900, Loss: 0.11416219919919968
step: 6000, Loss: 0.11341460794210434
step: 6100, Loss: 0.11546501517295837
step: 6200, Loss: 0.11408954858779907
step: 6300, Loss: 0.11643007397651672
step: 6400, Loss: 0.11329959332942963
step: 6500, Loss: 0.11717752367258072
step: 6600, Loss: 0.11365598440170288
step: 6700, Loss: 0.11454932391643524
step: 6800, Loss: 0.11291036009788513
step: 6900, Loss: 0.11595817655324936
step: 7000, Loss: 0.11317967623472214
step: 7100, Loss: 0.11307278275489807
step: 7200, Loss: 0.19641925394535065
step: 7300, Loss: 0.11341743171215057
step: 7400, Loss: 0.11384773254394531
step: 7500, Loss: 0.11336944997310638
step: 7600, Loss: 0.113909512758255
step: 7700, Loss: 0.1139461100101471
step: 7800, Loss: 0.11332352459430695
step: 7900, Loss: 0.11294029653072357
step: 8000, Loss: 0.11307641118764877
step: 8100, Loss: 0.11416283994913101
step: 8200, Loss: 0.11463094502687454
step: 8300, Loss: 0.11421165615320206
step: 8400, Loss: 0.11508781462907791
step: 8500, Loss: 0.11480965465307236
step: 8600, Loss: 0.11291292309761047
step: 8700, Loss: 0.11333201080560684
step: 8800, Loss: 0.11459605395793915
step: 8900, Loss: 0.11295022070407867
step: 9000, Loss: 0.11333371698856354
step: 9100, Loss: 0.1901535987854004
step: 9200, Loss: 0.1144593209028244
step: 9300, Loss: 0.1138230413198471
step: 9400, Loss: 0.11345554888248444
step: 9500, Loss: 0.11407169699668884
step: 9600, Loss: 0.11556489765644073
step: 9700, Loss: 0.11417390406131744
step: 9800, Loss: 0.1141580194234848
step: 9900, Loss: 0.11360742151737213
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.9761904761904762
F_score: 0.9879518072289156
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.12000229209661484
step: 100, Loss: 0.11495020240545273
step: 200, Loss: 0.11362585425376892
step: 300, Loss: 0.11415594816207886
step: 400, Loss: 0.11367098987102509
step: 500, Loss: 0.11375211924314499
step: 600, Loss: 0.1138107106089592
step: 700, Loss: 0.11351070553064346
step: 800, Loss: 0.11312301456928253
step: 900, Loss: 0.11451175808906555
step: 1000, Loss: 0.11388421058654785
step: 1100, Loss: 0.11396537721157074
step: 1200, Loss: 0.11262739449739456
step: 1300, Loss: 0.11291749775409698
step: 1400, Loss: 0.11307432502508163
step: 1500, Loss: 0.19244395196437836
step: 1600, Loss: 0.11346444487571716
step: 1700, Loss: 0.11388701945543289
step: 1800, Loss: 0.11369121074676514
step: 1900, Loss: 0.11291766166687012
step: 2000, Loss: 0.11326953023672104
step: 2100, Loss: 0.11288636922836304
step: 2200, Loss: 0.11435149610042572
step: 2300, Loss: 0.11342921853065491
step: 2400, Loss: 0.11459126323461533
step: 2500, Loss: 0.11211246252059937
step: 2600, Loss: 0.1140819862484932
step: 2700, Loss: 0.11453574895858765
step: 2800, Loss: 0.11292882263660431
step: 2900, Loss: 0.11425444483757019
step: 3000, Loss: 0.11446696519851685
step: 3100, Loss: 0.11293867230415344
step: 3200, Loss: 0.11400753259658813
step: 3300, Loss: 0.1142306923866272
step: 3400, Loss: 0.19599875807762146
step: 3500, Loss: 0.11349184811115265
step: 3600, Loss: 0.11391125619411469
step: 3700, Loss: 0.11337847262620926
step: 3800, Loss: 0.17814558744430542
step: 3900, Loss: 0.13599257171154022
step: 4000, Loss: 0.1276439130306244
step: 4100, Loss: 0.13121989369392395
step: 4200, Loss: 0.12082607299089432
step: 4300, Loss: 0.1249811053276062
step: 4400, Loss: 0.12420350313186646
step: 4500, Loss: 0.11834800243377686
step: 4600, Loss: 0.1200701892375946
step: 4700, Loss: 0.11940175294876099
step: 4800, Loss: 0.11528339982032776
step: 4900, Loss: 0.11699964106082916
step: 5000, Loss: 0.11581875383853912
step: 5100, Loss: 0.11511047929525375
step: 5200, Loss: 0.1155889555811882
step: 5300, Loss: 0.2072863131761551
step: 5400, Loss: 0.11595866829156876
step: 5500, Loss: 0.11477438360452652
step: 5600, Loss: 0.11644607782363892
step: 5700, Loss: 0.11503671109676361
step: 5800, Loss: 0.11539416760206223
step: 5900, Loss: 0.11716968566179276
step: 6000, Loss: 0.11627651751041412
step: 6100, Loss: 0.11575465649366379
step: 6200, Loss: 0.11548242717981339
step: 6300, Loss: 0.11571350693702698
step: 6400, Loss: 0.11370652168989182
step: 6500, Loss: 0.11679588258266449
step: 6600, Loss: 0.1145426332950592
step: 6700, Loss: 0.11396462470293045
step: 6800, Loss: 0.11491390317678452
step: 6900, Loss: 0.11609359830617905
step: 7000, Loss: 0.1179439127445221
step: 7100, Loss: 0.114564910531044
step: 7200, Loss: 0.2000560164451599
step: 7300, Loss: 0.11511015146970749
step: 7400, Loss: 0.11440165340900421
step: 7500, Loss: 0.11522549390792847
step: 7600, Loss: 0.11372142285108566
step: 7700, Loss: 0.1140090823173523
step: 7800, Loss: 0.11445894837379456
step: 7900, Loss: 0.11397633701562881
step: 8000, Loss: 0.11591564863920212
step: 8100, Loss: 0.1135026142001152
step: 8200, Loss: 0.11323605477809906
step: 8300, Loss: 0.11420023441314697
step: 8400, Loss: 0.11359573155641556
step: 8500, Loss: 0.1133752390742302
step: 8600, Loss: 0.11382463574409485
step: 8700, Loss: 0.11506545543670654
step: 8800, Loss: 0.11397553235292435
step: 8900, Loss: 0.11520510911941528
step: 9000, Loss: 0.11377859115600586
step: 9100, Loss: 0.1938697099685669
step: 9200, Loss: 0.11450834572315216
step: 9300, Loss: 0.11299333721399307
step: 9400, Loss: 0.11462383717298508
step: 9500, Loss: 0.11442746222019196
step: 9600, Loss: 0.11325088888406754
step: 9700, Loss: 0.11630897223949432
step: 9800, Loss: 0.11238022893667221
step: 9900, Loss: 0.11359301209449768
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.9761904761904762
F_score: 0.9879518072289156
subject 10 Avgacc: 0.9819444444444445 Avgfscore: 0.9828350674246431 
 Max acc:1.0, Max f score:1.0
******** mix subject_11 ********

[399, 361]
******fold 1******

Training... train_data length:684
step: 0, Loss: 49.793800354003906
step: 100, Loss: 7.382193088531494
step: 200, Loss: 6.073501110076904
step: 300, Loss: 5.099723815917969
step: 400, Loss: 5.158216953277588
step: 500, Loss: 2.3639309406280518
step: 600, Loss: 5.702984809875488
step: 700, Loss: 2.223881483078003
step: 800, Loss: 0.3065677285194397
step: 900, Loss: 0.20823319256305695
step: 1000, Loss: 0.20242680609226227
step: 1100, Loss: 0.176101416349411
step: 1200, Loss: 0.1944195032119751
step: 1300, Loss: 0.17785830795764923
step: 1400, Loss: 0.16414156556129456
step: 1500, Loss: 0.2328971028327942
step: 1600, Loss: 0.15743404626846313
step: 1700, Loss: 0.17168352007865906
step: 1800, Loss: 0.15154695510864258
step: 1900, Loss: 0.13400229811668396
step: 2000, Loss: 0.19643402099609375
step: 2100, Loss: 0.14176473021507263
step: 2200, Loss: 0.13492973148822784
step: 2300, Loss: 0.1342441290616989
step: 2400, Loss: 0.1393754780292511
step: 2500, Loss: 0.13567088544368744
step: 2600, Loss: 0.13014830648899078
step: 2700, Loss: 0.13820265233516693
step: 2800, Loss: 0.13264067471027374
step: 2900, Loss: 0.12594135105609894
step: 3000, Loss: 0.1300317943096161
step: 3100, Loss: 0.14026370644569397
step: 3200, Loss: 0.12979839742183685
step: 3300, Loss: 0.13025984168052673
step: 3400, Loss: 0.20627346634864807
step: 3500, Loss: 0.12445349246263504
step: 3600, Loss: 0.12603619694709778
step: 3700, Loss: 0.11845766007900238
step: 3800, Loss: 0.11533947288990021
step: 3900, Loss: 0.12054573744535446
step: 4000, Loss: 0.12346359342336655
step: 4100, Loss: 0.11719964444637299
step: 4200, Loss: 0.12470933794975281
step: 4300, Loss: 0.1176367700099945
step: 4400, Loss: 0.1326887458562851
step: 4500, Loss: 0.1187458410859108
step: 4600, Loss: 0.12047982215881348
step: 2700, Loss: 0.12756620347499847
step: 2800, Loss: 0.12792393565177917
step: 2900, Loss: 0.13253968954086304
step: 3000, Loss: 0.1361674964427948
step: 3100, Loss: 0.12351492792367935
step: 3200, Loss: 0.1358596533536911
step: 3300, Loss: 0.1220100075006485
step: 3400, Loss: 0.13729742169380188
step: 3500, Loss: 0.1316174566745758
step: 3600, Loss: 1.3528249263763428
step: 3700, Loss: 0.19544833898544312
step: 3800, Loss: 0.1646246761083603
step: 3900, Loss: 0.14303521811962128
step: 4000, Loss: 0.2240278720855713
step: 4100, Loss: 0.12921127676963806
step: 4200, Loss: 0.15700167417526245
step: 4300, Loss: 0.1334989368915558
step: 4400, Loss: 0.13813357055187225
step: 4500, Loss: 0.1348521113395691
step: 4600, Loss: 0.1538957804441452
step: 4700, Loss: 0.12467174977064133
step: 4800, Loss: 0.1366879642009735
step: 4900, Loss: 0.12645936012268066
step: 5000, Loss: 0.13555781543254852
step: 5100, Loss: 0.11878766119480133
step: 5200, Loss: 0.12792381644248962
step: 5300, Loss: 0.13074372708797455
step: 5400, Loss: 0.1257609874010086
step: 5500, Loss: 0.12161675840616226
step: 5600, Loss: 0.12965185940265656
step: 5700, Loss: 0.12475788593292236
step: 5800, Loss: 0.1263362169265747
step: 5900, Loss: 0.12101250141859055
step: 6000, Loss: 0.12799739837646484
step: 6100, Loss: 0.12483980506658554
step: 6200, Loss: 0.13160774111747742
step: 6300, Loss: 0.12352749705314636
step: 6400, Loss: 0.12834444642066956
step: 6500, Loss: 0.19725245237350464
step: 6600, Loss: 0.33544933795928955
step: 6700, Loss: 0.20198413729667664
step: 6800, Loss: 0.21337570250034332
step: 6900, Loss: 0.15825867652893066
step: 7000, Loss: 0.1480436623096466
step: 7100, Loss: 0.13754452764987946
step: 7200, Loss: 0.14312969148159027
step: 7300, Loss: 0.13915953040122986
step: 7400, Loss: 0.1359669417142868
step: 7500, Loss: 0.1310703456401825
step: 7600, Loss: 0.1434960514307022
step: 7700, Loss: 0.1407192200422287
step: 7800, Loss: 0.13803064823150635
step: 7900, Loss: 0.1302427053451538
step: 8000, Loss: 0.13449446856975555
step: 8100, Loss: 0.12497665733098984
step: 8200, Loss: 0.13738642632961273
step: 8300, Loss: 0.12054973840713501
step: 8400, Loss: 0.14615866541862488
step: 8500, Loss: 0.12588723003864288
step: 8600, Loss: 0.13630464673042297
step: 8700, Loss: 0.12161533534526825
step: 8800, Loss: 0.12860935926437378
step: 8900, Loss: 0.12070443481206894
step: 9000, Loss: 0.13683968782424927
step: 9100, Loss: 0.13855616748332977
step: 9200, Loss: 0.13150066137313843
step: 9300, Loss: 0.12216318398714066
step: 9400, Loss: 0.1360594481229782
step: 9500, Loss: 1.1415185928344727
step: 9600, Loss: 0.1775968074798584
step: 9700, Loss: 0.1602792590856552
step: 9800, Loss: 0.15369009971618652
step: 9900, Loss: 0.1335129737854004
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 1.0
recall: 0.8823529411764706
F_score: 0.9375
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.174398735165596
step: 100, Loss: 0.1487576812505722
step: 200, Loss: 0.16874894499778748
step: 300, Loss: 0.1403665989637375
step: 400, Loss: 0.1504075825214386
step: 500, Loss: 0.1349160522222519
step: 600, Loss: 0.14834369719028473
step: 700, Loss: 0.18576344847679138
step: 800, Loss: 0.3437870144844055
step: 900, Loss: 0.184696763753891
step: 1000, Loss: 0.16807390749454498
step: 1100, Loss: 0.15369126200675964
step: 1200, Loss: 0.17411097884178162
step: 1300, Loss: 0.1527656465768814
step: 1400, Loss: 0.15360093116760254
step: 1500, Loss: 0.15248332917690277
step: 1600, Loss: 0.15129846334457397
step: 1700, Loss: 0.14229950308799744
step: 1800, Loss: 0.14747069776058197
step: 1900, Loss: 0.13674995303153992
step: 2000, Loss: 0.14268548786640167
step: 2100, Loss: 0.12918440997600555
step: 2200, Loss: 0.13989366590976715
step: 2300, Loss: 0.12486199289560318
step: 2400, Loss: 0.14124616980552673
step: 2500, Loss: 0.1215466558933258
step: 2600, Loss: 0.1435588002204895
step: 2700, Loss: 0.12547306716442108
step: 2800, Loss: 0.14741675555706024
step: 2900, Loss: 0.1191021278500557
step: 3000, Loss: 0.1415790170431137
step: 3100, Loss: 0.12460070848464966
step: 3200, Loss: 0.13380882143974304
step: 3300, Loss: 0.12061566114425659
step: 3400, Loss: 0.1400955617427826
step: 3500, Loss: 0.1221226155757904
step: 3600, Loss: 0.133846253156662
step: 3700, Loss: 0.11963436752557755
step: 3800, Loss: 0.13171283900737762
step: 3900, Loss: 0.12164053320884705
step: 4000, Loss: 0.1280614137649536
step: 4100, Loss: 0.13234996795654297
step: 4200, Loss: 0.4194362163543701
step: 4300, Loss: 0.18780559301376343
step: 4400, Loss: 0.16637414693832397
step: 4500, Loss: 0.1653919368982315
step: 4600, Loss: 0.15281453728675842
step: 4700, Loss: 0.14984896779060364
step: 4800, Loss: 0.16280275583267212
step: 4900, Loss: 0.15506289899349213
step: 5000, Loss: 0.1462733894586563
step: 5100, Loss: 0.13200631737709045
step: 5200, Loss: 0.15086671710014343
step: 5300, Loss: 0.13027021288871765
step: 5400, Loss: 0.14504002034664154
step: 5500, Loss: 0.13169707357883453
step: 5600, Loss: 0.14879661798477173
step: 5700, Loss: 0.1231636330485344
step: 5800, Loss: 0.14647872745990753
step: 5900, Loss: 0.12608453631401062
step: 6000, Loss: 0.14536382257938385
step: 6100, Loss: 0.12396343797445297
step: 6200, Loss: 0.1277560144662857
step: 6300, Loss: 0.1301058679819107
step: 6400, Loss: 0.1399535983800888
step: 6500, Loss: 0.12429311126470566
step: 6600, Loss: 0.12562237679958344
step: 6700, Loss: 0.12132957577705383
step: 6800, Loss: 0.12835343182086945
step: 6900, Loss: 0.11924159526824951
step: 7000, Loss: 0.14644548296928406
step: 7100, Loss: 0.49681323766708374
step: 7200, Loss: 0.22218161821365356
step: 7300, Loss: 0.1786918193101883
step: 7400, Loss: 0.17400667071342468
step: 7500, Loss: 0.17065183818340302
step: 7600, Loss: 0.15171077847480774
step: 7700, Loss: 0.14255011081695557
step: 7800, Loss: 0.14707960188388824
step: 7900, Loss: 0.13146677613258362
step: 8000, Loss: 0.143385648727417
step: 8100, Loss: 0.12959960103034973
step: 8200, Loss: 0.14157995581626892
step: 8300, Loss: 0.12662148475646973
step: 8400, Loss: 0.13493798673152924
step: 8500, Loss: 0.12387895584106445
step: 8600, Loss: 0.1381496638059616
step: 8700, Loss: 0.12589570879936218
step: 8800, Loss: 0.1275143176317215
step: 8900, Loss: 0.11884524673223495
step: 9000, Loss: 0.13210797309875488
step: 9100, Loss: 0.12501215934753418
step: 9200, Loss: 0.12928466498851776
step: 9300, Loss: 0.11886373162269592
step: 9400, Loss: 0.13251452147960663
step: 9500, Loss: 0.12043118476867676
step: 9600, Loss: 0.12670880556106567
step: 9700, Loss: 0.12348552793264389
step: 9800, Loss: 0.1308477818965912
step: 9900, Loss: 0.12285543978214264
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.18878068029880524
step: 100, Loss: 0.17864851653575897
step: 200, Loss: 0.16337427496910095
step: 300, Loss: 0.1479606181383133
step: 400, Loss: 0.19136962294578552
step: 500, Loss: 0.14776988327503204
step: 600, Loss: 0.1690605878829956
step: 700, Loss: 0.13921129703521729
step: 800, Loss: 0.1559363305568695
step: 900, Loss: 0.15228594839572906
step: 1000, Loss: 0.13379313051700592
step: 1100, Loss: 0.1387048065662384
step: 1200, Loss: 0.1381078064441681
step: 1300, Loss: 0.13186709582805634
step: 1400, Loss: 0.13367435336112976
step: 1500, Loss: 0.1352982223033905
step: 1600, Loss: 0.8428391814231873
step: 1700, Loss: 0.681665301322937
step: 1800, Loss: 0.213041290640831
step: 1900, Loss: 0.2255571484565735
step: 2000, Loss: 0.23493441939353943
step: 2100, Loss: 0.1783251166343689
step: 2200, Loss: 0.18075363337993622
step: 2300, Loss: 0.16434365510940552
step: 2400, Loss: 0.15704314410686493
step: 2500, Loss: 0.14767326414585114
step: 2600, Loss: 0.178297221660614
step: 2700, Loss: 0.14470255374908447
step: 2800, Loss: 0.15789373219013214
step: 2900, Loss: 0.14180824160575867
step: 3000, Loss: 0.15150508284568787
step: 3100, Loss: 0.14218038320541382
step: 3200, Loss: 0.1427546739578247
step: 3300, Loss: 0.1351909339427948
step: 4700, Loss: 0.12238562107086182
step: 4800, Loss: 0.11767806112766266
step: 4900, Loss: 0.12710464000701904
step: 5000, Loss: 0.12620556354522705
step: 5100, Loss: 0.1323157548904419
step: 5200, Loss: 0.1196034625172615
step: 5300, Loss: 2.4640896320343018
step: 5400, Loss: 1.160955786705017
step: 5500, Loss: 0.5561812520027161
step: 5600, Loss: 0.3075558543205261
step: 5700, Loss: 0.39885711669921875
step: 5800, Loss: 0.23580415546894073
step: 5900, Loss: 0.17349840700626373
step: 6000, Loss: 0.16035990417003632
step: 6100, Loss: 0.1531340777873993
step: 6200, Loss: 0.1640988141298294
step: 6300, Loss: 0.1588369905948639
step: 6400, Loss: 0.1467887908220291
step: 6500, Loss: 0.14348295331001282
step: 6600, Loss: 0.15173077583312988
step: 6700, Loss: 0.13512413203716278
step: 6800, Loss: 0.14920376241207123
step: 6900, Loss: 0.13968464732170105
step: 7000, Loss: 0.1505996435880661
step: 7100, Loss: 0.12752902507781982
step: 7200, Loss: 0.22573703527450562
step: 7300, Loss: 0.13629291951656342
step: 7400, Loss: 0.1313786506652832
step: 7500, Loss: 0.1331728994846344
step: 7600, Loss: 0.1270046979188919
step: 7700, Loss: 0.1269359588623047
step: 7800, Loss: 0.12532198429107666
step: 7900, Loss: 0.12340642511844635
step: 8000, Loss: 0.12590010464191437
step: 8100, Loss: 0.12154433876276016
step: 8200, Loss: 0.12341096997261047
step: 8300, Loss: 0.12003228068351746
step: 8400, Loss: 0.12106156349182129
step: 8500, Loss: 0.1251598298549652
step: 8600, Loss: 0.11711637675762177
step: 8700, Loss: 0.12698985636234283
step: 8800, Loss: 0.12188198417425156
step: 8900, Loss: 0.12311696261167526
step: 9000, Loss: 0.1173732727766037
step: 9100, Loss: 0.2007015198469162
step: 9200, Loss: 0.12297166138887405
step: 9300, Loss: 0.12034551799297333
step: 9400, Loss: 0.1190420389175415
step: 9500, Loss: 0.11806405335664749
step: 9600, Loss: 0.12105792760848999
step: 9700, Loss: 0.1173551008105278
step: 9800, Loss: 0.1164252907037735
step: 9900, Loss: 0.1176375150680542
training successfully ended.
validating...
validate data length:76
acc: 0.75
precision: 0.723404255319149
recall: 0.8717948717948718
F_score: 0.7906976744186047
******fold 2******

Training... train_data length:684
step: 0, Loss: 4.738021373748779
step: 100, Loss: 0.15142062306404114
step: 200, Loss: 0.15792006254196167
step: 300, Loss: 0.14506518840789795
step: 400, Loss: 0.14062754809856415
step: 500, Loss: 0.13102509081363678
step: 600, Loss: 0.1290733963251114
step: 700, Loss: 0.12195491790771484
step: 800, Loss: 0.1302267462015152
step: 900, Loss: 0.12870648503303528
step: 1000, Loss: 0.11920766532421112
step: 1100, Loss: 0.1183517649769783
step: 1200, Loss: 0.11961963027715683
step: 1300, Loss: 0.1195753812789917
step: 1400, Loss: 0.1161225363612175
step: 1500, Loss: 0.19792364537715912
step: 1600, Loss: 0.118903249502182
step: 1700, Loss: 0.12208051234483719
step: 1800, Loss: 0.11830023676156998
step: 1900, Loss: 0.11941352486610413
step: 2000, Loss: 0.11547072231769562
step: 2100, Loss: 0.1172797828912735
step: 2200, Loss: 0.11676106601953506
step: 2300, Loss: 0.11713062971830368
step: 2400, Loss: 0.1150612160563469
step: 2500, Loss: 0.11666178703308105
step: 2600, Loss: 0.11574798822402954
step: 2700, Loss: 0.11898047477006912
step: 2800, Loss: 0.1186622828245163
step: 2900, Loss: 0.11422384530305862
step: 3000, Loss: 0.11503511667251587
step: 3100, Loss: 0.11535372585058212
step: 3200, Loss: 0.12078030407428741
step: 3300, Loss: 0.11679446697235107
step: 3400, Loss: 0.19923534989356995
step: 3500, Loss: 0.11509877443313599
step: 3600, Loss: 0.11643028259277344
step: 3700, Loss: 0.11627084016799927
step: 3800, Loss: 0.11592366546392441
step: 3900, Loss: 0.11740759760141373
step: 4000, Loss: 0.8425122499465942
step: 4100, Loss: 0.24275246262550354
step: 4200, Loss: 1.358902096748352
step: 4300, Loss: 0.28255829215049744
step: 4400, Loss: 0.17388461530208588
step: 4500, Loss: 0.1367870718240738
step: 4600, Loss: 0.1477513611316681
step: 4700, Loss: 0.1430894285440445
step: 4800, Loss: 0.15816283226013184
step: 4900, Loss: 0.12712448835372925
step: 5000, Loss: 0.14343154430389404
step: 5100, Loss: 0.14381957054138184
step: 5200, Loss: 0.13705553114414215
step: 5300, Loss: 0.22564247250556946
step: 5400, Loss: 0.12894758582115173
step: 5500, Loss: 0.12659193575382233
step: 5600, Loss: 0.13686609268188477
step: 5700, Loss: 0.12976530194282532
step: 5800, Loss: 0.13080330193042755
step: 5900, Loss: 0.12863147258758545
step: 6000, Loss: 0.12587451934814453
step: 6100, Loss: 0.12257355451583862
step: 6200, Loss: 0.1260145604610443
step: 6300, Loss: 0.1274065524339676
step: 6400, Loss: 0.11623121798038483
step: 6500, Loss: 0.12184964120388031
step: 6600, Loss: 0.1235903650522232
step: 6700, Loss: 0.12283389270305634
step: 6800, Loss: 0.11699747294187546
step: 6900, Loss: 0.12092022597789764
step: 7000, Loss: 0.1181754618883133
step: 7100, Loss: 0.11823795735836029
step: 7200, Loss: 0.19917131960391998
step: 7300, Loss: 0.11604742705821991
step: 7400, Loss: 0.1183173805475235
step: 7500, Loss: 0.11788509786128998
step: 7600, Loss: 0.11821170896291733
step: 7700, Loss: 0.12169491499662399
step: 7800, Loss: 0.11729024350643158
step: 7900, Loss: 0.11983188986778259
step: 8000, Loss: 0.11710906028747559
step: 8100, Loss: 0.11894261091947556
step: 8200, Loss: 0.11919448524713516
step: 8300, Loss: 0.11429544538259506
step: 8400, Loss: 0.11638785153627396
step: 8500, Loss: 0.11606620252132416
step: 8600, Loss: 0.11534756422042847
step: 8700, Loss: 0.11525595188140869
step: 8800, Loss: 0.11547484993934631
step: 8900, Loss: 0.11724719405174255
step: 9000, Loss: 0.11445286870002747
step: 9100, Loss: 0.19665277004241943
step: 9200, Loss: 0.11486229300498962
step: 9300, Loss: 0.1152404397726059
step: 9400, Loss: 0.11682023108005524
step: 9500, Loss: 0.11637113988399506
step: 9600, Loss: 0.11636564135551453
step: 9700, Loss: 0.11499568819999695
step: 9800, Loss: 0.11509501934051514
step: 9900, Loss: 0.11453957855701447
training successfully ended.
validating...
validate data length:76
acc: 0.9027777777777778
precision: 0.8888888888888888
recall: 0.9523809523809523
F_score: 0.9195402298850575
******fold 3******

Training... train_data length:684
step: 0, Loss: 1.109312653541565
step: 100, Loss: 0.14535793662071228
step: 200, Loss: 0.13142475485801697
step: 300, Loss: 0.12709940969944
step: 400, Loss: 0.11780304461717606
step: 500, Loss: 0.12031516432762146
step: 600, Loss: 0.12326254695653915
step: 700, Loss: 0.12099910527467728
step: 800, Loss: 0.12115004658699036
step: 900, Loss: 0.11804812401533127
step: 1000, Loss: 0.11879788339138031
step: 1100, Loss: 0.11987729370594025
step: 1200, Loss: 0.11949355900287628
step: 1300, Loss: 0.12039905786514282
step: 1400, Loss: 0.11784026771783829
step: 1500, Loss: 0.19431878626346588
step: 1600, Loss: 0.11580730974674225
step: 1700, Loss: 0.11571379005908966
step: 1800, Loss: 0.1191633865237236
step: 1900, Loss: 0.11687430739402771
step: 2000, Loss: 0.11602014303207397
step: 2100, Loss: 0.11645376682281494
step: 2200, Loss: 0.11446521431207657
step: 2300, Loss: 0.1161363422870636
step: 2400, Loss: 0.11498910188674927
step: 2500, Loss: 0.11459925025701523
step: 2600, Loss: 0.11493130773305893
step: 2700, Loss: 0.11563319712877274
step: 2800, Loss: 0.11573395133018494
step: 2900, Loss: 0.11569817364215851
step: 3000, Loss: 0.11715379357337952
step: 3100, Loss: 0.11345124989748001
step: 3200, Loss: 0.1164398267865181
step: 3300, Loss: 0.11541056632995605
step: 3400, Loss: 0.19642062485218048
step: 3500, Loss: 0.114972785115242
step: 3600, Loss: 0.1152944564819336
step: 3700, Loss: 0.11567607522010803
step: 3800, Loss: 0.11408303678035736
step: 3900, Loss: 0.11436805129051208
step: 4000, Loss: 0.11570355296134949
step: 4100, Loss: 0.11741166561841965
step: 4200, Loss: 0.11725638061761856
step: 4300, Loss: 0.11561373621225357
step: 4400, Loss: 0.11409269273281097
step: 4500, Loss: 0.11592409014701843
step: 4600, Loss: 0.11355922371149063
step: 4700, Loss: 0.12048368155956268
step: 4800, Loss: 0.37404751777648926
step: 4900, Loss: 0.34989097714424133
step: 5000, Loss: 0.16174763441085815
step: 5100, Loss: 0.14227841794490814
step: 5200, Loss: 0.15179918706417084
step: 3400, Loss: 0.1432074010372162
step: 3500, Loss: 0.14045444130897522
step: 3600, Loss: 0.14953090250492096
step: 3700, Loss: 0.13115562498569489
step: 3800, Loss: 0.14086033403873444
step: 3900, Loss: 0.13912120461463928
step: 4000, Loss: 0.13921786844730377
step: 4100, Loss: 0.13848073780536652
step: 4200, Loss: 0.13388210535049438
step: 4300, Loss: 0.13209059834480286
step: 4400, Loss: 0.13532933592796326
step: 4500, Loss: 0.12613627314567566
step: 4600, Loss: 0.1362747699022293
step: 4700, Loss: 0.12621881067752838
step: 4800, Loss: 0.14142292737960815
step: 4900, Loss: 0.12469886988401413
step: 5000, Loss: 0.13093994557857513
step: 5100, Loss: 0.1257876455783844
step: 5200, Loss: 0.1335531771183014
step: 5300, Loss: 0.12665998935699463
step: 5400, Loss: 0.14822827279567719
step: 5500, Loss: 0.14258059859275818
step: 5600, Loss: 0.3279210329055786
step: 5700, Loss: 0.19705238938331604
step: 5800, Loss: 0.17592814564704895
step: 5900, Loss: 0.1490756869316101
step: 6000, Loss: 0.15865617990493774
step: 6100, Loss: 0.13730180263519287
step: 6200, Loss: 0.16147534549236298
step: 6300, Loss: 0.1434953212738037
step: 6400, Loss: 0.1394624561071396
step: 6500, Loss: 0.1408543586730957
step: 6600, Loss: 0.13736464083194733
step: 6700, Loss: 0.13023826479911804
step: 6800, Loss: 0.13873738050460815
step: 6900, Loss: 0.13726972043514252
step: 7000, Loss: 0.13225190341472626
step: 7100, Loss: 0.12481103092432022
step: 7200, Loss: 0.14150111377239227
step: 7300, Loss: 0.12598833441734314
step: 7400, Loss: 0.13434410095214844
step: 7500, Loss: 0.1299860179424286
step: 7600, Loss: 0.13102981448173523
step: 7700, Loss: 0.121692955493927
step: 7800, Loss: 0.1252162754535675
step: 7900, Loss: 0.12692980468273163
step: 8000, Loss: 0.13684412837028503
step: 8100, Loss: 0.12241664528846741
step: 8200, Loss: 0.13573096692562103
step: 8300, Loss: 0.1245526447892189
step: 8400, Loss: 0.1228179931640625
step: 8500, Loss: 0.3478754162788391
step: 8600, Loss: 0.19785167276859283
step: 8700, Loss: 0.15385721623897552
step: 8800, Loss: 0.16326071321964264
step: 8900, Loss: 0.14542245864868164
step: 9000, Loss: 0.16446006298065186
step: 9100, Loss: 0.13246427476406097
step: 9200, Loss: 0.1430739015340805
step: 9300, Loss: 0.13738825917243958
step: 9400, Loss: 0.1431863009929657
step: 9500, Loss: 0.12699976563453674
step: 9600, Loss: 0.1339215785264969
step: 9700, Loss: 0.12699922919273376
step: 9800, Loss: 0.14282271265983582
step: 9900, Loss: 0.13017815351486206
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 1.0
recall: 0.9411764705882353
F_score: 0.9696969696969697
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.16140829026699066
step: 100, Loss: 0.1678042709827423
step: 200, Loss: 0.1538330316543579
step: 300, Loss: 0.164160817861557
step: 400, Loss: 0.16673140227794647
step: 500, Loss: 0.1463613063097
step: 600, Loss: 0.14415743947029114
step: 700, Loss: 0.1373191624879837
step: 800, Loss: 0.15059794485569
step: 900, Loss: 0.1286575198173523
step: 1000, Loss: 0.1463557630777359
step: 1100, Loss: 0.12954680621623993
step: 1200, Loss: 0.13535436987876892
step: 1300, Loss: 0.13425663113594055
step: 1400, Loss: 0.13365402817726135
step: 1500, Loss: 0.13333968818187714
step: 1600, Loss: 3.3576455116271973
step: 1700, Loss: 0.24511781334877014
step: 1800, Loss: 0.18786117434501648
step: 1900, Loss: 0.20530986785888672
step: 2000, Loss: 0.18018868565559387
step: 2100, Loss: 0.1908624768257141
step: 2200, Loss: 0.16778719425201416
step: 2300, Loss: 0.15305103361606598
step: 2400, Loss: 0.2730856239795685
step: 2500, Loss: 0.1720593422651291
step: 2600, Loss: 0.15884217619895935
step: 2700, Loss: 0.14356482028961182
step: 2800, Loss: 0.14962510764598846
step: 2900, Loss: 0.14217042922973633
step: 3000, Loss: 0.14867129921913147
step: 3100, Loss: 0.13568852841854095
step: 3200, Loss: 0.15145492553710938
step: 3300, Loss: 0.13792085647583008
step: 3400, Loss: 0.1414758712053299
step: 3500, Loss: 0.12905563414096832
step: 3600, Loss: 0.14101332426071167
step: 3700, Loss: 0.1317601352930069
step: 3800, Loss: 0.14230237901210785
step: 3900, Loss: 0.1391478180885315
step: 4000, Loss: 0.2086912840604782
step: 4100, Loss: 0.17257888615131378
step: 4200, Loss: 0.14948616921901703
step: 4300, Loss: 0.17463622987270355
step: 4400, Loss: 0.14661379158496857
step: 4500, Loss: 0.1456163376569748
step: 4600, Loss: 0.15443332493305206
step: 4700, Loss: 0.13502195477485657
step: 4800, Loss: 0.14082680642604828
step: 4900, Loss: 0.13939262926578522
step: 5000, Loss: 0.14635810256004333
step: 5100, Loss: 0.1323104351758957
step: 5200, Loss: 0.13441616296768188
step: 5300, Loss: 0.12972553074359894
step: 5400, Loss: 0.14154334366321564
step: 5500, Loss: 0.13121357560157776
step: 5600, Loss: 0.13045503199100494
step: 5700, Loss: 0.1242130771279335
step: 5800, Loss: 0.1290423423051834
step: 5900, Loss: 0.12852326035499573
step: 6000, Loss: 0.12699806690216064
step: 6100, Loss: 0.1259821504354477
step: 6200, Loss: 0.1393895149230957
step: 6300, Loss: 0.12575076520442963
step: 6400, Loss: 0.13575884699821472
step: 6500, Loss: 0.12784771621227264
step: 6600, Loss: 0.1300399750471115
step: 6700, Loss: 0.12439542263746262
step: 6800, Loss: 0.13195297122001648
step: 6900, Loss: 0.12298212945461273
step: 7000, Loss: 0.12809917330741882
step: 7100, Loss: 0.1265268474817276
step: 7200, Loss: 2.213566780090332
step: 7300, Loss: 0.16353395581245422
step: 7400, Loss: 0.1723501831293106
step: 7500, Loss: 0.1622191071510315
step: 7600, Loss: 0.1545500010251999
step: 7700, Loss: 0.1443174034357071
step: 7800, Loss: 0.15541431307792664
step: 7900, Loss: 0.13525593280792236
step: 8000, Loss: 0.13992080092430115
step: 8100, Loss: 0.13920113444328308
step: 8200, Loss: 0.1495887041091919
step: 8300, Loss: 0.13304632902145386
step: 8400, Loss: 0.1437276303768158
step: 8500, Loss: 0.13806232810020447
step: 8600, Loss: 0.132783442735672
step: 8700, Loss: 0.13141195476055145
step: 8800, Loss: 0.15899115800857544
step: 8900, Loss: 0.13191871345043182
step: 9000, Loss: 0.1389336735010147
step: 9100, Loss: 0.12329203635454178
step: 9200, Loss: 0.13347016274929047
step: 9300, Loss: 0.12449610233306885
step: 9400, Loss: 0.12693443894386292
step: 9500, Loss: 0.12134810537099838
step: 9600, Loss: 0.12877190113067627
step: 9700, Loss: 0.13240565359592438
step: 9800, Loss: 0.13261884450912476
step: 9900, Loss: 0.12302330136299133
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.9166666666666666
recall: 0.9166666666666666
F_score: 0.9166666666666666
subject 10 Avgacc: 0.9404166666666667 Avgfscore: 0.9423229144897226 
 Max acc:1.0, Max f score:1.0
******** mix subject_11 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 69.82080078125
step: 100, Loss: 6.758973121643066
step: 200, Loss: 2.2913966178894043
step: 300, Loss: 0.2826274633407593
step: 400, Loss: 0.4756324887275696
step: 500, Loss: 0.159965381026268
step: 600, Loss: 1.196426272392273
step: 700, Loss: 0.18061485886573792
step: 800, Loss: 0.3303186297416687
step: 900, Loss: 0.15114012360572815
step: 1000, Loss: 0.29818305373191833
step: 1100, Loss: 0.14463108777999878
step: 1200, Loss: 0.2629454433917999
step: 1300, Loss: 0.14333534240722656
step: 1400, Loss: 0.23892483115196228
step: 1500, Loss: 0.1375412940979004
step: 1600, Loss: 0.2337656468153
step: 1700, Loss: 0.13801005482673645
step: 1800, Loss: 0.22758162021636963
step: 1900, Loss: 0.1270999312400818
step: 2000, Loss: 0.19829463958740234
step: 2100, Loss: 0.12194957584142685
step: 2200, Loss: 0.16843181848526
step: 2300, Loss: 0.13100071251392365
step: 2400, Loss: 0.1598440408706665
step: 2500, Loss: 0.12275980412960052
step: 2600, Loss: 0.16214528679847717
step: 2700, Loss: 0.12044859677553177
step: 2800, Loss: 0.1374463438987732
step: 2900, Loss: 0.13434603810310364
step: 3000, Loss: 0.143354594707489
step: 3100, Loss: 0.11711296439170837
step: 3200, Loss: 0.14440299570560455
step: 3300, Loss: 0.11735950410366058
step: 3400, Loss: 0.14270344376564026
step: 3500, Loss: 0.11826273798942566
step: 3600, Loss: 0.1407928764820099
step: 5300, Loss: 0.2090865522623062
step: 5400, Loss: 0.13320131599903107
step: 5500, Loss: 0.1313772052526474
step: 5600, Loss: 0.13207708299160004
step: 5700, Loss: 0.1282511055469513
step: 5800, Loss: 0.13343602418899536
step: 5900, Loss: 0.12049347162246704
step: 6000, Loss: 0.12194854021072388
step: 6100, Loss: 0.12150721997022629
step: 6200, Loss: 0.11920368671417236
step: 6300, Loss: 0.12331882864236832
step: 6400, Loss: 0.12279026210308075
step: 6500, Loss: 0.12138640880584717
step: 6600, Loss: 0.12178942561149597
step: 6700, Loss: 0.11831632256507874
step: 6800, Loss: 0.12346357852220535
step: 6900, Loss: 0.11895386874675751
step: 7000, Loss: 0.11732516437768936
step: 7100, Loss: 0.13211801648139954
step: 7200, Loss: 0.1999485194683075
step: 7300, Loss: 0.11723268032073975
step: 7400, Loss: 0.115188829600811
step: 7500, Loss: 0.1209244430065155
step: 7600, Loss: 0.1177651435136795
step: 7700, Loss: 0.11448017507791519
step: 7800, Loss: 0.11612941324710846
step: 7900, Loss: 0.1197594553232193
step: 8000, Loss: 0.11630462110042572
step: 8100, Loss: 0.11514733731746674
step: 8200, Loss: 0.11867930740118027
step: 8300, Loss: 0.11688346415758133
step: 8400, Loss: 0.11695514619350433
step: 8500, Loss: 0.11420419067144394
step: 8600, Loss: 0.11674767732620239
step: 8700, Loss: 0.11579359322786331
step: 8800, Loss: 0.11732333898544312
step: 8900, Loss: 0.11435377597808838
step: 9000, Loss: 0.1141178160905838
step: 9100, Loss: 0.1924557387828827
step: 9200, Loss: 0.11526264995336533
step: 9300, Loss: 0.1141410544514656
step: 9400, Loss: 0.117486372590065
step: 9500, Loss: 0.11393178254365921
step: 9600, Loss: 0.11530652642250061
step: 9700, Loss: 0.11456392705440521
step: 9800, Loss: 0.1159813180565834
step: 9900, Loss: 0.11464006453752518
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.9
recall: 1.0
F_score: 0.9473684210526316
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.11559068411588669
step: 100, Loss: 0.137565478682518
step: 200, Loss: 0.12051026523113251
step: 300, Loss: 0.11861263960599899
step: 400, Loss: 0.11684715747833252
step: 500, Loss: 0.11792045831680298
step: 600, Loss: 0.11531753838062286
step: 700, Loss: 0.11671435832977295
step: 800, Loss: 0.11648642271757126
step: 900, Loss: 0.1152205765247345
step: 1000, Loss: 0.11492753773927689
step: 1100, Loss: 0.11604763567447662
step: 1200, Loss: 0.11749333143234253
step: 1300, Loss: 0.11436647176742554
step: 1400, Loss: 0.11614051461219788
step: 1500, Loss: 0.19247721135616302
step: 1600, Loss: 0.11735986918210983
step: 1700, Loss: 0.11406414955854416
step: 1800, Loss: 0.11395999789237976
step: 1900, Loss: 0.11551964282989502
step: 2000, Loss: 0.11531659960746765
step: 2100, Loss: 0.11354263126850128
step: 2200, Loss: 0.1164047122001648
step: 2300, Loss: 0.11412161588668823
step: 2400, Loss: 0.11440665274858475
step: 2500, Loss: 0.11338931322097778
step: 2600, Loss: 0.11592152714729309
step: 2700, Loss: 0.11390358954668045
step: 2800, Loss: 0.11439859867095947
step: 2900, Loss: 0.11368631571531296
step: 3000, Loss: 0.11334466934204102
step: 3100, Loss: 0.11457037180662155
step: 3200, Loss: 0.11342763900756836
step: 3300, Loss: 0.11398836970329285
step: 3400, Loss: 0.2004317343235016
step: 3500, Loss: 0.11668560653924942
step: 3600, Loss: 0.11422418802976608
step: 3700, Loss: 0.11546941101551056
step: 3800, Loss: 0.11469682306051254
step: 3900, Loss: 0.11369234323501587
step: 4000, Loss: 0.11588722467422485
step: 4100, Loss: 0.11503547430038452
step: 4200, Loss: 0.11983316391706467
step: 4300, Loss: 0.11556024849414825
step: 4400, Loss: 0.11916212737560272
step: 4500, Loss: 0.11474287509918213
step: 4600, Loss: 0.11629068851470947
step: 4700, Loss: 5.113027572631836
step: 4800, Loss: 1.0590014457702637
step: 4900, Loss: 0.26881158351898193
step: 5000, Loss: 0.1453263759613037
step: 5100, Loss: 0.13129910826683044
step: 5200, Loss: 0.13813525438308716
step: 5300, Loss: 0.22309505939483643
step: 5400, Loss: 0.12896139919757843
step: 5500, Loss: 0.12815344333648682
step: 5600, Loss: 0.1223263144493103
step: 5700, Loss: 0.12617038190364838
step: 5800, Loss: 0.12602603435516357
step: 5900, Loss: 0.12561681866645813
step: 6000, Loss: 0.12402638792991638
step: 6100, Loss: 0.11805437505245209
step: 6200, Loss: 0.11670490354299545
step: 6300, Loss: 0.11904006451368332
step: 6400, Loss: 0.11780944466590881
step: 6500, Loss: 0.11824051290750504
step: 6600, Loss: 0.11926963925361633
step: 6700, Loss: 0.11986474692821503
step: 6800, Loss: 0.11809276044368744
step: 6900, Loss: 0.11715678870677948
step: 7000, Loss: 0.11691129207611084
step: 7100, Loss: 0.11605272442102432
step: 7200, Loss: 0.20303116738796234
step: 7300, Loss: 0.11983781307935715
step: 7400, Loss: 0.11928708851337433
step: 7500, Loss: 0.11749889701604843
step: 7600, Loss: 0.11588312685489655
step: 7700, Loss: 0.11715365946292877
step: 7800, Loss: 0.11833083629608154
step: 7900, Loss: 0.11649820953607559
step: 8000, Loss: 0.11599655449390411
step: 8100, Loss: 0.1135963425040245
step: 8200, Loss: 0.11468411982059479
step: 8300, Loss: 0.11411934345960617
step: 8400, Loss: 0.11486675590276718
step: 8500, Loss: 0.11795192211866379
step: 8600, Loss: 0.11454485356807709
step: 8700, Loss: 0.11430377513170242
step: 8800, Loss: 0.11535157263278961
step: 8900, Loss: 0.11484768241643906
step: 9000, Loss: 0.11636827886104584
step: 9100, Loss: 0.19460809230804443
step: 9200, Loss: 0.11424949765205383
step: 9300, Loss: 0.11382794380187988
step: 9400, Loss: 0.11368010193109512
step: 9500, Loss: 0.11456028372049332
step: 9600, Loss: 0.11402859538793564
step: 9700, Loss: 0.11470536887645721
step: 9800, Loss: 0.11379251629114151
step: 9900, Loss: 0.11398957669734955
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9714285714285714
recall: 1.0
F_score: 0.9855072463768115
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.1688847690820694
step: 100, Loss: 0.12436520308256149
step: 200, Loss: 0.1199558675289154
step: 300, Loss: 0.11692490428686142
step: 400, Loss: 0.12138932198286057
step: 500, Loss: 0.11588156968355179
step: 600, Loss: 0.12444096803665161
step: 700, Loss: 0.11560004949569702
step: 800, Loss: 0.11676464974880219
step: 900, Loss: 0.11655914783477783
step: 1000, Loss: 0.11711932718753815
step: 1100, Loss: 0.1149115189909935
step: 1200, Loss: 0.11507493257522583
step: 1300, Loss: 0.1149488165974617
step: 1400, Loss: 0.11526279896497726
step: 1500, Loss: 0.19281576573848724
step: 1600, Loss: 0.11406496912240982
step: 1700, Loss: 0.11550482362508774
step: 1800, Loss: 0.11411140859127045
step: 1900, Loss: 0.11538684368133545
step: 2000, Loss: 0.11627338081598282
step: 2100, Loss: 0.11315521597862244
step: 2200, Loss: 0.11387956887483597
step: 2300, Loss: 0.11437416821718216
step: 2400, Loss: 0.11487027257680893
step: 2500, Loss: 0.11371400952339172
step: 2600, Loss: 0.11389826983213425
step: 2700, Loss: 0.11385142803192139
step: 2800, Loss: 0.11395609378814697
step: 2900, Loss: 0.11547449231147766
step: 3000, Loss: 0.11364544183015823
step: 3100, Loss: 0.11400783061981201
step: 3200, Loss: 0.11393868923187256
step: 3300, Loss: 0.11426084488630295
step: 3400, Loss: 0.19679203629493713
step: 3500, Loss: 0.11461573839187622
step: 3600, Loss: 0.11528623849153519
step: 3700, Loss: 0.11366425454616547
step: 3800, Loss: 0.1163267195224762
step: 3900, Loss: 0.11473763734102249
step: 4000, Loss: 0.11669434607028961
step: 4100, Loss: 0.11399529874324799
step: 4200, Loss: 6.630764484405518
step: 4300, Loss: 0.17305538058280945
step: 4400, Loss: 0.19815868139266968
step: 4500, Loss: 0.15272468328475952
step: 4600, Loss: 0.135906383395195
step: 4700, Loss: 0.13005128502845764
step: 4800, Loss: 0.12624356150627136
step: 4900, Loss: 0.13091212511062622
step: 5000, Loss: 0.12616363167762756
step: 5100, Loss: 0.12806445360183716
step: 5200, Loss: 0.12002742290496826
step: 5300, Loss: 0.21239539980888367
step: 5400, Loss: 0.12139201164245605
step: 5500, Loss: 0.1215694472193718
step: 5600, Loss: 0.1299685835838318
step: 5700, Loss: 0.12098333239555359
step: 5800, Loss: 0.11586857587099075
step: 3700, Loss: 0.13701242208480835
step: 3800, Loss: 0.369581937789917
step: 3900, Loss: 0.22776781022548676
step: 4000, Loss: 0.18362803757190704
step: 4100, Loss: 0.1454906463623047
step: 4200, Loss: 0.22514183819293976
step: 4300, Loss: 0.14383912086486816
step: 4400, Loss: 0.13876089453697205
step: 4500, Loss: 0.1310928463935852
step: 4600, Loss: 0.20328745245933533
step: 4700, Loss: 0.1284923255443573
step: 4800, Loss: 0.1352139711380005
step: 4900, Loss: 0.12359170615673065
step: 5000, Loss: 0.12997037172317505
step: 5100, Loss: 0.1315767765045166
step: 5200, Loss: 0.14196422696113586
step: 5300, Loss: 0.11861230432987213
step: 5400, Loss: 0.14344200491905212
step: 5500, Loss: 0.1177021786570549
step: 5600, Loss: 0.1279766857624054
step: 5700, Loss: 0.11940006911754608
step: 5800, Loss: 0.12474536150693893
step: 5900, Loss: 0.1194237768650055
step: 6000, Loss: 0.12239845097064972
step: 6100, Loss: 0.12206506729125977
step: 6200, Loss: 0.13930615782737732
step: 6300, Loss: 0.1174144595861435
step: 6400, Loss: 0.11896319687366486
step: 6500, Loss: 0.11680133640766144
step: 6600, Loss: 0.13430771231651306
step: 6700, Loss: 0.12230008840560913
step: 6800, Loss: 0.11996513605117798
step: 6900, Loss: 0.1172795295715332
step: 7000, Loss: 0.1693970263004303
step: 7100, Loss: 0.118147112429142
step: 7200, Loss: 0.11572236567735672
step: 7300, Loss: 0.11791073530912399
step: 7400, Loss: 0.11724177747964859
step: 7500, Loss: 0.11789107322692871
step: 7600, Loss: 0.12037794291973114
step: 7700, Loss: 0.11539463698863983
step: 7800, Loss: 0.11634308099746704
step: 7900, Loss: 0.12037129700183868
step: 8000, Loss: 0.12614695727825165
step: 8100, Loss: 0.11608342826366425
step: 8200, Loss: 0.11724443733692169
step: 8300, Loss: 0.11672326922416687
step: 8400, Loss: 0.12172693014144897
step: 8500, Loss: 0.11645615100860596
step: 8600, Loss: 0.11539918929338455
step: 8700, Loss: 0.11888811737298965
step: 8800, Loss: 0.11562079191207886
step: 8900, Loss: 0.11419100314378738
step: 9000, Loss: 0.11563638597726822
step: 9100, Loss: 0.11434780806303024
step: 9200, Loss: 0.1167578250169754
step: 9300, Loss: 0.11557809263467789
step: 9400, Loss: 0.11548283696174622
step: 9500, Loss: 0.11376731097698212
step: 9600, Loss: 0.11313460767269135
step: 9700, Loss: 0.11472397297620773
step: 9800, Loss: 0.1163000613451004
step: 9900, Loss: 0.11414022743701935
training successfully ended.
validating...
validate data length:32
acc: 0.75
precision: 0.625
recall: 0.8333333333333334
F_score: 0.7142857142857143
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.11602827906608582
step: 100, Loss: 0.12589704990386963
step: 200, Loss: 0.12388958781957626
step: 300, Loss: 0.11724266409873962
step: 400, Loss: 0.11755102872848511
step: 500, Loss: 0.11497771739959717
step: 600, Loss: 0.11590951681137085
step: 700, Loss: 0.11656883358955383
step: 800, Loss: 0.11516892910003662
step: 900, Loss: 0.11447218805551529
step: 1000, Loss: 0.11564784497022629
step: 1100, Loss: 0.11448706686496735
step: 1200, Loss: 0.11480551213026047
step: 1300, Loss: 0.11515802890062332
step: 1400, Loss: 0.1129315122961998
step: 1500, Loss: 0.11359667032957077
step: 1600, Loss: 0.11276975274085999
step: 1700, Loss: 0.115122489631176
step: 1800, Loss: 0.11356959491968155
step: 1900, Loss: 0.11478085815906525
step: 2000, Loss: 0.11459271609783173
step: 2100, Loss: 0.11343445628881454
step: 2200, Loss: 0.11375884711742401
step: 2300, Loss: 0.11303327977657318
step: 2400, Loss: 0.1144130602478981
step: 2500, Loss: 0.11314769089221954
step: 2600, Loss: 0.11526384204626083
step: 2700, Loss: 0.1140679270029068
step: 2800, Loss: 0.11655294895172119
step: 2900, Loss: 0.11430636793375015
step: 3000, Loss: 0.11363792419433594
step: 3100, Loss: 0.11663481593132019
step: 3200, Loss: 0.11566605418920517
step: 3300, Loss: 0.11468814313411713
step: 3400, Loss: 0.11506733298301697
step: 3500, Loss: 0.1147090420126915
step: 3600, Loss: 0.11719676852226257
step: 3700, Loss: 0.11532089859247208
step: 3800, Loss: 0.11444550007581711
step: 3900, Loss: 0.11462061107158661
step: 4000, Loss: 0.11454856395721436
step: 4100, Loss: 0.11471277475357056
step: 4200, Loss: 0.11476938426494598
step: 4300, Loss: 0.11367003619670868
step: 4400, Loss: 0.11379805207252502
step: 4500, Loss: 0.11548404395580292
step: 4600, Loss: 0.1151287853717804
step: 4700, Loss: 0.11602828651666641
step: 4800, Loss: 0.11330191791057587
step: 4900, Loss: 0.11443126201629639
step: 5000, Loss: 0.11514189094305038
step: 5100, Loss: 0.1136954128742218
step: 5200, Loss: 0.11542780697345734
step: 5300, Loss: 0.23322509229183197
step: 5400, Loss: 0.14130371809005737
step: 5500, Loss: 0.12463746964931488
step: 5600, Loss: 0.12875206768512726
step: 5700, Loss: 0.12305882573127747
step: 5800, Loss: 0.1236809492111206
step: 5900, Loss: 0.12385798990726471
step: 6000, Loss: 0.12227185070514679
step: 6100, Loss: 0.1173662543296814
step: 6200, Loss: 0.11806461215019226
step: 6300, Loss: 0.11717693507671356
step: 6400, Loss: 0.11909739673137665
step: 6500, Loss: 0.11493176221847534
step: 6600, Loss: 0.11605339497327805
step: 6700, Loss: 0.11637532711029053
step: 6800, Loss: 0.11705062538385391
step: 6900, Loss: 0.1146564707159996
step: 7000, Loss: 0.1159176453948021
step: 7100, Loss: 0.11499042063951492
step: 7200, Loss: 0.11413395404815674
step: 7300, Loss: 0.11554938554763794
step: 7400, Loss: 0.11451011896133423
step: 7500, Loss: 0.11387588083744049
step: 7600, Loss: 0.11585846543312073
step: 7700, Loss: 0.11395103484392166
step: 7800, Loss: 0.11468400061130524
step: 7900, Loss: 0.11456415802240372
step: 8000, Loss: 0.11356326937675476
step: 8100, Loss: 0.11447390913963318
step: 8200, Loss: 0.1154237911105156
step: 8300, Loss: 0.11339517682790756
step: 8400, Loss: 0.11285804957151413
step: 8500, Loss: 0.11403980851173401
step: 8600, Loss: 0.1157882809638977
step: 8700, Loss: 0.1137121245265007
step: 8800, Loss: 0.11347279697656631
step: 8900, Loss: 0.11416660249233246
step: 9000, Loss: 0.11529777944087982
step: 9100, Loss: 0.11435332149267197
step: 9200, Loss: 0.11408191174268723
step: 9300, Loss: 0.11449266970157623
step: 9400, Loss: 0.1133643388748169
step: 9500, Loss: 0.11413310468196869
step: 9600, Loss: 0.11313255876302719
step: 9700, Loss: 0.11354754120111465
step: 9800, Loss: 0.11401429027318954
step: 9900, Loss: 0.11348053812980652
training successfully ended.
validating...
validate data length:32
acc: 0.71875
precision: 0.6666666666666666
recall: 0.875
F_score: 0.7567567567567567
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.12102978676557541
step: 100, Loss: 0.13238263130187988
step: 200, Loss: 0.12364841252565384
step: 300, Loss: 0.11932597309350967
step: 400, Loss: 0.11737126111984253
step: 500, Loss: 0.11709827929735184
step: 600, Loss: 0.11930321902036667
step: 700, Loss: 0.11396364122629166
step: 800, Loss: 0.12285390496253967
step: 900, Loss: 0.11457210034132004
step: 1000, Loss: 0.11543367803096771
step: 1100, Loss: 0.11468110233545303
step: 1200, Loss: 0.11720052361488342
step: 1300, Loss: 0.11340374499559402
step: 1400, Loss: 0.1131151095032692
step: 1500, Loss: 0.11390137672424316
step: 1600, Loss: 0.11302219331264496
step: 1700, Loss: 0.11455900967121124
step: 1800, Loss: 0.11510182172060013
step: 1900, Loss: 0.11393706500530243
step: 2000, Loss: 0.11699852347373962
step: 2100, Loss: 0.11569739878177643
step: 2200, Loss: 0.11355122923851013
step: 2300, Loss: 0.1152942031621933
step: 2400, Loss: 0.11388062685728073
step: 2500, Loss: 0.11536439508199692
step: 2600, Loss: 0.11907073855400085
step: 2700, Loss: 0.11542627960443497
step: 2800, Loss: 0.11629872769117355
step: 2900, Loss: 0.11435108631849289
step: 3000, Loss: 0.11352168023586273
step: 3100, Loss: 0.11507970094680786
step: 3200, Loss: 0.11669956892728806
step: 3300, Loss: 0.11520956456661224
step: 3400, Loss: 0.11361654102802277
step: 3500, Loss: 0.11666899919509888
step: 3600, Loss: 0.11391523480415344
step: 3700, Loss: 0.11560487747192383
step: 3800, Loss: 0.11377108097076416
step: 3900, Loss: 0.11412181705236435
step: 4000, Loss: 0.11542718857526779
step: 4100, Loss: 0.11681823432445526
step: 4200, Loss: 0.11464744061231613
step: 5900, Loss: 0.11930765211582184
step: 6000, Loss: 0.12331108748912811
step: 6100, Loss: 0.11952333152294159
step: 6200, Loss: 0.11761446297168732
step: 6300, Loss: 0.12092085927724838
step: 6400, Loss: 0.11721985787153244
step: 6500, Loss: 0.11729520559310913
step: 6600, Loss: 0.1155007854104042
step: 6700, Loss: 0.11586272716522217
step: 6800, Loss: 0.12206900864839554
step: 6900, Loss: 0.12060205638408661
step: 7000, Loss: 0.11992523819208145
step: 7100, Loss: 0.11362802982330322
step: 7200, Loss: 0.2001710683107376
step: 7300, Loss: 0.11510871350765228
step: 7400, Loss: 0.11763999611139297
step: 7500, Loss: 0.11891104280948639
step: 7600, Loss: 0.11478245258331299
step: 7700, Loss: 0.11390392482280731
step: 7800, Loss: 0.11635430157184601
step: 7900, Loss: 0.11470802873373032
step: 8000, Loss: 0.11549076437950134
step: 8100, Loss: 0.11439782381057739
step: 8200, Loss: 0.11679673194885254
step: 8300, Loss: 0.11493043601512909
step: 8400, Loss: 0.1158980056643486
step: 8500, Loss: 0.11332279443740845
step: 8600, Loss: 0.1142093613743782
step: 8700, Loss: 0.11877356469631195
step: 8800, Loss: 0.11600925028324127
step: 8900, Loss: 0.11390777677297592
step: 9000, Loss: 0.11358257383108139
step: 9100, Loss: 0.19213280081748962
step: 9200, Loss: 0.11428817361593246
step: 9300, Loss: 0.11657827347517014
step: 9400, Loss: 0.11419613659381866
step: 9500, Loss: 0.11522124707698822
step: 9600, Loss: 0.11490561068058014
step: 9700, Loss: 0.11378304660320282
step: 9800, Loss: 0.11440227925777435
step: 9900, Loss: 0.11404655873775482
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9318181818181818
recall: 1.0
F_score: 0.9647058823529412
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.12182646989822388
step: 100, Loss: 0.13192707300186157
step: 200, Loss: 0.12377764284610748
step: 300, Loss: 0.12007324397563934
step: 400, Loss: 0.1203053817152977
step: 500, Loss: 0.11648492515087128
step: 600, Loss: 0.1143006831407547
step: 700, Loss: 0.1152375191450119
step: 800, Loss: 0.11984691023826599
step: 900, Loss: 0.11585888266563416
step: 1000, Loss: 0.11481304466724396
step: 1100, Loss: 0.11370867490768433
step: 1200, Loss: 0.11530755460262299
step: 1300, Loss: 0.1145637035369873
step: 1400, Loss: 0.11394327878952026
step: 1500, Loss: 0.18996666371822357
step: 1600, Loss: 0.11501379311084747
step: 1700, Loss: 0.11468242108821869
step: 1800, Loss: 0.11604184657335281
step: 1900, Loss: 0.11329125612974167
step: 2000, Loss: 0.11420684307813644
step: 2100, Loss: 0.11469252407550812
step: 2200, Loss: 0.11499331891536713
step: 2300, Loss: 0.11369796097278595
step: 2400, Loss: 0.11343532055616379
step: 2500, Loss: 0.1138581782579422
step: 2600, Loss: 0.11698201298713684
step: 2700, Loss: 0.11382007598876953
step: 2800, Loss: 0.1145160049200058
step: 2900, Loss: 0.11591722816228867
step: 3000, Loss: 0.1153518408536911
step: 3100, Loss: 0.11331193894147873
step: 3200, Loss: 0.11374959349632263
step: 3300, Loss: 0.11366099119186401
step: 3400, Loss: 0.1936970204114914
step: 3500, Loss: 0.113914355635643
step: 3600, Loss: 0.11245045065879822
step: 3700, Loss: 0.11471837759017944
step: 3800, Loss: 0.11516407132148743
step: 3900, Loss: 0.11565234512090683
step: 4000, Loss: 0.1152317076921463
step: 4100, Loss: 0.11454996466636658
step: 4200, Loss: 0.11503632366657257
step: 4300, Loss: 0.11556481570005417
step: 4400, Loss: 0.11421497911214828
step: 4500, Loss: 0.11448010802268982
step: 4600, Loss: 0.1151154488325119
step: 4700, Loss: 0.11483441293239594
step: 4800, Loss: 0.1159215122461319
step: 4900, Loss: 0.11619702726602554
step: 5000, Loss: 1.0207741260528564
step: 5100, Loss: 0.1593196988105774
step: 5200, Loss: 0.15072135627269745
step: 5300, Loss: 0.2501131296157837
step: 5400, Loss: 0.1318942755460739
step: 5500, Loss: 0.12803635001182556
step: 5600, Loss: 0.12248294055461884
step: 5700, Loss: 0.12224404513835907
step: 5800, Loss: 0.13043120503425598
step: 5900, Loss: 0.12270335108041763
step: 6000, Loss: 0.12186165899038315
step: 6100, Loss: 0.12293561547994614
step: 6200, Loss: 0.11855004727840424
step: 6300, Loss: 0.11504356563091278
step: 6400, Loss: 0.11823320388793945
step: 6500, Loss: 0.11780980229377747
step: 6600, Loss: 0.12153176963329315
step: 6700, Loss: 0.11855785548686981
step: 6800, Loss: 0.11986424773931503
step: 6900, Loss: 0.11968949437141418
step: 7000, Loss: 0.11702656745910645
step: 7100, Loss: 0.11672832071781158
step: 7200, Loss: 0.20097480714321136
step: 7300, Loss: 0.12141062319278717
step: 7400, Loss: 0.11867307126522064
step: 7500, Loss: 0.11918079107999802
step: 7600, Loss: 0.11721816658973694
step: 7700, Loss: 0.11595207452774048
step: 7800, Loss: 0.11861631274223328
step: 7900, Loss: 0.11586515605449677
step: 8000, Loss: 0.11573134362697601
step: 8100, Loss: 0.11470763385295868
step: 8200, Loss: 0.11537668108940125
step: 8300, Loss: 0.11616486310958862
step: 8400, Loss: 0.1167234480381012
step: 8500, Loss: 0.11578962206840515
step: 8600, Loss: 0.11602412164211273
step: 8700, Loss: 0.11438985913991928
step: 8800, Loss: 0.11418987810611725
step: 8900, Loss: 0.11663462966680527
step: 9000, Loss: 0.11381317675113678
step: 9100, Loss: 0.19528499245643616
step: 9200, Loss: 0.11526674032211304
step: 9300, Loss: 0.11316529661417007
step: 9400, Loss: 0.11539620161056519
step: 9500, Loss: 0.11778566241264343
step: 9600, Loss: 0.11447034031152725
step: 9700, Loss: 0.11375825107097626
step: 9800, Loss: 0.11516334116458893
step: 9900, Loss: 0.11529696732759476
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9767441860465116
recall: 1.0
F_score: 0.988235294117647
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11678879708051682
step: 100, Loss: 0.1477089822292328
step: 200, Loss: 0.12499852478504181
step: 300, Loss: 0.12237229198217392
step: 400, Loss: 0.12077578902244568
step: 500, Loss: 0.1154259517788887
step: 600, Loss: 0.11752619594335556
step: 700, Loss: 0.11475887894630432
step: 800, Loss: 0.11641694605350494
step: 900, Loss: 0.11600946635007858
step: 1000, Loss: 0.11395227909088135
step: 1100, Loss: 0.11486154794692993
step: 1200, Loss: 0.11552854627370834
step: 1300, Loss: 0.11435315757989883
step: 1400, Loss: 0.11509819328784943
step: 1500, Loss: 0.19129745662212372
step: 1600, Loss: 0.114322230219841
step: 1700, Loss: 0.11454561352729797
step: 1800, Loss: 0.11438886821269989
step: 1900, Loss: 0.11509647965431213
step: 2000, Loss: 0.11438658833503723
step: 2100, Loss: 0.11432456970214844
step: 2200, Loss: 0.11478409916162491
step: 2300, Loss: 0.11527073383331299
step: 2400, Loss: 0.11378172039985657
step: 2500, Loss: 0.11381504684686661
step: 2600, Loss: 0.11539756506681442
step: 2700, Loss: 0.11470445990562439
step: 2800, Loss: 0.11395710706710815
step: 2900, Loss: 0.1140282079577446
step: 3000, Loss: 0.11361535638570786
step: 3100, Loss: 0.11298636347055435
step: 3200, Loss: 0.11296883225440979
step: 3300, Loss: 0.1135854721069336
step: 3400, Loss: 0.19314627349376678
step: 3500, Loss: 0.11679276078939438
step: 3600, Loss: 0.11390925943851471
step: 3700, Loss: 0.11401145905256271
step: 3800, Loss: 0.11462688446044922
step: 3900, Loss: 0.11570916324853897
step: 4000, Loss: 0.11435359716415405
step: 4100, Loss: 0.11310668289661407
step: 4200, Loss: 0.11735570430755615
step: 4300, Loss: 0.11403728276491165
step: 4400, Loss: 0.11847171932458878
step: 4500, Loss: 0.11583695560693741
step: 4600, Loss: 0.7857714295387268
step: 4700, Loss: 0.315438449382782
step: 4800, Loss: 0.14903733134269714
step: 4900, Loss: 0.14148268103599548
step: 5000, Loss: 0.13118654489517212
step: 5100, Loss: 0.13962554931640625
step: 5200, Loss: 0.1334144026041031
step: 5300, Loss: 0.22213876247406006
step: 5400, Loss: 0.12327707558870316
step: 5500, Loss: 0.12594139575958252
step: 5600, Loss: 0.12895654141902924
step: 5700, Loss: 0.12175901234149933
step: 5800, Loss: 0.11960720270872116
step: 5900, Loss: 0.12471327185630798
step: 6000, Loss: 0.12436880171298981
step: 6100, Loss: 0.1166907399892807
step: 6200, Loss: 0.12021337449550629
step: 6300, Loss: 0.11845147609710693
step: 4300, Loss: 0.11487182974815369
step: 4400, Loss: 0.11418811231851578
step: 4500, Loss: 0.11457502841949463
step: 4600, Loss: 0.11561007052659988
step: 4700, Loss: 0.15172246098518372
step: 4800, Loss: 0.13362473249435425
step: 4900, Loss: 0.13304956257343292
step: 5000, Loss: 0.12703248858451843
step: 5100, Loss: 0.1187838464975357
step: 5200, Loss: 0.12865084409713745
step: 5300, Loss: 0.11711329221725464
step: 5400, Loss: 0.12424187362194061
step: 5500, Loss: 0.11860918253660202
step: 5600, Loss: 0.11750376224517822
step: 5700, Loss: 0.115215003490448
step: 5800, Loss: 0.11882162094116211
step: 5900, Loss: 0.1184978187084198
step: 6000, Loss: 0.11694250255823135
step: 6100, Loss: 0.11549949645996094
step: 6200, Loss: 0.11643264442682266
step: 6300, Loss: 0.11557390540838242
step: 6400, Loss: 0.11664687842130661
step: 6500, Loss: 0.1155051738023758
step: 6600, Loss: 0.11635277420282364
step: 6700, Loss: 0.11421959847211838
step: 6800, Loss: 0.11534237116575241
step: 6900, Loss: 0.11425106972455978
step: 7000, Loss: 0.1168263852596283
step: 7100, Loss: 0.11370065808296204
step: 7200, Loss: 0.11581700295209885
step: 7300, Loss: 0.11556032299995422
step: 7400, Loss: 0.11433793604373932
step: 7500, Loss: 0.11392547190189362
step: 7600, Loss: 0.11604742705821991
step: 7700, Loss: 0.11503605544567108
step: 7800, Loss: 0.1150062084197998
step: 7900, Loss: 0.11311725527048111
step: 8000, Loss: 0.11547759920358658
step: 8100, Loss: 0.11411122232675552
step: 8200, Loss: 0.11587405949831009
step: 8300, Loss: 0.11365959793329239
step: 8400, Loss: 0.11372826993465424
step: 8500, Loss: 0.11298753321170807
step: 8600, Loss: 0.11491280794143677
step: 8700, Loss: 0.11413618922233582
step: 8800, Loss: 0.11509588360786438
step: 8900, Loss: 0.1140822172164917
step: 9000, Loss: 0.11464409530162811
step: 9100, Loss: 0.11436756700277328
step: 9200, Loss: 0.11347498744726181
step: 9300, Loss: 0.11342134326696396
step: 9400, Loss: 0.11379416286945343
step: 9500, Loss: 0.1144600510597229
step: 9600, Loss: 0.1143568605184555
step: 9700, Loss: 0.11401307582855225
step: 9800, Loss: 0.11361685395240784
step: 9900, Loss: 0.11405281722545624
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7368421052631579
recall: 0.9333333333333333
F_score: 0.8235294117647058
******fold 4******

Training... train_data length:281
step: 0, Loss: 3.13926362991333
step: 100, Loss: 0.11986956000328064
step: 200, Loss: 0.12178495526313782
step: 300, Loss: 0.11513775587081909
step: 400, Loss: 0.11631885170936584
step: 500, Loss: 0.11538279056549072
step: 600, Loss: 0.1153314858675003
step: 700, Loss: 0.11498548090457916
step: 800, Loss: 0.11444838345050812
step: 900, Loss: 0.11364174634218216
step: 1000, Loss: 0.11435104161500931
step: 1100, Loss: 0.1149064153432846
step: 1200, Loss: 0.1151043176651001
step: 1300, Loss: 0.11367379128932953
step: 1400, Loss: 0.11411172151565552
step: 1500, Loss: 0.11364450305700302
step: 1600, Loss: 0.11620315164327621
step: 1700, Loss: 0.11454124748706818
step: 1800, Loss: 0.11554968357086182
step: 1900, Loss: 0.11354708671569824
step: 2000, Loss: 0.11418309807777405
step: 2100, Loss: 0.11431426554918289
step: 2200, Loss: 0.11451336741447449
step: 2300, Loss: 0.11472779512405396
step: 2400, Loss: 0.11388079822063446
step: 2500, Loss: 0.11372778564691544
step: 2600, Loss: 0.1152815893292427
step: 2700, Loss: 0.11418363451957703
step: 2800, Loss: 0.11952664703130722
step: 2900, Loss: 0.11461074650287628
step: 3000, Loss: 0.1141132041811943
step: 3100, Loss: 0.11421981453895569
step: 3200, Loss: 0.11395494639873505
step: 3300, Loss: 0.11374027281999588
step: 3400, Loss: 0.11500969529151917
step: 3500, Loss: 0.11467781662940979
step: 3600, Loss: 0.1151108667254448
step: 3700, Loss: 0.11477205157279968
step: 3800, Loss: 0.11587792634963989
step: 3900, Loss: 0.11419513821601868
step: 4000, Loss: 0.11541638523340225
step: 4100, Loss: 0.1136254370212555
step: 4200, Loss: 0.11598807573318481
step: 4300, Loss: 0.11464546620845795
step: 4400, Loss: 0.11483754962682724
step: 4500, Loss: 0.11384972184896469
step: 4600, Loss: 0.11494201421737671
step: 4700, Loss: 0.11704810708761215
step: 4800, Loss: 0.11270470172166824
step: 4900, Loss: 0.11491358280181885
step: 5000, Loss: 0.11368044465780258
step: 5100, Loss: 0.1153557151556015
step: 5200, Loss: 0.11461935937404633
step: 5300, Loss: 0.11378175765275955
step: 5400, Loss: 0.1149669885635376
step: 5500, Loss: 0.11549317836761475
step: 5600, Loss: 0.11430039256811142
step: 5700, Loss: 0.11418253183364868
step: 5800, Loss: 0.11578976362943649
step: 5900, Loss: 0.11347576975822449
step: 6000, Loss: 0.11418943852186203
step: 6100, Loss: 0.6887578964233398
step: 6200, Loss: 0.13171666860580444
step: 6300, Loss: 0.12792694568634033
step: 6400, Loss: 0.1225464940071106
step: 6500, Loss: 0.11829940229654312
step: 6600, Loss: 0.11872775107622147
step: 6700, Loss: 0.11787097901105881
step: 6800, Loss: 0.12411221861839294
step: 6900, Loss: 0.11892146617174149
step: 7000, Loss: 0.1196088120341301
step: 7100, Loss: 0.11858122050762177
step: 7200, Loss: 0.11617827415466309
step: 7300, Loss: 0.11698120832443237
step: 7400, Loss: 0.1156298816204071
step: 7500, Loss: 0.11488146334886551
step: 7600, Loss: 0.11454189568758011
step: 7700, Loss: 0.11622442305088043
step: 7800, Loss: 0.11828045547008514
step: 7900, Loss: 0.1167626678943634
step: 8000, Loss: 0.1159246563911438
step: 8100, Loss: 0.11497306823730469
step: 8200, Loss: 0.11525849997997284
step: 8300, Loss: 0.11467872560024261
step: 8400, Loss: 0.11383482068777084
step: 8500, Loss: 0.11676909029483795
step: 8600, Loss: 0.11809995770454407
step: 8700, Loss: 0.11451538652181625
step: 8800, Loss: 0.11388608068227768
step: 8900, Loss: 0.11397452652454376
step: 9000, Loss: 0.11505553126335144
step: 9100, Loss: 0.11413056403398514
step: 9200, Loss: 0.11613738536834717
step: 9300, Loss: 0.11394480615854263
step: 9400, Loss: 0.11347566545009613
step: 9500, Loss: 0.11466365307569504
step: 9600, Loss: 0.11393345892429352
step: 9700, Loss: 0.11487174034118652
step: 9800, Loss: 0.11430607736110687
step: 9900, Loss: 0.11425144970417023
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.9333333333333333
recall: 0.9333333333333333
F_score: 0.9333333333333333
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.33779966831207275
step: 100, Loss: 0.11753319203853607
step: 200, Loss: 0.11476941406726837
step: 300, Loss: 0.11501573026180267
step: 400, Loss: 0.11809506267309189
step: 500, Loss: 0.11421296000480652
step: 600, Loss: 0.11464700847864151
step: 700, Loss: 0.1138462945818901
step: 800, Loss: 0.11738327145576477
step: 900, Loss: 0.11334599554538727
step: 1000, Loss: 0.11473773419857025
step: 1100, Loss: 0.11382033675909042
step: 1200, Loss: 0.11333593726158142
step: 1300, Loss: 0.11346662789583206
step: 1400, Loss: 0.11345440149307251
step: 1500, Loss: 0.11354789137840271
step: 1600, Loss: 0.11377716809511185
step: 1700, Loss: 0.11365301162004471
step: 1800, Loss: 0.1137523502111435
step: 1900, Loss: 0.11370948702096939
step: 2000, Loss: 0.11384124308824539
step: 2100, Loss: 0.11381655931472778
step: 2200, Loss: 0.11620939522981644
step: 2300, Loss: 0.11334946751594543
step: 2400, Loss: 0.1148081049323082
step: 2500, Loss: 0.11413545906543732
step: 2600, Loss: 0.11352740228176117
step: 2700, Loss: 0.11368086189031601
step: 2800, Loss: 0.11409764736890793
step: 2900, Loss: 0.11335711926221848
step: 3000, Loss: 0.11311345547437668
step: 3100, Loss: 0.11275883764028549
step: 3200, Loss: 0.11381234228610992
step: 3300, Loss: 0.11316525936126709
step: 3400, Loss: 0.11340760439634323
step: 3500, Loss: 0.11269816756248474
step: 3600, Loss: 0.11438042670488358
step: 3700, Loss: 0.11341067403554916
step: 3800, Loss: 0.11373777687549591
step: 3900, Loss: 0.11344718188047409
step: 4000, Loss: 0.11401306092739105
step: 4100, Loss: 0.11622299998998642
step: 4200, Loss: 0.11269883066415787
step: 4300, Loss: 0.11393231153488159
step: 4400, Loss: 0.11369054019451141
step: 4500, Loss: 0.11331132054328918
step: 4600, Loss: 0.1155739277601242
step: 4700, Loss: 0.11395919322967529
step: 6400, Loss: 0.12165451049804688
step: 6500, Loss: 0.11671648919582367
step: 6600, Loss: 0.11840914189815521
step: 6700, Loss: 0.11825419962406158
step: 6800, Loss: 0.11930784583091736
step: 6900, Loss: 0.12074092030525208
step: 7000, Loss: 0.11678294837474823
step: 7100, Loss: 0.12247420847415924
step: 7200, Loss: 0.20310764014720917
step: 7300, Loss: 0.11732602119445801
step: 7400, Loss: 0.11622492969036102
step: 7500, Loss: 0.11528301984071732
step: 7600, Loss: 0.11706645041704178
step: 7700, Loss: 0.11629338562488556
step: 7800, Loss: 0.11754421889781952
step: 7900, Loss: 0.11538086831569672
step: 8000, Loss: 0.11531651020050049
step: 8100, Loss: 0.11533905565738678
step: 8200, Loss: 0.11536374688148499
step: 8300, Loss: 0.1149955689907074
step: 8400, Loss: 0.11522366851568222
step: 8500, Loss: 0.11425743997097015
step: 8600, Loss: 0.11472485959529877
step: 8700, Loss: 0.11344604939222336
step: 8800, Loss: 0.11494274437427521
step: 8900, Loss: 0.11480574309825897
step: 9000, Loss: 0.11341454088687897
step: 9100, Loss: 0.19508381187915802
step: 9200, Loss: 0.11525660753250122
step: 9300, Loss: 0.11499619483947754
step: 9400, Loss: 0.11473303288221359
step: 9500, Loss: 0.11422565579414368
step: 9600, Loss: 0.11447971314191818
step: 9700, Loss: 0.11512235552072525
step: 9800, Loss: 0.11365745961666107
step: 9900, Loss: 0.1137775182723999
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.12478908896446228
step: 100, Loss: 0.12185823917388916
step: 200, Loss: 0.11487702280282974
step: 300, Loss: 0.11946073919534683
step: 400, Loss: 0.11470111459493637
step: 500, Loss: 0.11710032820701599
step: 600, Loss: 0.11473741382360458
step: 700, Loss: 0.11566172540187836
step: 800, Loss: 0.11459043622016907
step: 900, Loss: 0.11385110765695572
step: 1000, Loss: 0.11491493135690689
step: 1100, Loss: 0.11452794075012207
step: 1200, Loss: 0.11415614932775497
step: 1300, Loss: 0.11315016448497772
step: 1400, Loss: 0.11450110375881195
step: 1500, Loss: 0.19161362946033478
step: 1600, Loss: 0.11511096358299255
step: 1700, Loss: 0.11482514441013336
step: 1800, Loss: 0.11459420621395111
step: 1900, Loss: 0.11585228145122528
step: 2000, Loss: 0.11422736942768097
step: 2100, Loss: 0.11368447542190552
step: 2200, Loss: 0.11483415216207504
step: 2300, Loss: 0.11321556568145752
step: 2400, Loss: 0.11502633988857269
step: 2500, Loss: 0.11344267427921295
step: 2600, Loss: 0.1161762923002243
step: 2700, Loss: 0.11275238543748856
step: 2800, Loss: 0.1161666214466095
step: 2900, Loss: 0.11434906721115112
step: 3000, Loss: 0.1156318411231041
step: 3100, Loss: 0.11433936655521393
step: 3200, Loss: 0.11535622179508209
step: 3300, Loss: 0.11389113962650299
step: 3400, Loss: 0.1914583146572113
step: 3500, Loss: 0.115654855966568
step: 3600, Loss: 0.11537045240402222
step: 3700, Loss: 1.7957221269607544
step: 3800, Loss: 0.16477499902248383
step: 3900, Loss: 0.1412423551082611
step: 4000, Loss: 0.1358461230993271
step: 4100, Loss: 0.12263698130846024
step: 4200, Loss: 0.1283305436372757
step: 4300, Loss: 0.12164195626974106
step: 4400, Loss: 0.11980844289064407
step: 4500, Loss: 0.12059192359447479
step: 4600, Loss: 0.1226746141910553
step: 4700, Loss: 0.12095512449741364
step: 4800, Loss: 0.12142064422369003
step: 4900, Loss: 0.11966970562934875
step: 5000, Loss: 0.12072277069091797
step: 5100, Loss: 0.11754438281059265
step: 5200, Loss: 0.11635599285364151
step: 5300, Loss: 0.2008970081806183
step: 5400, Loss: 0.11839780956506729
step: 5500, Loss: 0.11620144546031952
step: 5600, Loss: 0.11904023587703705
step: 5700, Loss: 0.11675603687763214
step: 5800, Loss: 0.11675690859556198
step: 5900, Loss: 0.11560152471065521
step: 6000, Loss: 0.11568877100944519
step: 6100, Loss: 0.1169164627790451
step: 6200, Loss: 0.11481817811727524
step: 6300, Loss: 0.11491972208023071
step: 6400, Loss: 0.1152309849858284
step: 6500, Loss: 0.11674412339925766
step: 6600, Loss: 0.11562031507492065
step: 6700, Loss: 0.11399326473474503
step: 6800, Loss: 0.11416392773389816
step: 6900, Loss: 0.11592909693717957
step: 7000, Loss: 0.11582189798355103
step: 7100, Loss: 0.11438412964344025
step: 7200, Loss: 0.19550397992134094
step: 7300, Loss: 0.11598584800958633
step: 7400, Loss: 0.11461207270622253
step: 7500, Loss: 0.11458902806043625
step: 7600, Loss: 0.11626613885164261
step: 7700, Loss: 0.11417293548583984
step: 7800, Loss: 0.1139814704656601
step: 7900, Loss: 0.11691363155841827
step: 8000, Loss: 0.11369165778160095
step: 8100, Loss: 0.11499779671430588
step: 8200, Loss: 0.11513136327266693
step: 8300, Loss: 0.11435368657112122
step: 8400, Loss: 0.11505040526390076
step: 8500, Loss: 0.11361779272556305
step: 8600, Loss: 0.11445926874876022
step: 8700, Loss: 0.11390360444784164
step: 8800, Loss: 0.114580899477005
step: 8900, Loss: 0.11336993426084518
step: 9000, Loss: 0.11348842084407806
step: 9100, Loss: 0.19066816568374634
step: 9200, Loss: 0.11427736282348633
step: 9300, Loss: 0.11459719389677048
step: 9400, Loss: 0.1131349429488182
step: 9500, Loss: 0.1136929988861084
step: 9600, Loss: 0.11387913674116135
step: 9700, Loss: 0.11472944915294647
step: 9800, Loss: 0.11546491831541061
step: 9900, Loss: 0.11331039667129517
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9230769230769231
recall: 1.0
F_score: 0.9600000000000001
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.12508587539196014
step: 100, Loss: 0.12305063009262085
step: 200, Loss: 0.11915023624897003
step: 300, Loss: 0.12313413619995117
step: 400, Loss: 0.1147426962852478
step: 500, Loss: 0.11421885341405869
step: 600, Loss: 0.11453290283679962
step: 700, Loss: 0.11784634739160538
step: 800, Loss: 0.11424139142036438
step: 900, Loss: 0.11407390236854553
step: 1000, Loss: 0.11550174653530121
step: 1100, Loss: 0.1142716184258461
step: 1200, Loss: 0.11435168981552124
step: 1300, Loss: 0.11577451229095459
step: 1400, Loss: 0.11427616328001022
step: 1500, Loss: 0.1940806359052658
step: 1600, Loss: 0.11379362642765045
step: 1700, Loss: 0.11499562859535217
step: 1800, Loss: 0.11592254042625427
step: 1900, Loss: 0.11354833096265793
step: 2000, Loss: 0.11440468579530716
step: 2100, Loss: 0.11642598360776901
step: 2200, Loss: 0.11425848305225372
step: 2300, Loss: 0.11406836658716202
step: 2400, Loss: 0.11483973264694214
step: 2500, Loss: 0.11343049257993698
step: 2600, Loss: 0.11517392098903656
step: 2700, Loss: 0.11546828597784042
step: 2800, Loss: 0.11325623840093613
step: 2900, Loss: 0.11484833061695099
step: 3000, Loss: 0.114870086312294
step: 3100, Loss: 0.11411897093057632
step: 3200, Loss: 0.11516588926315308
step: 3300, Loss: 0.11485747247934341
step: 3400, Loss: 0.19998794794082642
step: 3500, Loss: 0.11555822193622589
step: 3600, Loss: 0.11527347564697266
step: 3700, Loss: 0.11378329247236252
step: 3800, Loss: 0.11694421619176865
step: 3900, Loss: 0.7040701508522034
step: 4000, Loss: 0.15365184843540192
step: 4100, Loss: 0.1279931366443634
step: 4200, Loss: 0.12962082028388977
step: 4300, Loss: 0.1256496161222458
step: 4400, Loss: 0.123063825070858
step: 4500, Loss: 0.12685084342956543
step: 4600, Loss: 0.12333731353282928
step: 4700, Loss: 0.12041664123535156
step: 4800, Loss: 0.1213487982749939
step: 4900, Loss: 0.12058905512094498
step: 5000, Loss: 0.12081678956747055
step: 5100, Loss: 0.11653687804937363
step: 5200, Loss: 0.11729161441326141
step: 5300, Loss: 0.1984388530254364
step: 5400, Loss: 0.12247727066278458
step: 5500, Loss: 0.11589400470256805
step: 5600, Loss: 0.11990515142679214
step: 5700, Loss: 0.11551457643508911
step: 5800, Loss: 0.11797663569450378
step: 5900, Loss: 0.11537443101406097
step: 6000, Loss: 0.11568903177976608
step: 6100, Loss: 0.11634259670972824
step: 6200, Loss: 0.11507275700569153
step: 6300, Loss: 0.11686716228723526
step: 6400, Loss: 0.11628682911396027
step: 6500, Loss: 0.11461450159549713
step: 6600, Loss: 0.11528804153203964
step: 6700, Loss: 0.11564629524946213
step: 6800, Loss: 0.11465110629796982
step: 6900, Loss: 0.11466442048549652
step: 7000, Loss: 0.113136425614357
step: 4800, Loss: 0.11389030516147614
step: 4900, Loss: 0.11269006133079529
step: 5000, Loss: 0.11394036561250687
step: 5100, Loss: 0.11391344666481018
step: 5200, Loss: 0.11467387527227402
step: 5300, Loss: 0.11340344697237015
step: 5400, Loss: 0.11457450687885284
step: 5500, Loss: 1.4933063983917236
step: 5600, Loss: 0.16336479783058167
step: 5700, Loss: 0.14000873267650604
step: 5800, Loss: 0.13732695579528809
step: 5900, Loss: 0.12432172894477844
step: 6000, Loss: 0.13368839025497437
step: 6100, Loss: 0.12120593339204788
step: 6200, Loss: 0.12366635352373123
step: 6300, Loss: 0.12126030027866364
step: 6400, Loss: 0.12240678071975708
step: 6500, Loss: 0.11672276258468628
step: 6600, Loss: 0.12218596041202545
step: 6700, Loss: 0.11876485496759415
step: 6800, Loss: 0.11879505962133408
step: 6900, Loss: 0.11769382655620575
step: 7000, Loss: 0.11792415380477905
step: 7100, Loss: 0.11671657115221024
step: 7200, Loss: 0.11568880081176758
step: 7300, Loss: 0.11548038572072983
step: 7400, Loss: 0.11937277019023895
step: 7500, Loss: 0.11607279628515244
step: 7600, Loss: 0.121111199259758
step: 7700, Loss: 0.11556783318519592
step: 7800, Loss: 0.11652608215808868
step: 7900, Loss: 0.11845941096544266
step: 8000, Loss: 0.11621075123548508
step: 8100, Loss: 0.1156042143702507
step: 8200, Loss: 0.12249142676591873
step: 8300, Loss: 0.11389806866645813
step: 8400, Loss: 0.11349596828222275
step: 8500, Loss: 0.11508049815893173
step: 8600, Loss: 0.11448748409748077
step: 8700, Loss: 0.11532716453075409
step: 8800, Loss: 0.11338162422180176
step: 8900, Loss: 0.11353320628404617
step: 9000, Loss: 0.11586307734251022
step: 9100, Loss: 0.11715302616357803
step: 9200, Loss: 0.11575842648744583
step: 9300, Loss: 0.11521513760089874
step: 9400, Loss: 0.11646883934736252
step: 9500, Loss: 0.1136179193854332
step: 9600, Loss: 0.11359116435050964
step: 9700, Loss: 0.11457528173923492
step: 9800, Loss: 0.11425391584634781
step: 9900, Loss: 0.11491658538579941
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9473684210526315
recall: 1.0
F_score: 0.972972972972973
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.21594834327697754
step: 100, Loss: 0.11621513962745667
step: 200, Loss: 0.11837156116962433
step: 300, Loss: 0.11504000425338745
step: 400, Loss: 0.11333145946264267
step: 500, Loss: 0.11595390737056732
step: 600, Loss: 0.11416108161211014
step: 700, Loss: 0.1133938804268837
step: 800, Loss: 0.11363551020622253
step: 900, Loss: 0.11439606547355652
step: 1000, Loss: 0.11434408277273178
step: 1100, Loss: 0.11351758241653442
step: 1200, Loss: 0.1135374903678894
step: 1300, Loss: 0.11561953276395798
step: 1400, Loss: 0.1158623993396759
step: 1500, Loss: 0.11340080946683884
step: 1600, Loss: 0.11490920186042786
step: 1700, Loss: 0.11482129991054535
step: 1800, Loss: 0.11643964052200317
step: 1900, Loss: 0.11501061916351318
step: 2000, Loss: 0.11416494101285934
step: 2100, Loss: 0.11456948518753052
step: 2200, Loss: 0.11381086707115173
step: 2300, Loss: 0.11395205557346344
step: 2400, Loss: 0.11504524201154709
step: 2500, Loss: 0.11447257548570633
step: 2600, Loss: 0.11622003465890884
step: 2700, Loss: 0.11610700190067291
step: 2800, Loss: 0.11408066749572754
step: 2900, Loss: 0.11396000534296036
step: 3000, Loss: 5.5201640129089355
step: 3100, Loss: 0.17080844938755035
step: 3200, Loss: 0.12945052981376648
step: 3300, Loss: 0.12742450833320618
step: 3400, Loss: 0.12740710377693176
step: 3500, Loss: 0.12095372378826141
step: 3600, Loss: 0.12073132395744324
step: 3700, Loss: 0.12045730650424957
step: 3800, Loss: 0.12358348071575165
step: 3900, Loss: 0.1155899167060852
step: 4000, Loss: 0.11598401516675949
step: 4100, Loss: 0.11703848838806152
step: 4200, Loss: 0.11706866323947906
step: 4300, Loss: 0.11785951256752014
step: 4400, Loss: 0.11767313629388809
step: 4500, Loss: 0.11775372922420502
step: 4600, Loss: 0.11526471376419067
step: 4700, Loss: 0.11518168449401855
step: 4800, Loss: 0.1163189560174942
step: 4900, Loss: 0.11505217105150223
step: 5000, Loss: 0.11440490931272507
step: 5100, Loss: 0.11526481807231903
step: 5200, Loss: 0.11539215594530106
step: 5300, Loss: 0.11567525565624237
step: 5400, Loss: 0.11367536336183548
step: 5500, Loss: 0.11419481039047241
step: 5600, Loss: 0.116676926612854
step: 5700, Loss: 0.11434329301118851
step: 5800, Loss: 0.11569812148809433
step: 5900, Loss: 0.11359718441963196
step: 6000, Loss: 0.11527455598115921
step: 6100, Loss: 0.11393244564533234
step: 6200, Loss: 0.1154707744717598
step: 6300, Loss: 0.11414545029401779
step: 6400, Loss: 0.11602263152599335
step: 6500, Loss: 0.11483660340309143
step: 6600, Loss: 0.11562768369913101
step: 6700, Loss: 0.11333074420690536
step: 6800, Loss: 0.11385853588581085
step: 6900, Loss: 0.11654279381036758
step: 7000, Loss: 0.11548119783401489
step: 7100, Loss: 0.11349686235189438
step: 7200, Loss: 0.1137438639998436
step: 7300, Loss: 0.11315201222896576
step: 7400, Loss: 0.11511790007352829
step: 7500, Loss: 0.1137755960226059
step: 7600, Loss: 0.11317018419504166
step: 7700, Loss: 0.11364196240901947
step: 7800, Loss: 0.1160808652639389
step: 7900, Loss: 0.11350955069065094
step: 8000, Loss: 0.11310147494077682
step: 8100, Loss: 0.11330251395702362
step: 8200, Loss: 0.11396346241235733
step: 8300, Loss: 0.11474980413913727
step: 8400, Loss: 0.11394052952528
step: 8500, Loss: 0.11374012380838394
step: 8600, Loss: 0.12019101530313492
step: 8700, Loss: 0.1127883717417717
step: 8800, Loss: 0.11529885977506638
step: 8900, Loss: 0.11386916786432266
step: 9000, Loss: 0.11569824069738388
step: 9100, Loss: 0.11376094073057175
step: 9200, Loss: 0.11394812911748886
step: 9300, Loss: 0.11419813334941864
step: 9400, Loss: 0.1134410947561264
step: 9500, Loss: 0.11458654701709747
step: 9600, Loss: 0.11281818151473999
step: 9700, Loss: 0.11373726278543472
step: 9800, Loss: 0.11467530578374863
step: 9900, Loss: 0.11385857313871384
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 1.0
recall: 0.8888888888888888
F_score: 0.9411764705882353
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.20147013664245605
step: 100, Loss: 0.11552712321281433
step: 200, Loss: 0.115521140396595
step: 300, Loss: 0.11388536542654037
step: 400, Loss: 0.1140255406498909
step: 500, Loss: 0.11457324773073196
step: 600, Loss: 0.11845538765192032
step: 700, Loss: 0.11537953466176987
step: 800, Loss: 0.1133975237607956
step: 900, Loss: 0.11313706636428833
step: 1000, Loss: 0.11366543173789978
step: 1100, Loss: 0.1129995584487915
step: 1200, Loss: 0.11349096894264221
step: 1300, Loss: 0.11425825953483582
step: 1400, Loss: 0.11437204480171204
step: 1500, Loss: 0.11374165117740631
step: 1600, Loss: 0.11394107341766357
step: 1700, Loss: 0.11476603895425797
step: 1800, Loss: 0.1154005378484726
step: 1900, Loss: 0.11519449949264526
step: 2000, Loss: 0.11549771577119827
step: 2100, Loss: 0.11281342804431915
step: 2200, Loss: 0.1145692840218544
step: 2300, Loss: 0.11355945467948914
step: 2400, Loss: 0.11475595831871033
step: 2500, Loss: 0.11465807259082794
step: 2600, Loss: 0.11432894319295883
step: 2700, Loss: 0.11413069814443588
step: 2800, Loss: 0.11341121792793274
step: 2900, Loss: 0.1132882833480835
step: 3000, Loss: 0.1142401322722435
step: 3100, Loss: 0.11413712054491043
step: 3200, Loss: 0.11391305923461914
step: 3300, Loss: 0.11393296718597412
step: 3400, Loss: 0.11439724266529083
step: 3500, Loss: 0.16543039679527283
step: 3600, Loss: 0.12724870443344116
step: 3700, Loss: 0.12396204471588135
step: 3800, Loss: 0.12037818878889084
step: 3900, Loss: 0.12139852344989777
step: 4000, Loss: 0.1212955191731453
step: 4100, Loss: 0.12172447144985199
step: 4200, Loss: 0.1167011559009552
step: 4300, Loss: 0.1218797117471695
step: 4400, Loss: 0.11667682230472565
step: 4500, Loss: 0.11833430826663971
step: 4600, Loss: 0.12022919952869415
step: 4700, Loss: 0.11643370240926743
step: 4800, Loss: 0.11796094477176666
step: 4900, Loss: 0.11726467311382294
step: 5000, Loss: 0.11733639985322952
step: 5100, Loss: 0.1146620661020279
step: 5200, Loss: 0.11700145155191422
step: 7100, Loss: 0.11538855731487274
step: 7200, Loss: 0.1951276659965515
step: 7300, Loss: 0.11560402065515518
step: 7400, Loss: 0.11359038949012756
step: 7500, Loss: 0.11410268396139145
step: 7600, Loss: 0.11659005284309387
step: 7700, Loss: 0.11397735029459
step: 7800, Loss: 0.11372411251068115
step: 7900, Loss: 0.11328412592411041
step: 8000, Loss: 0.1133342757821083
step: 8100, Loss: 0.1139170229434967
step: 8200, Loss: 0.11562652885913849
step: 8300, Loss: 0.11324859410524368
step: 8400, Loss: 0.11695701628923416
step: 8500, Loss: 0.11639119684696198
step: 8600, Loss: 0.1136222779750824
step: 8700, Loss: 0.11416902393102646
step: 8800, Loss: 0.1137595847249031
step: 8900, Loss: 0.11367680132389069
step: 9000, Loss: 0.1135312020778656
step: 9100, Loss: 0.19243699312210083
step: 9200, Loss: 0.11324084550142288
step: 9300, Loss: 0.11454208195209503
step: 9400, Loss: 0.11414431035518646
step: 9500, Loss: 0.11432501673698425
step: 9600, Loss: 0.11396092176437378
step: 9700, Loss: 0.11570313572883606
step: 9800, Loss: 0.11574876308441162
step: 9900, Loss: 0.11432753503322601
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.9714285714285714
F_score: 0.9855072463768115
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.1264340579509735
step: 100, Loss: 0.12374009191989899
step: 200, Loss: 0.11692111194133759
step: 300, Loss: 0.12000833451747894
step: 400, Loss: 0.11496270447969437
step: 500, Loss: 0.11517278850078583
step: 600, Loss: 0.11407988518476486
step: 700, Loss: 0.11430544406175613
step: 800, Loss: 0.11479750275611877
step: 900, Loss: 0.1155293732881546
step: 1000, Loss: 0.11491107195615768
step: 1100, Loss: 0.11488945037126541
step: 1200, Loss: 0.11305750906467438
step: 1300, Loss: 0.11536092311143875
step: 1400, Loss: 0.11439476162195206
step: 1500, Loss: 0.19226159155368805
step: 1600, Loss: 0.11312276124954224
step: 1700, Loss: 0.11907528340816498
step: 1800, Loss: 0.11444756388664246
step: 1900, Loss: 0.11382871866226196
step: 2000, Loss: 0.11428079754114151
step: 2100, Loss: 0.11375248432159424
step: 2200, Loss: 0.11615817248821259
step: 2300, Loss: 0.11479634046554565
step: 2400, Loss: 0.11591105908155441
step: 2500, Loss: 0.11355667561292648
step: 2600, Loss: 0.11535749584436417
step: 2700, Loss: 0.11446782946586609
step: 2800, Loss: 0.11449526995420456
step: 2900, Loss: 0.11470088362693787
step: 3000, Loss: 0.11690522730350494
step: 3100, Loss: 0.11592204868793488
step: 3200, Loss: 0.11734537780284882
step: 3300, Loss: 1.057800531387329
step: 3400, Loss: 0.26679182052612305
step: 3500, Loss: 0.1450832486152649
step: 3600, Loss: 0.13138656318187714
step: 3700, Loss: 0.11888755112886429
step: 3800, Loss: 0.1292472630739212
step: 3900, Loss: 0.12602347135543823
step: 4000, Loss: 0.12230093777179718
step: 4100, Loss: 0.12109315395355225
step: 4200, Loss: 0.12335356324911118
step: 4300, Loss: 0.11770913004875183
step: 4400, Loss: 0.1172180324792862
step: 4500, Loss: 0.11927393823862076
step: 4600, Loss: 0.12071666866540909
step: 4700, Loss: 0.1177041158080101
step: 4800, Loss: 0.11702091991901398
step: 4900, Loss: 0.11595858633518219
step: 5000, Loss: 0.11774928867816925
step: 5100, Loss: 0.11637944728136063
step: 5200, Loss: 0.11912707984447479
step: 5300, Loss: 0.19633324444293976
step: 5400, Loss: 0.11760090291500092
step: 5500, Loss: 0.11569111049175262
step: 5600, Loss: 0.11484216153621674
step: 5700, Loss: 0.11608590185642242
step: 5800, Loss: 0.11721037328243256
step: 5900, Loss: 0.11398545652627945
step: 6000, Loss: 0.11489526927471161
step: 6100, Loss: 0.1163422018289566
step: 6200, Loss: 0.11535023897886276
step: 6300, Loss: 0.11391136795282364
step: 6400, Loss: 0.11315834522247314
step: 6500, Loss: 0.1151321604847908
step: 6600, Loss: 0.11502689868211746
step: 6700, Loss: 0.11434759199619293
step: 6800, Loss: 0.1133730411529541
step: 6900, Loss: 0.11397764831781387
step: 7000, Loss: 0.11444037407636642
step: 7100, Loss: 0.11414052546024323
step: 7200, Loss: 0.19341951608657837
step: 7300, Loss: 0.11414936184883118
step: 7400, Loss: 0.11544422060251236
step: 7500, Loss: 0.11409454047679901
step: 7600, Loss: 0.1143549233675003
step: 7700, Loss: 0.11419727653265
step: 7800, Loss: 0.11405384540557861
step: 7900, Loss: 0.11346606910228729
step: 8000, Loss: 0.11471228301525116
step: 8100, Loss: 0.11277822405099869
step: 8200, Loss: 0.11490606516599655
step: 8300, Loss: 0.11435452848672867
step: 8400, Loss: 0.11375022679567337
step: 8500, Loss: 0.11396920680999756
step: 8600, Loss: 0.11398512125015259
step: 8700, Loss: 0.1135740578174591
step: 8800, Loss: 0.11490995436906815
step: 8900, Loss: 0.1128511130809784
step: 9000, Loss: 0.11391817778348923
step: 9100, Loss: 0.19095061719417572
step: 9200, Loss: 0.114726722240448
step: 9300, Loss: 0.11383213847875595
step: 9400, Loss: 0.11440842598676682
step: 9500, Loss: 0.11449030041694641
step: 9600, Loss: 0.11463451385498047
step: 9700, Loss: 0.11448405683040619
step: 9800, Loss: 0.1151048094034195
step: 9900, Loss: 0.1138230562210083
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9743589743589743
recall: 1.0
F_score: 0.9870129870129869
subject 11 Avgacc: 0.9458333333333332 Avgfscore: 0.9528574981593492 
 Max acc:1.0, Max f score:1.0
******** mix subject_12 ********

[418, 342]
******fold 1******

Training... train_data length:684
step: 0, Loss: 48.96110534667969
step: 100, Loss: 11.524831771850586
step: 200, Loss: 1.937959909439087
step: 300, Loss: 0.6643435955047607
step: 400, Loss: 1.9852535724639893
step: 500, Loss: 0.21453769505023956
step: 600, Loss: 0.17210853099822998
step: 700, Loss: 0.1667742133140564
step: 800, Loss: 0.14357209205627441
step: 900, Loss: 0.14083673059940338
step: 1000, Loss: 0.12664933502674103
step: 1100, Loss: 0.13139945268630981
step: 1200, Loss: 0.12989270687103271
step: 1300, Loss: 0.12853381037712097
step: 1400, Loss: 0.12351298332214355
step: 1500, Loss: 0.21702054142951965
step: 1600, Loss: 0.12668579816818237
step: 1700, Loss: 0.12422623485326767
step: 1800, Loss: 0.12354077398777008
step: 1900, Loss: 0.12444436550140381
step: 2000, Loss: 0.12288085371255875
step: 2100, Loss: 0.12499485909938812
step: 2200, Loss: 0.12375172972679138
step: 2300, Loss: 0.15906989574432373
step: 2400, Loss: 0.13523666560649872
step: 2500, Loss: 0.11710874736309052
step: 2600, Loss: 0.12006901949644089
step: 2700, Loss: 0.11943119764328003
step: 2800, Loss: 0.12032703310251236
step: 2900, Loss: 0.11850665509700775
step: 3000, Loss: 0.11802640557289124
step: 3100, Loss: 0.11648695170879364
step: 3200, Loss: 0.11694702506065369
step: 3300, Loss: 0.11692306399345398
step: 3400, Loss: 0.20389503240585327
step: 3500, Loss: 0.11811991035938263
step: 3600, Loss: 0.11801384389400482
step: 3700, Loss: 0.11615964770317078
step: 3800, Loss: 0.11675402522087097
step: 3900, Loss: 0.11988198012113571
step: 4000, Loss: 0.1196652427315712
step: 4100, Loss: 0.11753373593091965
step: 4200, Loss: 0.11526712775230408
step: 4300, Loss: 0.11678694188594818
step: 4400, Loss: 0.11571678519248962
step: 4500, Loss: 0.11834347248077393
step: 4600, Loss: 0.11647898703813553
step: 4700, Loss: 0.11591563373804092
step: 4800, Loss: 0.11482420563697815
step: 4900, Loss: 0.11543511599302292
step: 5000, Loss: 0.11607790738344193
step: 5100, Loss: 0.11579087376594543
step: 5200, Loss: 0.11730234324932098
step: 5300, Loss: 0.19530104100704193
step: 5400, Loss: 0.11711009591817856
step: 5500, Loss: 0.11515682190656662
step: 5600, Loss: 0.11397331207990646
step: 5700, Loss: 0.11567245423793793
step: 5800, Loss: 0.1150885596871376
step: 5900, Loss: 0.11587405204772949
step: 6000, Loss: 0.131616473197937
step: 6100, Loss: 1.2222709655761719
step: 6200, Loss: 2.5770905017852783
step: 6300, Loss: 0.16357366740703583
step: 6400, Loss: 0.15747474133968353
step: 6500, Loss: 0.1495291143655777
step: 6600, Loss: 0.1338326632976532
step: 6700, Loss: 0.13071870803833008
step: 6800, Loss: 0.13012725114822388
step: 6900, Loss: 0.13170970976352692
step: 7000, Loss: 0.12423190474510193
step: 7100, Loss: 0.13296934962272644
step: 7200, Loss: 0.20820370316505432
step: 5300, Loss: 0.11710047721862793
step: 5400, Loss: 0.11625628173351288
step: 5500, Loss: 0.11367412656545639
step: 5600, Loss: 0.12324722111225128
step: 5700, Loss: 0.11427488923072815
step: 5800, Loss: 0.11384636163711548
step: 5900, Loss: 0.11673080921173096
step: 6000, Loss: 0.11401856690645218
step: 6100, Loss: 0.11455772072076797
step: 6200, Loss: 0.11635208129882812
step: 6300, Loss: 0.11490735411643982
step: 6400, Loss: 0.11401668190956116
step: 6500, Loss: 0.11334627121686935
step: 6600, Loss: 0.11976373940706253
step: 6700, Loss: 0.11400256305932999
step: 6800, Loss: 0.11249356716871262
step: 6900, Loss: 0.1136956587433815
step: 7000, Loss: 0.11461902409791946
step: 7100, Loss: 0.11646867543458939
step: 7200, Loss: 0.11508417874574661
step: 7300, Loss: 0.11422260850667953
step: 7400, Loss: 0.1153089702129364
step: 7500, Loss: 0.11315439641475677
step: 7600, Loss: 0.11545369774103165
step: 7700, Loss: 0.11509683728218079
step: 7800, Loss: 0.1126236692070961
step: 7900, Loss: 0.1140279620885849
step: 8000, Loss: 0.11414383351802826
step: 8100, Loss: 0.11380982398986816
step: 8200, Loss: 0.11500914394855499
step: 8300, Loss: 0.11294464766979218
step: 8400, Loss: 0.11515498161315918
step: 8500, Loss: 0.11307699978351593
step: 8600, Loss: 0.11389897763729095
step: 8700, Loss: 0.11395829170942307
step: 8800, Loss: 0.1146114394068718
step: 8900, Loss: 0.11497627943754196
step: 9000, Loss: 0.11326348781585693
step: 9100, Loss: 0.11413408815860748
step: 9200, Loss: 0.11402040719985962
step: 9300, Loss: 0.11326104402542114
step: 9400, Loss: 0.11385458707809448
step: 9500, Loss: 0.11340688169002533
step: 9600, Loss: 0.11957824230194092
step: 9700, Loss: 0.1146450862288475
step: 9800, Loss: 0.11329144984483719
step: 9900, Loss: 0.11337678134441376
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8235294117647058
recall: 0.8235294117647058
F_score: 0.8235294117647058
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.20175954699516296
step: 100, Loss: 0.11537507176399231
step: 200, Loss: 0.11517339199781418
step: 300, Loss: 0.11520764231681824
step: 400, Loss: 0.11602572351694107
step: 500, Loss: 0.1151459664106369
step: 600, Loss: 0.1201869398355484
step: 700, Loss: 0.1137540340423584
step: 800, Loss: 0.11351978033781052
step: 900, Loss: 0.11514516174793243
step: 1000, Loss: 0.1139225959777832
step: 1100, Loss: 0.1139877587556839
step: 1200, Loss: 0.11455032229423523
step: 1300, Loss: 0.11377618461847305
step: 1400, Loss: 0.11312916874885559
step: 1500, Loss: 0.11336787790060043
step: 1600, Loss: 0.112860307097435
step: 1700, Loss: 0.11431968212127686
step: 1800, Loss: 0.11460990458726883
step: 1900, Loss: 0.11410072445869446
step: 2000, Loss: 0.11385820806026459
step: 2100, Loss: 0.11362340301275253
step: 2200, Loss: 0.1150485947728157
step: 2300, Loss: 0.11359527707099915
step: 2400, Loss: 0.11327225714921951
step: 2500, Loss: 0.11402657628059387
step: 2600, Loss: 0.11294880509376526
step: 2700, Loss: 0.1134321391582489
step: 2800, Loss: 0.11431412398815155
step: 2900, Loss: 0.11392094194889069
step: 3000, Loss: 0.11307933926582336
step: 3100, Loss: 0.11422983556985855
step: 3200, Loss: 1.3590699434280396
step: 3300, Loss: 0.12714804708957672
step: 3400, Loss: 0.12135301530361176
step: 3500, Loss: 0.11749318242073059
step: 3600, Loss: 0.1232871264219284
step: 3700, Loss: 0.1165022999048233
step: 3800, Loss: 0.11698196828365326
step: 3900, Loss: 0.11511138081550598
step: 4000, Loss: 0.11687806248664856
step: 4100, Loss: 0.11546730995178223
step: 4200, Loss: 0.11847764253616333
step: 4300, Loss: 0.1150193065404892
step: 4400, Loss: 0.11639145761728287
step: 4500, Loss: 0.11355709284543991
step: 4600, Loss: 0.1161990836262703
step: 4700, Loss: 0.11555583029985428
step: 4800, Loss: 0.11511380970478058
step: 4900, Loss: 0.11470111459493637
step: 5000, Loss: 0.1142716035246849
step: 5100, Loss: 0.11325111985206604
step: 5200, Loss: 0.11653447151184082
step: 5300, Loss: 0.11413361132144928
step: 5400, Loss: 0.11576041579246521
step: 5500, Loss: 0.11559226363897324
step: 5600, Loss: 0.11575824022293091
step: 5700, Loss: 0.11420450359582901
step: 5800, Loss: 0.1152563989162445
step: 5900, Loss: 0.11561530828475952
step: 6000, Loss: 0.11529584228992462
step: 6100, Loss: 0.11399099230766296
step: 6200, Loss: 0.11349689960479736
step: 6300, Loss: 0.11484845727682114
step: 6400, Loss: 0.11414352059364319
step: 6500, Loss: 0.11618451029062271
step: 6600, Loss: 0.11707593500614166
step: 6700, Loss: 0.11611196398735046
step: 6800, Loss: 0.11484415084123611
step: 6900, Loss: 0.11287619918584824
step: 7000, Loss: 0.11331378668546677
step: 7100, Loss: 0.114143006503582
step: 7200, Loss: 0.11422936618328094
step: 7300, Loss: 0.11388162523508072
step: 7400, Loss: 0.11373724043369293
step: 7500, Loss: 0.11427317559719086
step: 7600, Loss: 0.114485964179039
step: 7700, Loss: 0.11346235871315002
step: 7800, Loss: 0.11434976011514664
step: 7900, Loss: 0.1135469377040863
step: 8000, Loss: 0.11389797925949097
step: 8100, Loss: 0.11483335494995117
step: 8200, Loss: 0.11495797336101532
step: 8300, Loss: 0.11360466480255127
step: 8400, Loss: 0.11654312908649445
step: 8500, Loss: 0.11410532891750336
step: 8600, Loss: 0.11423248052597046
step: 8700, Loss: 0.11434479057788849
step: 8800, Loss: 0.11434528976678848
step: 8900, Loss: 0.11360926926136017
step: 9000, Loss: 0.11384515464305878
step: 9100, Loss: 0.11362789571285248
step: 9200, Loss: 0.11424529552459717
step: 9300, Loss: 0.1132226288318634
step: 9400, Loss: 0.11497241258621216
step: 9500, Loss: 0.11336951702833176
step: 9600, Loss: 0.11499661952257156
step: 9700, Loss: 0.11303673684597015
step: 9800, Loss: 0.11416439712047577
step: 9900, Loss: 0.11335635185241699
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.17912623286247253
step: 100, Loss: 0.11422061920166016
step: 200, Loss: 0.1139746829867363
step: 300, Loss: 0.11376223713159561
step: 400, Loss: 0.11571557819843292
step: 500, Loss: 0.11395228654146194
step: 600, Loss: 0.11426978558301926
step: 700, Loss: 1.7403584718704224
step: 800, Loss: 0.12399265170097351
step: 900, Loss: 0.11595330387353897
step: 1000, Loss: 0.1162518858909607
step: 1100, Loss: 0.11574944853782654
step: 1200, Loss: 0.11922881752252579
step: 1300, Loss: 0.11706586927175522
step: 1400, Loss: 0.11875293403863907
step: 1500, Loss: 0.11389179527759552
step: 1600, Loss: 0.11611144989728928
step: 1700, Loss: 0.11362899839878082
step: 1800, Loss: 0.11458344757556915
step: 1900, Loss: 0.1138678565621376
step: 2000, Loss: 0.1139652207493782
step: 2100, Loss: 0.11523700505495071
step: 2200, Loss: 0.1143558919429779
step: 2300, Loss: 0.11592163890600204
step: 2400, Loss: 0.11398288607597351
step: 2500, Loss: 0.11395227909088135
step: 2600, Loss: 0.11498937755823135
step: 2700, Loss: 0.11373978853225708
step: 2800, Loss: 0.11491896957159042
step: 2900, Loss: 0.11342279613018036
step: 3000, Loss: 0.11421201378107071
step: 3100, Loss: 0.1142096072435379
step: 3200, Loss: 0.11363811045885086
step: 3300, Loss: 0.11394156515598297
step: 3400, Loss: 0.11587385833263397
step: 3500, Loss: 0.11426541209220886
step: 3600, Loss: 0.1150306761264801
step: 3700, Loss: 0.11294063180685043
step: 3800, Loss: 0.11435061693191528
step: 3900, Loss: 0.11258561909198761
step: 4000, Loss: 0.11649900674819946
step: 4100, Loss: 0.11538490653038025
step: 4200, Loss: 0.11325155943632126
step: 4300, Loss: 0.11379989236593246
step: 4400, Loss: 0.11498507112264633
step: 4500, Loss: 0.11396591365337372
step: 4600, Loss: 0.11526183038949966
step: 4700, Loss: 0.11422272771596909
step: 4800, Loss: 0.11404091119766235
step: 4900, Loss: 0.11498944461345673
step: 5000, Loss: 0.11395464092493057
step: 5100, Loss: 0.11322979629039764
step: 5200, Loss: 0.11489420384168625
step: 5300, Loss: 0.11433504521846771
step: 5400, Loss: 0.11413523554801941
step: 5500, Loss: 0.11263479292392731
step: 5600, Loss: 0.11394444108009338
step: 5700, Loss: 0.11388865113258362
step: 5800, Loss: 0.11575004458427429
step: 7300, Loss: 0.12532807886600494
step: 7400, Loss: 0.1272812932729721
step: 7500, Loss: 0.12196606397628784
step: 7600, Loss: 0.12471145391464233
step: 7700, Loss: 0.1215168833732605
step: 7800, Loss: 0.125192329287529
step: 7900, Loss: 0.11689913272857666
step: 8000, Loss: 0.11854144185781479
step: 8100, Loss: 0.1177365630865097
step: 8200, Loss: 0.12327207624912262
step: 8300, Loss: 0.12592294812202454
step: 8400, Loss: 0.1203664019703865
step: 8500, Loss: 0.12457005679607391
step: 8600, Loss: 0.11919155716896057
step: 8700, Loss: 0.1159748062491417
step: 8800, Loss: 0.11805745959281921
step: 8900, Loss: 0.11603054404258728
step: 9000, Loss: 0.11562419682741165
step: 9100, Loss: 0.2041853815317154
step: 9200, Loss: 0.11605292558670044
step: 9300, Loss: 0.11954697221517563
step: 9400, Loss: 0.11605553328990936
step: 9500, Loss: 0.11541896313428879
step: 9600, Loss: 0.11735998094081879
step: 9700, Loss: 0.11792635917663574
step: 9800, Loss: 0.11476422101259232
step: 9900, Loss: 0.11551636457443237
training successfully ended.
validating...
validate data length:76
acc: 0.6527777777777778
precision: 0.6122448979591837
recall: 0.8333333333333334
F_score: 0.7058823529411765
******fold 2******

Training... train_data length:684
step: 0, Loss: 4.37101411819458
step: 100, Loss: 0.14807510375976562
step: 200, Loss: 0.15087784826755524
step: 300, Loss: 0.12460043281316757
step: 400, Loss: 0.11852145940065384
step: 500, Loss: 0.12928587198257446
step: 600, Loss: 0.1313810795545578
step: 700, Loss: 0.12458709627389908
step: 800, Loss: 0.1198417991399765
step: 900, Loss: 0.11858004331588745
step: 1000, Loss: 0.12100093811750412
step: 1100, Loss: 0.11688625812530518
step: 1200, Loss: 0.11633440107107162
step: 1300, Loss: 0.118203304708004
step: 1400, Loss: 0.11730896681547165
step: 1500, Loss: 0.19738732278347015
step: 1600, Loss: 0.11723087728023529
step: 1700, Loss: 0.11767807602882385
step: 1800, Loss: 0.1152886375784874
step: 1900, Loss: 0.11382836848497391
step: 2000, Loss: 0.11652091145515442
step: 2100, Loss: 0.11818137019872665
step: 2200, Loss: 0.11607017368078232
step: 2300, Loss: 0.11604803800582886
step: 2400, Loss: 0.11849477887153625
step: 2500, Loss: 0.1168098971247673
step: 2600, Loss: 0.11594721674919128
step: 2700, Loss: 0.1156172826886177
step: 2800, Loss: 0.11468425393104553
step: 2900, Loss: 0.11451150476932526
step: 3000, Loss: 0.11546853184700012
step: 3100, Loss: 0.11601580679416656
step: 3200, Loss: 0.11462609469890594
step: 3300, Loss: 0.11493368446826935
step: 3400, Loss: 0.19947516918182373
step: 3500, Loss: 0.11397209763526917
step: 3600, Loss: 0.11477452516555786
step: 3700, Loss: 0.11480945348739624
step: 3800, Loss: 0.11532904952764511
step: 3900, Loss: 0.11448691040277481
step: 4000, Loss: 0.11762528866529465
step: 4100, Loss: 0.11564662307500839
step: 4200, Loss: 0.11447080969810486
step: 4300, Loss: 0.1154664009809494
step: 4400, Loss: 0.11553444713354111
step: 4500, Loss: 0.11793440580368042
step: 4600, Loss: 0.11374165862798691
step: 4700, Loss: 0.11652229726314545
step: 4800, Loss: 0.12271451205015182
step: 4900, Loss: 6.214025974273682
step: 5000, Loss: 0.43778687715530396
step: 5100, Loss: 0.20820559561252594
step: 5200, Loss: 0.13966359198093414
step: 5300, Loss: 0.21812322735786438
step: 5400, Loss: 0.12574316561222076
step: 5500, Loss: 0.12211628258228302
step: 5600, Loss: 0.12694406509399414
step: 5700, Loss: 0.1264859288930893
step: 5800, Loss: 0.12275272607803345
step: 5900, Loss: 0.13325555622577667
step: 6000, Loss: 0.12560352683067322
step: 6100, Loss: 0.12382817268371582
step: 6200, Loss: 0.12117935717105865
step: 6300, Loss: 0.12293849885463715
step: 6400, Loss: 0.1219157725572586
step: 6500, Loss: 0.11799999326467514
step: 6600, Loss: 0.12281061708927155
step: 6700, Loss: 0.12427739053964615
step: 6800, Loss: 0.11844439804553986
step: 6900, Loss: 0.11461658775806427
step: 7000, Loss: 0.11670233309268951
step: 7100, Loss: 0.11648058891296387
step: 7200, Loss: 0.1998709738254547
step: 7300, Loss: 0.1167045533657074
step: 7400, Loss: 0.1199169009923935
step: 7500, Loss: 0.11569540202617645
step: 7600, Loss: 0.11747629195451736
step: 7700, Loss: 0.11996760219335556
step: 7800, Loss: 0.11822766065597534
step: 7900, Loss: 0.11693878471851349
step: 8000, Loss: 0.11688138544559479
step: 8100, Loss: 0.11510790139436722
step: 8200, Loss: 0.11699073016643524
step: 8300, Loss: 0.1151028573513031
step: 8400, Loss: 0.11614740639925003
step: 8500, Loss: 0.11504296958446503
step: 8600, Loss: 0.11460115015506744
step: 8700, Loss: 0.1141083762049675
step: 8800, Loss: 0.11540322005748749
step: 8900, Loss: 0.1147790402173996
step: 9000, Loss: 0.1133318841457367
step: 9100, Loss: 0.19232387840747833
step: 9200, Loss: 0.11458681523799896
step: 9300, Loss: 0.11350439488887787
step: 9400, Loss: 0.11376829445362091
step: 9500, Loss: 0.11408138275146484
step: 9600, Loss: 0.11408029496669769
step: 9700, Loss: 0.11435945332050323
step: 9800, Loss: 0.11650702357292175
step: 9900, Loss: 0.11415623873472214
training successfully ended.
validating...
validate data length:76
acc: 0.8472222222222222
precision: 0.84
recall: 0.9333333333333333
F_score: 0.8842105263157894
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.12099321186542511
step: 100, Loss: 0.1351042538881302
step: 200, Loss: 0.12124134600162506
step: 300, Loss: 0.12176953256130219
step: 400, Loss: 0.12029056251049042
step: 500, Loss: 0.1176529973745346
step: 600, Loss: 0.11825967580080032
step: 700, Loss: 0.115528404712677
step: 800, Loss: 0.11887115240097046
step: 900, Loss: 0.11440487205982208
step: 1000, Loss: 0.11927478760480881
step: 1100, Loss: 0.11529001593589783
step: 1200, Loss: 0.11531367152929306
step: 1300, Loss: 0.11542000621557236
step: 1400, Loss: 0.11674673855304718
step: 1500, Loss: 0.1916191130876541
step: 1600, Loss: 0.11523234099149704
step: 1700, Loss: 0.11526428908109665
step: 1800, Loss: 0.11551135033369064
step: 1900, Loss: 0.11643978208303452
step: 2000, Loss: 0.11402557790279388
step: 2100, Loss: 0.11368873715400696
step: 2200, Loss: 0.1140654906630516
step: 2300, Loss: 0.1152433454990387
step: 2400, Loss: 0.11369597911834717
step: 2500, Loss: 0.11376053094863892
step: 2600, Loss: 0.11363248527050018
step: 2700, Loss: 0.11350592225790024
step: 2800, Loss: 0.11324180662631989
step: 2900, Loss: 0.11298707127571106
step: 3000, Loss: 0.11471328139305115
step: 3100, Loss: 0.1137775331735611
step: 3200, Loss: 0.11375759541988373
step: 3300, Loss: 0.11360053718090057
step: 3400, Loss: 0.1939241737127304
step: 3500, Loss: 0.11305420100688934
step: 3600, Loss: 0.11352979391813278
step: 3700, Loss: 0.11417413502931595
step: 3800, Loss: 0.11404582113027573
step: 3900, Loss: 0.1149151623249054
step: 4000, Loss: 0.11422177404165268
step: 4100, Loss: 0.11540418863296509
step: 4200, Loss: 0.11348748207092285
step: 4300, Loss: 0.1142696961760521
step: 4400, Loss: 0.12693524360656738
step: 4500, Loss: 0.11790747195482254
step: 4600, Loss: 0.11460030823945999
step: 4700, Loss: 0.1141657754778862
step: 4800, Loss: 0.1139337420463562
step: 4900, Loss: 0.8845118284225464
step: 5000, Loss: 0.25957250595092773
step: 5100, Loss: 0.1336352378129959
step: 5200, Loss: 0.13944990932941437
step: 5300, Loss: 0.21242153644561768
step: 5400, Loss: 0.12270272523164749
step: 5500, Loss: 0.12613791227340698
step: 5600, Loss: 0.12851828336715698
step: 5700, Loss: 0.12462225556373596
step: 5800, Loss: 0.12262791395187378
step: 5900, Loss: 0.12308790534734726
step: 6000, Loss: 0.11808378994464874
step: 6100, Loss: 0.1171770989894867
step: 6200, Loss: 0.11919309198856354
step: 6300, Loss: 0.1210605725646019
step: 6400, Loss: 0.11971321702003479
step: 6500, Loss: 0.1183614581823349
step: 6600, Loss: 0.11644286662340164
step: 6700, Loss: 0.11758026480674744
step: 6800, Loss: 0.12332560122013092
step: 6900, Loss: 0.11944730579853058
step: 7000, Loss: 0.11906491219997406
step: 7100, Loss: 0.1200941801071167
step: 7200, Loss: 0.19973713159561157
step: 7300, Loss: 0.11566568911075592
step: 7400, Loss: 0.11945977061986923
step: 7500, Loss: 0.11541848629713058
step: 7600, Loss: 0.11845412850379944
step: 7700, Loss: 0.11423766613006592
step: 5900, Loss: 0.1136791855096817
step: 6000, Loss: 0.11414355784654617
step: 6100, Loss: 0.1133287101984024
step: 6200, Loss: 0.11460413038730621
step: 6300, Loss: 0.11259131133556366
step: 6400, Loss: 0.11457131057977676
step: 6500, Loss: 0.11287058144807816
step: 6600, Loss: 0.1136731207370758
step: 6700, Loss: 0.11327193677425385
step: 6800, Loss: 0.11554914712905884
step: 6900, Loss: 0.11432214081287384
step: 7000, Loss: 0.11363136023283005
step: 7100, Loss: 0.113560251891613
step: 7200, Loss: 0.1152498871088028
step: 7300, Loss: 0.11272522807121277
step: 7400, Loss: 0.11383385956287384
step: 7500, Loss: 0.11329326033592224
step: 7600, Loss: 0.11579937487840652
step: 7700, Loss: 0.11358398199081421
step: 7800, Loss: 0.11401533335447311
step: 7900, Loss: 0.11507543176412582
step: 8000, Loss: 0.11505717039108276
step: 8100, Loss: 0.11378450691699982
step: 8200, Loss: 0.11389831453561783
step: 8300, Loss: 0.1240401566028595
step: 8400, Loss: 0.12297015637159348
step: 8500, Loss: 0.11760964244604111
step: 8600, Loss: 0.11737339198589325
step: 8700, Loss: 0.11543825268745422
step: 8800, Loss: 0.12033781409263611
step: 8900, Loss: 0.11686559021472931
step: 9000, Loss: 0.11680096387863159
step: 9100, Loss: 0.11625175923109055
step: 9200, Loss: 0.11728313565254211
step: 9300, Loss: 0.11514033377170563
step: 9400, Loss: 0.11806488782167435
step: 9500, Loss: 0.11442819237709045
step: 9600, Loss: 0.1154392808675766
step: 9700, Loss: 0.11473701149225235
step: 9800, Loss: 0.11714740097522736
step: 9900, Loss: 0.11422718316316605
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8823529411764706
recall: 0.8823529411764706
F_score: 0.8823529411764706
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.19231289625167847
step: 100, Loss: 0.11329758912324905
step: 200, Loss: 0.11535800993442535
step: 300, Loss: 0.11472910642623901
step: 400, Loss: 0.11435198038816452
step: 500, Loss: 0.1147465631365776
step: 600, Loss: 0.11469633132219315
step: 700, Loss: 0.1143597811460495
step: 800, Loss: 0.11606231331825256
step: 900, Loss: 0.11667022109031677
step: 1000, Loss: 0.11421643197536469
step: 1100, Loss: 0.1359519362449646
step: 1200, Loss: 0.14035871624946594
step: 1300, Loss: 0.12825314700603485
step: 1400, Loss: 0.1183205246925354
step: 1500, Loss: 0.11856406182050705
step: 1600, Loss: 0.11599685251712799
step: 1700, Loss: 0.11848045140504837
step: 1800, Loss: 0.11457483470439911
step: 1900, Loss: 0.11901800334453583
step: 2000, Loss: 0.11395756155252457
step: 2100, Loss: 0.11446697264909744
step: 2200, Loss: 0.1140998974442482
step: 2300, Loss: 0.11768223345279694
step: 2400, Loss: 0.11505557596683502
step: 2500, Loss: 0.11894413828849792
step: 2600, Loss: 0.11391504108905792
step: 2700, Loss: 0.11448819935321808
step: 2800, Loss: 0.11563393473625183
step: 2900, Loss: 0.1157510057091713
step: 3000, Loss: 0.11763710528612137
step: 3100, Loss: 0.11568480730056763
step: 3200, Loss: 0.11697392165660858
step: 3300, Loss: 0.11390455067157745
step: 3400, Loss: 0.11788580566644669
step: 3500, Loss: 0.11376554518938065
step: 3600, Loss: 0.11402162909507751
step: 3700, Loss: 0.11363314092159271
step: 3800, Loss: 0.11621986329555511
step: 3900, Loss: 0.11430640518665314
step: 4000, Loss: 0.11388780176639557
step: 4100, Loss: 0.1129525899887085
step: 4200, Loss: 0.11401886492967606
step: 4300, Loss: 0.11473389714956284
step: 4400, Loss: 0.11381593346595764
step: 4500, Loss: 0.11682215332984924
step: 4600, Loss: 0.11361126601696014
step: 4700, Loss: 0.11472322046756744
step: 4800, Loss: 0.11318594217300415
step: 4900, Loss: 0.11275769770145416
step: 5000, Loss: 0.11461737751960754
step: 5100, Loss: 0.11376245319843292
step: 5200, Loss: 0.1131310760974884
step: 5300, Loss: 0.11390276998281479
step: 5400, Loss: 0.11431685835123062
step: 5500, Loss: 0.11454284191131592
step: 5600, Loss: 0.1131804883480072
step: 5700, Loss: 0.11462761461734772
step: 5800, Loss: 0.11546619981527328
step: 5900, Loss: 0.11383239179849625
step: 6000, Loss: 0.1134939193725586
step: 6100, Loss: 0.11266274750232697
step: 6200, Loss: 0.11413455754518509
step: 6300, Loss: 0.11271005123853683
step: 6400, Loss: 0.12228713184595108
step: 6500, Loss: 0.11376239359378815
step: 6600, Loss: 0.1147937998175621
step: 6700, Loss: 0.11351190507411957
step: 6800, Loss: 0.11318983137607574
step: 6900, Loss: 0.11363409459590912
step: 7000, Loss: 0.11355128884315491
step: 7100, Loss: 0.11397126317024231
step: 7200, Loss: 0.11429192125797272
step: 7300, Loss: 0.11595047265291214
step: 7400, Loss: 0.11254789680242538
step: 7500, Loss: 0.11308170855045319
step: 7600, Loss: 0.11685071885585785
step: 7700, Loss: 0.11344016343355179
step: 7800, Loss: 0.11542393267154694
step: 7900, Loss: 0.11421123147010803
step: 8000, Loss: 0.11556874215602875
step: 8100, Loss: 0.1131625771522522
step: 8200, Loss: 0.1144687756896019
step: 8300, Loss: 0.1141505092382431
step: 8400, Loss: 0.11578013002872467
step: 8500, Loss: 0.11322321742773056
step: 8600, Loss: 0.11459299921989441
step: 8700, Loss: 0.11360007524490356
step: 8800, Loss: 0.11435878276824951
step: 8900, Loss: 0.11432383954524994
step: 9000, Loss: 0.11459238082170486
step: 9100, Loss: 0.11390207707881927
step: 9200, Loss: 0.11516338586807251
step: 9300, Loss: 0.11333481967449188
step: 9400, Loss: 0.11401860415935516
step: 9500, Loss: 0.11434168368577957
step: 9600, Loss: 0.13528656959533691
step: 9700, Loss: 0.1228971779346466
step: 9800, Loss: 0.12993793189525604
step: 9900, Loss: 0.12098485231399536
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8461538461538461
recall: 0.9166666666666666
F_score: 0.8799999999999999
subject 11 Avgacc: 0.8568750000000002 Avgfscore: 0.8631162819094509 
 Max acc:0.9666666666666667, Max f score:0.972972972972973
******** mix subject_12 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 29.00269889831543
step: 100, Loss: 1.1316618919372559
step: 200, Loss: 0.1298237293958664
step: 300, Loss: 0.14865247905254364
step: 400, Loss: 0.1391371339559555
step: 500, Loss: 0.14369307458400726
step: 600, Loss: 0.11696028709411621
step: 700, Loss: 0.1293003261089325
step: 800, Loss: 0.12003818899393082
step: 900, Loss: 0.13263705372810364
step: 1000, Loss: 0.1179770827293396
step: 1100, Loss: 0.1236325353384018
step: 1200, Loss: 0.11621202528476715
step: 1300, Loss: 0.12676554918289185
step: 1400, Loss: 0.12610794603824615
step: 1500, Loss: 0.12030234932899475
step: 1600, Loss: 0.1216079518198967
step: 1700, Loss: 0.1180504858493805
step: 1800, Loss: 0.11925481259822845
step: 1900, Loss: 0.11569447815418243
step: 2000, Loss: 0.11900103837251663
step: 2100, Loss: 0.11850002408027649
step: 2200, Loss: 0.11580834537744522
step: 2300, Loss: 0.11672879755496979
step: 2400, Loss: 0.11465689539909363
step: 2500, Loss: 0.1164940595626831
step: 2600, Loss: 0.1217370480298996
step: 2700, Loss: 0.11461609601974487
step: 2800, Loss: 0.11571642011404037
step: 2900, Loss: 0.12317788600921631
step: 3000, Loss: 0.11582578718662262
step: 3100, Loss: 0.11577881872653961
step: 3200, Loss: 0.11497123539447784
step: 3300, Loss: 0.16269224882125854
step: 3400, Loss: 0.13025841116905212
step: 3500, Loss: 0.13451768457889557
step: 3600, Loss: 0.1683274656534195
step: 3700, Loss: 0.1274053305387497
step: 3800, Loss: 0.12180395424365997
step: 3900, Loss: 0.1225963830947876
step: 4000, Loss: 0.12716753780841827
step: 4100, Loss: 0.12426472455263138
step: 4200, Loss: 0.11977437138557434
step: 4300, Loss: 0.11696532368659973
step: 4400, Loss: 0.11886358261108398
step: 4500, Loss: 0.1177038848400116
step: 4600, Loss: 0.1155015304684639
step: 4700, Loss: 0.1192944198846817
step: 4800, Loss: 0.1209782212972641
step: 4900, Loss: 0.11725493520498276
step: 5000, Loss: 0.11483611166477203
step: 5100, Loss: 0.12026306986808777
step: 5200, Loss: 0.11784283816814423
step: 5300, Loss: 0.11770523339509964
step: 5400, Loss: 0.11490307003259659
step: 5500, Loss: 0.1175159141421318
step: 5600, Loss: 0.11605916172266006
step: 5700, Loss: 0.11580248177051544
step: 5800, Loss: 0.11644211411476135
step: 5900, Loss: 0.11386499553918839
step: 7800, Loss: 0.11609305441379547
step: 7900, Loss: 0.11466766893863678
step: 8000, Loss: 0.11645844578742981
step: 8100, Loss: 0.11382642388343811
step: 8200, Loss: 0.11448373645544052
step: 8300, Loss: 0.114705890417099
step: 8400, Loss: 0.11591038107872009
step: 8500, Loss: 0.11417324841022491
step: 8600, Loss: 0.11491351574659348
step: 8700, Loss: 0.11466259509325027
step: 8800, Loss: 0.1141500324010849
step: 8900, Loss: 0.11368633806705475
step: 9000, Loss: 0.11480303853750229
step: 9100, Loss: 0.19250504672527313
step: 9200, Loss: 0.11467690020799637
step: 9300, Loss: 0.11482264846563339
step: 9400, Loss: 0.11539587378501892
step: 9500, Loss: 0.11370342969894409
step: 9600, Loss: 0.11349567770957947
step: 9700, Loss: 0.11414800584316254
step: 9800, Loss: 0.11473672837018967
step: 9900, Loss: 0.11421258747577667
training successfully ended.
validating...
validate data length:76
acc: 0.875
precision: 0.8913043478260869
recall: 0.9111111111111111
F_score: 0.9010989010989011
******fold 4******

Training... train_data length:684
step: 0, Loss: 2.312023639678955
step: 100, Loss: 0.1253889501094818
step: 200, Loss: 0.11775890737771988
step: 300, Loss: 0.11943470686674118
step: 400, Loss: 0.11454500257968903
step: 500, Loss: 0.1147976741194725
step: 600, Loss: 0.11486944556236267
step: 700, Loss: 0.11565610021352768
step: 800, Loss: 0.1141597256064415
step: 900, Loss: 0.11468475311994553
step: 1000, Loss: 0.11503536254167557
step: 1100, Loss: 0.11455661058425903
step: 1200, Loss: 0.11456702649593353
step: 1300, Loss: 0.11303238570690155
step: 1400, Loss: 0.11419566720724106
step: 1500, Loss: 0.19140565395355225
step: 1600, Loss: 0.11567245423793793
step: 1700, Loss: 0.11516141891479492
step: 1800, Loss: 0.11503776907920837
step: 1900, Loss: 0.11372053623199463
step: 2000, Loss: 0.11399189382791519
step: 2100, Loss: 0.11499687284231186
step: 2200, Loss: 0.11401020735502243
step: 2300, Loss: 0.11445147544145584
step: 2400, Loss: 0.11582542955875397
step: 2500, Loss: 0.11395741999149323
step: 2600, Loss: 0.11644703894853592
step: 2700, Loss: 0.11375557631254196
step: 2800, Loss: 0.11353045701980591
step: 2900, Loss: 0.11320684850215912
step: 3000, Loss: 0.1137731522321701
step: 3100, Loss: 0.1141810268163681
step: 3200, Loss: 0.11466207355260849
step: 3300, Loss: 0.11355554312467575
step: 3400, Loss: 0.19327151775360107
step: 3500, Loss: 0.11569187045097351
step: 3600, Loss: 0.11490720510482788
step: 3700, Loss: 0.11467571556568146
step: 3800, Loss: 0.11338028311729431
step: 3900, Loss: 0.11323259770870209
step: 4000, Loss: 0.11554896831512451
step: 4100, Loss: 0.11636103689670563
step: 4200, Loss: 0.11572440713644028
step: 4300, Loss: 3.2739388942718506
step: 4400, Loss: 0.18327537178993225
step: 4500, Loss: 0.13877908885478973
step: 4600, Loss: 0.1370023787021637
step: 4700, Loss: 0.12709090113639832
step: 4800, Loss: 0.12675286829471588
step: 4900, Loss: 0.13453759253025055
step: 5000, Loss: 0.11855046451091766
step: 5100, Loss: 0.1229894682765007
step: 5200, Loss: 0.12913016974925995
step: 5300, Loss: 0.20026792585849762
step: 5400, Loss: 0.12107694894075394
step: 5500, Loss: 0.12143115699291229
step: 5600, Loss: 0.11913757026195526
step: 5700, Loss: 0.11903315782546997
step: 5800, Loss: 0.11829540133476257
step: 5900, Loss: 0.12153377383947372
step: 6000, Loss: 0.11646334826946259
step: 6100, Loss: 0.115178182721138
step: 6200, Loss: 0.11542678624391556
step: 6300, Loss: 0.11839001625776291
step: 6400, Loss: 0.11833155155181885
step: 6500, Loss: 0.1161457896232605
step: 6600, Loss: 0.11574560403823853
step: 6700, Loss: 0.11609294265508652
step: 6800, Loss: 0.11500542610883713
step: 6900, Loss: 0.11410541087388992
step: 7000, Loss: 0.11640843749046326
step: 7100, Loss: 0.11784902960062027
step: 7200, Loss: 0.1920022964477539
step: 7300, Loss: 0.11507602035999298
step: 7400, Loss: 0.11524764448404312
step: 7500, Loss: 0.11358610540628433
step: 7600, Loss: 0.11492909491062164
step: 7700, Loss: 0.11413809657096863
step: 7800, Loss: 0.11389954388141632
step: 7900, Loss: 0.11354508250951767
step: 8000, Loss: 0.11422370374202728
step: 8100, Loss: 0.11339163780212402
step: 8200, Loss: 0.11299005895853043
step: 8300, Loss: 0.11409508436918259
step: 8400, Loss: 0.11374498158693314
step: 8500, Loss: 0.11419501900672913
step: 8600, Loss: 0.11388668417930603
step: 8700, Loss: 0.11442394554615021
step: 8800, Loss: 0.11382816731929779
step: 8900, Loss: 0.11449383199214935
step: 9000, Loss: 0.1141602173447609
step: 9100, Loss: 0.19079869985580444
step: 9200, Loss: 0.11249230802059174
step: 9300, Loss: 0.11300283670425415
step: 9400, Loss: 0.11361480504274368
step: 9500, Loss: 0.11393195390701294
step: 9600, Loss: 0.11586760729551315
step: 9700, Loss: 0.11506086587905884
step: 9800, Loss: 0.1127999871969223
step: 9900, Loss: 0.11394476145505905
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.8717948717948718
recall: 1.0
F_score: 0.9315068493150686
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.44671863317489624
step: 100, Loss: 0.1241944283246994
step: 200, Loss: 0.11824075877666473
step: 300, Loss: 0.11614631116390228
step: 400, Loss: 0.11613591760396957
step: 500, Loss: 0.11650896817445755
step: 600, Loss: 0.11534343659877777
step: 700, Loss: 0.11550043523311615
step: 800, Loss: 0.11399419605731964
step: 900, Loss: 0.11413972824811935
step: 1000, Loss: 0.11464475840330124
step: 1100, Loss: 0.11329203099012375
step: 1200, Loss: 0.11385148763656616
step: 1300, Loss: 0.11389286816120148
step: 1400, Loss: 0.11375749856233597
step: 1500, Loss: 0.19179753959178925
step: 1600, Loss: 0.11415962874889374
step: 1700, Loss: 0.11361514031887054
step: 1800, Loss: 0.11398157477378845
step: 1900, Loss: 0.11489169299602509
step: 2000, Loss: 0.11273612082004547
step: 2100, Loss: 0.11314321309328079
step: 2200, Loss: 0.11430603265762329
step: 2300, Loss: 0.11406300961971283
step: 2400, Loss: 0.11464663594961166
step: 2500, Loss: 0.11428524553775787
step: 2600, Loss: 0.11408720910549164
step: 2700, Loss: 0.11262203007936478
step: 2800, Loss: 0.11301133036613464
step: 2900, Loss: 0.11277873814105988
step: 3000, Loss: 0.11527054756879807
step: 3100, Loss: 0.11289466172456741
step: 3200, Loss: 0.11449480801820755
step: 3300, Loss: 0.11397526413202286
step: 3400, Loss: 0.19257396459579468
step: 3500, Loss: 0.11502846330404282
step: 3600, Loss: 0.11360333859920502
step: 3700, Loss: 0.11521977186203003
step: 3800, Loss: 0.11420480906963348
step: 3900, Loss: 0.11417563259601593
step: 4000, Loss: 0.11339478939771652
step: 4100, Loss: 0.11734679341316223
step: 4200, Loss: 0.11577586084604263
step: 4300, Loss: 0.11702460050582886
step: 4400, Loss: 0.11546431481838226
step: 4500, Loss: 0.11829011142253876
step: 4600, Loss: 0.11770707368850708
step: 4700, Loss: 0.11813939362764359
step: 4800, Loss: 0.12021534889936447
step: 4900, Loss: 0.12007850408554077
step: 5000, Loss: 0.11555014550685883
step: 5100, Loss: 0.11391066759824753
step: 5200, Loss: 0.1148139014840126
step: 5300, Loss: 0.19330695271492004
step: 5400, Loss: 0.12165552377700806
step: 5500, Loss: 0.21077726781368256
step: 5600, Loss: 0.14174768328666687
step: 5700, Loss: 0.12966501712799072
step: 5800, Loss: 0.1358087956905365
step: 5900, Loss: 0.12657806277275085
step: 6000, Loss: 0.12319392710924149
step: 6100, Loss: 0.12185126543045044
step: 6200, Loss: 0.12652866542339325
step: 6300, Loss: 0.12360262125730515
step: 6400, Loss: 0.12992118299007416
step: 6500, Loss: 0.12172013521194458
step: 6600, Loss: 0.12301260232925415
step: 6700, Loss: 0.12649044394493103
step: 6800, Loss: 0.12591661512851715
step: 6900, Loss: 0.11582305282354355
step: 7000, Loss: 0.12130159139633179
step: 7100, Loss: 0.12062954902648926
step: 7200, Loss: 0.20347346365451813
step: 7300, Loss: 0.11658617109060287
step: 7400, Loss: 0.11604301631450653
step: 7500, Loss: 0.11489380896091461
step: 7600, Loss: 0.11810760200023651
step: 7700, Loss: 0.11638910323381424
step: 7800, Loss: 0.11658081412315369
step: 7900, Loss: 0.11514278501272202
step: 8000, Loss: 0.11410552263259888
step: 8100, Loss: 0.11838085949420929
step: 8200, Loss: 0.11594507098197937
step: 6000, Loss: 0.12179846316576004
step: 6100, Loss: 0.1151454895734787
step: 6200, Loss: 0.11527540534734726
step: 6300, Loss: 0.11576074361801147
step: 6400, Loss: 0.1155238002538681
step: 6500, Loss: 0.11530721187591553
step: 6600, Loss: 0.11651580035686493
step: 6700, Loss: 0.11516665667295456
step: 6800, Loss: 0.1142912283539772
step: 6900, Loss: 0.11491364985704422
step: 7000, Loss: 0.11714740097522736
step: 7100, Loss: 0.11365659534931183
step: 7200, Loss: 0.11648236960172653
step: 7300, Loss: 0.11447370797395706
step: 7400, Loss: 0.11484060436487198
step: 7500, Loss: 0.11401506513357162
step: 7600, Loss: 0.12296171486377716
step: 7700, Loss: 0.1145646944642067
step: 7800, Loss: 0.11572836339473724
step: 7900, Loss: 0.11569345742464066
step: 8000, Loss: 0.11581163108348846
step: 8100, Loss: 0.11425691843032837
step: 8200, Loss: 0.11469148099422455
step: 8300, Loss: 0.11421599984169006
step: 8400, Loss: 0.1147519052028656
step: 8500, Loss: 0.11583445966243744
step: 8600, Loss: 0.11401522159576416
step: 8700, Loss: 0.11526064574718475
step: 8800, Loss: 0.11535963416099548
step: 8900, Loss: 0.11500094085931778
step: 9000, Loss: 0.11388713866472244
step: 9100, Loss: 0.1159386858344078
step: 9200, Loss: 0.1145465224981308
step: 9300, Loss: 0.11453843116760254
step: 9400, Loss: 0.11655522882938385
step: 9500, Loss: 0.11539660394191742
step: 9600, Loss: 0.11652736365795135
step: 9700, Loss: 0.11460121721029282
step: 9800, Loss: 0.11898037791252136
step: 9900, Loss: 0.11591674387454987
training successfully ended.
validating...
validate data length:32
acc: 0.5
precision: 0.4
recall: 0.6666666666666666
F_score: 0.5
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.17113932967185974
step: 100, Loss: 0.1249280795454979
step: 200, Loss: 0.1220572292804718
step: 300, Loss: 0.11691714823246002
step: 400, Loss: 0.12186030298471451
step: 500, Loss: 0.11760105192661285
step: 600, Loss: 0.11833218485116959
step: 700, Loss: 0.11698751896619797
step: 800, Loss: 0.1146751344203949
step: 900, Loss: 0.11527790129184723
step: 1000, Loss: 0.11663150787353516
step: 1100, Loss: 0.11705378443002701
step: 1200, Loss: 0.12069213390350342
step: 1300, Loss: 0.11361075937747955
step: 1400, Loss: 0.11577634513378143
step: 1500, Loss: 0.11637689173221588
step: 1600, Loss: 0.11472228169441223
step: 1700, Loss: 0.11492675542831421
step: 1800, Loss: 0.11645067483186722
step: 1900, Loss: 0.11523354053497314
step: 2000, Loss: 0.11451539397239685
step: 2100, Loss: 0.11498352885246277
step: 2200, Loss: 0.11619575321674347
step: 2300, Loss: 0.1150619238615036
step: 2400, Loss: 0.1163424551486969
step: 2500, Loss: 0.11427685618400574
step: 2600, Loss: 0.11459620296955109
step: 2700, Loss: 0.11300677806138992
step: 2800, Loss: 0.11512524634599686
step: 2900, Loss: 0.1134864017367363
step: 3000, Loss: 0.11302772909402847
step: 3100, Loss: 0.11532086879014969
step: 3200, Loss: 0.11835826933383942
step: 3300, Loss: 0.11498567461967468
step: 3400, Loss: 0.11520963162183762
step: 3500, Loss: 0.11304688453674316
step: 3600, Loss: 0.11490986496210098
step: 3700, Loss: 0.11335605382919312
step: 3800, Loss: 0.11385122686624527
step: 3900, Loss: 0.11519010365009308
step: 4000, Loss: 0.11722314357757568
step: 4100, Loss: 0.1219029352068901
step: 4200, Loss: 0.11515273153781891
step: 4300, Loss: 0.1156562864780426
step: 4400, Loss: 0.1150762140750885
step: 4500, Loss: 0.11559335887432098
step: 4600, Loss: 0.11377677321434021
step: 4700, Loss: 0.11520185321569443
step: 4800, Loss: 0.11487893760204315
step: 4900, Loss: 0.11410877853631973
step: 5000, Loss: 0.11592318117618561
step: 5100, Loss: 0.11384375393390656
step: 5200, Loss: 0.11728131026029587
step: 5300, Loss: 0.11486487090587616
step: 5400, Loss: 0.11418822407722473
step: 5500, Loss: 0.11423496156930923
step: 5600, Loss: 0.11779485642910004
step: 5700, Loss: 0.1823071837425232
step: 5800, Loss: 0.141184464097023
step: 5900, Loss: 0.13189639151096344
step: 6000, Loss: 0.1247815489768982
step: 6100, Loss: 0.126370370388031
step: 6200, Loss: 0.1256507784128189
step: 6300, Loss: 0.13245156407356262
step: 6400, Loss: 0.12609422206878662
step: 6500, Loss: 0.11987067013978958
step: 6600, Loss: 0.11780261993408203
step: 6700, Loss: 0.12049444019794464
step: 6800, Loss: 0.11839421838521957
step: 6900, Loss: 0.11919315159320831
step: 7000, Loss: 0.11634204536676407
step: 7100, Loss: 0.11773976683616638
step: 7200, Loss: 0.11989140510559082
step: 7300, Loss: 0.12329539656639099
step: 7400, Loss: 0.11856616288423538
step: 7500, Loss: 0.11577802896499634
step: 7600, Loss: 0.1184816062450409
step: 7700, Loss: 0.11623713374137878
step: 7800, Loss: 0.12862488627433777
step: 7900, Loss: 0.11501172184944153
step: 8000, Loss: 0.11755631864070892
step: 8100, Loss: 0.11444602906703949
step: 8200, Loss: 0.11615220457315445
step: 8300, Loss: 0.11833506077528
step: 8400, Loss: 0.11485698819160461
step: 8500, Loss: 0.11488759517669678
step: 8600, Loss: 0.11919170618057251
step: 8700, Loss: 0.11559531092643738
step: 8800, Loss: 0.11650560796260834
step: 8900, Loss: 0.11561498790979385
step: 9000, Loss: 0.11664064228534698
step: 9100, Loss: 0.11661678552627563
step: 9200, Loss: 0.12737244367599487
step: 9300, Loss: 0.11343590915203094
step: 9400, Loss: 0.11522986739873886
step: 9500, Loss: 0.11469962447881699
step: 9600, Loss: 0.11711820960044861
step: 9700, Loss: 0.11361108720302582
step: 9800, Loss: 0.11538216471672058
step: 9900, Loss: 0.11591683328151703
training successfully ended.
validating...
validate data length:32
acc: 0.78125
precision: 0.7647058823529411
recall: 0.8125
F_score: 0.787878787878788
******fold 3******

Training... train_data length:281
step: 0, Loss: 2.7837936878204346
step: 100, Loss: 0.11550136655569077
step: 200, Loss: 0.11958321183919907
step: 300, Loss: 0.11630243062973022
step: 400, Loss: 0.11599297821521759
step: 500, Loss: 0.11754490435123444
step: 600, Loss: 0.11646262556314468
step: 700, Loss: 0.11357537657022476
step: 800, Loss: 0.11371918022632599
step: 900, Loss: 0.11552499234676361
step: 1000, Loss: 0.1143622025847435
step: 1100, Loss: 0.1160123273730278
step: 1200, Loss: 0.11580722033977509
step: 1300, Loss: 0.11471503973007202
step: 1400, Loss: 0.11580199003219604
step: 1500, Loss: 0.11388374119997025
step: 1600, Loss: 0.1154523640871048
step: 1700, Loss: 0.11357436329126358
step: 1800, Loss: 0.11401336640119553
step: 1900, Loss: 0.11544758826494217
step: 2000, Loss: 0.11468248814344406
step: 2100, Loss: 0.1141887903213501
step: 2200, Loss: 0.11445081979036331
step: 2300, Loss: 0.11469332873821259
step: 2400, Loss: 0.11443115025758743
step: 2500, Loss: 0.11312730610370636
step: 2600, Loss: 0.11425027251243591
step: 2700, Loss: 0.11368216574192047
step: 2800, Loss: 0.11402031779289246
step: 2900, Loss: 0.11423514038324356
step: 3000, Loss: 0.11399563401937485
step: 3100, Loss: 0.11392918229103088
step: 3200, Loss: 0.11460259556770325
step: 3300, Loss: 0.11375321447849274
step: 3400, Loss: 0.11467533558607101
step: 3500, Loss: 0.11392995715141296
step: 3600, Loss: 0.11514709144830704
step: 3700, Loss: 0.11427277326583862
step: 3800, Loss: 0.11689817905426025
step: 3900, Loss: 0.11455261707305908
step: 4000, Loss: 0.11434850096702576
step: 4100, Loss: 0.11413648724555969
step: 4200, Loss: 0.11519129574298859
step: 4300, Loss: 0.11363711208105087
step: 4400, Loss: 0.1150234192609787
step: 4500, Loss: 0.1141194999217987
step: 4600, Loss: 0.11694468557834625
step: 4700, Loss: 0.1135043129324913
step: 4800, Loss: 0.11756112426519394
step: 4900, Loss: 0.11493690311908722
step: 5000, Loss: 0.11622294783592224
step: 5100, Loss: 0.11450076103210449
step: 5200, Loss: 0.11990144103765488
step: 5300, Loss: 0.11413907259702682
step: 5400, Loss: 0.11491984128952026
step: 5500, Loss: 0.11408072710037231
step: 5600, Loss: 0.11357201635837555
step: 5700, Loss: 0.11410336941480637
step: 5800, Loss: 0.11637650430202484
step: 5900, Loss: 0.11417949199676514
step: 6000, Loss: 0.11655940860509872
step: 6100, Loss: 0.11450380086898804
step: 6200, Loss: 0.11932507157325745
step: 6300, Loss: 0.11475668102502823
step: 6400, Loss: 0.1159711629152298
step: 6500, Loss: 0.11436362564563751
step: 8300, Loss: 0.11567194014787674
step: 8400, Loss: 0.11441853642463684
step: 8500, Loss: 0.11520599573850632
step: 8600, Loss: 0.11450061947107315
step: 8700, Loss: 0.11422739923000336
step: 8800, Loss: 0.1139090359210968
step: 8900, Loss: 0.11404505372047424
step: 9000, Loss: 0.11429691314697266
step: 9100, Loss: 0.1947014033794403
step: 9200, Loss: 0.11395831406116486
step: 9300, Loss: 0.11613460630178452
step: 9400, Loss: 0.11480952799320221
step: 9500, Loss: 0.1127740889787674
step: 9600, Loss: 0.11521217972040176
step: 9700, Loss: 0.11355317384004593
step: 9800, Loss: 0.11357463896274567
step: 9900, Loss: 0.11316151916980743
training successfully ended.
validating...
validate data length:76
acc: 0.9027777777777778
precision: 0.8536585365853658
recall: 0.9722222222222222
F_score: 0.9090909090909091
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.3626720905303955
step: 100, Loss: 0.12074390053749084
step: 200, Loss: 0.11826526373624802
step: 300, Loss: 0.11598512530326843
step: 400, Loss: 0.11527391523122787
step: 500, Loss: 0.11629518121480942
step: 600, Loss: 0.11620787531137466
step: 700, Loss: 0.11675310134887695
step: 800, Loss: 0.11531709134578705
step: 900, Loss: 0.11511462926864624
step: 1000, Loss: 0.11344306915998459
step: 1100, Loss: 0.11458580195903778
step: 1200, Loss: 0.11665719747543335
step: 1300, Loss: 0.11637154221534729
step: 1400, Loss: 0.11429159343242645
step: 1500, Loss: 0.19092370569705963
step: 1600, Loss: 0.11444015055894852
step: 1700, Loss: 0.1136380210518837
step: 1800, Loss: 0.11394599825143814
step: 1900, Loss: 0.11366376280784607
step: 2000, Loss: 0.11320687085390091
step: 2100, Loss: 0.11392373591661453
step: 2200, Loss: 0.11402623355388641
step: 2300, Loss: 0.1146617904305458
step: 2400, Loss: 0.11375458538532257
step: 2500, Loss: 0.11464360356330872
step: 2600, Loss: 0.11701705306768417
step: 2700, Loss: 0.11496968567371368
step: 2800, Loss: 0.11332597583532333
step: 2900, Loss: 0.11399232596158981
step: 3000, Loss: 0.11555135250091553
step: 3100, Loss: 0.11417104303836823
step: 3200, Loss: 0.11456720530986786
step: 3300, Loss: 0.11520249396562576
step: 3400, Loss: 0.1918664276599884
step: 3500, Loss: 0.1147739589214325
step: 3600, Loss: 0.11556262522935867
step: 3700, Loss: 0.1151416078209877
step: 3800, Loss: 0.11333858221769333
step: 3900, Loss: 0.11356749385595322
step: 4000, Loss: 0.11357980966567993
step: 4100, Loss: 0.11432958394289017
step: 4200, Loss: 0.11495800316333771
step: 4300, Loss: 0.11597581207752228
step: 4400, Loss: 0.11659267544746399
step: 4500, Loss: 0.11562365293502808
step: 4600, Loss: 0.11647510528564453
step: 4700, Loss: 0.11523312330245972
step: 4800, Loss: 0.11574089527130127
step: 4900, Loss: 0.11371194571256638
step: 5000, Loss: 0.1129155158996582
step: 5100, Loss: 0.11510290205478668
step: 5200, Loss: 0.12906073033809662
step: 5300, Loss: 0.2393733710050583
step: 5400, Loss: 0.1647389531135559
step: 5500, Loss: 0.12645436823368073
step: 5600, Loss: 0.1280285120010376
step: 5700, Loss: 0.12232299149036407
step: 5800, Loss: 0.1292249858379364
step: 5900, Loss: 0.12663090229034424
step: 6000, Loss: 0.1190526932477951
step: 6100, Loss: 0.12410717457532883
step: 6200, Loss: 0.12123657763004303
step: 6300, Loss: 0.12364670634269714
step: 6400, Loss: 0.11754083633422852
step: 6500, Loss: 0.12266728281974792
step: 6600, Loss: 0.12245841324329376
step: 6700, Loss: 0.11863389611244202
step: 6800, Loss: 0.11882684379816055
step: 6900, Loss: 0.11989174038171768
step: 7000, Loss: 0.11747028678655624
step: 7100, Loss: 0.11916007101535797
step: 7200, Loss: 0.2046409398317337
step: 7300, Loss: 0.11596108973026276
step: 7400, Loss: 0.11685813963413239
step: 7500, Loss: 0.11682207882404327
step: 7600, Loss: 0.11452493071556091
step: 7700, Loss: 0.114980548620224
step: 7800, Loss: 0.11551430821418762
step: 7900, Loss: 0.11587446928024292
step: 8000, Loss: 0.11416332423686981
step: 8100, Loss: 0.11313547194004059
step: 8200, Loss: 0.1181001216173172
step: 8300, Loss: 0.11473309993743896
step: 8400, Loss: 0.11498786509037018
step: 8500, Loss: 0.11370512843132019
step: 8600, Loss: 0.1154082790017128
step: 8700, Loss: 0.11567413806915283
step: 8800, Loss: 0.11486472189426422
step: 8900, Loss: 0.1144755482673645
step: 9000, Loss: 0.1142297014594078
step: 9100, Loss: 0.19783198833465576
step: 9200, Loss: 0.11575384438037872
step: 9300, Loss: 0.11535213142633438
step: 9400, Loss: 0.11646591871976852
step: 9500, Loss: 0.1138211116194725
step: 9600, Loss: 0.11351758241653442
step: 9700, Loss: 0.11431422829627991
step: 9800, Loss: 0.11445318162441254
step: 9900, Loss: 0.11450614035129547
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9302325581395349
recall: 1.0
F_score: 0.963855421686747
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.12521639466285706
step: 100, Loss: 0.11742689460515976
step: 200, Loss: 0.1174914687871933
step: 300, Loss: 0.11816565692424774
step: 400, Loss: 0.11436672508716583
step: 500, Loss: 0.11565929651260376
step: 600, Loss: 0.11524049937725067
step: 700, Loss: 0.11411285400390625
step: 800, Loss: 0.11365551501512527
step: 900, Loss: 0.11448326706886292
step: 1000, Loss: 0.11407305300235748
step: 1100, Loss: 0.11403243243694305
step: 1200, Loss: 0.11397279798984528
step: 1300, Loss: 0.11271356791257858
step: 1400, Loss: 0.11341302841901779
step: 1500, Loss: 0.19211840629577637
step: 1600, Loss: 0.11464329808950424
step: 1700, Loss: 0.11370158940553665
step: 1800, Loss: 0.1148672103881836
step: 1900, Loss: 0.11543594300746918
step: 2000, Loss: 0.11543574929237366
step: 2100, Loss: 0.11504971981048584
step: 2200, Loss: 0.11366105824708939
step: 2300, Loss: 0.11385181546211243
step: 2400, Loss: 0.11465781182050705
step: 2500, Loss: 0.11594496667385101
step: 2600, Loss: 0.11644135415554047
step: 2700, Loss: 0.1146979108452797
step: 2800, Loss: 0.11508828401565552
step: 2900, Loss: 0.11507513374090195
step: 3000, Loss: 0.11562035977840424
step: 3100, Loss: 0.1155516654253006
step: 3200, Loss: 0.11461420357227325
step: 3300, Loss: 0.11401613056659698
step: 3400, Loss: 0.19728679955005646
step: 3500, Loss: 0.11386995017528534
step: 3600, Loss: 0.11505033075809479
step: 3700, Loss: 0.22574442625045776
step: 3800, Loss: 0.14331701397895813
step: 3900, Loss: 0.1310344636440277
step: 4000, Loss: 0.13351793587207794
step: 4100, Loss: 0.12406182289123535
step: 4200, Loss: 0.1276438683271408
step: 4300, Loss: 0.12575624883174896
step: 4400, Loss: 0.12159120291471481
step: 4500, Loss: 0.1225929856300354
step: 4600, Loss: 0.12448832392692566
step: 4700, Loss: 0.12338479608297348
step: 4800, Loss: 0.12743207812309265
step: 4900, Loss: 0.11588750034570694
step: 5000, Loss: 0.11574125289916992
step: 5100, Loss: 0.11755290627479553
step: 5200, Loss: 0.1206749826669693
step: 5300, Loss: 0.20020464062690735
step: 5400, Loss: 0.11505109071731567
step: 5500, Loss: 0.11654948443174362
step: 5600, Loss: 0.1152864620089531
step: 5700, Loss: 0.11509750783443451
step: 5800, Loss: 0.11824660003185272
step: 5900, Loss: 0.11657451093196869
step: 6000, Loss: 0.11733192950487137
step: 6100, Loss: 0.117197684943676
step: 6200, Loss: 0.11751888692378998
step: 6300, Loss: 0.11372540146112442
step: 6400, Loss: 0.11534078419208527
step: 6500, Loss: 0.11496864259243011
step: 6600, Loss: 0.11814691126346588
step: 6700, Loss: 0.11840154975652695
step: 6800, Loss: 0.11509209871292114
step: 6900, Loss: 0.11467184126377106
step: 7000, Loss: 0.11637193709611893
step: 7100, Loss: 0.114162877202034
step: 7200, Loss: 0.19165700674057007
step: 7300, Loss: 0.11341030150651932
step: 7400, Loss: 0.1136179268360138
step: 7500, Loss: 0.11500139534473419
step: 7600, Loss: 0.11329303681850433
step: 7700, Loss: 0.11432518064975739
step: 7800, Loss: 0.11339958757162094
step: 7900, Loss: 0.11376261711120605
step: 8000, Loss: 0.11358226835727692
step: 8100, Loss: 0.11377693712711334
step: 8200, Loss: 0.11365047097206116
step: 8300, Loss: 0.11358222365379333
step: 8400, Loss: 0.11418680846691132
step: 8500, Loss: 0.11314092576503754
step: 8600, Loss: 0.11450619250535965
step: 8700, Loss: 0.11387338489294052
step: 6600, Loss: 0.1534125953912735
step: 6700, Loss: 0.1348981112241745
step: 6800, Loss: 0.12674705684185028
step: 6900, Loss: 0.12210845947265625
step: 7000, Loss: 0.12656666338443756
step: 7100, Loss: 0.11834795773029327
step: 7200, Loss: 0.12647637724876404
step: 7300, Loss: 0.11691881716251373
step: 7400, Loss: 0.11705833673477173
step: 7500, Loss: 0.11706364154815674
step: 7600, Loss: 0.12045548856258392
step: 7700, Loss: 0.12156569957733154
step: 7800, Loss: 0.1150820255279541
step: 7900, Loss: 0.11606474220752716
step: 8000, Loss: 0.11447460949420929
step: 8100, Loss: 0.11602987349033356
step: 8200, Loss: 0.11814549565315247
step: 8300, Loss: 0.1193261593580246
step: 8400, Loss: 0.11391381919384003
step: 8500, Loss: 0.11767899990081787
step: 8600, Loss: 0.11507203429937363
step: 8700, Loss: 0.11550463736057281
step: 8800, Loss: 0.1145586222410202
step: 8900, Loss: 0.11372923851013184
step: 9000, Loss: 0.1138167679309845
step: 9100, Loss: 0.1145528182387352
step: 9200, Loss: 0.11489942669868469
step: 9300, Loss: 0.11589013785123825
step: 9400, Loss: 0.11585025489330292
step: 9500, Loss: 0.11681261658668518
step: 9600, Loss: 0.11492997407913208
step: 9700, Loss: 0.11390164494514465
step: 9800, Loss: 0.114543117582798
step: 9900, Loss: 0.11558911204338074
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7058823529411765
recall: 0.8
F_score: 0.7500000000000001
******fold 4******

Training... train_data length:281
step: 0, Loss: 2.4116244316101074
step: 100, Loss: 0.11630367487668991
step: 200, Loss: 0.11719000339508057
step: 300, Loss: 0.11484343558549881
step: 400, Loss: 0.11692845076322556
step: 500, Loss: 0.11476486176252365
step: 600, Loss: 0.1146867573261261
step: 700, Loss: 0.11537414789199829
step: 800, Loss: 0.1151760071516037
step: 900, Loss: 0.11536583304405212
step: 1000, Loss: 0.11489652097225189
step: 1100, Loss: 0.1150001585483551
step: 1200, Loss: 0.11514750123023987
step: 1300, Loss: 0.11421804875135422
step: 1400, Loss: 0.1169913113117218
step: 1500, Loss: 0.11495057493448257
step: 1600, Loss: 0.11352784186601639
step: 1700, Loss: 0.11395761370658875
step: 1800, Loss: 0.11716768890619278
step: 1900, Loss: 0.11432552337646484
step: 2000, Loss: 0.11433441191911697
step: 2100, Loss: 0.11529140919446945
step: 2200, Loss: 0.11466077715158463
step: 2300, Loss: 0.11447878181934357
step: 2400, Loss: 0.11596465110778809
step: 2500, Loss: 0.11618899554014206
step: 2600, Loss: 0.11732812225818634
step: 2700, Loss: 0.11603385210037231
step: 2800, Loss: 0.11419850587844849
step: 2900, Loss: 0.11433251202106476
step: 3000, Loss: 0.11640885472297668
step: 3100, Loss: 0.11436381936073303
step: 3200, Loss: 0.11489414423704147
step: 3300, Loss: 0.11465908586978912
step: 3400, Loss: 0.11594805121421814
step: 3500, Loss: 0.11509428918361664
step: 3600, Loss: 0.11425527930259705
step: 3700, Loss: 0.11242663860321045
step: 3800, Loss: 0.11493701487779617
step: 3900, Loss: 0.1143961101770401
step: 4000, Loss: 0.11547771841287613
step: 4100, Loss: 0.11465160548686981
step: 4200, Loss: 0.11433236300945282
step: 4300, Loss: 0.11343765258789062
step: 4400, Loss: 0.11410702764987946
step: 4500, Loss: 0.11374590545892715
step: 4600, Loss: 0.11481216549873352
step: 4700, Loss: 0.11435539275407791
step: 4800, Loss: 0.11507077515125275
step: 4900, Loss: 0.11588357388973236
step: 5000, Loss: 0.1166829764842987
step: 5100, Loss: 0.11377904564142227
step: 5200, Loss: 0.11594787985086441
step: 5300, Loss: 0.11499172449111938
step: 5400, Loss: 0.11456157267093658
step: 5500, Loss: 0.11539773643016815
step: 5600, Loss: 0.11595183610916138
step: 5700, Loss: 0.11555013060569763
step: 5800, Loss: 0.11451940983533859
step: 5900, Loss: 0.11346013844013214
step: 6000, Loss: 0.1133798360824585
step: 6100, Loss: 0.11512423306703568
step: 6200, Loss: 0.1133393868803978
step: 6300, Loss: 0.11559142172336578
step: 6400, Loss: 0.11374161392450333
step: 6500, Loss: 0.11453849077224731
step: 6600, Loss: 0.11390164494514465
step: 6700, Loss: 0.11495673656463623
step: 6800, Loss: 0.11475972831249237
step: 6900, Loss: 0.11340644210577011
step: 7000, Loss: 0.11801120638847351
step: 7100, Loss: 0.9652289152145386
step: 7200, Loss: 0.15496495366096497
step: 7300, Loss: 0.14026981592178345
step: 7400, Loss: 0.1635032594203949
step: 7500, Loss: 0.12296566367149353
step: 7600, Loss: 0.1281156688928604
step: 7700, Loss: 0.12220160663127899
step: 7800, Loss: 0.12952065467834473
step: 7900, Loss: 0.1200862005352974
step: 8000, Loss: 0.12280456721782684
step: 8100, Loss: 0.11775669455528259
step: 8200, Loss: 0.12362492084503174
step: 8300, Loss: 0.11874761432409286
step: 8400, Loss: 0.1240270733833313
step: 8500, Loss: 0.1246727705001831
step: 8600, Loss: 0.12025035917758942
step: 8700, Loss: 0.11457814276218414
step: 8800, Loss: 0.12197668850421906
step: 8900, Loss: 0.11674250662326813
step: 9000, Loss: 0.11753430217504501
step: 9100, Loss: 0.11478190869092941
step: 9200, Loss: 0.1170385405421257
step: 9300, Loss: 0.11562734842300415
step: 9400, Loss: 0.122637540102005
step: 9500, Loss: 0.11817056685686111
step: 9600, Loss: 0.11662803590297699
step: 9700, Loss: 0.11605419218540192
step: 9800, Loss: 0.11559908092021942
step: 9900, Loss: 0.11539433151483536
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.8
recall: 0.5333333333333333
F_score: 0.64
******fold 5******

Training... train_data length:281
step: 0, Loss: 2.6022067070007324
step: 100, Loss: 0.11830288916826248
step: 200, Loss: 0.11830966919660568
step: 300, Loss: 0.11566399037837982
step: 400, Loss: 0.11572624742984772
step: 500, Loss: 0.11529404670000076
step: 600, Loss: 0.11726236343383789
step: 700, Loss: 0.11447679996490479
step: 800, Loss: 0.11443797498941422
step: 900, Loss: 0.11526677012443542
step: 1000, Loss: 0.11356481164693832
step: 1100, Loss: 0.11462973058223724
step: 1200, Loss: 0.11354003846645355
step: 1300, Loss: 0.11400631070137024
step: 1400, Loss: 0.11453435570001602
step: 1500, Loss: 0.11507386714220047
step: 1600, Loss: 0.11563679575920105
step: 1700, Loss: 0.11293832212686539
step: 1800, Loss: 0.11461133509874344
step: 1900, Loss: 0.11429312825202942
step: 2000, Loss: 0.11610039323568344
step: 2100, Loss: 0.11381063610315323
step: 2200, Loss: 0.11387995630502701
step: 2300, Loss: 0.11491496860980988
step: 2400, Loss: 0.11489948630332947
step: 2500, Loss: 0.11373624950647354
step: 2600, Loss: 0.11429356038570404
step: 2700, Loss: 0.1134808212518692
step: 2800, Loss: 0.11601515114307404
step: 2900, Loss: 0.11452433466911316
step: 3000, Loss: 0.11565256118774414
step: 3100, Loss: 0.11584928631782532
step: 3200, Loss: 0.11510883271694183
step: 3300, Loss: 0.11321614682674408
step: 3400, Loss: 0.11443091183900833
step: 3500, Loss: 0.11413486301898956
step: 3600, Loss: 0.11569422483444214
step: 3700, Loss: 0.11304701864719391
step: 3800, Loss: 0.11380594968795776
step: 3900, Loss: 0.11428652703762054
step: 4000, Loss: 0.11491428315639496
step: 4100, Loss: 0.11426714807748795
step: 4200, Loss: 0.11596120893955231
step: 4300, Loss: 0.11486174166202545
step: 4400, Loss: 0.11404816806316376
step: 4500, Loss: 0.11476373672485352
step: 4600, Loss: 0.11484203487634659
step: 4700, Loss: 0.1129176914691925
step: 4800, Loss: 0.1138821616768837
step: 4900, Loss: 0.11594868451356888
step: 5000, Loss: 0.11595321446657181
step: 5100, Loss: 0.11395232379436493
step: 5200, Loss: 0.11414659023284912
step: 5300, Loss: 0.11490552127361298
step: 5400, Loss: 0.11539941281080246
step: 5500, Loss: 0.11316242814064026
step: 5600, Loss: 0.11509917676448822
step: 5700, Loss: 0.11381088197231293
step: 5800, Loss: 0.11505376547574997
step: 5900, Loss: 0.1156976968050003
step: 6000, Loss: 0.116350457072258
step: 6100, Loss: 0.11428533494472504
step: 6200, Loss: 0.11467540264129639
step: 6300, Loss: 0.1139162927865982
step: 6400, Loss: 0.11522790044546127
step: 6500, Loss: 0.11585775762796402
step: 6600, Loss: 0.11632685363292694
step: 6700, Loss: 0.6152476668357849
step: 6800, Loss: 0.14184510707855225
step: 6900, Loss: 0.1293647140264511
step: 7000, Loss: 0.13456352055072784
step: 7100, Loss: 0.12307029962539673
step: 8800, Loss: 0.11246968805789948
step: 8900, Loss: 0.1139087975025177
step: 9000, Loss: 0.11429516971111298
step: 9100, Loss: 0.19196602702140808
step: 9200, Loss: 0.1132865697145462
step: 9300, Loss: 0.11368021368980408
step: 9400, Loss: 0.11321841925382614
step: 9500, Loss: 0.11386851221323013
step: 9600, Loss: 0.11691537499427795
step: 9700, Loss: 0.11350594460964203
step: 9800, Loss: 0.11319839954376221
step: 9900, Loss: 0.11308098584413528
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.8666666666666667
recall: 1.0
F_score: 0.9285714285714286
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.12981602549552917
step: 100, Loss: 0.11678092181682587
step: 200, Loss: 0.11840257048606873
step: 300, Loss: 0.1175115779042244
step: 400, Loss: 0.11351374536752701
step: 500, Loss: 0.11562182009220123
step: 600, Loss: 0.11508986353874207
step: 700, Loss: 0.11386463791131973
step: 800, Loss: 0.11382859945297241
step: 900, Loss: 0.11443442106246948
step: 1000, Loss: 0.1138601303100586
step: 1100, Loss: 0.1149420291185379
step: 1200, Loss: 0.11247898638248444
step: 1300, Loss: 0.11427046358585358
step: 1400, Loss: 0.11448792368173599
step: 1500, Loss: 0.19075408577919006
step: 1600, Loss: 0.11426493525505066
step: 1700, Loss: 0.1139855831861496
step: 1800, Loss: 0.11350446194410324
step: 1900, Loss: 0.1135735884308815
step: 2000, Loss: 0.11339905858039856
step: 2100, Loss: 0.11369647085666656
step: 2200, Loss: 0.11386334896087646
step: 2300, Loss: 0.1138075590133667
step: 2400, Loss: 0.114444300532341
step: 2500, Loss: 0.11407355964183807
step: 2600, Loss: 0.11364497989416122
step: 2700, Loss: 0.11362189054489136
step: 2800, Loss: 0.11384771764278412
step: 2900, Loss: 0.1186303049325943
step: 3000, Loss: 0.11607274413108826
step: 3100, Loss: 0.1150224506855011
step: 3200, Loss: 3.8709876537323
step: 3300, Loss: 0.25324228405952454
step: 3400, Loss: 0.2226720154285431
step: 3500, Loss: 0.12808345258235931
step: 3600, Loss: 0.13004985451698303
step: 3700, Loss: 0.1317613571882248
step: 3800, Loss: 0.12373509258031845
step: 3900, Loss: 0.1251942217350006
step: 4000, Loss: 0.12603095173835754
step: 4100, Loss: 0.12295769155025482
step: 4200, Loss: 0.12010836601257324
step: 4300, Loss: 0.11758330464363098
step: 4400, Loss: 0.12253836542367935
step: 4500, Loss: 0.12226512283086777
step: 4600, Loss: 0.11542758345603943
step: 4700, Loss: 0.11759494990110397
step: 4800, Loss: 0.11590138077735901
step: 4900, Loss: 0.1241350844502449
step: 5000, Loss: 0.11579137295484543
step: 5100, Loss: 0.11473193764686584
step: 5200, Loss: 0.11567503213882446
step: 5300, Loss: 0.19586890935897827
step: 5400, Loss: 0.11496774852275848
step: 5500, Loss: 0.11589886248111725
step: 5600, Loss: 0.11672628670930862
step: 5700, Loss: 0.11548615247011185
step: 5800, Loss: 0.11440275609493256
step: 5900, Loss: 0.11557435244321823
step: 6000, Loss: 0.11568498611450195
step: 6100, Loss: 0.11626621335744858
step: 6200, Loss: 0.11595824360847473
step: 6300, Loss: 0.11461398005485535
step: 6400, Loss: 0.1146102100610733
step: 6500, Loss: 0.11377403885126114
step: 6600, Loss: 0.11494926363229752
step: 6700, Loss: 0.11398141086101532
step: 6800, Loss: 0.1151425912976265
step: 6900, Loss: 0.1137155294418335
step: 7000, Loss: 0.11659461259841919
step: 7100, Loss: 0.11438401788473129
step: 7200, Loss: 0.19124293327331543
step: 7300, Loss: 0.11354347318410873
step: 7400, Loss: 0.11296688765287399
step: 7500, Loss: 0.11385855078697205
step: 7600, Loss: 0.11382966488599777
step: 7700, Loss: 0.11360906809568405
step: 7800, Loss: 0.11336595565080643
step: 7900, Loss: 0.11325466632843018
step: 8000, Loss: 0.11391027271747589
step: 8100, Loss: 0.11347703635692596
step: 8200, Loss: 0.11421758681535721
step: 8300, Loss: 0.11464262753725052
step: 8400, Loss: 0.1130182147026062
step: 8500, Loss: 0.1147974506020546
step: 8600, Loss: 0.1131962388753891
step: 8700, Loss: 0.11666445434093475
step: 8800, Loss: 0.11522682756185532
step: 8900, Loss: 0.11353001743555069
step: 9000, Loss: 0.113599494099617
step: 9100, Loss: 0.19411826133728027
step: 9200, Loss: 0.11425109207630157
step: 9300, Loss: 0.11627514660358429
step: 9400, Loss: 0.11416248232126236
step: 9500, Loss: 0.1130337342619896
step: 9600, Loss: 0.11806271970272064
step: 9700, Loss: 0.11597374826669693
step: 9800, Loss: 0.11605776101350784
step: 9900, Loss: 0.11389021575450897
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9285714285714286
recall: 1.0
F_score: 0.962962962962963
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11892221122980118
step: 100, Loss: 0.11833121627569199
step: 200, Loss: 0.11535532772541046
step: 300, Loss: 0.11782649904489517
step: 400, Loss: 0.11467985063791275
step: 500, Loss: 0.11557665467262268
step: 600, Loss: 0.11401781439781189
step: 700, Loss: 0.11555667221546173
step: 800, Loss: 0.11449302732944489
step: 900, Loss: 0.11370109021663666
step: 1000, Loss: 0.11363708972930908
step: 1100, Loss: 0.11460714042186737
step: 1200, Loss: 0.11355975270271301
step: 1300, Loss: 0.11476784944534302
step: 1400, Loss: 0.11382387578487396
step: 1500, Loss: 0.1925981640815735
step: 1600, Loss: 0.11356373876333237
step: 1700, Loss: 0.11339303106069565
step: 1800, Loss: 0.11443464457988739
step: 1900, Loss: 0.1145290732383728
step: 2000, Loss: 0.11419393867254257
step: 2100, Loss: 0.11347267031669617
step: 2200, Loss: 0.11605334281921387
step: 2300, Loss: 0.11575006693601608
step: 2400, Loss: 0.11364167928695679
step: 2500, Loss: 0.11351852118968964
step: 2600, Loss: 0.11438308656215668
step: 2700, Loss: 0.11592376977205276
step: 2800, Loss: 0.11457332968711853
step: 2900, Loss: 0.11414840817451477
step: 3000, Loss: 0.1146571934223175
step: 3100, Loss: 0.11432245373725891
step: 3200, Loss: 0.11355361342430115
step: 3300, Loss: 0.11387855559587479
step: 3400, Loss: 0.19591839611530304
step: 3500, Loss: 0.11318782716989517
step: 3600, Loss: 0.11503913998603821
step: 3700, Loss: 0.11498352140188217
step: 3800, Loss: 0.2594120502471924
step: 3900, Loss: 0.2359844297170639
step: 4000, Loss: 0.157979354262352
step: 4100, Loss: 0.13100245594978333
step: 4200, Loss: 0.1321888417005539
step: 4300, Loss: 0.12066467106342316
step: 4400, Loss: 0.12824970483779907
step: 4500, Loss: 0.12724724411964417
step: 4600, Loss: 0.12627410888671875
step: 4700, Loss: 0.12238779664039612
step: 4800, Loss: 0.12538975477218628
step: 4900, Loss: 0.119819276034832
step: 5000, Loss: 0.12099325656890869
step: 5100, Loss: 0.12301715463399887
step: 5200, Loss: 0.11853395402431488
step: 5300, Loss: 0.2028672844171524
step: 5400, Loss: 0.115433469414711
step: 5500, Loss: 0.11691600829362869
step: 5600, Loss: 0.12325375527143478
step: 5700, Loss: 0.11958396434783936
step: 5800, Loss: 0.11516153812408447
step: 5900, Loss: 0.11442379653453827
step: 6000, Loss: 0.11682608723640442
step: 6100, Loss: 0.11663602292537689
step: 6200, Loss: 0.11541657149791718
step: 6300, Loss: 0.11429882794618607
step: 6400, Loss: 0.1159229427576065
step: 6500, Loss: 0.11704041063785553
step: 6600, Loss: 0.11775857210159302
step: 6700, Loss: 0.11520113050937653
step: 6800, Loss: 0.11461199074983597
step: 6900, Loss: 0.11564823985099792
step: 7000, Loss: 0.11720483005046844
step: 7100, Loss: 0.11451780796051025
step: 7200, Loss: 0.19620978832244873
step: 7300, Loss: 0.11349916458129883
step: 7400, Loss: 0.11476510018110275
step: 7500, Loss: 0.11615797877311707
step: 7600, Loss: 0.11523115634918213
step: 7700, Loss: 0.11444718390703201
step: 7800, Loss: 0.11372323334217072
step: 7900, Loss: 0.11434804648160934
step: 8000, Loss: 0.1142706573009491
step: 8100, Loss: 0.11385621875524521
step: 8200, Loss: 0.11335588991641998
step: 8300, Loss: 0.11433131992816925
step: 8400, Loss: 0.1147090345621109
step: 8500, Loss: 0.11459139734506607
step: 8600, Loss: 0.11371838301420212
step: 8700, Loss: 0.11346692591905594
step: 8800, Loss: 0.11342724412679672
step: 8900, Loss: 0.11474274843931198
step: 9000, Loss: 0.11498995125293732
step: 9100, Loss: 0.19381380081176758
step: 9200, Loss: 0.11302460730075836
step: 9300, Loss: 0.11512419581413269
step: 7200, Loss: 0.1268717497587204
step: 7300, Loss: 0.11927895247936249
step: 7400, Loss: 0.12267617881298065
step: 7500, Loss: 0.12100232392549515
step: 7600, Loss: 0.12910887598991394
step: 7700, Loss: 0.11736074090003967
step: 7800, Loss: 0.1221972107887268
step: 7900, Loss: 0.11537084728479385
step: 8000, Loss: 0.11875627934932709
step: 8100, Loss: 0.11667772382497787
step: 8200, Loss: 0.11898167431354523
step: 8300, Loss: 0.11518756300210953
step: 8400, Loss: 0.11992849409580231
step: 8500, Loss: 0.11528339236974716
step: 8600, Loss: 0.11581233143806458
step: 8700, Loss: 0.11745869368314743
step: 8800, Loss: 0.12229986488819122
step: 8900, Loss: 0.11515520513057709
step: 9000, Loss: 0.11573352664709091
step: 9100, Loss: 0.11365845799446106
step: 9200, Loss: 0.11590194702148438
step: 9300, Loss: 0.11513790488243103
step: 9400, Loss: 0.11617646366357803
step: 9500, Loss: 0.11495451629161835
step: 9600, Loss: 0.11478835344314575
step: 9700, Loss: 0.11384974420070648
step: 9800, Loss: 0.11542030423879623
step: 9900, Loss: 0.11386615037918091
training successfully ended.
validating...
validate data length:31
acc: 0.6666666666666666
precision: 0.6666666666666666
recall: 0.8888888888888888
F_score: 0.761904761904762
******fold 6******

Training... train_data length:281
step: 0, Loss: 2.0987415313720703
step: 100, Loss: 0.11683037877082825
step: 200, Loss: 0.1168556958436966
step: 300, Loss: 0.11573134362697601
step: 400, Loss: 0.11742746084928513
step: 500, Loss: 0.11532396078109741
step: 600, Loss: 0.1150120422244072
step: 700, Loss: 0.11437499523162842
step: 800, Loss: 0.11499100178480148
step: 900, Loss: 0.1159362867474556
step: 1000, Loss: 0.11582096666097641
step: 1100, Loss: 0.11424301564693451
step: 1200, Loss: 0.11413999646902084
step: 1300, Loss: 0.11462315171957016
step: 1400, Loss: 0.11553296446800232
step: 1500, Loss: 0.11369172483682632
step: 1600, Loss: 0.11421844363212585
step: 1700, Loss: 0.11479125171899796
step: 1800, Loss: 0.11654544621706009
step: 1900, Loss: 0.11475101113319397
step: 2000, Loss: 0.11601702123880386
step: 2100, Loss: 0.11529373377561569
step: 2200, Loss: 0.11812327802181244
step: 2300, Loss: 0.11382211744785309
step: 2400, Loss: 0.11469055712223053
step: 2500, Loss: 0.11308878660202026
step: 2600, Loss: 0.11484253406524658
step: 2700, Loss: 0.11570388078689575
step: 2800, Loss: 0.1147826611995697
step: 2900, Loss: 0.11353255063295364
step: 3000, Loss: 0.11403889954090118
step: 3100, Loss: 0.11369501054286957
step: 3200, Loss: 0.11770395934581757
step: 3300, Loss: 0.1145668476819992
step: 3400, Loss: 0.11367225646972656
step: 3500, Loss: 0.11385183036327362
step: 3600, Loss: 0.11589441448450089
step: 3700, Loss: 0.11409907042980194
step: 3800, Loss: 0.11800556629896164
step: 3900, Loss: 0.11445392668247223
step: 4000, Loss: 0.11594302952289581
step: 4100, Loss: 0.1148006021976471
step: 4200, Loss: 0.11448252946138382
step: 4300, Loss: 0.11487168073654175
step: 4400, Loss: 0.11526486277580261
step: 4500, Loss: 0.11356634646654129
step: 4600, Loss: 0.11521921306848526
step: 4700, Loss: 0.11531518399715424
step: 4800, Loss: 0.11794888973236084
step: 4900, Loss: 0.1156676709651947
step: 5000, Loss: 0.11744660884141922
step: 5100, Loss: 0.11454637348651886
step: 5200, Loss: 0.11507678031921387
step: 5300, Loss: 0.11674809455871582
step: 5400, Loss: 0.11561637371778488
step: 5500, Loss: 0.11704028397798538
step: 5600, Loss: 0.11504017561674118
step: 5700, Loss: 0.11341703683137894
step: 5800, Loss: 0.11494674533605576
step: 5900, Loss: 0.11542703211307526
step: 6000, Loss: 0.11528858542442322
step: 6100, Loss: 0.11418894678354263
step: 6200, Loss: 0.11458716541528702
step: 6300, Loss: 1.4805784225463867
step: 6400, Loss: 0.1493963599205017
step: 6500, Loss: 0.1344400942325592
step: 6600, Loss: 0.12806865572929382
step: 6700, Loss: 0.12172006070613861
step: 6800, Loss: 0.12290261685848236
step: 6900, Loss: 0.12211048603057861
step: 7000, Loss: 0.11943620443344116
step: 7100, Loss: 0.12289358675479889
step: 7200, Loss: 0.12117043137550354
step: 7300, Loss: 0.12101346254348755
step: 7400, Loss: 0.11850187182426453
step: 7500, Loss: 0.11639480292797089
step: 7600, Loss: 0.12053164094686508
step: 7700, Loss: 0.119743213057518
step: 7800, Loss: 0.11763853579759598
step: 7900, Loss: 0.11623973399400711
step: 8000, Loss: 0.11434194445610046
step: 8100, Loss: 0.11510103940963745
step: 8200, Loss: 0.11801189184188843
step: 8300, Loss: 0.11447997391223907
step: 8400, Loss: 0.11666368693113327
step: 8500, Loss: 0.11421870440244675
step: 8600, Loss: 0.11702126264572144
step: 8700, Loss: 0.11578476428985596
step: 8800, Loss: 0.1159486472606659
step: 8900, Loss: 0.11579524725675583
step: 9000, Loss: 0.11583691835403442
step: 9100, Loss: 0.11357811838388443
step: 9200, Loss: 0.11478041112422943
step: 9300, Loss: 0.11436009407043457
step: 9400, Loss: 0.11383111774921417
step: 9500, Loss: 0.11419966071844101
step: 9600, Loss: 0.11897028982639313
step: 9700, Loss: 0.11508892476558685
step: 9800, Loss: 0.1147417426109314
step: 9900, Loss: 0.11558811366558075
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.9411764705882353
recall: 0.8888888888888888
F_score: 0.9142857142857143
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.18127095699310303
step: 100, Loss: 0.11778450757265091
step: 200, Loss: 0.11712156236171722
step: 300, Loss: 0.11616653949022293
step: 400, Loss: 0.1184849664568901
step: 500, Loss: 0.11742706596851349
step: 600, Loss: 0.11488702148199081
step: 700, Loss: 0.11409959197044373
step: 800, Loss: 0.11391505599021912
step: 900, Loss: 0.11415815353393555
step: 1000, Loss: 0.1162499338388443
step: 1100, Loss: 0.11551876366138458
step: 1200, Loss: 0.11498968303203583
step: 1300, Loss: 0.11635296791791916
step: 1400, Loss: 0.1147068664431572
step: 1500, Loss: 0.11625456809997559
step: 1600, Loss: 0.11478734016418457
step: 1700, Loss: 0.11464658379554749
step: 1800, Loss: 0.11410587280988693
step: 1900, Loss: 0.11656896770000458
step: 2000, Loss: 0.11477992683649063
step: 2100, Loss: 0.11491584777832031
step: 2200, Loss: 0.11643370240926743
step: 2300, Loss: 0.11349045485258102
step: 2400, Loss: 0.11447394639253616
step: 2500, Loss: 0.11538030207157135
step: 2600, Loss: 0.11485517770051956
step: 2700, Loss: 0.11376471817493439
step: 2800, Loss: 0.11437835544347763
step: 2900, Loss: 0.11447803676128387
step: 3000, Loss: 0.11445160955190659
step: 3100, Loss: 0.11346575617790222
step: 3200, Loss: 0.11442232877016068
step: 3300, Loss: 0.11371462792158127
step: 3400, Loss: 0.1152876615524292
step: 3500, Loss: 0.11377713829278946
step: 3600, Loss: 0.11450494825839996
step: 3700, Loss: 0.1154027208685875
step: 3800, Loss: 0.11427280306816101
step: 3900, Loss: 0.11459215730428696
step: 4000, Loss: 0.11498880386352539
step: 4100, Loss: 0.11872059106826782
step: 4200, Loss: 0.11955365538597107
step: 4300, Loss: 0.11372807621955872
step: 4400, Loss: 0.11336854100227356
step: 4500, Loss: 0.11393524706363678
step: 4600, Loss: 0.1164703369140625
step: 4700, Loss: 0.11340861022472382
step: 4800, Loss: 0.11543983221054077
step: 4900, Loss: 0.11434520035982132
step: 5000, Loss: 0.11451473832130432
step: 5100, Loss: 0.11510147154331207
step: 5200, Loss: 0.11475209891796112
step: 5300, Loss: 0.1149243637919426
step: 5400, Loss: 0.11455217748880386
step: 5500, Loss: 0.11380735039710999
step: 5600, Loss: 0.11331487447023392
step: 5700, Loss: 0.1132483035326004
step: 5800, Loss: 0.11610019207000732
step: 5900, Loss: 0.11548209935426712
step: 6000, Loss: 0.11391565203666687
step: 6100, Loss: 0.11538270115852356
step: 6200, Loss: 0.11688347160816193
step: 6300, Loss: 0.11423693597316742
step: 6400, Loss: 0.25269559025764465
step: 6500, Loss: 0.13048186898231506
step: 6600, Loss: 0.13009744882583618
step: 6700, Loss: 0.12078790366649628
step: 6800, Loss: 0.1240568459033966
step: 6900, Loss: 0.11924514174461365
step: 7000, Loss: 0.13186566531658173
step: 7100, Loss: 0.11754667013883591
step: 7200, Loss: 0.1265065222978592
step: 7300, Loss: 0.12056238949298859
step: 7400, Loss: 0.11810874193906784
step: 7500, Loss: 0.11862737685441971
step: 7600, Loss: 0.11865013092756271
step: 9400, Loss: 0.11358457803726196
step: 9500, Loss: 0.11350452899932861
step: 9600, Loss: 0.11433963477611542
step: 9700, Loss: 0.11322730034589767
step: 9800, Loss: 0.11330797523260117
step: 9900, Loss: 0.11370182782411575
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9318181818181818
recall: 1.0
F_score: 0.9647058823529412
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.12567850947380066
step: 100, Loss: 0.11709452420473099
step: 200, Loss: 0.12003970146179199
step: 300, Loss: 0.12007425725460052
step: 400, Loss: 0.11507630348205566
step: 500, Loss: 0.11550779640674591
step: 600, Loss: 0.11560758948326111
step: 700, Loss: 0.11520992964506149
step: 800, Loss: 0.11410781741142273
step: 900, Loss: 0.11519600450992584
step: 1000, Loss: 0.11394580453634262
step: 1100, Loss: 0.11381250619888306
step: 1200, Loss: 0.11476510018110275
step: 1300, Loss: 0.11312570422887802
step: 1400, Loss: 0.113835409283638
step: 1500, Loss: 0.1903759241104126
step: 1600, Loss: 0.11388759315013885
step: 1700, Loss: 0.11519396305084229
step: 1800, Loss: 0.11445771157741547
step: 1900, Loss: 0.11391274631023407
step: 2000, Loss: 0.11380825936794281
step: 2100, Loss: 0.11301711946725845
step: 2200, Loss: 0.1139959990978241
step: 2300, Loss: 0.11419650912284851
step: 2400, Loss: 0.1138654425740242
step: 2500, Loss: 0.11726834625005722
step: 2600, Loss: 0.11626623570919037
step: 2700, Loss: 0.1171661913394928
step: 2800, Loss: 0.11548106372356415
step: 2900, Loss: 0.11907672137022018
step: 3000, Loss: 0.11437608301639557
step: 3100, Loss: 0.1156206876039505
step: 3200, Loss: 0.1154949814081192
step: 3300, Loss: 0.1145666316151619
step: 3400, Loss: 0.1958751231431961
step: 3500, Loss: 0.11431502550840378
step: 3600, Loss: 0.11701041460037231
step: 3700, Loss: 0.11802460253238678
step: 3800, Loss: 0.11415740102529526
step: 3900, Loss: 0.11727769672870636
step: 4000, Loss: 0.11521515250205994
step: 4100, Loss: 0.11431561410427094
step: 4200, Loss: 0.1156219020485878
step: 4300, Loss: 0.11452922970056534
step: 4400, Loss: 0.11781558394432068
step: 4500, Loss: 0.11638109385967255
step: 4600, Loss: 1.3572932481765747
step: 4700, Loss: 0.14186683297157288
step: 4800, Loss: 0.13864780962467194
step: 4900, Loss: 0.1368267834186554
step: 5000, Loss: 0.12063457816839218
step: 5100, Loss: 0.12679427862167358
step: 5200, Loss: 0.12837474048137665
step: 5300, Loss: 0.21313248574733734
step: 5400, Loss: 0.12133652716875076
step: 5500, Loss: 0.12244901061058044
step: 5600, Loss: 0.12158378958702087
step: 5700, Loss: 0.11971701681613922
step: 5800, Loss: 0.12426452338695526
step: 5900, Loss: 0.11971118301153183
step: 6000, Loss: 0.11606066673994064
step: 6100, Loss: 0.11945749819278717
step: 6200, Loss: 0.12188617885112762
step: 6300, Loss: 0.11962592601776123
step: 6400, Loss: 0.11549121886491776
step: 6500, Loss: 0.11564482748508453
step: 6600, Loss: 0.11524083465337753
step: 6700, Loss: 0.12282828986644745
step: 6800, Loss: 0.11596052348613739
step: 6900, Loss: 0.11451289057731628
step: 7000, Loss: 0.11548013240098953
step: 7100, Loss: 0.11931074410676956
step: 7200, Loss: 0.19614370167255402
step: 7300, Loss: 0.11478124558925629
step: 7400, Loss: 0.11436722427606583
step: 7500, Loss: 0.11529940366744995
step: 7600, Loss: 0.11395284533500671
step: 7700, Loss: 0.11489033699035645
step: 7800, Loss: 0.11336586624383926
step: 7900, Loss: 0.11308424174785614
step: 8000, Loss: 0.11385795474052429
step: 8100, Loss: 0.11315059661865234
step: 8200, Loss: 0.11509378254413605
step: 8300, Loss: 0.11395832896232605
step: 8400, Loss: 0.11404210329055786
step: 8500, Loss: 0.11410196870565414
step: 8600, Loss: 0.11357349157333374
step: 8700, Loss: 0.11347353458404541
step: 8800, Loss: 0.11354511231184006
step: 8900, Loss: 0.11438494175672531
step: 9000, Loss: 0.11461232602596283
step: 9100, Loss: 0.19057318568229675
step: 9200, Loss: 0.11333861947059631
step: 9300, Loss: 0.1138126403093338
step: 9400, Loss: 0.11423259228467941
step: 9500, Loss: 0.11450985074043274
step: 9600, Loss: 0.11347091197967529
step: 9700, Loss: 0.11387839913368225
step: 9800, Loss: 0.11370506137609482
step: 9900, Loss: 0.11330379545688629
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.8974358974358975
recall: 1.0
F_score: 0.945945945945946
subject 12 Avgacc: 0.8944444444444443 Avgfscore: 0.909783118028187 
 Max acc:0.9583333333333334, Max f score:0.9647058823529412
******** mix subject_13 ********

[285, 475]
******fold 1******

Training... train_data length:855
step: 0, Loss: 44.09394073486328
step: 100, Loss: 5.938954830169678
step: 200, Loss: 1.3087440729141235
step: 300, Loss: 0.22310185432434082
step: 400, Loss: 0.22277745604515076
step: 500, Loss: 0.20107004046440125
step: 600, Loss: 0.15162117779254913
step: 700, Loss: 0.14810381829738617
step: 800, Loss: 0.16362115740776062
step: 900, Loss: 0.1445935219526291
step: 1000, Loss: 0.12871313095092773
step: 1100, Loss: 0.14814800024032593
step: 1200, Loss: 0.1413475126028061
step: 1300, Loss: 0.13841524720191956
step: 1400, Loss: 0.13590404391288757
step: 1500, Loss: 0.1281791478395462
step: 1600, Loss: 0.12487789988517761
step: 1700, Loss: 0.13945356011390686
step: 1800, Loss: 0.13417348265647888
step: 1900, Loss: 0.1294110119342804
step: 2000, Loss: 0.12874644994735718
step: 2100, Loss: 0.13732099533081055
step: 2200, Loss: 0.12624241411685944
step: 2300, Loss: 0.12941387295722961
step: 2400, Loss: 0.12532450258731842
step: 2500, Loss: 0.12133581936359406
step: 2600, Loss: 0.1261260062456131
step: 2700, Loss: 0.12181925773620605
step: 2800, Loss: 0.11742299795150757
step: 2900, Loss: 0.12230274826288223
step: 3000, Loss: 0.12680496275424957
step: 3100, Loss: 0.1210591271519661
step: 3200, Loss: 3.800654649734497
step: 3300, Loss: 0.6947141289710999
step: 3400, Loss: 0.14900977909564972
step: 3500, Loss: 0.15152281522750854
step: 3600, Loss: 0.15619541704654694
step: 3700, Loss: 0.13805195689201355
step: 3800, Loss: 0.136036217212677
step: 3900, Loss: 0.13873767852783203
step: 4000, Loss: 0.12398029863834381
step: 4100, Loss: 0.12372983992099762
step: 4200, Loss: 0.14242033660411835
step: 4300, Loss: 0.1290411353111267
step: 4400, Loss: 0.12262138724327087
step: 4500, Loss: 0.12934288382530212
step: 4600, Loss: 0.12284746021032333
step: 4700, Loss: 0.12201186269521713
step: 4800, Loss: 0.1373710185289383
step: 4900, Loss: 0.1239040419459343
step: 5000, Loss: 0.12114851176738739
step: 5100, Loss: 0.12710462510585785
step: 5200, Loss: 0.12315624952316284
step: 5300, Loss: 0.12005041539669037
step: 5400, Loss: 0.12932413816452026
step: 5500, Loss: 0.1221645176410675
step: 5600, Loss: 0.11990546435117722
step: 5700, Loss: 0.11911329627037048
step: 5800, Loss: 0.11618832498788834
step: 5900, Loss: 0.11896732449531555
step: 6000, Loss: 0.12501153349876404
step: 6100, Loss: 0.11972186714410782
step: 6200, Loss: 0.11865608394145966
step: 6300, Loss: 0.1214153990149498
step: 6400, Loss: 0.11419427394866943
step: 6500, Loss: 0.11563536524772644
step: 6600, Loss: 0.12253640592098236
step: 6700, Loss: 0.1189226284623146
step: 6800, Loss: 0.11903801560401917
step: 6900, Loss: 0.11788903921842575
step: 7000, Loss: 0.1361103355884552
step: 7100, Loss: 0.11726006865501404
step: 7200, Loss: 0.11943221837282181
step: 7300, Loss: 0.11763279139995575
step: 7400, Loss: 0.11798053234815598
step: 7500, Loss: 0.11625286936759949
step: 7600, Loss: 0.11458340287208557
step: 7700, Loss: 0.1162167638540268
step: 7800, Loss: 0.11737516522407532
step: 7900, Loss: 0.11568465828895569
step: 8000, Loss: 0.12044800072908401
step: 8100, Loss: 0.11839346587657928
step: 8200, Loss: 0.11525217443704605
step: 8300, Loss: 0.11651269346475601
step: 8400, Loss: 0.11939382553100586
step: 8500, Loss: 0.11687742173671722
step: 8600, Loss: 0.11768826097249985
step: 8700, Loss: 0.1163107305765152
step: 8800, Loss: 0.1170286238193512
step: 8900, Loss: 0.112823486328125
step: 9000, Loss: 0.11822816729545593
step: 9100, Loss: 0.11612889915704727
step: 9200, Loss: 0.11879371106624603
step: 9300, Loss: 0.11508674919605255
step: 9400, Loss: 0.11532966047525406
step: 9500, Loss: 0.11542566120624542
step: 9600, Loss: 0.11769169569015503
step: 9700, Loss: 0.11380591243505478
step: 9800, Loss: 0.11601903289556503
step: 9900, Loss: 0.11492446064949036
training successfully ended.
validating...
validate data length:95
acc: 0.9318181818181818
precision: 0.9038461538461539
recall: 0.9791666666666666
F_score: 0.9400000000000001
******fold 2******

Training... train_data length:855
step: 0, Loss: 0.12022776156663895
step: 100, Loss: 0.1245579943060875
step: 200, Loss: 0.11794707179069519
step: 300, Loss: 0.11867164820432663
step: 400, Loss: 0.11463381350040436
step: 500, Loss: 0.12141936272382736
step: 600, Loss: 0.11587512493133545
step: 700, Loss: 0.11543881893157959
step: 800, Loss: 0.11668342351913452
step: 900, Loss: 0.11511017382144928
step: 1000, Loss: 0.11438728868961334
step: 1100, Loss: 0.11856827884912491
step: 1200, Loss: 0.11508951336145401
step: 1300, Loss: 0.11856527626514435
step: 1400, Loss: 0.11440129578113556
step: 1500, Loss: 0.11545705050230026
step: 1600, Loss: 0.11528992652893066
step: 1700, Loss: 0.11499599367380142
step: 1800, Loss: 0.11516234278678894
step: 1900, Loss: 0.11364303529262543
step: 2000, Loss: 0.1146024838089943
step: 2100, Loss: 0.11600174009799957
step: 2200, Loss: 0.11427047848701477
step: 2300, Loss: 0.11662270128726959
step: 2400, Loss: 0.11334722489118576
step: 2500, Loss: 0.11615107953548431
step: 2600, Loss: 0.11543306708335876
step: 2700, Loss: 0.11427386105060577
step: 2800, Loss: 0.1149013563990593
step: 2900, Loss: 0.11516016721725464
step: 3000, Loss: 0.11530672013759613
step: 3100, Loss: 0.1156463548541069
step: 3200, Loss: 0.11886309832334518
step: 3300, Loss: 0.7417484521865845
step: 3400, Loss: 0.18085838854312897
step: 3500, Loss: 0.1424197107553482
step: 3600, Loss: 0.1337474286556244
step: 3700, Loss: 0.12686175107955933
step: 3800, Loss: 0.12705260515213013
step: 3900, Loss: 0.12265478074550629
step: 4000, Loss: 0.12229438871145248
step: 4100, Loss: 0.12344643473625183
step: 4200, Loss: 0.12278291583061218
step: 4300, Loss: 0.1209384948015213
step: 4400, Loss: 0.1192815899848938
step: 4500, Loss: 0.12030963599681854
step: 4600, Loss: 0.11963701248168945
step: 4700, Loss: 0.11954628676176071
step: 4800, Loss: 0.11830389499664307
step: 4900, Loss: 0.11890903860330582
step: 5000, Loss: 0.1188889667391777
step: 5100, Loss: 0.11605485528707504
step: 5200, Loss: 0.11635208874940872
step: 5300, Loss: 0.11950519680976868
step: 5400, Loss: 0.11719013005495071
step: 5500, Loss: 0.11590304970741272
step: 5600, Loss: 0.11559372395277023
step: 5700, Loss: 0.11517591774463654
step: 5800, Loss: 0.11602076143026352
step: 5900, Loss: 0.11730295419692993
step: 6000, Loss: 0.11704806983470917
step: 6100, Loss: 0.11617967486381531
step: 6200, Loss: 0.11850626021623611
step: 6300, Loss: 0.11573367565870285
step: 6400, Loss: 0.11394587904214859
step: 6500, Loss: 0.11572928726673126
step: 6600, Loss: 0.1144154816865921
step: 6700, Loss: 0.1152416542172432
step: 6800, Loss: 0.11559699475765228
step: 6900, Loss: 0.11477324366569519
step: 7000, Loss: 0.114610955119133
step: 7100, Loss: 0.11604182422161102
step: 7200, Loss: 0.11415316164493561
step: 7300, Loss: 0.11412705481052399
step: 7400, Loss: 0.1155422031879425
step: 7500, Loss: 0.11350040882825851
step: 7600, Loss: 0.11587335914373398
step: 7700, Loss: 0.11555559933185577
step: 7800, Loss: 0.11431965976953506
step: 7900, Loss: 0.11456742882728577
step: 8000, Loss: 0.11354333162307739
step: 8100, Loss: 0.11353300511837006
step: 8200, Loss: 0.11389794200658798
step: 8300, Loss: 0.11594866961240768
step: 8400, Loss: 0.11471032351255417
step: 8500, Loss: 0.11393101513385773
step: 8600, Loss: 0.11471288651227951
step: 8700, Loss: 0.11430556327104568
step: 8800, Loss: 0.11452844738960266
step: 8900, Loss: 0.11522451788187027
step: 9000, Loss: 0.1145932525396347
step: 9100, Loss: 0.11428352445363998
step: 9200, Loss: 0.1153792291879654
step: 9300, Loss: 0.11341602355241776
step: 9400, Loss: 0.11519382894039154
step: 9500, Loss: 0.11381645500659943
step: 9600, Loss: 0.11387988924980164
step: 9700, Loss: 0.1145385205745697
step: 9800, Loss: 0.11560579389333725
step: 9900, Loss: 0.11424330621957779
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 0.926829268292683
recall: 1.0
F_score: 0.9620253164556963
******fold 3******

Training... train_data length:855
step: 0, Loss: 0.6701538562774658
step: 100, Loss: 0.11809093505144119
step: 200, Loss: 0.11616739630699158
step: 300, Loss: 0.11459147930145264
step: 400, Loss: 0.11580870300531387
step: 500, Loss: 0.11627237498760223
step: 600, Loss: 0.11413590610027313
step: 700, Loss: 0.1140948235988617
step: 800, Loss: 0.11408892273902893
step: 900, Loss: 0.11398432403802872
step: 1000, Loss: 0.11424785852432251
step: 1100, Loss: 0.11634395271539688
step: 1200, Loss: 0.11268295347690582
step: 1300, Loss: 0.1135210394859314
step: 1400, Loss: 0.11336696892976761
step: 1500, Loss: 0.11376998573541641
step: 1600, Loss: 0.11426106095314026
step: 1700, Loss: 0.11550083756446838
step: 1800, Loss: 0.11338245123624802
step: 1900, Loss: 0.11411266028881073
step: 2000, Loss: 0.11503444612026215
step: 2100, Loss: 0.11402396112680435
step: 2200, Loss: 0.11422661691904068
step: 2300, Loss: 0.11414094269275665
step: 2400, Loss: 0.11461781710386276
step: 2500, Loss: 0.11579899489879608
step: 2600, Loss: 0.11575984209775925
step: 2700, Loss: 0.11469689756631851
step: 2800, Loss: 0.11415722966194153
step: 2900, Loss: 0.11365173012018204
step: 3000, Loss: 0.11737845838069916
step: 3100, Loss: 0.11614947021007538
step: 3200, Loss: 0.11494316905736923
step: 3300, Loss: 0.11825425922870636
step: 3400, Loss: 0.11514521390199661
step: 3500, Loss: 0.11848488450050354
step: 3600, Loss: 0.11760272085666656
step: 3700, Loss: 0.11729523539543152
step: 3800, Loss: 0.11383640021085739
step: 3900, Loss: 0.11520594358444214
step: 4000, Loss: 0.11738987267017365
step: 4100, Loss: 0.1141895055770874
step: 4200, Loss: 0.11397546529769897
step: 4300, Loss: 0.11484282463788986
step: 4400, Loss: 0.11652477085590363
step: 4500, Loss: 0.11434157937765121
step: 4600, Loss: 0.11637081205844879
step: 4700, Loss: 0.2591060400009155
step: 4800, Loss: 0.1746511161327362
step: 4900, Loss: 0.1337282359600067
step: 5000, Loss: 0.13623356819152832
step: 5100, Loss: 0.13352292776107788
step: 5200, Loss: 0.12857654690742493
step: 5300, Loss: 0.1287156045436859
step: 5400, Loss: 0.12454360723495483
step: 5500, Loss: 0.1220766007900238
step: 5600, Loss: 0.12406009435653687
step: 5700, Loss: 0.12110653519630432
step: 5800, Loss: 0.1199425682425499
step: 5900, Loss: 0.12014058232307434
step: 6000, Loss: 0.11875174194574356
step: 6100, Loss: 0.12152770161628723
step: 6200, Loss: 0.11764995753765106
step: 6300, Loss: 0.11951785534620285
step: 6400, Loss: 0.11804698407649994
step: 6500, Loss: 0.11846516281366348
step: 6600, Loss: 0.1153612732887268
step: 6700, Loss: 0.11744429171085358
step: 6800, Loss: 0.11745841801166534
step: 6900, Loss: 0.11784738302230835
step: 7000, Loss: 0.11676301062107086
step: 7100, Loss: 0.11506928503513336
step: 7200, Loss: 0.11721177399158478
step: 7300, Loss: 0.11626717448234558
step: 7400, Loss: 0.1167723760008812
step: 7500, Loss: 0.11465026438236237
step: 7600, Loss: 0.11614011973142624
step: 7700, Loss: 0.11729218065738678
step: 7800, Loss: 0.11553028970956802
step: 7900, Loss: 0.11443217098712921
step: 8000, Loss: 0.11340148746967316
step: 8100, Loss: 0.11546273529529572
step: 8200, Loss: 0.11577336490154266
step: 8300, Loss: 0.11557309329509735
step: 8400, Loss: 0.11570262908935547
step: 8500, Loss: 0.11596386879682541
step: 8600, Loss: 0.11516552418470383
step: 8700, Loss: 0.11462756991386414
step: 8800, Loss: 0.11456240713596344
step: 8900, Loss: 0.11448942869901657
step: 9000, Loss: 0.11457444727420807
step: 9100, Loss: 0.11339713633060455
step: 9200, Loss: 0.11654938012361526
step: 9300, Loss: 0.11289358139038086
step: 9400, Loss: 0.11480124294757843
step: 9500, Loss: 0.11395404487848282
step: 9600, Loss: 0.11591725051403046
step: 9700, Loss: 0.8906199336051941
step: 9800, Loss: 0.14430618286132812
step: 9900, Loss: 0.1307477056980133
step: 7700, Loss: 0.11476904898881912
step: 7800, Loss: 0.12095895409584045
step: 7900, Loss: 0.11530537903308868
step: 8000, Loss: 0.11442729830741882
step: 8100, Loss: 0.11623948812484741
step: 8200, Loss: 0.11757819354534149
step: 8300, Loss: 0.11573784798383713
step: 8400, Loss: 0.11946241557598114
step: 8500, Loss: 0.11569532752037048
step: 8600, Loss: 0.11670319736003876
step: 8700, Loss: 0.11383864283561707
step: 8800, Loss: 0.11492077261209488
step: 8900, Loss: 0.11539279669523239
step: 9000, Loss: 0.11575973033905029
step: 9100, Loss: 0.11448338627815247
step: 9200, Loss: 0.11480299383401871
step: 9300, Loss: 0.11448565125465393
step: 9400, Loss: 0.11476530879735947
step: 9500, Loss: 0.11507101356983185
step: 9600, Loss: 0.11815911531448364
step: 9700, Loss: 0.11317912489175797
step: 9800, Loss: 0.11420466005802155
step: 9900, Loss: 0.11447207629680634
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9333333333333333
recall: 0.8235294117647058
F_score: 0.8749999999999999
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.16982054710388184
step: 100, Loss: 0.11773687601089478
step: 200, Loss: 0.11505397409200668
step: 300, Loss: 0.11656481772661209
step: 400, Loss: 0.11508392542600632
step: 500, Loss: 0.11629705131053925
step: 600, Loss: 0.11456657201051712
step: 700, Loss: 0.11594561487436295
step: 800, Loss: 0.11710675805807114
step: 900, Loss: 0.11514569818973541
step: 1000, Loss: 0.11380138993263245
step: 1100, Loss: 0.11257540434598923
step: 1200, Loss: 0.11673334240913391
step: 1300, Loss: 0.11547736823558807
step: 1400, Loss: 0.1151326522231102
step: 1500, Loss: 0.11318853497505188
step: 1600, Loss: 0.11404572427272797
step: 1700, Loss: 0.11375438421964645
step: 1800, Loss: 0.11566412448883057
step: 1900, Loss: 0.1147681176662445
step: 2000, Loss: 0.11392823606729507
step: 2100, Loss: 0.11363554745912552
step: 2200, Loss: 0.11407332867383957
step: 2300, Loss: 0.1148865818977356
step: 2400, Loss: 0.11447260528802872
step: 2500, Loss: 0.11586882174015045
step: 2600, Loss: 0.1134500503540039
step: 2700, Loss: 0.11602577567100525
step: 2800, Loss: 0.11556768417358398
step: 2900, Loss: 0.11369147896766663
step: 3000, Loss: 0.11577372997999191
step: 3100, Loss: 0.11397863924503326
step: 3200, Loss: 0.1138799637556076
step: 3300, Loss: 0.11596964299678802
step: 3400, Loss: 0.11519132554531097
step: 3500, Loss: 0.11401423811912537
step: 3600, Loss: 0.11416199058294296
step: 3700, Loss: 0.11469565331935883
step: 3800, Loss: 0.11384864896535873
step: 3900, Loss: 0.11369352787733078
step: 4000, Loss: 0.11564989387989044
step: 4100, Loss: 0.1143641471862793
step: 4200, Loss: 0.1160288006067276
step: 4300, Loss: 0.1167140081524849
step: 4400, Loss: 0.11460062861442566
step: 4500, Loss: 0.11434413492679596
step: 4600, Loss: 0.11694159358739853
step: 4700, Loss: 0.11500602215528488
step: 4800, Loss: 0.11405376344919205
step: 4900, Loss: 0.11399556696414948
step: 5000, Loss: 0.1167457103729248
step: 5100, Loss: 0.11331365257501602
step: 5200, Loss: 0.11688048392534256
step: 5300, Loss: 0.11469623446464539
step: 5400, Loss: 0.11756071448326111
step: 5500, Loss: 0.11623847484588623
step: 5600, Loss: 0.11501891165971756
step: 5700, Loss: 0.1166388988494873
step: 5800, Loss: 0.11393270641565323
step: 5900, Loss: 0.11292542517185211
step: 6000, Loss: 0.11399735510349274
step: 6100, Loss: 0.11599627137184143
step: 6200, Loss: 0.11379536986351013
step: 6300, Loss: 0.11272366344928741
step: 6400, Loss: 0.1167442724108696
step: 6500, Loss: 0.11490949243307114
step: 6600, Loss: 0.11556704342365265
step: 6700, Loss: 0.22971653938293457
step: 6800, Loss: 0.13019150495529175
step: 6900, Loss: 0.13367028534412384
step: 7000, Loss: 0.12694060802459717
step: 7100, Loss: 0.12211477011442184
step: 7200, Loss: 0.12236843258142471
step: 7300, Loss: 0.1177157461643219
step: 7400, Loss: 0.11984563618898392
step: 7500, Loss: 0.11925346404314041
step: 7600, Loss: 0.12044376134872437
step: 7700, Loss: 0.11719122529029846
step: 7800, Loss: 0.11743441969156265
step: 7900, Loss: 0.11483358591794968
step: 8000, Loss: 0.11698075383901596
step: 8100, Loss: 0.11568042635917664
step: 8200, Loss: 0.11697231233119965
step: 8300, Loss: 0.11520971357822418
step: 8400, Loss: 0.1158728301525116
step: 8500, Loss: 0.11661121994256973
step: 8600, Loss: 0.11710972338914871
step: 8700, Loss: 0.11834308505058289
step: 8800, Loss: 0.11495824158191681
step: 8900, Loss: 0.11553583294153214
step: 9000, Loss: 0.11695843935012817
step: 9100, Loss: 0.114470936357975
step: 9200, Loss: 0.1160132884979248
step: 9300, Loss: 0.11550143361091614
step: 9400, Loss: 0.11416053026914597
step: 9500, Loss: 0.11632887274026871
step: 9600, Loss: 0.1144791841506958
step: 9700, Loss: 0.1142244040966034
step: 9800, Loss: 0.11490801721811295
step: 9900, Loss: 0.11465896666049957
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9230769230769231
recall: 0.8
F_score: 0.8571428571428571
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.16934429109096527
step: 100, Loss: 0.11714355647563934
step: 200, Loss: 0.11723259091377258
step: 300, Loss: 0.11514520645141602
step: 400, Loss: 0.11532606184482574
step: 500, Loss: 0.11590389907360077
step: 600, Loss: 0.11542122066020966
step: 700, Loss: 0.1167139858007431
step: 800, Loss: 0.11621247231960297
step: 900, Loss: 0.11692728847265244
step: 1000, Loss: 0.11573265492916107
step: 1100, Loss: 0.11447270214557648
step: 1200, Loss: 0.11490034312009811
step: 1300, Loss: 0.11402472853660583
step: 1400, Loss: 0.11539129912853241
step: 1500, Loss: 0.11421891301870346
step: 1600, Loss: 0.11487292498350143
step: 1700, Loss: 0.11352820694446564
step: 1800, Loss: 0.11543732136487961
step: 1900, Loss: 0.11563677340745926
step: 2000, Loss: 0.1149686798453331
step: 2100, Loss: 0.11389724165201187
step: 2200, Loss: 0.11518979072570801
step: 2300, Loss: 0.11388000100851059
step: 2400, Loss: 0.11497574299573898
step: 2500, Loss: 0.11369992792606354
step: 2600, Loss: 0.1155766099691391
step: 2700, Loss: 0.11456012725830078
step: 2800, Loss: 0.11475790292024612
step: 2900, Loss: 0.11787831783294678
step: 3000, Loss: 0.11440777778625488
step: 3100, Loss: 0.11458873748779297
step: 3200, Loss: 0.11517585068941116
step: 3300, Loss: 0.11517743021249771
step: 3400, Loss: 0.11368921399116516
step: 3500, Loss: 0.1173037737607956
step: 3600, Loss: 0.11718939244747162
step: 3700, Loss: 0.11294098198413849
step: 3800, Loss: 0.11460647732019424
step: 3900, Loss: 0.11507062613964081
step: 4000, Loss: 0.11381229758262634
step: 4100, Loss: 0.11618918180465698
step: 4200, Loss: 0.11548006534576416
step: 4300, Loss: 0.11535575240850449
step: 4400, Loss: 0.11472480744123459
step: 4500, Loss: 0.11394338309764862
step: 4600, Loss: 0.11496733129024506
step: 4700, Loss: 0.11427567899227142
step: 4800, Loss: 0.11382615566253662
step: 4900, Loss: 0.1144838035106659
step: 5000, Loss: 0.11471005529165268
step: 5100, Loss: 0.11332443356513977
step: 5200, Loss: 0.1150716170668602
step: 5300, Loss: 0.11419612914323807
step: 5400, Loss: 0.11735290288925171
step: 5500, Loss: 0.11387624591588974
step: 5600, Loss: 0.11421159654855728
step: 5700, Loss: 0.11492328345775604
step: 5800, Loss: 0.11622248589992523
step: 5900, Loss: 0.1142469048500061
step: 6000, Loss: 0.11583675444126129
step: 6100, Loss: 0.11422738432884216
step: 6200, Loss: 0.11494233459234238
step: 6300, Loss: 0.11360339820384979
step: 6400, Loss: 0.11552297323942184
step: 6500, Loss: 0.11605070531368256
step: 6600, Loss: 0.11559219658374786
step: 6700, Loss: 0.11516723036766052
step: 6800, Loss: 5.182737827301025
step: 6900, Loss: 0.12808701395988464
step: 7000, Loss: 0.1238948255777359
step: 7100, Loss: 0.1219714879989624
step: 7200, Loss: 0.12465417385101318
step: 7300, Loss: 0.1220848485827446
step: 7400, Loss: 0.12699100375175476
step: 7500, Loss: 0.11988475173711777
step: 7600, Loss: 0.11827899515628815
step: 7700, Loss: 0.11540853977203369
step: 7800, Loss: 0.11841108649969101
step: 7900, Loss: 0.12149108946323395
step: 8000, Loss: 0.1208440363407135
step: 8100, Loss: 0.1158197671175003
training successfully ended.
validating...
validate data length:95
acc: 0.9772727272727273
precision: 1.0
recall: 0.9523809523809523
F_score: 0.975609756097561
******fold 4******

Training... train_data length:855
step: 0, Loss: 0.3342633545398712
step: 100, Loss: 0.13241468369960785
step: 200, Loss: 0.12580503523349762
step: 300, Loss: 0.12389720231294632
step: 400, Loss: 0.11885952949523926
step: 500, Loss: 0.12030427157878876
step: 600, Loss: 0.12016431242227554
step: 700, Loss: 0.11668258905410767
step: 800, Loss: 0.11747843027114868
step: 900, Loss: 0.12233108282089233
step: 1000, Loss: 0.11953286826610565
step: 1100, Loss: 0.11482895910739899
step: 1200, Loss: 0.11561242491006851
step: 1300, Loss: 0.11717063188552856
step: 1400, Loss: 0.11906290054321289
step: 1500, Loss: 0.11475824564695358
step: 1600, Loss: 0.11754535138607025
step: 1700, Loss: 0.11334144324064255
step: 1800, Loss: 0.11542665213346481
step: 1900, Loss: 0.11474098265171051
step: 2000, Loss: 0.11638836562633514
step: 2100, Loss: 0.11526110768318176
step: 2200, Loss: 0.11671140789985657
step: 2300, Loss: 0.11515302956104279
step: 2400, Loss: 0.11276575177907944
step: 2500, Loss: 0.11703521013259888
step: 2600, Loss: 0.11449962854385376
step: 2700, Loss: 0.11436035484075546
step: 2800, Loss: 0.1145210862159729
step: 2900, Loss: 0.11377378553152084
step: 3000, Loss: 0.11440423130989075
step: 3100, Loss: 0.1146620661020279
step: 3200, Loss: 0.11440152674913406
step: 3300, Loss: 0.11537417024374008
step: 3400, Loss: 0.1141965389251709
step: 3500, Loss: 0.11392201483249664
step: 3600, Loss: 0.11423899233341217
step: 3700, Loss: 0.11404386162757874
step: 3800, Loss: 0.11507061868906021
step: 3900, Loss: 0.11384208500385284
step: 4000, Loss: 0.11300935596227646
step: 4100, Loss: 0.11472770571708679
step: 4200, Loss: 0.11373026669025421
step: 4300, Loss: 0.11493159830570221
step: 4400, Loss: 0.11418537050485611
step: 4500, Loss: 4.032734394073486
step: 4600, Loss: 0.15940113365650177
step: 4700, Loss: 0.14140520989894867
step: 4800, Loss: 0.13844013214111328
step: 4900, Loss: 0.1274982988834381
step: 5000, Loss: 0.12947528064250946
step: 5100, Loss: 0.12295234203338623
step: 5200, Loss: 0.11908948421478271
step: 5300, Loss: 0.12152951955795288
step: 5400, Loss: 0.12040573358535767
step: 5500, Loss: 0.11863988637924194
step: 5600, Loss: 0.11991717666387558
step: 5700, Loss: 0.11977745592594147
step: 5800, Loss: 0.11437813937664032
step: 5900, Loss: 0.11895845830440521
step: 6000, Loss: 0.11989323049783707
step: 6100, Loss: 0.11647806316614151
step: 6200, Loss: 0.11830011010169983
step: 6300, Loss: 0.11536575853824615
step: 6400, Loss: 0.11536110192537308
step: 6500, Loss: 0.11838202178478241
step: 6600, Loss: 0.11570626497268677
step: 6700, Loss: 0.11668409407138824
step: 6800, Loss: 0.11505365371704102
step: 6900, Loss: 0.11904074996709824
step: 7000, Loss: 0.11441908776760101
step: 7100, Loss: 0.11440588533878326
step: 7200, Loss: 0.11463292688131332
step: 7300, Loss: 0.11586257070302963
step: 7400, Loss: 0.11478018760681152
step: 7500, Loss: 0.11433229595422745
step: 7600, Loss: 0.11428473889827728
step: 7700, Loss: 0.11465851217508316
step: 7800, Loss: 0.11426807940006256
step: 7900, Loss: 0.11398039013147354
step: 8000, Loss: 0.11397894471883774
step: 8100, Loss: 0.1151319369673729
step: 8200, Loss: 0.11413052678108215
step: 8300, Loss: 0.11427268385887146
step: 8400, Loss: 0.11472108215093613
step: 8500, Loss: 0.11513473093509674
step: 8600, Loss: 0.11401915550231934
step: 8700, Loss: 0.11460007727146149
step: 8800, Loss: 0.11473296582698822
step: 8900, Loss: 0.1151595488190651
step: 9000, Loss: 0.1149887889623642
step: 9100, Loss: 0.1151033490896225
step: 9200, Loss: 0.11353596299886703
step: 9300, Loss: 0.11474651843309402
step: 9400, Loss: 0.11308994144201279
step: 9500, Loss: 0.11332931369543076
step: 9600, Loss: 0.11377131193876266
step: 9700, Loss: 0.11334351450204849
step: 9800, Loss: 0.11600364744663239
step: 9900, Loss: 0.1132846474647522
training successfully ended.
validating...
validate data length:95
acc: 0.9772727272727273
precision: 0.9761904761904762
recall: 0.9761904761904762
F_score: 0.9761904761904762
******fold 5******

Training... train_data length:855
step: 0, Loss: 0.289612352848053
step: 100, Loss: 0.132653146982193
step: 200, Loss: 0.12461039423942566
step: 300, Loss: 0.12056566029787064
step: 400, Loss: 0.11898216605186462
step: 500, Loss: 0.1189694032073021
step: 600, Loss: 0.1213110014796257
step: 700, Loss: 0.12291218340396881
step: 800, Loss: 0.1175866425037384
step: 900, Loss: 0.11579311639070511
step: 1000, Loss: 0.11814889311790466
step: 1100, Loss: 0.1170017272233963
step: 1200, Loss: 0.11552207916975021
step: 1300, Loss: 0.11394528299570084
step: 1400, Loss: 0.11575005948543549
step: 1500, Loss: 0.11586125940084457
step: 1600, Loss: 0.11626803874969482
step: 1700, Loss: 0.11645394563674927
step: 1800, Loss: 0.1144966408610344
step: 1900, Loss: 0.11554200202226639
step: 2000, Loss: 0.11605880409479141
step: 2100, Loss: 0.11495647579431534
step: 2200, Loss: 0.11467812955379486
step: 2300, Loss: 0.11368869990110397
step: 2400, Loss: 0.11564174294471741
step: 2500, Loss: 0.11669989675283432
step: 2600, Loss: 0.11700451374053955
step: 2700, Loss: 0.11553159356117249
step: 2800, Loss: 0.11422400176525116
step: 2900, Loss: 0.11386041343212128
step: 3000, Loss: 0.11357875168323517
step: 3100, Loss: 0.11484164744615555
step: 3200, Loss: 0.11392410844564438
step: 3300, Loss: 0.11620211601257324
step: 3400, Loss: 0.11553692817687988
step: 3500, Loss: 0.11625033617019653
step: 3600, Loss: 0.11454883962869644
step: 3700, Loss: 0.11311495304107666
step: 3800, Loss: 0.11513462662696838
step: 3900, Loss: 0.11377943307161331
step: 4000, Loss: 0.11512121558189392
step: 4100, Loss: 0.11444098502397537
step: 4200, Loss: 0.11319757997989655
step: 4300, Loss: 0.11456319689750671
step: 4400, Loss: 0.1145077645778656
step: 4500, Loss: 0.11367037892341614
step: 4600, Loss: 0.11449456959962845
step: 4700, Loss: 0.226394921541214
step: 4800, Loss: 0.20692750811576843
step: 4900, Loss: 0.129920095205307
step: 5000, Loss: 0.12516462802886963
step: 5100, Loss: 0.12252399325370789
step: 5200, Loss: 0.12648184597492218
step: 5300, Loss: 0.12082934379577637
step: 5400, Loss: 0.12544384598731995
step: 5500, Loss: 0.11782780289649963
step: 5600, Loss: 0.11990686506032944
step: 5700, Loss: 0.11678072810173035
step: 5800, Loss: 0.1181294173002243
step: 5900, Loss: 0.11714110523462296
step: 6000, Loss: 0.11889658868312836
step: 6100, Loss: 0.11667737364768982
step: 6200, Loss: 0.11695238202810287
step: 6300, Loss: 0.11580076813697815
step: 6400, Loss: 0.11842840164899826
step: 6500, Loss: 0.11594966053962708
step: 6600, Loss: 0.1172129437327385
step: 6700, Loss: 0.11836358159780502
step: 6800, Loss: 0.1158871054649353
step: 6900, Loss: 0.11464489996433258
step: 7000, Loss: 0.11787210404872894
step: 7100, Loss: 0.11526936292648315
step: 7200, Loss: 0.11817456036806107
step: 7300, Loss: 0.11575429141521454
step: 7400, Loss: 0.1143430545926094
step: 7500, Loss: 0.11503785848617554
step: 7600, Loss: 0.1148412674665451
step: 7700, Loss: 0.11568610370159149
step: 7800, Loss: 0.11620043963193893
step: 7900, Loss: 0.11425764113664627
step: 8000, Loss: 0.11451663076877594
step: 8100, Loss: 0.11436954140663147
step: 8200, Loss: 0.11470343917608261
step: 8300, Loss: 0.11420075595378876
step: 8400, Loss: 0.11602655053138733
step: 8500, Loss: 0.11512699723243713
step: 8600, Loss: 0.11661238968372345
step: 8700, Loss: 0.11361641436815262
step: 8800, Loss: 0.1137429028749466
step: 8900, Loss: 0.11421619355678558
step: 9000, Loss: 0.11472573131322861
step: 9100, Loss: 0.11413241922855377
step: 9200, Loss: 0.11624614894390106
step: 9300, Loss: 0.11374935507774353
step: 9400, Loss: 0.11544878035783768
step: 9500, Loss: 0.11425943672657013
step: 9600, Loss: 0.11604798585176468
step: 9700, Loss: 0.1145426481962204
step: 9800, Loss: 0.11479935795068741
step: 9900, Loss: 0.11372064054012299
training successfully ended.
validating...
validate data length:95
acc: 0.9886363636363636
precision: 1.0
recall: 0.9807692307692307
F_score: 0.9902912621359222
******fold 6******

step: 8200, Loss: 0.11772837489843369
step: 8300, Loss: 0.11751513183116913
step: 8400, Loss: 0.11793138086795807
step: 8500, Loss: 0.1178632527589798
step: 8600, Loss: 0.11632408201694489
step: 8700, Loss: 0.11512807011604309
step: 8800, Loss: 0.11827798187732697
step: 8900, Loss: 0.11483001708984375
step: 9000, Loss: 0.1185750663280487
step: 9100, Loss: 0.11478595435619354
step: 9200, Loss: 0.11800862848758698
step: 9300, Loss: 0.11606188118457794
step: 9400, Loss: 0.11580236256122589
step: 9500, Loss: 0.1150389015674591
step: 9600, Loss: 0.11538100242614746
step: 9700, Loss: 0.1147322729229927
step: 9800, Loss: 0.11624807864427567
step: 9900, Loss: 0.1157875806093216
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.8571428571428571
recall: 0.7058823529411765
F_score: 0.7741935483870968
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.15837429463863373
step: 100, Loss: 0.11957216262817383
step: 200, Loss: 0.11892375349998474
step: 300, Loss: 0.11648045480251312
step: 400, Loss: 0.11719156056642532
step: 500, Loss: 0.11604724079370499
step: 600, Loss: 0.11457668989896774
step: 700, Loss: 0.11816798150539398
step: 800, Loss: 0.11459928750991821
step: 900, Loss: 0.11457926779985428
step: 1000, Loss: 0.11549746990203857
step: 1100, Loss: 0.11329242587089539
step: 1200, Loss: 0.11577580869197845
step: 1300, Loss: 0.11856891214847565
step: 1400, Loss: 0.11538548767566681
step: 1500, Loss: 0.11370334774255753
step: 1600, Loss: 0.11391344666481018
step: 1700, Loss: 0.1154770478606224
step: 1800, Loss: 0.11597852408885956
step: 1900, Loss: 0.11497853696346283
step: 2000, Loss: 0.11438624560832977
step: 2100, Loss: 0.11549340933561325
step: 2200, Loss: 0.11352281272411346
step: 2300, Loss: 0.11327636986970901
step: 2400, Loss: 0.11416304111480713
step: 2500, Loss: 0.11475835740566254
step: 2600, Loss: 0.11497916281223297
step: 2700, Loss: 0.11440587043762207
step: 2800, Loss: 0.11363222450017929
step: 2900, Loss: 0.1159122884273529
step: 3000, Loss: 0.11471385508775711
step: 3100, Loss: 0.11334824562072754
step: 3200, Loss: 0.11389937996864319
step: 3300, Loss: 0.11467055231332779
step: 3400, Loss: 0.11670056730508804
step: 3500, Loss: 0.11362430453300476
step: 3600, Loss: 0.11551117897033691
step: 3700, Loss: 0.11542672663927078
step: 3800, Loss: 0.11437799781560898
step: 3900, Loss: 0.11343264579772949
step: 4000, Loss: 0.11551467329263687
step: 4100, Loss: 0.11566488444805145
step: 4200, Loss: 0.11398329585790634
step: 4300, Loss: 0.11484555900096893
step: 4400, Loss: 0.11385130137205124
step: 4500, Loss: 0.11424588412046432
step: 4600, Loss: 0.1168127954006195
step: 4700, Loss: 0.11533806473016739
step: 4800, Loss: 0.11526310443878174
step: 4900, Loss: 0.11408041417598724
step: 5000, Loss: 0.11314515769481659
step: 5100, Loss: 0.11433431506156921
step: 5200, Loss: 0.11515240371227264
step: 5300, Loss: 0.11386757344007492
step: 5400, Loss: 0.11289746314287186
step: 5500, Loss: 0.11494710296392441
step: 5600, Loss: 0.11500103771686554
step: 5700, Loss: 0.11412861198186874
step: 5800, Loss: 0.11349853128194809
step: 5900, Loss: 0.11378414183855057
step: 6000, Loss: 0.11407619714736938
step: 6100, Loss: 0.1151837408542633
step: 6200, Loss: 0.8962106704711914
step: 6300, Loss: 0.14686617255210876
step: 6400, Loss: 0.134405255317688
step: 6500, Loss: 0.12412310391664505
step: 6600, Loss: 0.12798625230789185
step: 6700, Loss: 0.12138321995735168
step: 6800, Loss: 0.13599175214767456
step: 6900, Loss: 0.12133586406707764
step: 7000, Loss: 0.12226516008377075
step: 7100, Loss: 0.11545547097921371
step: 7200, Loss: 0.1187334954738617
step: 7300, Loss: 0.11790259927511215
step: 7400, Loss: 0.12260617315769196
step: 7500, Loss: 0.11730538308620453
step: 7600, Loss: 0.1172940582036972
step: 7700, Loss: 0.11882956326007843
step: 7800, Loss: 0.1179775595664978
step: 7900, Loss: 0.11636687815189362
step: 8000, Loss: 0.11675029247999191
step: 8100, Loss: 0.11491268128156662
step: 8200, Loss: 0.11519457399845123
step: 8300, Loss: 0.11521974205970764
step: 8400, Loss: 0.11637914180755615
step: 8500, Loss: 0.1148643046617508
step: 8600, Loss: 0.11858803033828735
step: 8700, Loss: 0.11462530493736267
step: 8800, Loss: 0.11615295708179474
step: 8900, Loss: 0.11676614731550217
step: 9000, Loss: 0.1151031106710434
step: 9100, Loss: 0.11532403528690338
step: 9200, Loss: 0.11650031805038452
step: 9300, Loss: 0.11395308375358582
step: 9400, Loss: 0.11557479202747345
step: 9500, Loss: 0.11635982245206833
step: 9600, Loss: 0.11940081417560577
step: 9700, Loss: 0.11444152891635895
step: 9800, Loss: 0.11551207304000854
step: 9900, Loss: 0.1150968074798584
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7058823529411765
recall: 1.0
F_score: 0.8275862068965517
subject 12 Avgacc: 0.7614583333333332 Avgfscore: 0.768799187649577 
 Max acc:0.9, Max f score:0.9142857142857143
******** mix subject_13 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 46.82318878173828
step: 100, Loss: 2.552271604537964
step: 200, Loss: 0.1572331041097641
step: 300, Loss: 0.1365222930908203
step: 400, Loss: 0.13863244652748108
step: 500, Loss: 0.1412113755941391
step: 600, Loss: 0.13193009793758392
step: 700, Loss: 0.12746384739875793
step: 800, Loss: 0.1303154081106186
step: 900, Loss: 0.12997443974018097
step: 1000, Loss: 0.12147399038076401
step: 1100, Loss: 0.1212640106678009
step: 1200, Loss: 0.11843304336071014
step: 1300, Loss: 0.11932483315467834
step: 1400, Loss: 0.1651909351348877
step: 1500, Loss: 0.13596467673778534
step: 1600, Loss: 0.135778546333313
step: 1700, Loss: 0.13317610323429108
step: 1800, Loss: 0.13523200154304504
step: 1900, Loss: 0.13037273287773132
step: 2000, Loss: 0.1301378309726715
step: 2100, Loss: 0.123650461435318
step: 2200, Loss: 0.12442527711391449
step: 2300, Loss: 0.1295069307088852
step: 2400, Loss: 0.12230129539966583
step: 2500, Loss: 0.11786855012178421
step: 2600, Loss: 0.11830278486013412
step: 2700, Loss: 0.1219119280576706
step: 2800, Loss: 0.11954216659069061
step: 2900, Loss: 0.12149961292743683
step: 3000, Loss: 0.11874055862426758
step: 3100, Loss: 0.11718487739562988
step: 3200, Loss: 0.11615195870399475
step: 3300, Loss: 0.12089036405086517
step: 3400, Loss: 0.11589467525482178
step: 3500, Loss: 0.11830584704875946
step: 3600, Loss: 0.11612037569284439
step: 3700, Loss: 0.12056691944599152
step: 3800, Loss: 0.11526860296726227
step: 3900, Loss: 0.11782333999872208
step: 4000, Loss: 0.11547739803791046
step: 4100, Loss: 0.11874356865882874
step: 4200, Loss: 0.11775296181440353
step: 4300, Loss: 0.11863989382982254
step: 4400, Loss: 0.1172570139169693
step: 4500, Loss: 0.11793908476829529
step: 4600, Loss: 0.11611738055944443
step: 4700, Loss: 0.11719459295272827
step: 4800, Loss: 0.11382690072059631
step: 4900, Loss: 0.11604949086904526
step: 5000, Loss: 0.11438102275133133
step: 5100, Loss: 0.11664148420095444
step: 5200, Loss: 0.11397473514080048
step: 5300, Loss: 0.11796658486127853
step: 5400, Loss: 0.11370498687028885
step: 5500, Loss: 0.12211839854717255
step: 5600, Loss: 0.11685800552368164
step: 5700, Loss: 0.1160094141960144
step: 5800, Loss: 0.11441513895988464
step: 5900, Loss: 0.12058644741773605
step: 6000, Loss: 0.11382611095905304
step: 6100, Loss: 0.11635153740644455
step: 6200, Loss: 0.11478421092033386
step: 6300, Loss: 0.11578421294689178
step: 6400, Loss: 0.11468596011400223
step: 6500, Loss: 0.12030408531427383
step: 6600, Loss: 0.11402493715286255
step: 6700, Loss: 0.11557668447494507
step: 6800, Loss: 0.11612321436405182
step: 6900, Loss: 0.11564839631319046
step: 7000, Loss: 1.408386468887329
step: 7100, Loss: 0.15740378201007843
step: 7200, Loss: 0.13688188791275024
step: 7300, Loss: 0.13367696106433868
step: 7400, Loss: 0.13181909918785095
step: 7500, Loss: 0.12794415652751923
step: 7600, Loss: 0.12476544082164764
step: 7700, Loss: 0.12245668470859528
step: 7800, Loss: 0.12358862906694412
step: 7900, Loss: 0.12042651325464249
step: 8000, Loss: 0.11919510364532471
step: 8100, Loss: 0.11983009427785873
step: 8200, Loss: 0.11901094019412994
Training... train_data length:855
step: 0, Loss: 0.25001460313796997
step: 100, Loss: 0.11669273674488068
step: 200, Loss: 0.11550647765398026
step: 300, Loss: 0.11439112573862076
step: 400, Loss: 0.11421284079551697
step: 500, Loss: 0.11365694552659988
step: 600, Loss: 0.11523911356925964
step: 700, Loss: 0.11569230258464813
step: 800, Loss: 0.11520331352949142
step: 900, Loss: 0.11387471854686737
step: 1000, Loss: 0.1136239692568779
step: 1100, Loss: 0.11396336555480957
step: 1200, Loss: 0.114070363342762
step: 1300, Loss: 0.11474468559026718
step: 1400, Loss: 0.11418382823467255
step: 1500, Loss: 0.11487401276826859
step: 1600, Loss: 0.1140010803937912
step: 1700, Loss: 0.11387611925601959
step: 1800, Loss: 0.11444730311632156
step: 1900, Loss: 0.11531999707221985
step: 2000, Loss: 0.11332046240568161
step: 2100, Loss: 0.11352988332509995
step: 2200, Loss: 0.11380606889724731
step: 2300, Loss: 0.11465874314308167
step: 2400, Loss: 0.11438820511102676
step: 2500, Loss: 0.1132957711815834
step: 2600, Loss: 0.11337917298078537
step: 2700, Loss: 0.11430548876523972
step: 2800, Loss: 0.1138729527592659
step: 2900, Loss: 0.11432147026062012
step: 3000, Loss: 0.1136990338563919
step: 3100, Loss: 0.11618103086948395
step: 3200, Loss: 0.11370399594306946
step: 3300, Loss: 0.1148543506860733
step: 3400, Loss: 0.11444365978240967
step: 3500, Loss: 0.11425258964300156
step: 3600, Loss: 0.11337704956531525
step: 3700, Loss: 0.11302976310253143
step: 3800, Loss: 0.1162368506193161
step: 3900, Loss: 0.11269326508045197
step: 4000, Loss: 0.11277750879526138
step: 4100, Loss: 0.11433951556682587
step: 4200, Loss: 0.11424151808023453
step: 4300, Loss: 0.11481254547834396
step: 4400, Loss: 0.11532401293516159
step: 4500, Loss: 0.11379006505012512
step: 4600, Loss: 0.1149214506149292
step: 4700, Loss: 0.113444022834301
step: 4800, Loss: 0.11801087111234665
step: 4900, Loss: 0.5427414178848267
step: 5000, Loss: 0.14665165543556213
step: 5100, Loss: 0.13563980162143707
step: 5200, Loss: 0.12720626592636108
step: 5300, Loss: 0.1256171315908432
step: 5400, Loss: 0.12308241426944733
step: 5500, Loss: 0.1232011616230011
step: 5600, Loss: 0.12191323935985565
step: 5700, Loss: 0.11857414245605469
step: 5800, Loss: 0.11906185001134872
step: 5900, Loss: 0.11941183358430862
step: 6000, Loss: 0.1201000064611435
step: 6100, Loss: 0.12056350708007812
step: 6200, Loss: 0.1179288998246193
step: 6300, Loss: 0.11687853932380676
step: 6400, Loss: 0.11810201406478882
step: 6500, Loss: 0.11838597059249878
step: 6600, Loss: 0.11658964306116104
step: 6700, Loss: 0.11725625395774841
step: 6800, Loss: 0.11645473539829254
step: 6900, Loss: 0.11567693948745728
step: 7000, Loss: 0.1146438792347908
step: 7100, Loss: 0.11724182963371277
step: 7200, Loss: 0.11681883037090302
step: 7300, Loss: 0.116364024579525
step: 7400, Loss: 0.11851859092712402
step: 7500, Loss: 0.11471016705036163
step: 7600, Loss: 0.11792895942926407
step: 7700, Loss: 0.11533674597740173
step: 7800, Loss: 0.11697950214147568
step: 7900, Loss: 0.11467444151639938
step: 8000, Loss: 0.11477335542440414
step: 8100, Loss: 0.11444513499736786
step: 8200, Loss: 0.11447308212518692
step: 8300, Loss: 0.11470507830381393
step: 8400, Loss: 0.11427419632673264
step: 8500, Loss: 0.11323695629835129
step: 8600, Loss: 0.11418326944112778
step: 8700, Loss: 0.11331799626350403
step: 8800, Loss: 0.11314520984888077
step: 8900, Loss: 0.11395218968391418
step: 9000, Loss: 0.11530133336782455
step: 9100, Loss: 0.11373730003833771
step: 9200, Loss: 0.11432848870754242
step: 9300, Loss: 0.11486802995204926
step: 9400, Loss: 0.11513712257146835
step: 9500, Loss: 0.11306800693273544
step: 9600, Loss: 0.11454556882381439
step: 9700, Loss: 0.11427421122789383
step: 9800, Loss: 0.1139155849814415
step: 9900, Loss: 0.11365804076194763
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:855
step: 0, Loss: 0.11591053009033203
step: 100, Loss: 0.11644700169563293
step: 200, Loss: 0.1326121985912323
step: 300, Loss: 0.1184995174407959
step: 400, Loss: 0.6320089101791382
step: 500, Loss: 0.14051216840744019
step: 600, Loss: 0.12215279042720795
step: 700, Loss: 0.1280292272567749
step: 800, Loss: 0.11998847872018814
step: 900, Loss: 0.11964823305606842
step: 1000, Loss: 0.11838935315608978
step: 1100, Loss: 0.11895760893821716
step: 1200, Loss: 0.11601065844297409
step: 1300, Loss: 0.11690575629472733
step: 1400, Loss: 0.11729066073894501
step: 1500, Loss: 0.11706370860338211
step: 1600, Loss: 0.11608856171369553
step: 1700, Loss: 0.11632102727890015
step: 1800, Loss: 0.11418863385915756
step: 1900, Loss: 0.11723244190216064
step: 2000, Loss: 0.11497102677822113
step: 2100, Loss: 0.11698579788208008
step: 2200, Loss: 0.11456788331270218
step: 2300, Loss: 0.1145409345626831
step: 2400, Loss: 0.1151096522808075
step: 2500, Loss: 0.11639948934316635
step: 2600, Loss: 0.11428765952587128
step: 2700, Loss: 0.11538641899824142
step: 2800, Loss: 0.11351556330919266
step: 2900, Loss: 0.11335569620132446
step: 3000, Loss: 0.11437474191188812
step: 3100, Loss: 0.1166205182671547
step: 3200, Loss: 0.11617531627416611
step: 3300, Loss: 0.11496811360120773
step: 3400, Loss: 0.11377235502004623
step: 3500, Loss: 0.11553432792425156
step: 3600, Loss: 0.11586818844079971
step: 3700, Loss: 0.11383642256259918
step: 3800, Loss: 0.11410130560398102
step: 3900, Loss: 0.1136179119348526
step: 4000, Loss: 0.11376268416643143
step: 4100, Loss: 0.1140391081571579
step: 4200, Loss: 0.11354435980319977
step: 4300, Loss: 0.11364483833312988
step: 4400, Loss: 0.1137450709939003
step: 4500, Loss: 0.11377431452274323
step: 4600, Loss: 0.11488120257854462
step: 4700, Loss: 0.11437016725540161
step: 4800, Loss: 0.11482931673526764
step: 4900, Loss: 0.11398965120315552
step: 5000, Loss: 0.11384281516075134
step: 5100, Loss: 0.11488772183656693
step: 5200, Loss: 0.11358141899108887
step: 5300, Loss: 0.11276829987764359
step: 5400, Loss: 0.11308314651250839
step: 5500, Loss: 0.1132662445306778
step: 5600, Loss: 0.1133069396018982
step: 5700, Loss: 0.11370163410902023
step: 5800, Loss: 0.11536356806755066
step: 5900, Loss: 0.11515524238348007
step: 6000, Loss: 0.146176278591156
step: 6100, Loss: 0.4077373147010803
step: 6200, Loss: 0.13781581819057465
step: 6300, Loss: 0.12078328430652618
step: 6400, Loss: 0.12385492026805878
step: 6500, Loss: 0.12086968123912811
step: 6600, Loss: 0.1220216155052185
step: 6700, Loss: 0.12077683955430984
step: 6800, Loss: 0.1195283830165863
step: 6900, Loss: 0.11526664346456528
step: 7000, Loss: 0.11860780417919159
step: 7100, Loss: 0.11810028553009033
step: 7200, Loss: 0.11657358705997467
step: 7300, Loss: 0.11576460301876068
step: 7400, Loss: 0.11646071821451187
step: 7500, Loss: 0.11553782224655151
step: 7600, Loss: 0.11565621942281723
step: 7700, Loss: 0.11646084487438202
step: 7800, Loss: 0.1130475252866745
step: 7900, Loss: 0.1157103180885315
step: 8000, Loss: 0.11656726896762848
step: 8100, Loss: 0.11453771591186523
step: 8200, Loss: 0.1162102073431015
step: 8300, Loss: 0.11459241807460785
step: 8400, Loss: 0.11438460648059845
step: 8500, Loss: 0.11671712249517441
step: 8600, Loss: 0.11523854732513428
step: 8700, Loss: 0.11303429305553436
step: 8800, Loss: 0.11536511033773422
step: 8900, Loss: 0.11462762951850891
step: 9000, Loss: 0.11406517773866653
step: 9100, Loss: 0.11590278893709183
step: 9200, Loss: 0.11516831815242767
step: 9300, Loss: 0.1139201819896698
step: 9400, Loss: 0.11493903398513794
step: 9500, Loss: 0.11443517357110977
step: 9600, Loss: 0.11492646485567093
step: 9700, Loss: 0.11402949690818787
step: 9800, Loss: 0.11454314738512039
step: 9900, Loss: 0.11360553652048111
training successfully ended.
validating...
validate data length:95
acc: 0.9772727272727273
precision: 0.9782608695652174
recall: 0.9782608695652174
F_score: 0.9782608695652174
******fold 8******

Training... train_data length:855
step: 0, Loss: 0.11546596884727478
step: 100, Loss: 0.11802671104669571
step: 200, Loss: 0.119304358959198
step: 300, Loss: 0.11422564089298248
step: 400, Loss: 0.11431485414505005
step: 500, Loss: 0.11604747176170349
step: 8300, Loss: 0.12093682587146759
step: 8400, Loss: 0.1196589544415474
step: 8500, Loss: 0.11845173686742783
step: 8600, Loss: 0.11621163785457611
step: 8700, Loss: 0.11985252797603607
step: 8800, Loss: 0.1173272356390953
step: 8900, Loss: 0.119438037276268
step: 9000, Loss: 0.11463236808776855
step: 9100, Loss: 0.11750409007072449
step: 9200, Loss: 0.11616742610931396
step: 9300, Loss: 0.11623445153236389
step: 9400, Loss: 0.11697722971439362
step: 9500, Loss: 0.11984626948833466
step: 9600, Loss: 0.11688034236431122
step: 9700, Loss: 0.11986708641052246
step: 9800, Loss: 0.11664026975631714
step: 9900, Loss: 0.11561199277639389
training successfully ended.
validating...
validate data length:32
acc: 0.46875
precision: 0.35294117647058826
recall: 0.5
F_score: 0.41379310344827586
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.20651128888130188
step: 100, Loss: 0.13333731889724731
step: 200, Loss: 0.12252122163772583
step: 300, Loss: 0.12282523512840271
step: 400, Loss: 0.12196618318557739
step: 500, Loss: 0.11983733624219894
step: 600, Loss: 0.11917947232723236
step: 700, Loss: 0.12001537531614304
step: 800, Loss: 0.11678628623485565
step: 900, Loss: 0.11752871423959732
step: 1000, Loss: 0.11755461245775223
step: 1100, Loss: 0.11869953572750092
step: 1200, Loss: 0.1147584393620491
step: 1300, Loss: 0.11760276556015015
step: 1400, Loss: 0.11544527858495712
step: 1500, Loss: 0.11677546799182892
step: 1600, Loss: 0.11553995311260223
step: 1700, Loss: 0.11526408046483994
step: 1800, Loss: 0.11792735010385513
step: 1900, Loss: 0.11675068736076355
step: 2000, Loss: 0.11421534419059753
step: 2100, Loss: 0.11387437582015991
step: 2200, Loss: 0.1151382178068161
step: 2300, Loss: 0.11680677533149719
step: 2400, Loss: 0.11637591570615768
step: 2500, Loss: 0.1141817718744278
step: 2600, Loss: 0.11438125371932983
step: 2700, Loss: 0.11556417495012283
step: 2800, Loss: 0.11492882668972015
step: 2900, Loss: 0.1189383938908577
step: 3000, Loss: 0.11371281743049622
step: 3100, Loss: 0.1148034930229187
step: 3200, Loss: 0.11559474468231201
step: 3300, Loss: 0.11561333388090134
step: 3400, Loss: 0.11633056402206421
step: 3500, Loss: 0.113991379737854
step: 3600, Loss: 0.11455884575843811
step: 3700, Loss: 0.12193702906370163
step: 3800, Loss: 0.12483742833137512
step: 3900, Loss: 0.11492566764354706
step: 4000, Loss: 0.12061047554016113
step: 4100, Loss: 0.11492592096328735
step: 4200, Loss: 0.11557459831237793
step: 4300, Loss: 0.11537213623523712
step: 4400, Loss: 0.11474627256393433
step: 4500, Loss: 0.11556658148765564
step: 4600, Loss: 0.1190844401717186
step: 4700, Loss: 0.1149141862988472
step: 4800, Loss: 0.11484412103891373
step: 4900, Loss: 0.11464536190032959
step: 5000, Loss: 0.11471042037010193
step: 5100, Loss: 0.11578691750764847
step: 5200, Loss: 0.11612696945667267
step: 5300, Loss: 0.11564528197050095
step: 5400, Loss: 0.11386911571025848
step: 5500, Loss: 0.1151401549577713
step: 5600, Loss: 0.1130804494023323
step: 5700, Loss: 0.11335942149162292
step: 5800, Loss: 0.11546143889427185
step: 5900, Loss: 0.11580819636583328
step: 6000, Loss: 0.11525517702102661
step: 6100, Loss: 0.11571938544511795
step: 6200, Loss: 0.11458240449428558
step: 6300, Loss: 0.17118582129478455
step: 6400, Loss: 0.1284223049879074
step: 6500, Loss: 0.125844344496727
step: 6600, Loss: 0.1206468716263771
step: 6700, Loss: 0.12039702385663986
step: 6800, Loss: 0.11796283721923828
step: 6900, Loss: 0.12528705596923828
step: 7000, Loss: 0.12256835401058197
step: 7100, Loss: 0.1245955377817154
step: 7200, Loss: 0.11834442615509033
step: 7300, Loss: 0.1357208490371704
step: 7400, Loss: 0.11835181713104248
step: 7500, Loss: 0.11938916146755219
step: 7600, Loss: 0.11790154129266739
step: 7700, Loss: 0.12057168781757355
step: 7800, Loss: 0.11929759383201599
step: 7900, Loss: 0.12005992233753204
step: 8000, Loss: 0.11595484614372253
step: 8100, Loss: 0.11669860780239105
step: 8200, Loss: 0.11524148285388947
step: 8300, Loss: 0.11647555232048035
step: 8400, Loss: 0.11849647760391235
step: 8500, Loss: 0.11810623854398727
step: 8600, Loss: 0.11518870294094086
step: 8700, Loss: 0.12074971199035645
step: 8800, Loss: 0.11557915806770325
step: 8900, Loss: 0.11628710478544235
step: 9000, Loss: 0.1167936697602272
step: 9100, Loss: 0.11674898117780685
step: 9200, Loss: 0.11478288471698761
step: 9300, Loss: 0.11566347628831863
step: 9400, Loss: 0.11483120918273926
step: 9500, Loss: 0.11682110279798508
step: 9600, Loss: 0.11495108902454376
step: 9700, Loss: 0.11390170454978943
step: 9800, Loss: 0.11389224976301193
step: 9900, Loss: 0.11592323333024979
training successfully ended.
validating...
validate data length:32
acc: 0.84375
precision: 0.7894736842105263
recall: 0.9375
F_score: 0.8571428571428572
******fold 3******

Training... train_data length:281
step: 0, Loss: 2.4969186782836914
step: 100, Loss: 0.12025294452905655
step: 200, Loss: 0.11785237491130829
step: 300, Loss: 0.11631335318088531
step: 400, Loss: 0.11930319666862488
step: 500, Loss: 0.11567381024360657
step: 600, Loss: 0.11522440612316132
step: 700, Loss: 0.11453872919082642
step: 800, Loss: 0.1147012710571289
step: 900, Loss: 0.11592856049537659
step: 1000, Loss: 0.11455407738685608
step: 1100, Loss: 0.11462453752756119
step: 1200, Loss: 0.11560944467782974
step: 1300, Loss: 0.1171901598572731
step: 1400, Loss: 0.11569535732269287
step: 1500, Loss: 0.11513090133666992
step: 1600, Loss: 0.11519282311201096
step: 1700, Loss: 0.11410172283649445
step: 1800, Loss: 0.11536283791065216
step: 1900, Loss: 0.11393418908119202
step: 2000, Loss: 0.11552207171916962
step: 2100, Loss: 0.11453588306903839
step: 2200, Loss: 0.11389319598674774
step: 2300, Loss: 0.11584579199552536
step: 2400, Loss: 0.114209845662117
step: 2500, Loss: 0.11642944812774658
step: 2600, Loss: 0.11544854938983917
step: 2700, Loss: 0.11568721383810043
step: 2800, Loss: 0.1149965152144432
step: 2900, Loss: 0.11340067535638809
step: 3000, Loss: 0.11589032411575317
step: 3100, Loss: 0.11706100404262543
step: 3200, Loss: 0.11613888293504715
step: 3300, Loss: 0.11499141901731491
step: 3400, Loss: 0.1148354560136795
step: 3500, Loss: 0.11408679187297821
step: 3600, Loss: 0.11595392972230911
step: 3700, Loss: 0.11546453088521957
step: 3800, Loss: 0.11461086571216583
step: 3900, Loss: 0.11429059505462646
step: 4000, Loss: 0.11552215367555618
step: 4100, Loss: 0.11546605080366135
step: 4200, Loss: 0.1143174096941948
step: 4300, Loss: 0.11608521640300751
step: 4400, Loss: 0.11591096967458725
step: 4500, Loss: 0.11467999219894409
step: 4600, Loss: 0.11565317213535309
step: 4700, Loss: 0.11555005609989166
step: 4800, Loss: 0.11639269441366196
step: 4900, Loss: 0.11779404431581497
step: 5000, Loss: 0.1141851544380188
step: 5100, Loss: 0.11701004207134247
step: 5200, Loss: 0.9071637392044067
step: 5300, Loss: 0.1471359133720398
step: 5400, Loss: 0.13228657841682434
step: 5500, Loss: 0.12666654586791992
step: 5600, Loss: 0.1251508891582489
step: 5700, Loss: 0.12049825489521027
step: 5800, Loss: 0.12283307313919067
step: 5900, Loss: 0.1197924017906189
step: 6000, Loss: 0.12077832967042923
step: 6100, Loss: 0.11884292960166931
step: 6200, Loss: 0.12038401514291763
step: 6300, Loss: 0.1168552115559578
step: 6400, Loss: 0.11772389709949493
step: 6500, Loss: 0.11502374708652496
step: 6600, Loss: 0.11687512695789337
step: 6700, Loss: 0.11835260689258575
step: 6800, Loss: 0.11729858815670013
step: 6900, Loss: 0.11673562973737717
step: 7000, Loss: 0.11766761541366577
step: 7100, Loss: 0.11501946300268173
step: 7200, Loss: 0.11728019267320633
step: 7300, Loss: 0.11536620557308197
step: 7400, Loss: 0.11909942328929901
step: 7500, Loss: 0.1230277419090271
step: 7600, Loss: 0.11771565675735474
step: 7700, Loss: 0.11447019129991531
step: 7800, Loss: 0.11441081762313843
step: 7900, Loss: 0.11641939729452133
step: 8000, Loss: 0.11360723525285721
step: 8100, Loss: 0.11827616393566132
step: 8200, Loss: 0.1159626841545105
step: 8300, Loss: 0.11367838084697723
step: 8400, Loss: 0.11437797546386719
step: 8500, Loss: 0.11744332313537598
step: 8600, Loss: 0.11442703753709793
step: 8700, Loss: 0.11508646607398987
step: 8800, Loss: 0.11488589644432068
step: 600, Loss: 0.11418168991804123
step: 700, Loss: 0.11455289274454117
step: 800, Loss: 0.1150282472372055
step: 900, Loss: 0.11386612802743912
step: 1000, Loss: 0.11600015312433243
step: 1100, Loss: 0.11447486281394958
step: 1200, Loss: 0.11612491309642792
step: 1300, Loss: 0.1149829626083374
step: 1400, Loss: 0.11604722589254379
step: 1500, Loss: 0.11545245349407196
step: 1600, Loss: 0.11417414247989655
step: 1700, Loss: 0.11468394100666046
step: 1800, Loss: 0.11415039002895355
step: 1900, Loss: 2.433946371078491
step: 2000, Loss: 0.13720101118087769
step: 2100, Loss: 0.12200050055980682
step: 2200, Loss: 0.1207524910569191
step: 2300, Loss: 0.12284794449806213
step: 2400, Loss: 0.11882814764976501
step: 2500, Loss: 0.12107112258672714
step: 2600, Loss: 0.11761817336082458
step: 2700, Loss: 0.12338192015886307
step: 2800, Loss: 0.11647951602935791
step: 2900, Loss: 0.11684338003396988
step: 3000, Loss: 0.11747442930936813
step: 3100, Loss: 0.1161172017455101
step: 3200, Loss: 0.11655404418706894
step: 3300, Loss: 0.1173877865076065
step: 3400, Loss: 0.11555252224206924
step: 3500, Loss: 0.11664030700922012
step: 3600, Loss: 0.1147918552160263
step: 3700, Loss: 0.11564639955759048
step: 3800, Loss: 0.11846651136875153
step: 3900, Loss: 0.11534672975540161
step: 4000, Loss: 0.11535950750112534
step: 4100, Loss: 0.11728748679161072
step: 4200, Loss: 0.11548548936843872
step: 4300, Loss: 0.11705543100833893
step: 4400, Loss: 0.11475716531276703
step: 4500, Loss: 0.11425696313381195
step: 4600, Loss: 0.11481063067913055
step: 4700, Loss: 0.11401166021823883
step: 4800, Loss: 0.11441797018051147
step: 4900, Loss: 0.11488349735736847
step: 5000, Loss: 0.11367401480674744
step: 5100, Loss: 0.11438079178333282
step: 5200, Loss: 0.1150306835770607
step: 5300, Loss: 0.11355777084827423
step: 5400, Loss: 0.11542215943336487
step: 5500, Loss: 0.1133996769785881
step: 5600, Loss: 0.11611886322498322
step: 5700, Loss: 0.11378120630979538
step: 5800, Loss: 0.1149999350309372
step: 5900, Loss: 0.11411851644515991
step: 6000, Loss: 0.11439873278141022
step: 6100, Loss: 0.11505933105945587
step: 6200, Loss: 0.11433197557926178
step: 6300, Loss: 0.11343793570995331
step: 6400, Loss: 0.11356356739997864
step: 6500, Loss: 0.11392835527658463
step: 6600, Loss: 0.11454237252473831
step: 6700, Loss: 0.11315000802278519
step: 6800, Loss: 0.11415820568799973
step: 6900, Loss: 0.11424727737903595
step: 7000, Loss: 0.1140298992395401
step: 7100, Loss: 0.11423055827617645
step: 7200, Loss: 0.11488267779350281
step: 7300, Loss: 0.11336653679609299
step: 7400, Loss: 0.11490548402070999
step: 7500, Loss: 0.11422465741634369
step: 7600, Loss: 0.11327299475669861
step: 7700, Loss: 0.11357734352350235
step: 7800, Loss: 0.11333442479372025
step: 7900, Loss: 0.11524935066699982
step: 8000, Loss: 0.11347805708646774
step: 8100, Loss: 0.11472197622060776
step: 8200, Loss: 0.1147952452301979
step: 8300, Loss: 0.11379141360521317
step: 8400, Loss: 0.11384235322475433
step: 8500, Loss: 0.11409850418567657
step: 8600, Loss: 0.11519817262887955
step: 8700, Loss: 0.11541593819856644
step: 8800, Loss: 0.11719946563243866
step: 8900, Loss: 0.11389166116714478
step: 9000, Loss: 0.11529552936553955
step: 9100, Loss: 0.1143495962023735
step: 9200, Loss: 0.11556636542081833
step: 9300, Loss: 0.11412300169467926
step: 9400, Loss: 1.764614462852478
step: 9500, Loss: 0.13697563111782074
step: 9600, Loss: 0.13435985147953033
step: 9700, Loss: 0.12532882392406464
step: 9800, Loss: 0.12336348742246628
step: 9900, Loss: 0.12407037615776062
training successfully ended.
validating...
validate data length:95
acc: 0.9772727272727273
precision: 1.0
recall: 0.9555555555555556
F_score: 0.9772727272727273
******fold 9******

Training... train_data length:855
step: 0, Loss: 0.11554066091775894
step: 100, Loss: 0.11880605667829514
step: 200, Loss: 0.11536908149719238
step: 300, Loss: 0.11362094432115555
step: 400, Loss: 0.11533346027135849
step: 500, Loss: 0.11558352410793304
step: 600, Loss: 0.11257673799991608
step: 700, Loss: 0.11374302208423615
step: 800, Loss: 0.1134888082742691
step: 900, Loss: 0.11662732809782028
step: 1000, Loss: 0.11488209664821625
step: 1100, Loss: 0.1141914650797844
step: 1200, Loss: 0.11382071673870087
step: 1300, Loss: 0.11455561965703964
step: 1400, Loss: 0.11383791267871857
step: 1500, Loss: 0.11363886296749115
step: 1600, Loss: 0.11447149515151978
step: 1700, Loss: 0.11375229060649872
step: 1800, Loss: 0.5770834684371948
step: 1900, Loss: 0.14692723751068115
step: 2000, Loss: 0.1250840574502945
step: 2100, Loss: 0.12921711802482605
step: 2200, Loss: 0.12100132554769516
step: 2300, Loss: 0.1209230124950409
step: 2400, Loss: 0.11796502768993378
step: 2500, Loss: 0.12148482352495193
step: 2600, Loss: 0.11634986847639084
step: 2700, Loss: 0.11959271132946014
step: 2800, Loss: 0.1182091236114502
step: 2900, Loss: 0.11548572778701782
step: 3000, Loss: 0.11966771632432938
step: 3100, Loss: 0.11761923134326935
step: 3200, Loss: 0.1165623813867569
step: 3300, Loss: 0.11724856495857239
step: 3400, Loss: 0.11660867184400558
step: 3500, Loss: 0.11475193500518799
step: 3600, Loss: 0.11579766124486923
step: 3700, Loss: 0.11589827388525009
step: 3800, Loss: 0.11438079923391342
step: 3900, Loss: 0.11596792936325073
step: 4000, Loss: 0.11541365087032318
step: 4100, Loss: 0.11480553448200226
step: 4200, Loss: 0.11464042961597443
step: 4300, Loss: 0.11624632030725479
step: 4400, Loss: 0.11350793391466141
step: 4500, Loss: 0.11781185865402222
step: 4600, Loss: 0.11423751711845398
step: 4700, Loss: 0.11729049682617188
step: 4800, Loss: 0.11559019237756729
step: 4900, Loss: 0.11453131586313248
step: 5000, Loss: 0.11384838074445724
step: 5100, Loss: 0.11444593966007233
step: 5200, Loss: 0.11471384763717651
step: 5300, Loss: 0.1147843673825264
step: 5400, Loss: 0.11609400808811188
step: 5500, Loss: 0.11397993564605713
step: 5600, Loss: 0.1161331757903099
step: 5700, Loss: 0.11302980780601501
step: 5800, Loss: 0.11540438234806061
step: 5900, Loss: 0.1135149598121643
step: 6000, Loss: 0.11590267717838287
step: 6100, Loss: 0.11467063426971436
step: 6200, Loss: 0.11389510333538055
step: 6300, Loss: 0.11299097537994385
step: 6400, Loss: 0.11443931609392166
step: 6500, Loss: 0.1137867420911789
step: 6600, Loss: 0.11390702426433563
step: 6700, Loss: 0.11674480140209198
step: 6800, Loss: 0.11454088985919952
step: 6900, Loss: 0.11326742172241211
step: 7000, Loss: 0.11368848383426666
step: 7100, Loss: 0.11440613865852356
step: 7200, Loss: 0.11381630599498749
step: 7300, Loss: 0.11485867947340012
step: 7400, Loss: 0.11412066221237183
step: 7500, Loss: 0.11371485888957977
step: 7600, Loss: 0.11415599286556244
step: 7700, Loss: 0.11353492736816406
step: 7800, Loss: 0.1144762709736824
step: 7900, Loss: 0.1136036217212677
step: 8000, Loss: 0.11412933468818665
step: 8100, Loss: 0.11386562138795853
step: 8200, Loss: 0.11426058411598206
step: 8300, Loss: 0.11285870522260666
step: 8400, Loss: 0.115120530128479
step: 8500, Loss: 0.11370236426591873
step: 8600, Loss: 0.11356636136770248
step: 8700, Loss: 0.11412733048200607
step: 8800, Loss: 0.11452209204435349
step: 8900, Loss: 0.1141115352511406
step: 9000, Loss: 0.11441595107316971
step: 9100, Loss: 0.11407314240932465
step: 9200, Loss: 0.11482395231723785
step: 9300, Loss: 0.11525620520114899
step: 9400, Loss: 0.11474478989839554
step: 9500, Loss: 0.11271905899047852
step: 9600, Loss: 0.11431211978197098
step: 9700, Loss: 0.1133091002702713
step: 9800, Loss: 0.11477286368608475
step: 9900, Loss: 0.22552652657032013
training successfully ended.
validating...
validate data length:95
acc: 0.9886363636363636
precision: 0.9795918367346939
recall: 1.0
F_score: 0.9896907216494846
******fold 10******

Training... train_data length:855
step: 0, Loss: 0.11467284709215164
step: 100, Loss: 0.11521857976913452
step: 200, Loss: 0.11461086571216583
step: 300, Loss: 0.11303766071796417
step: 400, Loss: 0.11403089016675949
step: 500, Loss: 0.11390739679336548
step: 600, Loss: 0.1157776340842247
step: 700, Loss: 0.11568108201026917
step: 800, Loss: 0.11373316496610641
step: 900, Loss: 0.11483398079872131
step: 1000, Loss: 0.11592385172843933
step: 8900, Loss: 0.11557596176862717
step: 9000, Loss: 0.11371631175279617
step: 9100, Loss: 0.11451941728591919
step: 9200, Loss: 0.11495446413755417
step: 9300, Loss: 0.11426199972629547
step: 9400, Loss: 0.11465884745121002
step: 9500, Loss: 0.11518813669681549
step: 9600, Loss: 0.11514745652675629
step: 9700, Loss: 0.11479417979717255
step: 9800, Loss: 0.11537811160087585
step: 9900, Loss: 0.11483292281627655
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8235294117647058
recall: 0.9333333333333333
F_score: 0.8749999999999999
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.12343648821115494
step: 100, Loss: 0.11906442791223526
step: 200, Loss: 0.11989407241344452
step: 300, Loss: 0.11754238605499268
step: 400, Loss: 0.11455953866243362
step: 500, Loss: 0.11774885654449463
step: 600, Loss: 0.11448998749256134
step: 700, Loss: 0.11396678537130356
step: 800, Loss: 0.1160144954919815
step: 900, Loss: 0.11453292518854141
step: 1000, Loss: 0.11688970029354095
step: 1100, Loss: 0.11344660818576813
step: 1200, Loss: 0.11530597507953644
step: 1300, Loss: 0.113139308989048
step: 1400, Loss: 0.11335035413503647
step: 1500, Loss: 0.11416658014059067
step: 1600, Loss: 0.11488038301467896
step: 1700, Loss: 0.11498339474201202
step: 1800, Loss: 0.11455703526735306
step: 1900, Loss: 0.11697002500295639
step: 2000, Loss: 0.11427415907382965
step: 2100, Loss: 0.11375851929187775
step: 2200, Loss: 0.11422053724527359
step: 2300, Loss: 0.11720792949199677
step: 2400, Loss: 0.11347801238298416
step: 2500, Loss: 0.11482126265764236
step: 2600, Loss: 0.1136971190571785
step: 2700, Loss: 0.11464224010705948
step: 2800, Loss: 0.11547389626502991
step: 2900, Loss: 0.11465345323085785
step: 3000, Loss: 0.11281836032867432
step: 3100, Loss: 0.11536510288715363
step: 3200, Loss: 0.11402342468500137
step: 3300, Loss: 0.11412131786346436
step: 3400, Loss: 0.11509105563163757
step: 3500, Loss: 0.11498448252677917
step: 3600, Loss: 0.11292668431997299
step: 3700, Loss: 0.11581140756607056
step: 3800, Loss: 0.11638911068439484
step: 3900, Loss: 0.11463917791843414
step: 4000, Loss: 0.11438655853271484
step: 4100, Loss: 0.1142607033252716
step: 4200, Loss: 0.11400438845157623
step: 4300, Loss: 0.11442043632268906
step: 4400, Loss: 0.11552244424819946
step: 4500, Loss: 0.11347576975822449
step: 4600, Loss: 0.11346212774515152
step: 4700, Loss: 0.11415478587150574
step: 4800, Loss: 0.1152573823928833
step: 4900, Loss: 0.11377134919166565
step: 5000, Loss: 0.11401154845952988
step: 5100, Loss: 0.1146576926112175
step: 5200, Loss: 0.11709370464086533
step: 5300, Loss: 0.11411617696285248
step: 5400, Loss: 0.11487451195716858
step: 5500, Loss: 0.1208706945180893
step: 5600, Loss: 0.3163908123970032
step: 5700, Loss: 0.13140243291854858
step: 5800, Loss: 0.12785300612449646
step: 5900, Loss: 0.1301860511302948
step: 6000, Loss: 0.1248260959982872
step: 6100, Loss: 0.12091205269098282
step: 6200, Loss: 0.11830923706293106
step: 6300, Loss: 0.12343405187129974
step: 6400, Loss: 0.12416107952594757
step: 6500, Loss: 0.1555810123682022
step: 6600, Loss: 0.12361025810241699
step: 6700, Loss: 0.12092418223619461
step: 6800, Loss: 0.11819525063037872
step: 6900, Loss: 0.11666364967823029
step: 7000, Loss: 0.11693325638771057
step: 7100, Loss: 0.11513346433639526
step: 7200, Loss: 0.1189715713262558
step: 7300, Loss: 0.12080225348472595
step: 7400, Loss: 0.11831548810005188
step: 7500, Loss: 0.11732673645019531
step: 7600, Loss: 0.12049560248851776
step: 7700, Loss: 0.11662183701992035
step: 7800, Loss: 0.11608049273490906
step: 7900, Loss: 0.11508027464151382
step: 8000, Loss: 0.11645074188709259
step: 8100, Loss: 0.11653165519237518
step: 8200, Loss: 0.11481235921382904
step: 8300, Loss: 0.12040948867797852
step: 8400, Loss: 0.11487765610218048
step: 8500, Loss: 0.11478917300701141
step: 8600, Loss: 0.11520758271217346
step: 8700, Loss: 0.11411652714014053
step: 8800, Loss: 0.1156664788722992
step: 8900, Loss: 0.11533702909946442
step: 9000, Loss: 0.11642814427614212
step: 9100, Loss: 0.1163279116153717
step: 9200, Loss: 0.11545398831367493
step: 9300, Loss: 0.11409907788038254
step: 9400, Loss: 0.11346608400344849
step: 9500, Loss: 0.11851481348276138
step: 9600, Loss: 0.11422516405582428
step: 9700, Loss: 0.11552301049232483
step: 9800, Loss: 0.1150607317686081
step: 9900, Loss: 0.11824022233486176
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8235294117647058
recall: 0.9333333333333333
F_score: 0.8749999999999999
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.12236692011356354
step: 100, Loss: 0.12454776465892792
step: 200, Loss: 0.11496437340974808
step: 300, Loss: 0.12031462788581848
step: 400, Loss: 0.11497136950492859
step: 500, Loss: 0.11741368472576141
step: 600, Loss: 0.11471311748027802
step: 700, Loss: 0.11578671634197235
step: 800, Loss: 0.11479519307613373
step: 900, Loss: 0.11551161110401154
step: 1000, Loss: 0.11422885954380035
step: 1100, Loss: 0.11450041830539703
step: 1200, Loss: 0.11592337489128113
step: 1300, Loss: 0.11601968109607697
step: 1400, Loss: 0.11319398134946823
step: 1500, Loss: 0.11655136197805405
step: 1600, Loss: 0.11754849553108215
step: 1700, Loss: 0.11577142775058746
step: 1800, Loss: 0.11544017493724823
step: 1900, Loss: 0.116844043135643
step: 2000, Loss: 0.11298301815986633
step: 2100, Loss: 0.11757776141166687
step: 2200, Loss: 0.11414742469787598
step: 2300, Loss: 0.11497943103313446
step: 2400, Loss: 0.11421504616737366
step: 2500, Loss: 0.11515970528125763
step: 2600, Loss: 0.11569331586360931
step: 2700, Loss: 0.11497244238853455
step: 2800, Loss: 0.11423706263303757
step: 2900, Loss: 0.11911894381046295
step: 3000, Loss: 0.11385893076658249
step: 3100, Loss: 0.11295899748802185
step: 3200, Loss: 0.11502643674612045
step: 3300, Loss: 0.11446337401866913
step: 3400, Loss: 0.11461149901151657
step: 3500, Loss: 0.11580411344766617
step: 3600, Loss: 0.11397315561771393
step: 3700, Loss: 0.12014312297105789
step: 3800, Loss: 0.1132272481918335
step: 3900, Loss: 0.11556514352560043
step: 4000, Loss: 0.11384349316358566
step: 4100, Loss: 0.11425526440143585
step: 4200, Loss: 0.11425222456455231
step: 4300, Loss: 0.11460860073566437
step: 4400, Loss: 0.11437442898750305
step: 4500, Loss: 0.11551647633314133
step: 4600, Loss: 0.1157803013920784
step: 4700, Loss: 0.11458035558462143
step: 4800, Loss: 0.11397720873355865
step: 4900, Loss: 0.11390095949172974
step: 5000, Loss: 0.11371148377656937
step: 5100, Loss: 0.11525936424732208
step: 5200, Loss: 0.11432544142007828
step: 5300, Loss: 0.11717860400676727
step: 5400, Loss: 0.11457105726003647
step: 5500, Loss: 0.1183067113161087
step: 5600, Loss: 0.11397626996040344
step: 5700, Loss: 0.11559539288282394
step: 5800, Loss: 0.11468173563480377
step: 5900, Loss: 0.11444661021232605
step: 6000, Loss: 0.11400489509105682
step: 6100, Loss: 0.11545459926128387
step: 6200, Loss: 1.244341254234314
step: 6300, Loss: 0.14748302102088928
step: 6400, Loss: 0.14144742488861084
step: 6500, Loss: 0.13851961493492126
step: 6600, Loss: 0.13119444251060486
step: 6700, Loss: 0.12965601682662964
step: 6800, Loss: 0.11985267698764801
step: 6900, Loss: 0.12488872557878494
step: 7000, Loss: 0.12197968363761902
step: 7100, Loss: 0.12209967523813248
step: 7200, Loss: 0.12280674278736115
step: 7300, Loss: 0.12014313042163849
step: 7400, Loss: 0.11577222496271133
step: 7500, Loss: 0.12246111035346985
step: 7600, Loss: 0.12015721201896667
step: 7700, Loss: 0.11710435152053833
step: 7800, Loss: 0.1169903427362442
step: 7900, Loss: 0.1169065609574318
step: 8000, Loss: 0.11563147604465485
step: 8100, Loss: 0.11594948172569275
step: 8200, Loss: 0.11677610129117966
step: 8300, Loss: 0.11390456557273865
step: 8400, Loss: 0.11462191492319107
step: 8500, Loss: 0.11546903848648071
step: 8600, Loss: 0.11732311546802521
step: 8700, Loss: 0.11647269129753113
step: 8800, Loss: 0.11532953381538391
step: 8900, Loss: 0.11539645493030548
step: 9000, Loss: 0.11598558723926544
step: 9100, Loss: 0.11462539434432983
step: 9200, Loss: 0.11518082022666931
step: 1100, Loss: 0.11615851521492004
step: 1200, Loss: 0.1137087270617485
step: 1300, Loss: 0.11480868607759476
step: 1400, Loss: 0.11456815153360367
step: 1500, Loss: 0.5093773603439331
step: 1600, Loss: 1.9493682384490967
step: 1700, Loss: 0.1392716020345688
step: 1800, Loss: 0.13206395506858826
step: 1900, Loss: 0.14086858928203583
step: 2000, Loss: 0.12444856762886047
step: 2100, Loss: 0.11969352513551712
step: 2200, Loss: 0.12201160192489624
step: 2300, Loss: 0.12130144238471985
step: 2400, Loss: 0.11833146214485168
step: 2500, Loss: 0.11942704021930695
step: 2600, Loss: 0.11958852410316467
step: 2700, Loss: 0.11902835220098495
step: 2800, Loss: 0.11705389618873596
step: 2900, Loss: 0.1160733699798584
step: 3000, Loss: 0.11704204976558685
step: 3100, Loss: 0.1162223145365715
step: 3200, Loss: 0.11753677576780319
step: 3300, Loss: 0.1161155179142952
step: 3400, Loss: 0.11652477830648422
step: 3500, Loss: 0.11545535176992416
step: 3600, Loss: 0.11666547507047653
step: 3700, Loss: 0.1180202066898346
step: 3800, Loss: 0.11702284216880798
step: 3900, Loss: 0.1166626513004303
step: 4000, Loss: 0.11567021161317825
step: 4100, Loss: 0.11690989136695862
step: 4200, Loss: 0.11548596620559692
step: 4300, Loss: 0.11739735305309296
step: 4400, Loss: 0.11652175337076187
step: 4500, Loss: 0.11487101763486862
step: 4600, Loss: 0.11455526202917099
step: 4700, Loss: 0.11440560221672058
step: 4800, Loss: 0.11448020488023758
step: 4900, Loss: 0.11558964848518372
step: 5000, Loss: 0.11587129533290863
step: 5100, Loss: 0.11556066572666168
step: 5200, Loss: 0.11536240577697754
step: 5300, Loss: 0.11447100341320038
step: 5400, Loss: 0.11518976092338562
step: 5500, Loss: 0.11490137875080109
step: 5600, Loss: 0.11448192596435547
step: 5700, Loss: 0.11397302150726318
step: 5800, Loss: 0.11454694718122482
step: 5900, Loss: 0.11424294114112854
step: 6000, Loss: 0.1142800971865654
step: 6100, Loss: 0.11544282734394073
step: 6200, Loss: 0.11493444442749023
step: 6300, Loss: 0.11481410264968872
step: 6400, Loss: 0.11487144231796265
step: 6500, Loss: 0.11572901159524918
step: 6600, Loss: 0.11385923624038696
step: 6700, Loss: 0.11528527736663818
step: 6800, Loss: 0.11442802846431732
step: 6900, Loss: 0.1146860346198082
step: 7000, Loss: 0.11339438706636429
step: 7100, Loss: 0.11318599432706833
step: 7200, Loss: 0.11410856992006302
step: 7300, Loss: 0.11282915621995926
step: 7400, Loss: 0.1139487698674202
step: 7500, Loss: 0.11674629896879196
step: 7600, Loss: 0.11289771646261215
step: 7700, Loss: 0.11395666003227234
step: 7800, Loss: 0.11408033967018127
step: 7900, Loss: 0.11507876217365265
step: 8000, Loss: 0.11393974721431732
step: 8100, Loss: 0.11386474967002869
step: 8200, Loss: 0.11410922557115555
step: 8300, Loss: 0.11318628489971161
step: 8400, Loss: 0.11410076171159744
step: 8500, Loss: 0.11342482268810272
step: 8600, Loss: 0.11383385211229324
step: 8700, Loss: 0.11425697058439255
step: 8800, Loss: 0.11414690315723419
step: 8900, Loss: 0.11424210667610168
step: 9000, Loss: 0.11423403769731522
step: 9100, Loss: 0.1194433867931366
step: 9200, Loss: 0.11367974430322647
step: 9300, Loss: 0.11429059505462646
step: 9400, Loss: 2.3015081882476807
step: 9500, Loss: 0.22206661105155945
step: 9600, Loss: 0.12993264198303223
step: 9700, Loss: 0.1351250559091568
step: 9800, Loss: 0.1238253116607666
step: 9900, Loss: 0.12354560196399689
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 13 Avgacc: 0.978409090909091 Avgfscore: 0.9789341129367084 
 Max acc:1.0, Max f score:1.0
******** mix subject_14 ********

[380, 380]
******fold 1******

Training... train_data length:684
step: 0, Loss: 31.237947463989258
step: 100, Loss: 5.547846794128418
step: 200, Loss: 3.332566499710083
step: 300, Loss: 2.8929426670074463
step: 400, Loss: 1.0790305137634277
step: 500, Loss: 0.43840017914772034
step: 600, Loss: 0.1674197018146515
step: 700, Loss: 0.13512076437473297
step: 800, Loss: 0.1475418210029602
step: 900, Loss: 0.15138213336467743
step: 1000, Loss: 0.1318526268005371
step: 1100, Loss: 0.13831046223640442
step: 1200, Loss: 0.13547269999980927
step: 1300, Loss: 0.13711383938789368
step: 1400, Loss: 0.13122175633907318
step: 1500, Loss: 0.2005622237920761
step: 1600, Loss: 0.13926999270915985
step: 1700, Loss: 0.13203677535057068
step: 1800, Loss: 0.13721033930778503
step: 1900, Loss: 0.13350696861743927
step: 2000, Loss: 0.12893174588680267
step: 2100, Loss: 0.13298290967941284
step: 2200, Loss: 0.1270207017660141
step: 2300, Loss: 0.13026639819145203
step: 2400, Loss: 0.12649492919445038
step: 2500, Loss: 0.12983867526054382
step: 2600, Loss: 0.12076706439256668
step: 2700, Loss: 0.12013315409421921
step: 2800, Loss: 0.12248744815587997
step: 2900, Loss: 0.11959069967269897
step: 3000, Loss: 0.12158231437206268
step: 3100, Loss: 0.12242390215396881
step: 3200, Loss: 0.12161621451377869
step: 3300, Loss: 0.12006624042987823
step: 3400, Loss: 0.20095720887184143
step: 3500, Loss: 0.12263816595077515
step: 3600, Loss: 0.11967732012271881
step: 3700, Loss: 0.12102137506008148
step: 3800, Loss: 0.12144321203231812
step: 3900, Loss: 0.1161387711763382
step: 4000, Loss: 0.11917184293270111
step: 4100, Loss: 0.11596938967704773
step: 4200, Loss: 0.12157893180847168
step: 4300, Loss: 0.11728038638830185
step: 4400, Loss: 0.1176355630159378
step: 4500, Loss: 0.11703953146934509
step: 4600, Loss: 0.11359928548336029
step: 4700, Loss: 0.11532267183065414
step: 4800, Loss: 0.11621216684579849
step: 4900, Loss: 0.1156294047832489
step: 5000, Loss: 0.1157241091132164
step: 5100, Loss: 0.11578681319952011
step: 5200, Loss: 0.1148746907711029
step: 5300, Loss: 0.19975489377975464
step: 5400, Loss: 0.11505328118801117
step: 5500, Loss: 0.11538182199001312
step: 5600, Loss: 0.11599882692098618
step: 5700, Loss: 0.11602845788002014
step: 5800, Loss: 0.11868710070848465
step: 5900, Loss: 0.12367038428783417
step: 6000, Loss: 11.875484466552734
step: 6100, Loss: 0.6299933791160583
step: 6200, Loss: 0.16533374786376953
step: 6300, Loss: 0.16635820269584656
step: 6400, Loss: 0.14107343554496765
step: 6500, Loss: 0.1411832720041275
step: 6600, Loss: 0.12765608727931976
step: 6700, Loss: 0.13031457364559174
step: 6800, Loss: 0.13255220651626587
step: 6900, Loss: 0.13347357511520386
step: 7000, Loss: 0.12680214643478394
step: 7100, Loss: 0.12476244568824768
step: 7200, Loss: 0.21276845037937164
step: 7300, Loss: 0.1322687566280365
step: 7400, Loss: 0.12810365855693817
step: 7500, Loss: 0.12593698501586914
step: 7600, Loss: 0.12429884821176529
step: 7700, Loss: 0.1263478696346283
step: 7800, Loss: 0.11797177791595459
step: 7900, Loss: 0.12047439813613892
step: 8000, Loss: 0.12016694247722626
step: 8100, Loss: 0.11823581159114838
step: 8200, Loss: 0.12339620292186737
step: 8300, Loss: 0.12017739564180374
step: 8400, Loss: 0.11885397136211395
step: 8500, Loss: 0.11824812740087509
step: 8600, Loss: 0.11849861592054367
step: 8700, Loss: 0.12099361419677734
step: 8800, Loss: 0.11884793639183044
step: 8900, Loss: 0.11860912293195724
step: 9000, Loss: 0.11524668335914612
step: 9100, Loss: 0.20407260954380035
step: 9200, Loss: 0.11998079717159271
step: 9300, Loss: 0.11861114203929901
step: 9400, Loss: 0.1191333681344986
step: 9500, Loss: 0.11874914169311523
step: 9600, Loss: 0.11603863537311554
step: 9700, Loss: 0.11491798609495163
step: 9800, Loss: 0.11484506726264954
step: 9900, Loss: 0.11710076034069061
training successfully ended.
validating...
validate data length:76
acc: 0.7222222222222222
precision: 0.6511627906976745
recall: 0.8484848484848485
F_score: 0.736842105263158
******fold 2******

Training... train_data length:684
step: 0, Loss: 5.254690647125244
step: 100, Loss: 0.13955926895141602
step: 200, Loss: 0.12658414244651794
step: 300, Loss: 0.12618482112884521
step: 400, Loss: 0.12286166846752167
step: 500, Loss: 0.12404607981443405
step: 600, Loss: 0.11764116585254669
step: 700, Loss: 0.11806747317314148
step: 800, Loss: 0.11769892275333405
step: 900, Loss: 0.11716730892658234
step: 1000, Loss: 0.11483451724052429
step: 1100, Loss: 0.11778835952281952
step: 1200, Loss: 0.11996237188577652
step: 1300, Loss: 0.11621116101741791
step: 9300, Loss: 0.11716128885746002
step: 9400, Loss: 0.11545511335134506
step: 9500, Loss: 0.11628309637308121
step: 9600, Loss: 0.11637697368860245
step: 9700, Loss: 0.11385352164506912
step: 9800, Loss: 0.1140424907207489
step: 9900, Loss: 0.11525900661945343
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8823529411764706
recall: 0.8333333333333334
F_score: 0.8571428571428571
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.1304204761981964
step: 100, Loss: 0.12131991982460022
step: 200, Loss: 0.11584389954805374
step: 300, Loss: 0.11545340716838837
step: 400, Loss: 0.11572705954313278
step: 500, Loss: 0.11386136710643768
step: 600, Loss: 0.11499270796775818
step: 700, Loss: 0.1168522909283638
step: 800, Loss: 0.1158965453505516
step: 900, Loss: 0.11513158679008484
step: 1000, Loss: 0.11695096641778946
step: 1100, Loss: 0.11485493928194046
step: 1200, Loss: 0.1148916631937027
step: 1300, Loss: 0.11512013524770737
step: 1400, Loss: 0.11404000222682953
step: 1500, Loss: 0.11453709006309509
step: 1600, Loss: 0.11459452658891678
step: 1700, Loss: 0.11326533555984497
step: 1800, Loss: 0.11442055553197861
step: 1900, Loss: 0.11509470641613007
step: 2000, Loss: 0.11526995152235031
step: 2100, Loss: 0.11541157960891724
step: 2200, Loss: 0.11481237411499023
step: 2300, Loss: 0.11488364636898041
step: 2400, Loss: 0.11364426463842392
step: 2500, Loss: 0.11356305330991745
step: 2600, Loss: 0.11374481767416
step: 2700, Loss: 0.11498801410198212
step: 2800, Loss: 0.11418303847312927
step: 2900, Loss: 0.11425845324993134
step: 3000, Loss: 0.1137605533003807
step: 3100, Loss: 0.1126708835363388
step: 3200, Loss: 0.11494529247283936
step: 3300, Loss: 0.11741968989372253
step: 3400, Loss: 0.11387462168931961
step: 3500, Loss: 0.11477778851985931
step: 3600, Loss: 0.11487646400928497
step: 3700, Loss: 0.11400863528251648
step: 3800, Loss: 0.11317282915115356
step: 3900, Loss: 0.11468969285488129
step: 4000, Loss: 0.11416364461183548
step: 4100, Loss: 0.11382798850536346
step: 4200, Loss: 0.11451197415590286
step: 4300, Loss: 0.11486556380987167
step: 4400, Loss: 0.1145663633942604
step: 4500, Loss: 0.9728658199310303
step: 4600, Loss: 0.1413889229297638
step: 4700, Loss: 0.13401861488819122
step: 4800, Loss: 0.1301916241645813
step: 4900, Loss: 0.12983742356300354
step: 5000, Loss: 0.12568388879299164
step: 5100, Loss: 0.12620039284229279
step: 5200, Loss: 0.12062157690525055
step: 5300, Loss: 0.12036222219467163
step: 5400, Loss: 0.11878908425569534
step: 5500, Loss: 0.11948306858539581
step: 5600, Loss: 0.11730209738016129
step: 5700, Loss: 0.11842132359743118
step: 5800, Loss: 0.12078410387039185
step: 5900, Loss: 0.11647842079401016
step: 6000, Loss: 0.11903849244117737
step: 6100, Loss: 0.11943067610263824
step: 6200, Loss: 0.11744032055139542
step: 6300, Loss: 0.11813333630561829
step: 6400, Loss: 0.11676457524299622
step: 6500, Loss: 0.11779351532459259
step: 6600, Loss: 0.12001550197601318
step: 6700, Loss: 0.11749420315027237
step: 6800, Loss: 0.1178051009774208
step: 6900, Loss: 0.11792384088039398
step: 7000, Loss: 0.11684966087341309
step: 7100, Loss: 0.11579149961471558
step: 7200, Loss: 0.11556766927242279
step: 7300, Loss: 0.1166178286075592
step: 7400, Loss: 0.11724558472633362
step: 7500, Loss: 0.11565740406513214
step: 7600, Loss: 0.11455283313989639
step: 7700, Loss: 0.11526968330144882
step: 7800, Loss: 0.11392330378293991
step: 7900, Loss: 0.11426140367984772
step: 8000, Loss: 0.11415351182222366
step: 8100, Loss: 0.11595800518989563
step: 8200, Loss: 0.11517836153507233
step: 8300, Loss: 0.11676548421382904
step: 8400, Loss: 0.11545062810182571
step: 8500, Loss: 0.11605588346719742
step: 8600, Loss: 0.11396460235118866
step: 8700, Loss: 0.11502645909786224
step: 8800, Loss: 0.11601130664348602
step: 8900, Loss: 0.11434520035982132
step: 9000, Loss: 0.1146090105175972
step: 9100, Loss: 0.11551950126886368
step: 9200, Loss: 0.11553065478801727
step: 9300, Loss: 0.11549396812915802
step: 9400, Loss: 0.11377034336328506
step: 9500, Loss: 0.11782947182655334
step: 9600, Loss: 0.11610845476388931
step: 9700, Loss: 0.11546146869659424
step: 9800, Loss: 0.11351042240858078
step: 9900, Loss: 0.1143098846077919
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.85
recall: 0.9444444444444444
F_score: 0.8947368421052632
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.13467472791671753
step: 100, Loss: 0.11872655153274536
step: 200, Loss: 0.11809148639440536
step: 300, Loss: 0.11713135242462158
step: 400, Loss: 0.11485671997070312
step: 500, Loss: 0.11515935510396957
step: 600, Loss: 0.1147860586643219
step: 700, Loss: 0.11334304511547089
step: 800, Loss: 0.11599910259246826
step: 900, Loss: 0.11399830132722855
step: 1000, Loss: 0.11502199620008469
step: 1100, Loss: 0.116256944835186
step: 1200, Loss: 0.11467105150222778
step: 1300, Loss: 0.11471743881702423
step: 1400, Loss: 0.11447611451148987
step: 1500, Loss: 0.1136227697134018
step: 1600, Loss: 0.11393294483423233
step: 1700, Loss: 0.11482997238636017
step: 1800, Loss: 0.11506326496601105
step: 1900, Loss: 0.1140265092253685
step: 2000, Loss: 0.11486563831567764
step: 2100, Loss: 0.1132640540599823
step: 2200, Loss: 0.11456115543842316
step: 2300, Loss: 0.11350551247596741
step: 2400, Loss: 0.11532812565565109
step: 2500, Loss: 0.11337088793516159
step: 2600, Loss: 0.11474847793579102
step: 2700, Loss: 0.1155763566493988
step: 2800, Loss: 0.11557891964912415
step: 2900, Loss: 0.11549625545740128
step: 3000, Loss: 0.11332230269908905
step: 3100, Loss: 0.11347382515668869
step: 3200, Loss: 0.11356489360332489
step: 3300, Loss: 0.11352062970399857
step: 3400, Loss: 0.11533419042825699
step: 3500, Loss: 0.1137220561504364
step: 3600, Loss: 0.11498179286718369
step: 3700, Loss: 0.1148378998041153
step: 3800, Loss: 0.11453497409820557
step: 3900, Loss: 0.11566242575645447
step: 4000, Loss: 0.11534802615642548
step: 4100, Loss: 0.1146952360868454
step: 4200, Loss: 0.1134195476770401
step: 4300, Loss: 0.11544039845466614
step: 4400, Loss: 0.11355733871459961
step: 4500, Loss: 0.11628429591655731
step: 4600, Loss: 0.11423350125551224
step: 4700, Loss: 0.11454885452985764
step: 4800, Loss: 0.11730876564979553
step: 4900, Loss: 0.11394114792346954
step: 5000, Loss: 0.1146642193198204
step: 5100, Loss: 0.11357679963111877
step: 5200, Loss: 0.11305698752403259
step: 5300, Loss: 0.11491059511899948
step: 5400, Loss: 0.11300690472126007
step: 5500, Loss: 0.11350421607494354
step: 5600, Loss: 3.8794853687286377
step: 5700, Loss: 0.17936386168003082
step: 5800, Loss: 0.12914782762527466
step: 5900, Loss: 0.1340067982673645
step: 6000, Loss: 0.13172300159931183
step: 6100, Loss: 0.11941243708133698
step: 6200, Loss: 0.12382767349481583
step: 6300, Loss: 0.1183510273694992
step: 6400, Loss: 0.1190740168094635
step: 6500, Loss: 0.11638356000185013
step: 6600, Loss: 0.12225960195064545
step: 6700, Loss: 0.11605830490589142
step: 6800, Loss: 0.11528949439525604
step: 6900, Loss: 0.11723972856998444
step: 7000, Loss: 0.11553973704576492
step: 7100, Loss: 0.12186604738235474
step: 7200, Loss: 0.11518217623233795
step: 7300, Loss: 0.11756451427936554
step: 7400, Loss: 0.11585583537817001
step: 7500, Loss: 0.11765982955694199
step: 7600, Loss: 0.11698400229215622
step: 7700, Loss: 0.11417499929666519
step: 7800, Loss: 0.11556623876094818
step: 7900, Loss: 0.11859632283449173
step: 8000, Loss: 0.11522146314382553
step: 8100, Loss: 0.115316241979599
step: 8200, Loss: 0.11528029292821884
step: 8300, Loss: 0.11456859111785889
step: 8400, Loss: 0.11467649042606354
step: 8500, Loss: 0.11427245289087296
step: 8600, Loss: 0.11383610218763351
step: 8700, Loss: 0.11418692767620087
step: 8800, Loss: 0.11888861656188965
step: 8900, Loss: 0.11549574136734009
step: 9000, Loss: 0.11386609077453613
step: 9100, Loss: 0.11980187892913818
step: 9200, Loss: 0.11437813937664032
step: 9300, Loss: 0.11329453438520432
step: 9400, Loss: 0.11426082253456116
step: 9500, Loss: 0.11370552331209183
step: 9600, Loss: 0.11432628333568573
step: 9700, Loss: 0.11582069098949432
step: 1400, Loss: 0.1174510046839714
step: 1500, Loss: 0.19137321412563324
step: 1600, Loss: 0.11664660274982452
step: 1700, Loss: 0.11658793687820435
step: 1800, Loss: 0.11778266727924347
step: 1900, Loss: 0.11665524542331696
step: 2000, Loss: 0.1149992048740387
step: 2100, Loss: 0.11531923711299896
step: 2200, Loss: 0.11471789330244064
step: 2300, Loss: 1.491877794265747
step: 2400, Loss: 0.6313000917434692
step: 2500, Loss: 0.1507687121629715
step: 2600, Loss: 0.14115077257156372
step: 2700, Loss: 0.13407030701637268
step: 2800, Loss: 0.13566797971725464
step: 2900, Loss: 0.1343180239200592
step: 3000, Loss: 0.12488746643066406
step: 3100, Loss: 0.13501359522342682
step: 3200, Loss: 0.12427595257759094
step: 3300, Loss: 0.12293791770935059
step: 3400, Loss: 0.2027531862258911
step: 3500, Loss: 0.1243944764137268
step: 3600, Loss: 0.1233772337436676
step: 3700, Loss: 0.1276692897081375
step: 3800, Loss: 0.12442243099212646
step: 3900, Loss: 0.12016891688108444
step: 4000, Loss: 0.11940145492553711
step: 4100, Loss: 0.12001001089811325
step: 4200, Loss: 0.11893080919981003
step: 4300, Loss: 0.12226565927267075
step: 4400, Loss: 0.11908350139856339
step: 4500, Loss: 0.11817153543233871
step: 4600, Loss: 0.1154099628329277
step: 4700, Loss: 0.1195574700832367
step: 4800, Loss: 0.118806853890419
step: 4900, Loss: 0.1166946217417717
step: 5000, Loss: 0.12208016216754913
step: 5100, Loss: 0.11491941660642624
step: 5200, Loss: 0.11537645012140274
step: 5300, Loss: 0.19562645256519318
step: 5400, Loss: 0.1162453293800354
step: 5500, Loss: 0.11751998960971832
step: 5600, Loss: 0.12086419761180878
step: 5700, Loss: 0.11812608689069748
step: 5800, Loss: 0.11616291850805283
step: 5900, Loss: 0.11422748863697052
step: 6000, Loss: 0.11461303383111954
step: 6100, Loss: 0.11651673913002014
step: 6200, Loss: 0.11661114543676376
step: 6300, Loss: 0.11567836254835129
step: 6400, Loss: 0.11427368223667145
step: 6500, Loss: 0.11348006129264832
step: 6600, Loss: 0.11460105329751968
step: 6700, Loss: 0.11362279206514359
step: 6800, Loss: 0.11778168380260468
step: 6900, Loss: 0.1165744960308075
step: 7000, Loss: 0.1137998178601265
step: 7100, Loss: 0.11343590170145035
step: 7200, Loss: 0.19196899235248566
step: 7300, Loss: 0.11359085142612457
step: 7400, Loss: 0.11500771343708038
step: 7500, Loss: 0.1149396225810051
step: 7600, Loss: 0.11416785418987274
step: 7700, Loss: 0.11798809468746185
step: 7800, Loss: 0.1148187667131424
step: 7900, Loss: 0.11467060446739197
step: 8000, Loss: 0.11331204324960709
step: 8100, Loss: 0.1144360601902008
step: 8200, Loss: 0.115216925740242
step: 8300, Loss: 0.11396326869726181
step: 8400, Loss: 0.1144462525844574
step: 8500, Loss: 0.11580801010131836
step: 8600, Loss: 0.1149318739771843
step: 8700, Loss: 0.11393497884273529
step: 8800, Loss: 0.11624497920274734
step: 8900, Loss: 0.11541928350925446
step: 9000, Loss: 0.1137000247836113
step: 9100, Loss: 0.19234037399291992
step: 9200, Loss: 0.11323671787977219
step: 9300, Loss: 0.11474289745092392
step: 9400, Loss: 0.12277115881443024
step: 9500, Loss: 0.12058143317699432
step: 9600, Loss: 0.11435277760028839
step: 9700, Loss: 0.11410702764987946
step: 9800, Loss: 0.1184813603758812
step: 9900, Loss: 0.11687478423118591
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.9230769230769231
recall: 0.9230769230769231
F_score: 0.9230769230769231
******fold 3******

Training... train_data length:684
step: 0, Loss: 1.1613365411758423
step: 100, Loss: 0.12422299385070801
step: 200, Loss: 0.12246455997228622
step: 300, Loss: 0.11866579949855804
step: 400, Loss: 0.1174253299832344
step: 500, Loss: 0.11766485869884491
step: 600, Loss: 0.1148686334490776
step: 700, Loss: 0.11456137895584106
step: 800, Loss: 0.11517229676246643
step: 900, Loss: 0.1142348125576973
step: 1000, Loss: 0.11554478108882904
step: 1100, Loss: 0.11455623060464859
step: 1200, Loss: 0.11552970111370087
step: 1300, Loss: 0.11616051197052002
step: 1400, Loss: 0.1146974042057991
step: 1500, Loss: 0.19373789429664612
step: 1600, Loss: 0.11438462138175964
step: 1700, Loss: 0.11360183358192444
step: 1800, Loss: 0.11370063573122025
step: 1900, Loss: 0.11457625031471252
step: 2000, Loss: 0.11288268119096756
step: 2100, Loss: 0.11513248085975647
step: 2200, Loss: 0.11360616981983185
step: 2300, Loss: 0.1154513955116272
step: 2400, Loss: 0.11488049477338791
step: 2500, Loss: 0.11523984372615814
step: 2600, Loss: 0.1130686029791832
step: 2700, Loss: 0.11372822523117065
step: 2800, Loss: 0.1137607991695404
step: 2900, Loss: 0.11458331346511841
step: 3000, Loss: 0.11446212977170944
step: 3100, Loss: 0.11475565284490585
step: 3200, Loss: 0.11354805529117584
step: 3300, Loss: 0.11319884657859802
step: 3400, Loss: 0.19119170308113098
step: 3500, Loss: 0.11445795744657516
step: 3600, Loss: 0.11467550694942474
step: 3700, Loss: 0.11432778835296631
step: 3800, Loss: 0.11906934529542923
step: 3900, Loss: 0.11679980158805847
step: 4000, Loss: 0.11496071517467499
step: 4100, Loss: 0.11295685917139053
step: 4200, Loss: 0.11421119421720505
step: 4300, Loss: 0.1158970296382904
step: 4400, Loss: 0.11530067026615143
step: 4500, Loss: 0.11454039067029953
step: 4600, Loss: 0.11439052224159241
step: 4700, Loss: 0.11585668474435806
step: 4800, Loss: 4.871289253234863
step: 4900, Loss: 0.204170361161232
step: 5000, Loss: 0.15506470203399658
step: 5100, Loss: 0.13609102368354797
step: 5200, Loss: 0.13706240057945251
step: 5300, Loss: 0.23247340321540833
step: 5400, Loss: 0.1319262981414795
step: 5500, Loss: 0.12972891330718994
step: 5600, Loss: 0.12385784089565277
step: 5700, Loss: 0.12664589285850525
step: 5800, Loss: 0.13024505972862244
step: 5900, Loss: 0.12126246094703674
step: 6000, Loss: 0.12118732184171677
step: 6100, Loss: 0.11728362739086151
step: 6200, Loss: 0.122406505048275
step: 6300, Loss: 0.12724846601486206
step: 6400, Loss: 0.12060253322124481
step: 6500, Loss: 0.12091975659132004
step: 6600, Loss: 0.12029268592596054
step: 6700, Loss: 0.1204783096909523
step: 6800, Loss: 0.11707893759012222
step: 6900, Loss: 0.11741693317890167
step: 7000, Loss: 0.1176736056804657
step: 7100, Loss: 0.11552731692790985
step: 7200, Loss: 0.19700105488300323
step: 7300, Loss: 0.11722537875175476
step: 7400, Loss: 0.11636495590209961
step: 7500, Loss: 0.11845998466014862
step: 7600, Loss: 0.11639109253883362
step: 7700, Loss: 0.1169787272810936
step: 7800, Loss: 0.11489494889974594
step: 7900, Loss: 0.1146821603178978
step: 8000, Loss: 0.11486732959747314
step: 8100, Loss: 0.11468186229467392
step: 8200, Loss: 0.1183241680264473
step: 8300, Loss: 0.11487895995378494
step: 8400, Loss: 0.11423096060752869
step: 8500, Loss: 0.11772855371236801
step: 8600, Loss: 0.11646498739719391
step: 8700, Loss: 0.11495733261108398
step: 8800, Loss: 0.11492955684661865
step: 8900, Loss: 0.11416099965572357
step: 9000, Loss: 0.11412035673856735
step: 9100, Loss: 0.19327640533447266
step: 9200, Loss: 0.11390639841556549
step: 9300, Loss: 0.11411501467227936
step: 9400, Loss: 0.11539080739021301
step: 9500, Loss: 0.11308906227350235
step: 9600, Loss: 0.11573272943496704
step: 9700, Loss: 0.11402737349271774
step: 9800, Loss: 0.11345613747835159
step: 9900, Loss: 0.11374646425247192
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9473684210526315
recall: 0.972972972972973
F_score: 0.9599999999999999
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.11623977869749069
step: 100, Loss: 0.12824012339115143
step: 200, Loss: 0.11989095062017441
step: 300, Loss: 0.12077553570270538
step: 400, Loss: 0.11707507073879242
step: 500, Loss: 0.1156553104519844
step: 600, Loss: 0.11861368268728256
step: 700, Loss: 0.1153482049703598
step: 800, Loss: 0.11578232049942017
step: 900, Loss: 0.11590774357318878
step: 1000, Loss: 0.1150847002863884
step: 1100, Loss: 0.11398747563362122
step: 1200, Loss: 0.11433194577693939
step: 1300, Loss: 0.1151331439614296
step: 1400, Loss: 0.11327175050973892
step: 1500, Loss: 0.19309286773204803
step: 1600, Loss: 0.11372892558574677
step: 1700, Loss: 0.11336887627840042
step: 1800, Loss: 0.11435870826244354
step: 9800, Loss: 0.11380241811275482
step: 9900, Loss: 0.1128956526517868
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8421052631578947
recall: 0.9411764705882353
F_score: 0.8888888888888888
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.12623026967048645
step: 100, Loss: 0.12440908700227737
step: 200, Loss: 0.11675365269184113
step: 300, Loss: 0.11829127371311188
step: 400, Loss: 0.11747436225414276
step: 500, Loss: 0.11632540822029114
step: 600, Loss: 0.11567080020904541
step: 700, Loss: 0.11724942922592163
step: 800, Loss: 0.1144733875989914
step: 900, Loss: 0.11515715718269348
step: 1000, Loss: 0.11439970135688782
step: 1100, Loss: 0.11551279574632645
step: 1200, Loss: 0.11516257375478745
step: 1300, Loss: 0.11593221127986908
step: 1400, Loss: 0.11450910568237305
step: 1500, Loss: 0.11301988363265991
step: 1600, Loss: 0.11367587745189667
step: 1700, Loss: 0.11538470536470413
step: 1800, Loss: 0.11454452574253082
step: 1900, Loss: 0.11364828795194626
step: 2000, Loss: 0.11468252539634705
step: 2100, Loss: 0.11399596929550171
step: 2200, Loss: 0.11441590636968613
step: 2300, Loss: 0.11589975655078888
step: 2400, Loss: 0.11736607551574707
step: 2500, Loss: 0.11496530473232269
step: 2600, Loss: 0.11503905802965164
step: 2700, Loss: 0.1162261813879013
step: 2800, Loss: 0.11485733091831207
step: 2900, Loss: 0.11466038972139359
step: 3000, Loss: 0.1146489828824997
step: 3100, Loss: 0.11568720638751984
step: 3200, Loss: 0.11506107449531555
step: 3300, Loss: 0.11437413841485977
step: 3400, Loss: 0.11341820657253265
step: 3500, Loss: 0.11454135179519653
step: 3600, Loss: 0.11537006497383118
step: 3700, Loss: 0.11647719144821167
step: 3800, Loss: 0.11481964588165283
step: 3900, Loss: 0.11589732766151428
step: 4000, Loss: 0.11305103451013565
step: 4100, Loss: 0.11355337500572205
step: 4200, Loss: 0.1149677187204361
step: 4300, Loss: 0.11430344730615616
step: 4400, Loss: 0.11371880769729614
step: 4500, Loss: 0.11269013583660126
step: 4600, Loss: 0.11578862369060516
step: 4700, Loss: 0.11404150724411011
step: 4800, Loss: 0.11416877061128616
step: 4900, Loss: 0.1141987144947052
step: 5000, Loss: 0.11561106145381927
step: 5100, Loss: 0.11289144307374954
step: 5200, Loss: 0.11490943282842636
step: 5300, Loss: 0.11352568119764328
step: 5400, Loss: 0.11336776614189148
step: 5500, Loss: 0.11495807766914368
step: 5600, Loss: 0.11536954343318939
step: 5700, Loss: 0.1149972528219223
step: 5800, Loss: 0.1135285496711731
step: 5900, Loss: 0.1143449917435646
step: 6000, Loss: 0.11423230171203613
step: 6100, Loss: 0.11402793973684311
step: 6200, Loss: 0.11533044278621674
step: 6300, Loss: 0.5998142957687378
step: 6400, Loss: 0.13618311285972595
step: 6500, Loss: 0.1421072781085968
step: 6600, Loss: 0.1217813789844513
step: 6700, Loss: 0.12793374061584473
step: 6800, Loss: 0.12294112145900726
step: 6900, Loss: 0.12602217495441437
step: 7000, Loss: 0.11745662987232208
step: 7100, Loss: 0.1242184042930603
step: 7200, Loss: 0.11655529588460922
step: 7300, Loss: 0.12092868983745575
step: 7400, Loss: 0.11572064459323883
step: 7500, Loss: 0.12004043161869049
step: 7600, Loss: 0.1157284826040268
step: 7700, Loss: 0.12232570350170135
step: 7800, Loss: 0.11503593623638153
step: 7900, Loss: 0.11612050235271454
step: 8000, Loss: 0.1143336296081543
step: 8100, Loss: 0.11871547996997833
step: 8200, Loss: 0.11496621370315552
step: 8300, Loss: 0.11469440907239914
step: 8400, Loss: 0.1156034916639328
step: 8500, Loss: 0.11715790629386902
step: 8600, Loss: 0.1154206246137619
step: 8700, Loss: 0.11568617820739746
step: 8800, Loss: 0.11433377116918564
step: 8900, Loss: 0.11625971645116806
step: 9000, Loss: 0.11583144962787628
step: 9100, Loss: 0.11586894094944
step: 9200, Loss: 0.11717867851257324
step: 9300, Loss: 0.1184329092502594
step: 9400, Loss: 0.11433927714824677
step: 9500, Loss: 0.1158311665058136
step: 9600, Loss: 0.11513406038284302
step: 9700, Loss: 0.11569239944219589
step: 9800, Loss: 0.11343707889318466
step: 9900, Loss: 0.11579443514347076
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.12806080281734467
step: 100, Loss: 0.12393593788146973
step: 200, Loss: 0.11587259918451309
step: 300, Loss: 0.11487767100334167
step: 400, Loss: 0.11490064859390259
step: 500, Loss: 0.11493894457817078
step: 600, Loss: 0.11482356488704681
step: 700, Loss: 0.11459065228700638
step: 800, Loss: 0.11511339247226715
step: 900, Loss: 0.11331385374069214
step: 1000, Loss: 0.11379994451999664
step: 1100, Loss: 0.11791233718395233
step: 1200, Loss: 0.1140732616186142
step: 1300, Loss: 0.11454784870147705
step: 1400, Loss: 0.11497463285923004
step: 1500, Loss: 0.11406461149454117
step: 1600, Loss: 0.1163012906908989
step: 1700, Loss: 0.11435944586992264
step: 1800, Loss: 0.11434721946716309
step: 1900, Loss: 0.11344143748283386
step: 2000, Loss: 0.11511243879795074
step: 2100, Loss: 0.11589289456605911
step: 2200, Loss: 0.11447569727897644
step: 2300, Loss: 0.11347584426403046
step: 2400, Loss: 0.11468670517206192
step: 2500, Loss: 0.1140107735991478
step: 2600, Loss: 0.11921043694019318
step: 2700, Loss: 0.11490971595048904
step: 2800, Loss: 0.11435188353061676
step: 2900, Loss: 0.11497706919908524
step: 3000, Loss: 0.11489695310592651
step: 3100, Loss: 0.1148756816983223
step: 3200, Loss: 0.11334814131259918
step: 3300, Loss: 0.11566682904958725
step: 3400, Loss: 0.11393540352582932
step: 3500, Loss: 0.11381634324789047
step: 3600, Loss: 0.11525285989046097
step: 3700, Loss: 0.11342868208885193
step: 3800, Loss: 0.11400870233774185
step: 3900, Loss: 0.1158880814909935
step: 4000, Loss: 0.11435428261756897
step: 4100, Loss: 0.11661523580551147
step: 4200, Loss: 0.11383221298456192
step: 4300, Loss: 0.11600688099861145
step: 4400, Loss: 0.11320238560438156
step: 4500, Loss: 0.11641846597194672
step: 4600, Loss: 0.11388367414474487
step: 4700, Loss: 0.11511875689029694
step: 4800, Loss: 0.1142546683549881
step: 4900, Loss: 0.11478009819984436
step: 5000, Loss: 0.11564991623163223
step: 5100, Loss: 0.11617808789014816
step: 5200, Loss: 0.11394695937633514
step: 5300, Loss: 0.11625903099775314
step: 5400, Loss: 0.1137140691280365
step: 5500, Loss: 0.11325851082801819
step: 5600, Loss: 0.11339884996414185
step: 5700, Loss: 0.12137982249259949
step: 5800, Loss: 0.15303762257099152
step: 5900, Loss: 0.13615499436855316
step: 6000, Loss: 0.131087064743042
step: 6100, Loss: 0.13382303714752197
step: 6200, Loss: 0.1276922971010208
step: 6300, Loss: 0.1220451071858406
step: 6400, Loss: 0.122441366314888
step: 6500, Loss: 0.12032884359359741
step: 6600, Loss: 0.1181224137544632
step: 6700, Loss: 0.1194041520357132
step: 6800, Loss: 0.11573192477226257
step: 6900, Loss: 0.12060369551181793
step: 7000, Loss: 0.11996052414178848
step: 7100, Loss: 0.11780650913715363
step: 7200, Loss: 0.121455617249012
step: 7300, Loss: 0.11964809149503708
step: 7400, Loss: 0.1169087141752243
step: 7500, Loss: 0.11780055612325668
step: 7600, Loss: 0.119892418384552
step: 7700, Loss: 0.1175093799829483
step: 7800, Loss: 0.11494030058383942
step: 7900, Loss: 0.11569157987833023
step: 8000, Loss: 0.11723436415195465
step: 8100, Loss: 0.11517897248268127
step: 8200, Loss: 0.11559469252824783
step: 8300, Loss: 0.11370282620191574
step: 8400, Loss: 0.11585795879364014
step: 8500, Loss: 0.11840496957302094
step: 8600, Loss: 0.11549670994281769
step: 8700, Loss: 0.11755678802728653
step: 8800, Loss: 0.11423662304878235
step: 8900, Loss: 0.11339535564184189
step: 9000, Loss: 0.11465877294540405
step: 9100, Loss: 0.11349032074213028
step: 9200, Loss: 0.11453650891780853
step: 9300, Loss: 0.11408746242523193
step: 9400, Loss: 0.11371055245399475
step: 9500, Loss: 0.11538688838481903
step: 9600, Loss: 0.11452756077051163
step: 9700, Loss: 0.1149473786354065
step: 9800, Loss: 0.11504200100898743
step: 9900, Loss: 0.11529311537742615
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8235294117647058
recall: 0.8235294117647058
step: 1900, Loss: 0.11549051851034164
step: 2000, Loss: 0.11443693935871124
step: 2100, Loss: 0.11423859000205994
step: 2200, Loss: 0.114805668592453
step: 2300, Loss: 0.11499930173158646
step: 2400, Loss: 0.11260967701673508
step: 2500, Loss: 0.11443178355693817
step: 2600, Loss: 0.1141546219587326
step: 2700, Loss: 0.11357205361127853
step: 2800, Loss: 0.11549487709999084
step: 2900, Loss: 0.1144091784954071
step: 3000, Loss: 0.11318458616733551
step: 3100, Loss: 0.11455283313989639
step: 3200, Loss: 0.11347286403179169
step: 3300, Loss: 0.11455951631069183
step: 3400, Loss: 0.1930215060710907
step: 3500, Loss: 0.1150345578789711
step: 3600, Loss: 0.11408376693725586
step: 3700, Loss: 0.11386805772781372
step: 3800, Loss: 0.11455739289522171
step: 3900, Loss: 0.11399564146995544
step: 4000, Loss: 0.1140931248664856
step: 4100, Loss: 0.11490626633167267
step: 4200, Loss: 0.11337562650442123
step: 4300, Loss: 0.1148165762424469
step: 4400, Loss: 0.11506371200084686
step: 4500, Loss: 0.1138816624879837
step: 4600, Loss: 0.11454833298921585
step: 4700, Loss: 0.11469081044197083
step: 4800, Loss: 0.11786326766014099
step: 4900, Loss: 0.11436101794242859
step: 5000, Loss: 0.1151004284620285
step: 5100, Loss: 0.1916431486606598
step: 5200, Loss: 0.23096568882465363
step: 5300, Loss: 0.2253788262605667
step: 5400, Loss: 0.13061629235744476
step: 5500, Loss: 0.12377139180898666
step: 5600, Loss: 0.12672308087348938
step: 5700, Loss: 0.12678563594818115
step: 5800, Loss: 0.12422636151313782
step: 5900, Loss: 0.12059151381254196
step: 6000, Loss: 0.12342504411935806
step: 6100, Loss: 0.11924374103546143
step: 6200, Loss: 0.11995813250541687
step: 6300, Loss: 0.11909066885709763
step: 6400, Loss: 0.11931478977203369
step: 6500, Loss: 0.11730746924877167
step: 6600, Loss: 0.11670848727226257
step: 6700, Loss: 0.11885146796703339
step: 6800, Loss: 0.11605211347341537
step: 6900, Loss: 0.12150558829307556
step: 7000, Loss: 0.11913486570119858
step: 7100, Loss: 0.11711228638887405
step: 7200, Loss: 0.19716212153434753
step: 7300, Loss: 0.11766674369573593
step: 7400, Loss: 0.11567509174346924
step: 7500, Loss: 0.11588332802057266
step: 7600, Loss: 0.11574404686689377
step: 7700, Loss: 0.11535629630088806
step: 7800, Loss: 0.11506160348653793
step: 7900, Loss: 0.11785122752189636
step: 8000, Loss: 0.11378875374794006
step: 8100, Loss: 0.11386673152446747
step: 8200, Loss: 0.11469196528196335
step: 8300, Loss: 0.1154731810092926
step: 8400, Loss: 0.11388866603374481
step: 8500, Loss: 0.11507454514503479
step: 8600, Loss: 0.11690796166658401
step: 8700, Loss: 0.11485400795936584
step: 8800, Loss: 0.11474042385816574
step: 8900, Loss: 0.1137767881155014
step: 9000, Loss: 0.1153397187590599
step: 9100, Loss: 0.19272491335868835
step: 9200, Loss: 0.1140572652220726
step: 9300, Loss: 0.11248672008514404
step: 9400, Loss: 0.11285170912742615
step: 9500, Loss: 0.11356903612613678
step: 9600, Loss: 0.11416857689619064
step: 9700, Loss: 0.11461751163005829
step: 9800, Loss: 0.11346802860498428
step: 9900, Loss: 0.11292339861392975
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9090909090909091
recall: 1.0
F_score: 0.9523809523809523
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.11494870483875275
step: 100, Loss: 0.12826849520206451
step: 200, Loss: 0.1240883469581604
step: 300, Loss: 0.11708369851112366
step: 400, Loss: 0.11703263968229294
step: 500, Loss: 0.11827421933412552
step: 600, Loss: 0.11915844678878784
step: 700, Loss: 0.11551731824874878
step: 800, Loss: 0.11540091782808304
step: 900, Loss: 0.11635789275169373
step: 1000, Loss: 0.11456296592950821
step: 1100, Loss: 0.11653818190097809
step: 1200, Loss: 0.11380814760923386
step: 1300, Loss: 0.11415152251720428
step: 1400, Loss: 0.11419187486171722
step: 1500, Loss: 0.19104354083538055
step: 1600, Loss: 0.11533095687627792
step: 1700, Loss: 0.11644542217254639
step: 1800, Loss: 0.11473049223423004
step: 1900, Loss: 0.11301928758621216
step: 2000, Loss: 0.11386167258024216
step: 2100, Loss: 0.11363638937473297
step: 2200, Loss: 0.1148214191198349
step: 2300, Loss: 0.1151968240737915
step: 2400, Loss: 0.11378981918096542
step: 2500, Loss: 0.11416956782341003
step: 2600, Loss: 0.11377406120300293
step: 2700, Loss: 0.1140308529138565
step: 2800, Loss: 0.11488309502601624
step: 2900, Loss: 0.11315014213323593
step: 3000, Loss: 0.11351184546947479
step: 3100, Loss: 0.11382145434617996
step: 3200, Loss: 0.11313265562057495
step: 3300, Loss: 0.11366262286901474
step: 3400, Loss: 0.1936480700969696
step: 3500, Loss: 0.11294467747211456
step: 3600, Loss: 0.11296973377466202
step: 3700, Loss: 0.11452291160821915
step: 3800, Loss: 0.11397551000118256
step: 3900, Loss: 0.11308018863201141
step: 4000, Loss: 0.11381734907627106
step: 4100, Loss: 0.11524757742881775
step: 4200, Loss: 0.11434867233037949
step: 4300, Loss: 0.1142536848783493
step: 4400, Loss: 0.11575078964233398
step: 4500, Loss: 0.11611267924308777
step: 4600, Loss: 0.11734846979379654
step: 4700, Loss: 0.11448348313570023
step: 4800, Loss: 0.11569324880838394
step: 4900, Loss: 0.11523307859897614
step: 5000, Loss: 0.11357742547988892
step: 5100, Loss: 0.11488770693540573
step: 5200, Loss: 0.11560674011707306
step: 5300, Loss: 0.1944633424282074
step: 5400, Loss: 0.11602377146482468
step: 5500, Loss: 0.1143273115158081
step: 5600, Loss: 0.11678808927536011
step: 5700, Loss: 0.11682906746864319
step: 5800, Loss: 0.16009093821048737
step: 5900, Loss: 0.18013909459114075
step: 6000, Loss: 0.1411876082420349
step: 6100, Loss: 0.12524457275867462
step: 6200, Loss: 0.12631526589393616
step: 6300, Loss: 0.13445940613746643
step: 6400, Loss: 0.12379292398691177
step: 6500, Loss: 0.1209956482052803
step: 6600, Loss: 0.1361360400915146
step: 6700, Loss: 0.12037304043769836
step: 6800, Loss: 0.1227387934923172
step: 6900, Loss: 0.12218931317329407
step: 7000, Loss: 0.12749896943569183
step: 7100, Loss: 0.12135382741689682
step: 7200, Loss: 0.20082685351371765
step: 7300, Loss: 0.11745736002922058
step: 7400, Loss: 0.11869898438453674
step: 7500, Loss: 0.1179298385977745
step: 7600, Loss: 0.12103085219860077
step: 7700, Loss: 0.11791038513183594
step: 7800, Loss: 0.11848028004169464
step: 7900, Loss: 0.1184864193201065
step: 8000, Loss: 0.1154317706823349
step: 8100, Loss: 0.11505873501300812
step: 8200, Loss: 0.11747779697179794
step: 8300, Loss: 0.11447083950042725
step: 8400, Loss: 0.11596432328224182
step: 8500, Loss: 0.11640191078186035
step: 8600, Loss: 0.11688186228275299
step: 8700, Loss: 0.11556907743215561
step: 8800, Loss: 0.11476011574268341
step: 8900, Loss: 0.11603327840566635
step: 9000, Loss: 0.11513790488243103
step: 9100, Loss: 0.19290821254253387
step: 9200, Loss: 0.11346624791622162
step: 9300, Loss: 0.11611147224903107
step: 9400, Loss: 0.11396722495555878
step: 9500, Loss: 0.11498021334409714
step: 9600, Loss: 0.11480861902236938
step: 9700, Loss: 0.11371662467718124
step: 9800, Loss: 0.11456822603940964
step: 9900, Loss: 0.11350210011005402
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9743589743589743
recall: 0.9743589743589743
F_score: 0.9743589743589743
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.7456490993499756
step: 100, Loss: 0.12388463318347931
step: 200, Loss: 0.12414410710334778
step: 300, Loss: 0.11867669969797134
step: 400, Loss: 0.1157602071762085
step: 500, Loss: 0.11493746936321259
step: 600, Loss: 0.11540399491786957
step: 700, Loss: 0.11562567204236984
step: 800, Loss: 0.11670124530792236
step: 900, Loss: 0.11665770411491394
step: 1000, Loss: 0.11636562645435333
step: 1100, Loss: 0.1136077493429184
step: 1200, Loss: 0.11536078155040741
step: 1300, Loss: 0.11484566330909729
step: 1400, Loss: 0.116029292345047
step: 1500, Loss: 0.19268959760665894
step: 1600, Loss: 0.11466898024082184
step: 1700, Loss: 0.11396174132823944
step: 1800, Loss: 0.11570006608963013
step: 1900, Loss: 0.11448651552200317
step: 2000, Loss: 0.11428696662187576
step: 2100, Loss: 0.11423812806606293
step: 2200, Loss: 0.11432504653930664
step: 2300, Loss: 0.11420831084251404
F_score: 0.8235294117647058
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.12711836397647858
step: 100, Loss: 0.12268468737602234
step: 200, Loss: 0.12004286050796509
step: 300, Loss: 0.11726333945989609
step: 400, Loss: 0.11470776796340942
step: 500, Loss: 0.1190025582909584
step: 600, Loss: 0.1146407350897789
step: 700, Loss: 0.11551184952259064
step: 800, Loss: 0.11583307385444641
step: 900, Loss: 0.11468352377414703
step: 1000, Loss: 0.11499800533056259
step: 1100, Loss: 0.11443748325109482
step: 1200, Loss: 0.11639410257339478
step: 1300, Loss: 0.11376189440488815
step: 1400, Loss: 0.11349955201148987
step: 1500, Loss: 0.11497648805379868
step: 1600, Loss: 0.11379120498895645
step: 1700, Loss: 0.11433228850364685
step: 1800, Loss: 0.11451162397861481
step: 1900, Loss: 0.11554291099309921
step: 2000, Loss: 0.11409568786621094
step: 2100, Loss: 0.11516273021697998
step: 2200, Loss: 0.11296319216489792
step: 2300, Loss: 0.11603833734989166
step: 2400, Loss: 0.11389194428920746
step: 2500, Loss: 0.11384574323892593
step: 2600, Loss: 0.11282271146774292
step: 2700, Loss: 0.114753857254982
step: 2800, Loss: 0.11363860964775085
step: 2900, Loss: 0.1146918460726738
step: 3000, Loss: 0.11354829370975494
step: 3100, Loss: 0.11561331152915955
step: 3200, Loss: 0.11355902999639511
step: 3300, Loss: 0.11508072912693024
step: 3400, Loss: 0.1137382984161377
step: 3500, Loss: 0.11514291912317276
step: 3600, Loss: 0.11507561802864075
step: 3700, Loss: 0.11391079425811768
step: 3800, Loss: 0.11505738645792007
step: 3900, Loss: 0.11388027667999268
step: 4000, Loss: 0.11430638283491135
step: 4100, Loss: 0.11460168659687042
step: 4200, Loss: 0.11472812294960022
step: 4300, Loss: 0.11459678411483765
step: 4400, Loss: 0.11397220194339752
step: 4500, Loss: 0.11567672342061996
step: 4600, Loss: 0.11362866312265396
step: 4700, Loss: 0.11583027243614197
step: 4800, Loss: 0.1133197546005249
step: 4900, Loss: 0.11487958580255508
step: 5000, Loss: 1.3773393630981445
step: 5100, Loss: 0.14529138803482056
step: 5200, Loss: 0.13177378475666046
step: 5300, Loss: 0.13860024511814117
step: 5400, Loss: 0.1270122081041336
step: 5500, Loss: 0.12891867756843567
step: 5600, Loss: 0.1180763691663742
step: 5700, Loss: 0.12695029377937317
step: 5800, Loss: 0.12116913497447968
step: 5900, Loss: 0.12194584310054779
step: 6000, Loss: 0.12093169242143631
step: 6100, Loss: 0.12647834420204163
step: 6200, Loss: 0.11689667403697968
step: 6300, Loss: 0.12093280255794525
step: 6400, Loss: 0.11767967790365219
step: 6500, Loss: 0.11839541792869568
step: 6600, Loss: 0.11489316076040268
step: 6700, Loss: 0.11660312116146088
step: 6800, Loss: 0.11872737854719162
step: 6900, Loss: 0.11535239964723587
step: 7000, Loss: 0.11514361202716827
step: 7100, Loss: 0.11636246740818024
step: 7200, Loss: 0.1165645569562912
step: 7300, Loss: 0.11862392723560333
step: 7400, Loss: 0.11721514165401459
step: 7500, Loss: 0.11651916056871414
step: 7600, Loss: 0.11640822142362595
step: 7700, Loss: 0.11570213735103607
step: 7800, Loss: 0.11706747114658356
step: 7900, Loss: 0.117508664727211
step: 8000, Loss: 0.11471119523048401
step: 8100, Loss: 0.1165909394621849
step: 8200, Loss: 0.11558863520622253
step: 8300, Loss: 0.1166510283946991
step: 8400, Loss: 0.11504032462835312
step: 8500, Loss: 0.11491265147924423
step: 8600, Loss: 0.11470235139131546
step: 8700, Loss: 0.1262820065021515
step: 8800, Loss: 0.11487570405006409
step: 8900, Loss: 0.1156584769487381
step: 9000, Loss: 0.11338820308446884
step: 9100, Loss: 0.1162516176700592
step: 9200, Loss: 0.1163400411605835
step: 9300, Loss: 0.11534135043621063
step: 9400, Loss: 0.1149386614561081
step: 9500, Loss: 0.11470995843410492
step: 9600, Loss: 0.1150926873087883
step: 9700, Loss: 0.11445996910333633
step: 9800, Loss: 0.1139247864484787
step: 9900, Loss: 0.11405229568481445
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.6666666666666666
recall: 1.0
F_score: 0.8
subject 13 Avgacc: 0.8045833333333334 Avgfscore: 0.8123943637912202 
 Max acc:0.8666666666666667, Max f score:0.8947368421052632
******** mix subject_14 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 33.91822052001953
step: 100, Loss: 2.6501996517181396
step: 200, Loss: 0.1366572380065918
step: 300, Loss: 0.14374247193336487
step: 400, Loss: 0.12679459154605865
step: 500, Loss: 0.12685568630695343
step: 600, Loss: 0.12559962272644043
step: 700, Loss: 0.12406283617019653
step: 800, Loss: 0.1261693686246872
step: 900, Loss: 0.11992254853248596
step: 1000, Loss: 0.12388229370117188
step: 1100, Loss: 0.12483992427587509
step: 1200, Loss: 0.12366160750389099
step: 1300, Loss: 0.11920338124036789
step: 1400, Loss: 0.125011146068573
step: 1500, Loss: 0.11680907011032104
step: 1600, Loss: 0.1211370900273323
step: 1700, Loss: 0.11915843188762665
step: 1800, Loss: 0.11775103956460953
step: 1900, Loss: 0.1183006688952446
step: 2000, Loss: 0.11805807799100876
step: 2100, Loss: 0.12575556337833405
step: 2200, Loss: 0.11978739500045776
step: 2300, Loss: 0.12227332592010498
step: 2400, Loss: 0.11963032186031342
step: 2500, Loss: 0.1169755682349205
step: 2600, Loss: 0.11540932953357697
step: 2700, Loss: 0.12050265818834305
step: 2800, Loss: 0.118641696870327
step: 2900, Loss: 0.11952341347932816
step: 3000, Loss: 0.6024328470230103
step: 3100, Loss: 0.15240950882434845
step: 3200, Loss: 0.13453635573387146
step: 3300, Loss: 0.14182297885417938
step: 3400, Loss: 0.13652878999710083
step: 3500, Loss: 0.1402384638786316
step: 3600, Loss: 0.1369946002960205
step: 3700, Loss: 0.12775075435638428
step: 3800, Loss: 0.11993519216775894
step: 3900, Loss: 0.12539012730121613
step: 4000, Loss: 0.12030907720327377
step: 4100, Loss: 0.12551432847976685
step: 4200, Loss: 0.11658082902431488
step: 4300, Loss: 0.12452320754528046
step: 4400, Loss: 0.11631179600954056
step: 4500, Loss: 0.12035258114337921
step: 4600, Loss: 0.11894527077674866
step: 4700, Loss: 0.1222318559885025
step: 4800, Loss: 0.11800909042358398
step: 4900, Loss: 0.12139143794775009
step: 5000, Loss: 0.117937371134758
step: 5100, Loss: 0.11661815643310547
step: 5200, Loss: 0.1219840943813324
step: 5300, Loss: 0.120023712515831
step: 5400, Loss: 0.11533728986978531
step: 5500, Loss: 0.11673782020807266
step: 5600, Loss: 0.11495783179998398
step: 5700, Loss: 0.11535695195198059
step: 5800, Loss: 0.11557861417531967
step: 5900, Loss: 0.11754452437162399
step: 6000, Loss: 0.1162957027554512
step: 6100, Loss: 0.11409323662519455
step: 6200, Loss: 0.11440902203321457
step: 6300, Loss: 0.11403531581163406
step: 6400, Loss: 0.11538571119308472
step: 6500, Loss: 0.11670786887407303
step: 6600, Loss: 0.1164339929819107
step: 6700, Loss: 0.11914348602294922
step: 6800, Loss: 0.11392000317573547
step: 6900, Loss: 0.11405573040246964
step: 7000, Loss: 0.11519797146320343
step: 7100, Loss: 0.11391331255435944
step: 7200, Loss: 0.11455884575843811
step: 7300, Loss: 0.11512541770935059
step: 7400, Loss: 0.11641865223646164
step: 7500, Loss: 0.11569264531135559
step: 7600, Loss: 0.11637862026691437
step: 7700, Loss: 0.11465273052453995
step: 7800, Loss: 0.1152060478925705
step: 7900, Loss: 0.11427345871925354
step: 8000, Loss: 0.11673301458358765
step: 8100, Loss: 0.11464245617389679
step: 8200, Loss: 0.11956113576889038
step: 8300, Loss: 0.12280028313398361
step: 8400, Loss: 0.11606746912002563
step: 8500, Loss: 0.11502658575773239
step: 8600, Loss: 0.1152220293879509
step: 8700, Loss: 0.11577825248241425
step: 8800, Loss: 0.11385447531938553
step: 8900, Loss: 0.11644142866134644
step: 9000, Loss: 0.1155305877327919
step: 9100, Loss: 0.11482865363359451
step: 9200, Loss: 0.11485034227371216
step: 9300, Loss: 0.11875276267528534
step: 9400, Loss: 0.11589829623699188
step: 9500, Loss: 0.11527132242918015
step: 9600, Loss: 0.11661266535520554
step: 9700, Loss: 0.11474652588367462
step: 9800, Loss: 0.11442048847675323
step: 9900, Loss: 0.11589443683624268
training successfully ended.
validating...
validate data length:32
acc: 0.65625
precision: 0.5238095238095238
recall: 0.9166666666666666
F_score: 0.6666666666666667
******fold 2******

Training... train_data length:280
step: 2400, Loss: 0.11329314112663269
step: 2500, Loss: 0.11328187584877014
step: 2600, Loss: 0.1136273592710495
step: 2700, Loss: 0.1158987432718277
step: 2800, Loss: 0.11376558989286423
step: 2900, Loss: 0.11359970271587372
step: 3000, Loss: 0.11325552314519882
step: 3100, Loss: 0.11476732045412064
step: 3200, Loss: 0.11441736668348312
step: 3300, Loss: 0.11404331773519516
step: 3400, Loss: 0.19164445996284485
step: 3500, Loss: 0.1133967712521553
step: 3600, Loss: 0.11306512355804443
step: 3700, Loss: 0.11367146670818329
step: 3800, Loss: 0.11369803547859192
step: 3900, Loss: 0.11472710967063904
step: 4000, Loss: 0.11416950076818466
step: 4100, Loss: 0.11311642825603485
step: 4200, Loss: 0.11524465680122375
step: 4300, Loss: 0.1148901954293251
step: 4400, Loss: 0.11408506333827972
step: 4500, Loss: 0.11388569325208664
step: 4600, Loss: 0.11418727040290833
step: 4700, Loss: 0.1143074780702591
step: 4800, Loss: 0.11605013906955719
step: 4900, Loss: 0.2150982916355133
step: 5000, Loss: 0.17399591207504272
step: 5100, Loss: 0.13881167769432068
step: 5200, Loss: 0.12773391604423523
step: 5300, Loss: 0.22494150698184967
step: 5400, Loss: 0.12297284603118896
step: 5500, Loss: 0.1261363923549652
step: 5600, Loss: 0.12253405153751373
step: 5700, Loss: 0.12515389919281006
step: 5800, Loss: 0.11893443763256073
step: 5900, Loss: 0.1193763017654419
step: 6000, Loss: 0.12455964088439941
step: 6100, Loss: 0.11765318363904953
step: 6200, Loss: 0.12136155366897583
step: 6300, Loss: 0.12269161641597748
step: 6400, Loss: 0.11691071093082428
step: 6500, Loss: 0.11612052470445633
step: 6600, Loss: 0.11924321949481964
step: 6700, Loss: 0.11831144988536835
step: 6800, Loss: 0.11772314459085464
step: 6900, Loss: 0.11739727109670639
step: 7000, Loss: 0.11596983671188354
step: 7100, Loss: 0.12336388230323792
step: 7200, Loss: 0.19870637357234955
step: 7300, Loss: 0.11643673479557037
step: 7400, Loss: 0.117381252348423
step: 7500, Loss: 0.11419856548309326
step: 7600, Loss: 0.1180514469742775
step: 7700, Loss: 0.11580771952867508
step: 7800, Loss: 0.11549945175647736
step: 7900, Loss: 0.11428641527891159
step: 8000, Loss: 0.11658932268619537
step: 8100, Loss: 0.11504831910133362
step: 8200, Loss: 0.1147054135799408
step: 8300, Loss: 0.11512012034654617
step: 8400, Loss: 0.1155039519071579
step: 8500, Loss: 0.11638054251670837
step: 8600, Loss: 0.11650267988443375
step: 8700, Loss: 0.11532935500144958
step: 8800, Loss: 0.11331384629011154
step: 8900, Loss: 0.11488976329565048
step: 9000, Loss: 0.11504752933979034
step: 9100, Loss: 0.1950492560863495
step: 9200, Loss: 0.11337107419967651
step: 9300, Loss: 0.11345353722572327
step: 9400, Loss: 0.11355295777320862
step: 9500, Loss: 0.11341660469770432
step: 9600, Loss: 0.11333489418029785
step: 9700, Loss: 0.11361929774284363
step: 9800, Loss: 0.11494851112365723
step: 9900, Loss: 0.11386127769947052
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9736842105263158
recall: 1.0
F_score: 0.9866666666666666
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11377689242362976
step: 100, Loss: 0.11906048655509949
step: 200, Loss: 0.11590732634067535
step: 300, Loss: 0.1162252351641655
step: 400, Loss: 0.11622323095798492
step: 500, Loss: 0.11619917303323746
step: 600, Loss: 0.11447679996490479
step: 700, Loss: 0.11563603579998016
step: 800, Loss: 0.11337701976299286
step: 900, Loss: 0.1142880767583847
step: 1000, Loss: 0.11414869874715805
step: 1100, Loss: 0.11294427514076233
step: 1200, Loss: 0.1129315048456192
step: 1300, Loss: 0.11521492153406143
step: 1400, Loss: 0.11376375705003738
step: 1500, Loss: 0.19591088593006134
step: 1600, Loss: 0.11429554224014282
step: 1700, Loss: 0.11471733450889587
step: 1800, Loss: 0.11389049887657166
step: 1900, Loss: 0.11290813237428665
step: 2000, Loss: 0.11780267208814621
step: 2100, Loss: 0.1156809851527214
step: 2200, Loss: 0.11438475549221039
step: 2300, Loss: 0.11404785513877869
step: 2400, Loss: 0.11288953572511673
step: 2500, Loss: 0.1138841062784195
step: 2600, Loss: 0.1145138144493103
step: 2700, Loss: 0.11357250064611435
step: 2800, Loss: 0.11468515545129776
step: 2900, Loss: 0.11390923708677292
step: 3000, Loss: 0.11316051334142685
step: 3100, Loss: 0.11416066437959671
step: 3200, Loss: 0.11470519006252289
step: 3300, Loss: 0.1147354319691658
step: 3400, Loss: 0.1937621682882309
step: 3500, Loss: 0.1185944527387619
step: 3600, Loss: 0.31291767954826355
step: 3700, Loss: 0.1492835283279419
step: 3800, Loss: 0.12575291097164154
step: 3900, Loss: 0.12390667200088501
step: 4000, Loss: 0.12185728549957275
step: 4100, Loss: 0.12479941546916962
step: 4200, Loss: 0.1170375868678093
step: 4300, Loss: 0.1229969933629036
step: 4400, Loss: 0.12305684387683868
step: 4500, Loss: 0.11827709525823593
step: 4600, Loss: 0.11662323027849197
step: 4700, Loss: 0.11891210824251175
step: 4800, Loss: 0.12219421565532684
step: 4900, Loss: 0.11618620157241821
step: 5000, Loss: 0.12127725034952164
step: 5100, Loss: 0.11812811344861984
step: 5200, Loss: 0.11721892654895782
step: 5300, Loss: 0.20071154832839966
step: 5400, Loss: 0.11893440037965775
step: 5500, Loss: 0.1190597340464592
step: 5600, Loss: 0.11620451509952545
step: 5700, Loss: 0.12017641961574554
step: 5800, Loss: 0.11868663877248764
step: 5900, Loss: 0.11498626321554184
step: 6000, Loss: 0.11644173413515091
step: 6100, Loss: 0.11531762033700943
step: 6200, Loss: 0.11579160392284393
step: 6300, Loss: 0.11671439558267593
step: 6400, Loss: 0.11516477912664413
step: 6500, Loss: 0.11561895906925201
step: 6600, Loss: 0.11419070512056351
step: 6700, Loss: 0.11603239178657532
step: 6800, Loss: 0.11499569565057755
step: 6900, Loss: 0.11476372182369232
step: 7000, Loss: 0.11530248820781708
step: 7100, Loss: 0.11514267325401306
step: 7200, Loss: 0.19485384225845337
step: 7300, Loss: 0.11493932455778122
step: 7400, Loss: 0.11395996063947678
step: 7500, Loss: 0.11549640446901321
step: 7600, Loss: 0.11547195911407471
step: 7700, Loss: 0.11414501070976257
step: 7800, Loss: 0.11407366394996643
step: 7900, Loss: 0.11372192949056625
step: 8000, Loss: 0.1129913404583931
step: 8100, Loss: 0.11403004825115204
step: 8200, Loss: 0.11375240981578827
step: 8300, Loss: 0.11457093805074692
step: 8400, Loss: 0.11381202191114426
step: 8500, Loss: 0.1134481132030487
step: 8600, Loss: 0.11433272063732147
step: 8700, Loss: 0.11390877515077591
step: 8800, Loss: 0.11416111886501312
step: 8900, Loss: 0.1129060611128807
step: 9000, Loss: 0.1139691099524498
step: 9100, Loss: 0.19026340544223785
step: 9200, Loss: 0.11345168948173523
step: 9300, Loss: 0.11376731842756271
step: 9400, Loss: 0.1142835021018982
step: 9500, Loss: 0.11321274936199188
step: 9600, Loss: 0.11331118643283844
step: 9700, Loss: 0.11264128983020782
step: 9800, Loss: 0.11413755267858505
step: 9900, Loss: 0.11380837857723236
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9705882352941176
recall: 1.0
F_score: 0.9850746268656716
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11504051834344864
step: 100, Loss: 0.1160828024148941
step: 200, Loss: 0.11634373664855957
step: 300, Loss: 0.11798848956823349
step: 400, Loss: 0.11461905390024185
step: 500, Loss: 0.1135987862944603
step: 600, Loss: 0.11427110433578491
step: 700, Loss: 0.11495427042245865
step: 800, Loss: 0.11341555416584015
step: 900, Loss: 0.11451154947280884
step: 1000, Loss: 0.11375828087329865
step: 1100, Loss: 0.11369030177593231
step: 1200, Loss: 0.11365155875682831
step: 1300, Loss: 0.11325038969516754
step: 1400, Loss: 0.11357961595058441
step: 1500, Loss: 0.19274257123470306
step: 1600, Loss: 0.11391500383615494
step: 1700, Loss: 0.11363448202610016
step: 1800, Loss: 0.11367575079202652
step: 1900, Loss: 0.11507914215326309
step: 2000, Loss: 0.11388448625802994
step: 2100, Loss: 0.11317721009254456
step: 2200, Loss: 0.11365340650081635
step: 2300, Loss: 0.11415032297372818
step: 2400, Loss: 0.1144309937953949
step: 2500, Loss: 0.1150476485490799
step: 2600, Loss: 0.11568799614906311
step: 2700, Loss: 0.1146039366722107
step: 2800, Loss: 0.11372675746679306
step: 0, Loss: 2.7571322917938232
step: 100, Loss: 0.12448355555534363
step: 200, Loss: 0.11755067110061646
step: 300, Loss: 0.11879034340381622
step: 400, Loss: 0.11994519084692001
step: 500, Loss: 0.12072356790304184
step: 600, Loss: 0.11729484051465988
step: 700, Loss: 0.11462556570768356
step: 800, Loss: 0.11555058509111404
step: 900, Loss: 0.11483646929264069
step: 1000, Loss: 0.11546184867620468
step: 1100, Loss: 0.11497616767883301
step: 1200, Loss: 0.11500359326601028
step: 1300, Loss: 0.1135023981332779
step: 1400, Loss: 0.11534427106380463
step: 1500, Loss: 0.11482077836990356
step: 1600, Loss: 0.11435555666685104
step: 1700, Loss: 0.11301207542419434
step: 1800, Loss: 0.11802689731121063
step: 1900, Loss: 0.1137285903096199
step: 2000, Loss: 0.11440172046422958
step: 2100, Loss: 0.11362477391958237
step: 2200, Loss: 0.11418797075748444
step: 2300, Loss: 0.11559482663869858
step: 2400, Loss: 0.11484578251838684
step: 2500, Loss: 0.11370094120502472
step: 2600, Loss: 0.11344587802886963
step: 2700, Loss: 0.11434400081634521
step: 2800, Loss: 0.25052034854888916
step: 2900, Loss: 0.1566949188709259
step: 3000, Loss: 0.14659859240055084
step: 3100, Loss: 0.14668554067611694
step: 3200, Loss: 0.1334531307220459
step: 3300, Loss: 0.12917977571487427
step: 3400, Loss: 0.12521305680274963
step: 3500, Loss: 0.1306661069393158
step: 3600, Loss: 0.12263840436935425
step: 3700, Loss: 0.12436290830373764
step: 3800, Loss: 0.12263675779104233
step: 3900, Loss: 0.1221838966012001
step: 4000, Loss: 0.12438692897558212
step: 4100, Loss: 0.12302064895629883
step: 4200, Loss: 0.1167914941906929
step: 4300, Loss: 0.11861933767795563
step: 4400, Loss: 0.1196688860654831
step: 4500, Loss: 0.12098715454339981
step: 4600, Loss: 0.11645843833684921
step: 4700, Loss: 0.11657366901636124
step: 4800, Loss: 0.11522920429706573
step: 4900, Loss: 0.11915076524019241
step: 5000, Loss: 0.11627844721078873
step: 5100, Loss: 0.11621101945638657
step: 5200, Loss: 0.11485646665096283
step: 5300, Loss: 0.11978627741336823
step: 5400, Loss: 0.11622440069913864
step: 5500, Loss: 0.11468784511089325
step: 5600, Loss: 0.11421412974596024
step: 5700, Loss: 0.11636906862258911
step: 5800, Loss: 0.11500947177410126
step: 5900, Loss: 0.11431003361940384
step: 6000, Loss: 0.11589745432138443
step: 6100, Loss: 0.11583532392978668
step: 6200, Loss: 0.1162363737821579
step: 6300, Loss: 0.11306649446487427
step: 6400, Loss: 0.1149011179804802
step: 6500, Loss: 0.11533689498901367
step: 6600, Loss: 0.11510727554559708
step: 6700, Loss: 0.11529605090618134
step: 6800, Loss: 0.11599622666835785
step: 6900, Loss: 0.11479976773262024
step: 7000, Loss: 0.11496581882238388
step: 7100, Loss: 0.11422187089920044
step: 7200, Loss: 0.11507907509803772
step: 7300, Loss: 0.114516481757164
step: 7400, Loss: 0.1152229756116867
step: 7500, Loss: 0.11552402377128601
step: 7600, Loss: 0.1134248822927475
step: 7700, Loss: 0.11395813524723053
step: 7800, Loss: 0.1143449917435646
step: 7900, Loss: 0.11511589586734772
step: 8000, Loss: 0.11331604421138763
step: 8100, Loss: 0.1151052936911583
step: 8200, Loss: 0.11573953926563263
step: 8300, Loss: 0.11362786591053009
step: 8400, Loss: 0.11408361047506332
step: 8500, Loss: 0.11449713259935379
step: 8600, Loss: 0.11416687071323395
step: 8700, Loss: 0.11573682725429535
step: 8800, Loss: 0.11631570756435394
step: 8900, Loss: 0.1161520779132843
step: 9000, Loss: 0.11478017270565033
step: 9100, Loss: 0.11526231467723846
step: 9200, Loss: 0.11358048766851425
step: 9300, Loss: 0.11518104374408722
step: 9400, Loss: 0.11520988494157791
step: 9500, Loss: 0.11438076198101044
step: 9600, Loss: 0.11443574726581573
step: 9700, Loss: 0.11405067145824432
step: 9800, Loss: 0.11530202627182007
step: 9900, Loss: 0.11362942308187485
training successfully ended.
validating...
validate data length:32
acc: 0.84375
precision: 0.7894736842105263
recall: 0.9375
F_score: 0.8571428571428572
******fold 3******

Training... train_data length:281
step: 0, Loss: 1.5988022089004517
step: 100, Loss: 0.11867467314004898
step: 200, Loss: 0.11572030186653137
step: 300, Loss: 0.11530013382434845
step: 400, Loss: 0.11698546260595322
step: 500, Loss: 0.1149841919541359
step: 600, Loss: 0.11420789361000061
step: 700, Loss: 0.11307160556316376
step: 800, Loss: 0.11536571383476257
step: 900, Loss: 0.11434005945920944
step: 1000, Loss: 0.11479814350605011
step: 1100, Loss: 0.1141679584980011
step: 1200, Loss: 0.11327160894870758
step: 1300, Loss: 0.11441533267498016
step: 1400, Loss: 0.11395116150379181
step: 1500, Loss: 0.11451831459999084
step: 1600, Loss: 0.11335745453834534
step: 1700, Loss: 0.11499730497598648
step: 1800, Loss: 0.11411430686712265
step: 1900, Loss: 0.11449643969535828
step: 2000, Loss: 0.11338566988706589
step: 2100, Loss: 0.1132839247584343
step: 2200, Loss: 0.11443306505680084
step: 2300, Loss: 0.11427737027406693
step: 2400, Loss: 0.1135224848985672
step: 2500, Loss: 0.11413303017616272
step: 2600, Loss: 0.11425440013408661
step: 2700, Loss: 0.11341825127601624
step: 2800, Loss: 0.11619147658348083
step: 2900, Loss: 0.11365341395139694
step: 3000, Loss: 0.11433324217796326
step: 3100, Loss: 0.11421213299036026
step: 3200, Loss: 0.11461938172578812
step: 3300, Loss: 0.11519847065210342
step: 3400, Loss: 0.11415642499923706
step: 3500, Loss: 0.11516420543193817
step: 3600, Loss: 0.11457861214876175
step: 3700, Loss: 0.1174350157380104
step: 3800, Loss: 0.11519896239042282
step: 3900, Loss: 0.11451969295740128
step: 4000, Loss: 0.11523091048002243
step: 4100, Loss: 0.11393433064222336
step: 4200, Loss: 0.11603502929210663
step: 4300, Loss: 0.1143656075000763
step: 4400, Loss: 0.11396109312772751
step: 4500, Loss: 0.11717094480991364
step: 4600, Loss: 0.11450975388288498
step: 4700, Loss: 0.11616368591785431
step: 4800, Loss: 0.11578531563282013
step: 4900, Loss: 0.11502279341220856
step: 5000, Loss: 0.11464346200227737
step: 5100, Loss: 0.11541669070720673
step: 5200, Loss: 0.11376403272151947
step: 5300, Loss: 0.11385755985975266
step: 5400, Loss: 0.1147802323102951
step: 5500, Loss: 0.11606436967849731
step: 5600, Loss: 0.11417987197637558
step: 5700, Loss: 0.11447830498218536
step: 5800, Loss: 0.11301802843809128
step: 5900, Loss: 0.1137072741985321
step: 6000, Loss: 0.1141912192106247
step: 6100, Loss: 0.11411963403224945
step: 6200, Loss: 0.11424068361520767
step: 6300, Loss: 0.11508478224277496
step: 6400, Loss: 0.11351273953914642
step: 6500, Loss: 0.11522268503904343
step: 6600, Loss: 0.11468581855297089
step: 6700, Loss: 0.11476929485797882
step: 6800, Loss: 0.11379962414503098
step: 6900, Loss: 0.4660910964012146
step: 7000, Loss: 0.16172125935554504
step: 7100, Loss: 0.12925578653812408
step: 7200, Loss: 0.1348927617073059
step: 7300, Loss: 0.1286676824092865
step: 7400, Loss: 0.12882113456726074
step: 7500, Loss: 0.12214959412813187
step: 7600, Loss: 0.12587741017341614
step: 7700, Loss: 0.12065213918685913
step: 7800, Loss: 0.11855337768793106
step: 7900, Loss: 0.11854669451713562
step: 8000, Loss: 0.11831134557723999
step: 8100, Loss: 0.11733351647853851
step: 8200, Loss: 0.11799784749746323
step: 8300, Loss: 0.12497712671756744
step: 8400, Loss: 0.1198602169752121
step: 8500, Loss: 0.11709717661142349
step: 8600, Loss: 0.11741665005683899
step: 8700, Loss: 0.11621060967445374
step: 8800, Loss: 0.11646926403045654
step: 8900, Loss: 0.11732372641563416
step: 9000, Loss: 0.1154128760099411
step: 9100, Loss: 0.11699855327606201
step: 9200, Loss: 0.11508030444383621
step: 9300, Loss: 0.11650721728801727
step: 9400, Loss: 0.11523003876209259
step: 9500, Loss: 0.11477361619472504
step: 9600, Loss: 0.11571717262268066
step: 9700, Loss: 0.11449822783470154
step: 9800, Loss: 0.114585742354393
step: 9900, Loss: 0.11540374159812927
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.6842105263157895
recall: 0.8666666666666667
F_score: 0.7647058823529413
******fold 4******

Training... train_data length:281
step: 0, Loss: 1.4717466831207275
step: 100, Loss: 0.12080125510692596
step: 200, Loss: 0.11581870913505554
step: 300, Loss: 0.11522407084703445
step: 400, Loss: 0.11417322605848312
step: 2900, Loss: 0.11351688951253891
step: 3000, Loss: 0.11353226751089096
step: 3100, Loss: 0.11378192156553268
step: 3200, Loss: 0.4170919954776764
step: 3300, Loss: 0.26079225540161133
step: 3400, Loss: 0.24522067606449127
step: 3500, Loss: 0.1297287791967392
step: 3600, Loss: 0.12695211172103882
step: 3700, Loss: 0.13174214959144592
step: 3800, Loss: 0.12320659309625626
step: 3900, Loss: 0.11887440830469131
step: 4000, Loss: 0.12019272148609161
step: 4100, Loss: 0.11863373219966888
step: 4200, Loss: 0.11664005368947983
step: 4300, Loss: 0.12499231845140457
step: 4400, Loss: 0.1321209967136383
step: 4500, Loss: 0.1190730631351471
step: 4600, Loss: 0.11515449732542038
step: 4700, Loss: 0.11804382503032684
step: 4800, Loss: 0.11722788214683533
step: 4900, Loss: 0.11694563925266266
step: 5000, Loss: 0.1148143783211708
step: 5100, Loss: 0.11990652233362198
step: 5200, Loss: 0.11976046860218048
step: 5300, Loss: 0.20902061462402344
step: 5400, Loss: 0.11592601239681244
step: 5500, Loss: 0.11593268811702728
step: 5600, Loss: 0.11663991212844849
step: 5700, Loss: 0.11711077392101288
step: 5800, Loss: 0.11522651463747025
step: 5900, Loss: 0.11573488265275955
step: 6000, Loss: 0.11482768505811691
step: 6100, Loss: 0.1126793771982193
step: 6200, Loss: 0.11517278850078583
step: 6300, Loss: 0.11492843925952911
step: 6400, Loss: 0.11557111889123917
step: 6500, Loss: 0.11420794576406479
step: 6600, Loss: 0.11549527198076248
step: 6700, Loss: 0.11547274887561798
step: 6800, Loss: 0.1144934743642807
step: 6900, Loss: 0.11452813446521759
step: 7000, Loss: 0.11491677910089493
step: 7100, Loss: 0.11461231112480164
step: 7200, Loss: 0.19702719151973724
step: 7300, Loss: 0.11397146433591843
step: 7400, Loss: 0.11544065177440643
step: 7500, Loss: 0.11409573256969452
step: 7600, Loss: 0.11337457597255707
step: 7700, Loss: 0.1163109838962555
step: 7800, Loss: 0.11483829468488693
step: 7900, Loss: 0.11354043334722519
step: 8000, Loss: 0.11352237313985825
step: 8100, Loss: 0.1143176406621933
step: 8200, Loss: 0.11249154806137085
step: 8300, Loss: 0.11355823278427124
step: 8400, Loss: 0.1136457547545433
step: 8500, Loss: 0.11363770812749863
step: 8600, Loss: 0.11338935047388077
step: 8700, Loss: 0.11471694707870483
step: 8800, Loss: 0.11297557502985
step: 8900, Loss: 0.11329418420791626
step: 9000, Loss: 0.11463397741317749
step: 9100, Loss: 0.19324195384979248
step: 9200, Loss: 0.1148938238620758
step: 9300, Loss: 0.1145232766866684
step: 9400, Loss: 0.11290787160396576
step: 9500, Loss: 0.11489806324243546
step: 9600, Loss: 0.11403578519821167
step: 9700, Loss: 0.1138603687286377
step: 9800, Loss: 0.11416256427764893
step: 9900, Loss: 0.11345308274030685
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.868421052631579
recall: 1.0
F_score: 0.9295774647887324
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11498560011386871
step: 100, Loss: 0.12449664622545242
step: 200, Loss: 0.117466039955616
step: 300, Loss: 0.1163410171866417
step: 400, Loss: 0.11560947448015213
step: 500, Loss: 0.11436453461647034
step: 600, Loss: 0.11582138389348984
step: 700, Loss: 0.1143316775560379
step: 800, Loss: 0.11335072666406631
step: 900, Loss: 0.11313392221927643
step: 1000, Loss: 0.11509837955236435
step: 1100, Loss: 0.11424989253282547
step: 1200, Loss: 0.11433501541614532
step: 1300, Loss: 0.11373420059680939
step: 1400, Loss: 0.11400220543146133
step: 1500, Loss: 0.19269506633281708
step: 1600, Loss: 0.11358007788658142
step: 1700, Loss: 0.11380919069051743
step: 1800, Loss: 0.11344237625598907
step: 1900, Loss: 0.11415845155715942
step: 2000, Loss: 0.11407957971096039
step: 2100, Loss: 0.11339736729860306
step: 2200, Loss: 0.11348433047533035
step: 2300, Loss: 0.11285477876663208
step: 2400, Loss: 0.1132226437330246
step: 2500, Loss: 0.11525136977434158
step: 2600, Loss: 0.11381018906831741
step: 2700, Loss: 0.11420769989490509
step: 2800, Loss: 0.11405801773071289
step: 2900, Loss: 0.11357615888118744
step: 3000, Loss: 0.11465457081794739
step: 3100, Loss: 0.11232080310583115
step: 3200, Loss: 0.11371149122714996
step: 3300, Loss: 0.11385105550289154
step: 3400, Loss: 0.1919499635696411
step: 3500, Loss: 0.11507753282785416
step: 3600, Loss: 0.1148679330945015
step: 3700, Loss: 0.1138063371181488
step: 3800, Loss: 0.11547934263944626
step: 3900, Loss: 0.1150929182767868
step: 4000, Loss: 0.1203603446483612
step: 4100, Loss: 1.0594420433044434
step: 4200, Loss: 0.32731789350509644
step: 4300, Loss: 0.1326122283935547
step: 4400, Loss: 0.12603537738323212
step: 4500, Loss: 0.12659358978271484
step: 4600, Loss: 0.12091100960969925
step: 4700, Loss: 0.11978290975093842
step: 4800, Loss: 0.12184857577085495
step: 4900, Loss: 0.11968468874692917
step: 5000, Loss: 0.1199987381696701
step: 5100, Loss: 0.11933670938014984
step: 5200, Loss: 0.122174933552742
step: 5300, Loss: 0.20119184255599976
step: 5400, Loss: 0.11973194777965546
step: 5500, Loss: 0.11930952221155167
step: 5600, Loss: 0.1222655326128006
step: 5700, Loss: 0.11690344661474228
step: 5800, Loss: 0.11551176011562347
step: 5900, Loss: 0.11900248378515244
step: 6000, Loss: 0.11723221838474274
step: 6100, Loss: 0.11514879763126373
step: 6200, Loss: 0.11757346987724304
step: 6300, Loss: 0.11715447157621384
step: 6400, Loss: 0.11491686850786209
step: 6500, Loss: 0.11718181520700455
step: 6600, Loss: 0.11529356241226196
step: 6700, Loss: 0.11633803695440292
step: 6800, Loss: 0.11496859788894653
step: 6900, Loss: 0.11636736243963242
step: 7000, Loss: 0.11500634253025055
step: 7100, Loss: 0.11933384090662003
step: 7200, Loss: 0.1976257860660553
step: 7300, Loss: 0.11388616263866425
step: 7400, Loss: 0.11446486413478851
step: 7500, Loss: 0.11559580266475677
step: 7600, Loss: 0.11414429545402527
step: 7700, Loss: 0.11355199664831161
step: 7800, Loss: 0.11545613408088684
step: 7900, Loss: 0.11412832885980606
step: 8000, Loss: 0.11628814041614532
step: 8100, Loss: 0.11534932255744934
step: 8200, Loss: 0.11349056661128998
step: 8300, Loss: 0.11471587419509888
step: 8400, Loss: 0.11352923512458801
step: 8500, Loss: 0.11429646611213684
step: 8600, Loss: 0.11409109085798264
step: 8700, Loss: 0.11500398814678192
step: 8800, Loss: 0.1136050745844841
step: 8900, Loss: 0.11475341767072678
step: 9000, Loss: 0.11335621774196625
step: 9100, Loss: 0.1924746036529541
step: 9200, Loss: 0.11289940774440765
step: 9300, Loss: 0.11247760057449341
step: 9400, Loss: 0.11592966318130493
step: 9500, Loss: 0.11430515348911285
step: 9600, Loss: 0.11389867216348648
step: 9700, Loss: 0.11374149471521378
step: 9800, Loss: 0.11372993141412735
step: 9900, Loss: 0.11413682997226715
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11844077706336975
step: 100, Loss: 0.1178751289844513
step: 200, Loss: 0.11850026994943619
step: 300, Loss: 0.11615458875894547
step: 400, Loss: 0.11423028260469437
step: 500, Loss: 0.11419279873371124
step: 600, Loss: 0.11516988277435303
step: 700, Loss: 0.11479850858449936
step: 800, Loss: 0.1138102188706398
step: 900, Loss: 0.11478284746408463
step: 1000, Loss: 0.11553400754928589
step: 1100, Loss: 0.11438508331775665
step: 1200, Loss: 0.11395900696516037
step: 1300, Loss: 0.11480198800563812
step: 1400, Loss: 0.11382561177015305
step: 1500, Loss: 0.19137859344482422
step: 1600, Loss: 0.114349365234375
step: 1700, Loss: 0.11518356204032898
step: 1800, Loss: 0.11395449191331863
step: 1900, Loss: 0.11350661516189575
step: 2000, Loss: 0.11447957158088684
step: 2100, Loss: 0.11371342092752457
step: 2200, Loss: 0.11342456936836243
step: 2300, Loss: 0.11355189234018326
step: 2400, Loss: 0.11411973088979721
step: 2500, Loss: 0.11359158903360367
step: 2600, Loss: 0.11371378600597382
step: 2700, Loss: 0.11397644877433777
step: 2800, Loss: 0.11390924453735352
step: 2900, Loss: 0.11471910029649734
step: 3000, Loss: 0.11437298357486725
step: 3100, Loss: 0.11371716856956482
step: 3200, Loss: 0.11555137485265732
step: 3300, Loss: 0.11369369179010391
step: 3400, Loss: 0.19629637897014618
step: 3500, Loss: 0.11427871882915497
step: 500, Loss: 0.11409589648246765
step: 600, Loss: 0.11394836008548737
step: 700, Loss: 0.11514516174793243
step: 800, Loss: 0.11398700624704361
step: 900, Loss: 0.11403094232082367
step: 1000, Loss: 0.11452043801546097
step: 1100, Loss: 0.11436381936073303
step: 1200, Loss: 0.11621232330799103
step: 1300, Loss: 0.11320067942142487
step: 1400, Loss: 0.11511073261499405
step: 1500, Loss: 0.11334087699651718
step: 1600, Loss: 0.1126684844493866
step: 1700, Loss: 0.11342404782772064
step: 1800, Loss: 0.11586874723434448
step: 1900, Loss: 0.11397241055965424
step: 2000, Loss: 0.11464296281337738
step: 2100, Loss: 0.11561988294124603
step: 2200, Loss: 0.11430235207080841
step: 2300, Loss: 0.11325500905513763
step: 2400, Loss: 0.11593759804964066
step: 2500, Loss: 0.11331098526716232
step: 2600, Loss: 0.11355718970298767
step: 2700, Loss: 0.11431130766868591
step: 2800, Loss: 0.11401530355215073
step: 2900, Loss: 0.1149718314409256
step: 3000, Loss: 0.11422504484653473
step: 3100, Loss: 0.11365668475627899
step: 3200, Loss: 0.11715909838676453
step: 3300, Loss: 0.11583162099123001
step: 3400, Loss: 0.11353465914726257
step: 3500, Loss: 0.11468590795993805
step: 3600, Loss: 0.11549610644578934
step: 3700, Loss: 0.11568712443113327
step: 3800, Loss: 0.11382108926773071
step: 3900, Loss: 0.11568668484687805
step: 4000, Loss: 0.11465489119291306
step: 4100, Loss: 0.11349546909332275
step: 4200, Loss: 0.11404697597026825
step: 4300, Loss: 0.11378113180398941
step: 4400, Loss: 0.11450383067131042
step: 4500, Loss: 0.11451301723718643
step: 4600, Loss: 0.11325223743915558
step: 4700, Loss: 0.11387908458709717
step: 4800, Loss: 0.11451955139636993
step: 4900, Loss: 0.11365332454442978
step: 5000, Loss: 0.11426980048418045
step: 5100, Loss: 0.11298080533742905
step: 5200, Loss: 0.11331607401371002
step: 5300, Loss: 0.11312055587768555
step: 5400, Loss: 0.11942849308252335
step: 5500, Loss: 0.1501639038324356
step: 5600, Loss: 0.1366288810968399
step: 5700, Loss: 0.12411416321992874
step: 5800, Loss: 0.12641091644763947
step: 5900, Loss: 0.12535212934017181
step: 6000, Loss: 0.11943398416042328
step: 6100, Loss: 0.11795197427272797
step: 6200, Loss: 0.12267705798149109
step: 6300, Loss: 0.11905085295438766
step: 6400, Loss: 0.12012169510126114
step: 6500, Loss: 0.11664050072431564
step: 6600, Loss: 0.12159594148397446
step: 6700, Loss: 0.11752162873744965
step: 6800, Loss: 0.11976346373558044
step: 6900, Loss: 0.11573000997304916
step: 7000, Loss: 0.1238970160484314
step: 7100, Loss: 0.11626702547073364
step: 7200, Loss: 0.1184476912021637
step: 7300, Loss: 0.11485899984836578
step: 7400, Loss: 0.11490893363952637
step: 7500, Loss: 0.11434374749660492
step: 7600, Loss: 0.11628445982933044
step: 7700, Loss: 0.11348354071378708
step: 7800, Loss: 0.11689148843288422
step: 7900, Loss: 0.11385129392147064
step: 8000, Loss: 0.11580108106136322
step: 8100, Loss: 0.11488847434520721
step: 8200, Loss: 0.1135406643152237
step: 8300, Loss: 0.11505761742591858
step: 8400, Loss: 0.113441102206707
step: 8500, Loss: 0.11458677798509598
step: 8600, Loss: 0.11540760844945908
step: 8700, Loss: 0.11571133136749268
step: 8800, Loss: 0.11559950560331345
step: 8900, Loss: 0.11517927795648575
step: 9000, Loss: 0.11736638844013214
step: 9100, Loss: 0.1160714328289032
step: 9200, Loss: 0.11330635845661163
step: 9300, Loss: 0.1142985001206398
step: 9400, Loss: 0.1153474748134613
step: 9500, Loss: 0.1162593886256218
step: 9600, Loss: 0.11448504030704498
step: 9700, Loss: 0.11502082645893097
step: 9800, Loss: 0.11573491245508194
step: 9900, Loss: 0.11361799389123917
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8571428571428571
recall: 0.8
F_score: 0.8275862068965518
******fold 5******

Training... train_data length:281
step: 0, Loss: 1.2333836555480957
step: 100, Loss: 0.12013259530067444
step: 200, Loss: 0.11707815527915955
step: 300, Loss: 0.11481408774852753
step: 400, Loss: 0.11680552363395691
step: 500, Loss: 0.11337503045797348
step: 600, Loss: 0.11362485587596893
step: 700, Loss: 0.11379798501729965
step: 800, Loss: 0.11651401966810226
step: 900, Loss: 0.11411938071250916
step: 1000, Loss: 0.1139230951666832
step: 1100, Loss: 0.11501967161893845
step: 1200, Loss: 0.11287017166614532
step: 1300, Loss: 0.11328426748514175
step: 1400, Loss: 0.1147875189781189
step: 1500, Loss: 0.11419684439897537
step: 1600, Loss: 0.11473292112350464
step: 1700, Loss: 0.11573819071054459
step: 1800, Loss: 0.11393055319786072
step: 1900, Loss: 0.11453559249639511
step: 2000, Loss: 0.1141509860754013
step: 2100, Loss: 0.11580108106136322
step: 2200, Loss: 0.11374405771493912
step: 2300, Loss: 0.1149480789899826
step: 2400, Loss: 0.11360199004411697
step: 2500, Loss: 0.11489710956811905
step: 2600, Loss: 0.11512982845306396
step: 2700, Loss: 0.11524765193462372
step: 2800, Loss: 0.11395782977342606
step: 2900, Loss: 0.1153024435043335
step: 3000, Loss: 0.114438995718956
step: 3100, Loss: 0.11548864096403122
step: 3200, Loss: 0.11339154839515686
step: 3300, Loss: 0.11572577059268951
step: 3400, Loss: 0.11612473428249359
step: 3500, Loss: 0.11517860740423203
step: 3600, Loss: 0.11483199149370193
step: 3700, Loss: 0.11653360724449158
step: 3800, Loss: 0.1156371459364891
step: 3900, Loss: 0.11486487835645676
step: 4000, Loss: 0.11634903401136398
step: 4100, Loss: 0.1146053671836853
step: 4200, Loss: 0.11324787884950638
step: 4300, Loss: 0.11540445685386658
step: 4400, Loss: 0.11498997360467911
step: 4500, Loss: 0.11526702344417572
step: 4600, Loss: 0.11614274978637695
step: 4700, Loss: 0.11646393686532974
step: 4800, Loss: 0.11582668125629425
step: 4900, Loss: 0.1149350255727768
step: 5000, Loss: 0.11618450284004211
step: 5100, Loss: 0.11587017774581909
step: 5200, Loss: 0.11378011107444763
step: 5300, Loss: 0.11727114766836166
step: 5400, Loss: 0.1134481132030487
step: 5500, Loss: 0.1149509996175766
step: 5600, Loss: 0.11375655233860016
step: 5700, Loss: 0.1143237054347992
step: 5800, Loss: 0.11683016270399094
step: 5900, Loss: 0.7551179528236389
step: 6000, Loss: 0.13826367259025574
step: 6100, Loss: 0.13057610392570496
step: 6200, Loss: 0.12078562378883362
step: 6300, Loss: 0.12122564017772675
step: 6400, Loss: 0.118043914437294
step: 6500, Loss: 0.12225943803787231
step: 6600, Loss: 0.11984409391880035
step: 6700, Loss: 0.11878960579633713
step: 6800, Loss: 0.11765927076339722
step: 6900, Loss: 0.11857672035694122
step: 7000, Loss: 0.1178819090127945
step: 7100, Loss: 0.11990224570035934
step: 7200, Loss: 0.11588509380817413
step: 7300, Loss: 0.11594202369451523
step: 7400, Loss: 0.11533273011445999
step: 7500, Loss: 0.116334468126297
step: 7600, Loss: 0.11619589477777481
step: 7700, Loss: 0.11581588536500931
step: 7800, Loss: 0.11608095467090607
step: 7900, Loss: 0.11832159012556076
step: 8000, Loss: 0.11477099359035492
step: 8100, Loss: 0.11814280599355698
step: 8200, Loss: 0.11930564790964127
step: 8300, Loss: 0.11557546257972717
step: 8400, Loss: 0.11649393290281296
step: 8500, Loss: 0.11732365936040878
step: 8600, Loss: 0.11434035748243332
step: 8700, Loss: 0.11531232297420502
step: 8800, Loss: 0.11475174129009247
step: 8900, Loss: 0.11483030766248703
step: 9000, Loss: 0.11352719366550446
step: 9100, Loss: 0.11474517732858658
step: 9200, Loss: 0.11426275968551636
step: 9300, Loss: 0.11463981866836548
step: 9400, Loss: 0.11346863210201263
step: 9500, Loss: 0.11486347764730453
step: 9600, Loss: 0.11501296609640121
step: 9700, Loss: 0.11390071362257004
step: 9800, Loss: 0.1146664172410965
step: 9900, Loss: 0.1147131398320198
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.75
recall: 1.0
F_score: 0.8571428571428571
******fold 6******

Training... train_data length:281
step: 0, Loss: 1.3471179008483887
step: 100, Loss: 0.12052243202924728
step: 200, Loss: 0.11799907684326172
step: 300, Loss: 0.11521248519420624
step: 400, Loss: 0.11500099301338196
step: 500, Loss: 0.11367791146039963
step: 600, Loss: 0.11357888579368591
step: 700, Loss: 0.11487295478582382
step: 800, Loss: 0.11466380953788757
step: 900, Loss: 0.11368206143379211
step: 1000, Loss: 0.11418699473142624
step: 3600, Loss: 0.11401356011629105
step: 3700, Loss: 0.11525456607341766
step: 3800, Loss: 0.11370343714952469
step: 3900, Loss: 0.11463866382837296
step: 4000, Loss: 0.11360429227352142
step: 4100, Loss: 0.11381867527961731
step: 4200, Loss: 0.11643996834754944
step: 4300, Loss: 2.7631595134735107
step: 4400, Loss: 0.15574145317077637
step: 4500, Loss: 0.1668260395526886
step: 4600, Loss: 0.13806435465812683
step: 4700, Loss: 0.12563985586166382
step: 4800, Loss: 0.12519115209579468
step: 4900, Loss: 0.12281745672225952
step: 5000, Loss: 0.12180435657501221
step: 5100, Loss: 0.12240830063819885
step: 5200, Loss: 0.12061140686273575
step: 5300, Loss: 0.2068120390176773
step: 5400, Loss: 0.1213541254401207
step: 5500, Loss: 0.12057666480541229
step: 5600, Loss: 0.12401086837053299
step: 5700, Loss: 0.12101056426763535
step: 5800, Loss: 0.11814053356647491
step: 5900, Loss: 0.116445392370224
step: 6000, Loss: 0.11657337099313736
step: 6100, Loss: 0.12047930806875229
step: 6200, Loss: 0.11559423059225082
step: 6300, Loss: 0.1160045713186264
step: 6400, Loss: 0.1191021278500557
step: 6500, Loss: 0.11697065830230713
step: 6600, Loss: 0.11769050359725952
step: 6700, Loss: 0.11565607041120529
step: 6800, Loss: 0.11722492426633835
step: 6900, Loss: 0.1168355643749237
step: 7000, Loss: 0.11555670201778412
step: 7100, Loss: 0.11500436067581177
step: 7200, Loss: 0.19240246713161469
step: 7300, Loss: 0.11489348858594894
step: 7400, Loss: 0.11699968576431274
step: 7500, Loss: 0.11453016102313995
step: 7600, Loss: 0.1166466474533081
step: 7700, Loss: 0.11427182704210281
step: 7800, Loss: 0.11583008617162704
step: 7900, Loss: 0.11419312655925751
step: 8000, Loss: 0.11551893502473831
step: 8100, Loss: 0.11421807110309601
step: 8200, Loss: 0.11420205235481262
step: 8300, Loss: 0.11409942060709
step: 8400, Loss: 0.11371342837810516
step: 8500, Loss: 0.11505958437919617
step: 8600, Loss: 0.1147688627243042
step: 8700, Loss: 0.114476278424263
step: 8800, Loss: 0.11617548763751984
step: 8900, Loss: 0.11335723102092743
step: 9000, Loss: 0.11373471468687057
step: 9100, Loss: 0.1916826069355011
step: 9200, Loss: 0.11486589908599854
step: 9300, Loss: 0.11446664482355118
step: 9400, Loss: 0.11409745365381241
step: 9500, Loss: 0.11396484076976776
step: 9600, Loss: 0.11317136138677597
step: 9700, Loss: 0.11322931200265884
step: 9800, Loss: 0.1134391725063324
step: 9900, Loss: 0.11360906809568405
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9473684210526315
recall: 1.0
F_score: 0.972972972972973
subject 14 Avgacc: 0.9402777777777777 Avgfscore: 0.9420950686374052 
 Max acc:1.0, Max f score:1.0
******** mix subject_15 ********

[361, 399]
******fold 1******

Training... train_data length:684
step: 0, Loss: 40.49241256713867
step: 100, Loss: 5.121257305145264
step: 200, Loss: 1.4073894023895264
step: 300, Loss: 0.29300954937934875
step: 400, Loss: 0.15823526680469513
step: 500, Loss: 0.1673518717288971
step: 600, Loss: 0.16665062308311462
step: 700, Loss: 0.13187897205352783
step: 800, Loss: 0.15174174308776855
step: 900, Loss: 0.15879538655281067
step: 1000, Loss: 0.141669362783432
step: 1100, Loss: 0.1397733986377716
step: 1200, Loss: 0.13021719455718994
step: 1300, Loss: 0.1333315223455429
step: 1400, Loss: 0.13298265635967255
step: 1500, Loss: 0.2141590714454651
step: 1600, Loss: 0.12872356176376343
step: 1700, Loss: 0.1190197691321373
step: 1800, Loss: 0.1324024647474289
step: 1900, Loss: 0.12252339720726013
step: 2000, Loss: 0.12734222412109375
step: 2100, Loss: 0.1190539002418518
step: 2200, Loss: 0.1300862431526184
step: 2300, Loss: 0.12012132257223129
step: 2400, Loss: 0.12134304642677307
step: 2500, Loss: 0.12352339923381805
step: 2600, Loss: 0.11817994713783264
step: 2700, Loss: 0.11677317321300507
step: 2800, Loss: 0.12153943628072739
step: 2900, Loss: 0.11589449644088745
step: 3000, Loss: 0.11824290454387665
step: 3100, Loss: 0.11646173894405365
step: 3200, Loss: 0.11701637506484985
step: 3300, Loss: 0.12047939002513885
step: 3400, Loss: 4.963379383087158
step: 3500, Loss: 0.21134819090366364
step: 3600, Loss: 0.15107499063014984
step: 3700, Loss: 0.1625947505235672
step: 3800, Loss: 0.12390127032995224
step: 3900, Loss: 0.14081168174743652
step: 4000, Loss: 0.1457907259464264
step: 4100, Loss: 0.13973195850849152
step: 4200, Loss: 0.13647647202014923
step: 4300, Loss: 0.12165515869855881
step: 4400, Loss: 0.12542647123336792
step: 4500, Loss: 0.12202771008014679
step: 4600, Loss: 0.12957632541656494
step: 4700, Loss: 0.12716616690158844
step: 4800, Loss: 0.12407123297452927
step: 4900, Loss: 0.12462469935417175
step: 5000, Loss: 0.1194116622209549
step: 5100, Loss: 0.12396426498889923
step: 5200, Loss: 0.12010060250759125
step: 5300, Loss: 0.20315425097942352
step: 5400, Loss: 0.12006139010190964
step: 5500, Loss: 0.11691637337207794
step: 5600, Loss: 0.12561437487602234
step: 5700, Loss: 0.1173960417509079
step: 5800, Loss: 0.12213200330734253
step: 5900, Loss: 0.12204287946224213
step: 6000, Loss: 0.12064413726329803
step: 6100, Loss: 0.11928127706050873
step: 6200, Loss: 0.11648660898208618
step: 6300, Loss: 0.11508455872535706
step: 6400, Loss: 0.11437086015939713
step: 6500, Loss: 0.11781594157218933
step: 6600, Loss: 0.11859855055809021
step: 6700, Loss: 0.1171787902712822
step: 6800, Loss: 0.11478094011545181
step: 6900, Loss: 0.11649177968502045
step: 7000, Loss: 0.11595865339040756
step: 7100, Loss: 0.11691021174192429
step: 7200, Loss: 0.1957073211669922
step: 7300, Loss: 0.11386936902999878
step: 7400, Loss: 0.11484141647815704
step: 7500, Loss: 0.11642878502607346
step: 7600, Loss: 0.11496692895889282
step: 7700, Loss: 0.11554063111543655
step: 7800, Loss: 0.11644670367240906
step: 7900, Loss: 0.11420607566833496
step: 8000, Loss: 0.1151159405708313
step: 8100, Loss: 0.1137535497546196
step: 8200, Loss: 0.11456704884767532
step: 8300, Loss: 0.11431543529033661
step: 8400, Loss: 0.11395032703876495
step: 8500, Loss: 0.11389992386102676
step: 8600, Loss: 0.11641629785299301
step: 8700, Loss: 0.11416591703891754
step: 8800, Loss: 0.11487814784049988
step: 8900, Loss: 0.11471426486968994
step: 9000, Loss: 0.1155906617641449
step: 9100, Loss: 0.19524617493152618
step: 9200, Loss: 0.11418401449918747
step: 9300, Loss: 0.11370441317558289
step: 9400, Loss: 0.11664789915084839
step: 9500, Loss: 0.11408886313438416
step: 9600, Loss: 0.11628025770187378
step: 9700, Loss: 0.11478035151958466
step: 9800, Loss: 0.11438101530075073
step: 9900, Loss: 0.12015750259160995
training successfully ended.
validating...
validate data length:76
acc: 0.8333333333333334
precision: 0.7777777777777778
recall: 0.875
F_score: 0.823529411764706
******fold 2******

Training... train_data length:684
step: 0, Loss: 0.2795042097568512
step: 100, Loss: 0.12418162822723389
step: 200, Loss: 0.12625126540660858
step: 300, Loss: 0.12202365696430206
step: 400, Loss: 0.11616455018520355
step: 500, Loss: 0.1163402646780014
step: 600, Loss: 0.11783672124147415
step: 700, Loss: 0.11863692849874496
step: 800, Loss: 0.11419902741909027
step: 900, Loss: 0.11637843400239944
step: 1000, Loss: 0.11335627734661102
step: 1100, Loss: 0.114564448595047
step: 1200, Loss: 0.11559771746397018
step: 1300, Loss: 0.11375168710947037
step: 1400, Loss: 0.11327265202999115
step: 1500, Loss: 0.1908891797065735
step: 1600, Loss: 0.11406607180833817
step: 1700, Loss: 0.11630827188491821
step: 1800, Loss: 0.11389518529176712
step: 1900, Loss: 0.1141364797949791
step: 2000, Loss: 0.1140630766749382
step: 2100, Loss: 0.11489520221948624
step: 2200, Loss: 0.11412756145000458
step: 2300, Loss: 0.11339540779590607
step: 2400, Loss: 0.11480572074651718
step: 2500, Loss: 0.11463607847690582
step: 2600, Loss: 0.11315066367387772
step: 2700, Loss: 0.11367466300725937
step: 2800, Loss: 0.11408501863479614
step: 2900, Loss: 0.1142704114317894
step: 3000, Loss: 0.1142079159617424
step: 3100, Loss: 0.11450685560703278
step: 3200, Loss: 0.11537349969148636
step: 3300, Loss: 0.11572117358446121
step: 3400, Loss: 0.19503840804100037
step: 3500, Loss: 0.11454892158508301
step: 3600, Loss: 0.11427591741085052
step: 3700, Loss: 0.11377642303705215
step: 1100, Loss: 0.11478165537118912
step: 1200, Loss: 0.11490854620933533
step: 1300, Loss: 0.11266394704580307
step: 1400, Loss: 0.11452445387840271
step: 1500, Loss: 0.11345487833023071
step: 1600, Loss: 0.11374274641275406
step: 1700, Loss: 0.11260052025318146
step: 1800, Loss: 0.11387402564287186
step: 1900, Loss: 0.11411357671022415
step: 2000, Loss: 0.1130824014544487
step: 2100, Loss: 0.1133158802986145
step: 2200, Loss: 0.1141299232840538
step: 2300, Loss: 0.11375173926353455
step: 2400, Loss: 0.11547780781984329
step: 2500, Loss: 0.11335564404726028
step: 2600, Loss: 0.11354047060012817
step: 2700, Loss: 0.11413607001304626
step: 2800, Loss: 0.11281141638755798
step: 2900, Loss: 0.11529598385095596
step: 3000, Loss: 0.1138090193271637
step: 3100, Loss: 0.11451765894889832
step: 3200, Loss: 0.11440646648406982
step: 3300, Loss: 0.11384227871894836
step: 3400, Loss: 0.11370149254798889
step: 3500, Loss: 0.11415137350559235
step: 3600, Loss: 0.11459757387638092
step: 3700, Loss: 0.1150817722082138
step: 3800, Loss: 0.11409533023834229
step: 3900, Loss: 0.11436689645051956
step: 4000, Loss: 0.11409822106361389
step: 4100, Loss: 0.11446705460548401
step: 4200, Loss: 0.11573613435029984
step: 4300, Loss: 0.11360673606395721
step: 4400, Loss: 0.1166362538933754
step: 4500, Loss: 0.11487363278865814
step: 4600, Loss: 0.11508608609437943
step: 4700, Loss: 0.11402655392885208
step: 4800, Loss: 0.11526449024677277
step: 4900, Loss: 0.11444464325904846
step: 5000, Loss: 0.11587224900722504
step: 5100, Loss: 0.11462105065584183
step: 5200, Loss: 0.11649803072214127
step: 5300, Loss: 0.1139395534992218
step: 5400, Loss: 0.11527454107999802
step: 5500, Loss: 0.11511805653572083
step: 5600, Loss: 0.11505893617868423
step: 5700, Loss: 0.11774551868438721
step: 5800, Loss: 0.11466924846172333
step: 5900, Loss: 0.12151108682155609
step: 6000, Loss: 0.11576823890209198
step: 6100, Loss: 0.11384031921625137
step: 6200, Loss: 0.114325612783432
step: 6300, Loss: 0.11527315527200699
step: 6400, Loss: 0.11508011817932129
step: 6500, Loss: 0.3336118757724762
step: 6600, Loss: 0.1486358791589737
step: 6700, Loss: 0.13171939551830292
step: 6800, Loss: 0.1362607479095459
step: 6900, Loss: 0.1232142299413681
step: 7000, Loss: 0.12323538959026337
step: 7100, Loss: 0.12123116105794907
step: 7200, Loss: 0.12153521180152893
step: 7300, Loss: 0.12012194097042084
step: 7400, Loss: 0.11651475727558136
step: 7500, Loss: 0.11868512630462646
step: 7600, Loss: 0.11825098097324371
step: 7700, Loss: 0.11716847121715546
step: 7800, Loss: 0.11795773357152939
step: 7900, Loss: 0.11758971959352493
step: 8000, Loss: 0.11653156578540802
step: 8100, Loss: 0.11529171466827393
step: 8200, Loss: 0.11833324283361435
step: 8300, Loss: 0.11864358931779861
step: 8400, Loss: 0.1179809495806694
step: 8500, Loss: 0.1164231076836586
step: 8600, Loss: 0.11626958847045898
step: 8700, Loss: 0.11535760760307312
step: 8800, Loss: 0.11728540062904358
step: 8900, Loss: 0.11485492438077927
step: 9000, Loss: 0.11562863737344742
step: 9100, Loss: 0.11611246317625046
step: 9200, Loss: 0.11444812268018723
step: 9300, Loss: 0.11349420994520187
step: 9400, Loss: 0.11513583362102509
step: 9500, Loss: 0.11566593497991562
step: 9600, Loss: 0.11664307862520218
step: 9700, Loss: 0.1145208477973938
step: 9800, Loss: 0.11533921957015991
step: 9900, Loss: 0.11714932322502136
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.75
recall: 0.8333333333333334
F_score: 0.7894736842105262
******fold 7******

Training... train_data length:281
step: 0, Loss: 1.6401550769805908
step: 100, Loss: 0.11684982478618622
step: 200, Loss: 0.11548124253749847
step: 300, Loss: 0.11703672260046005
step: 400, Loss: 0.11359886825084686
step: 500, Loss: 0.11468695104122162
step: 600, Loss: 0.11428231000900269
step: 700, Loss: 0.11447860300540924
step: 800, Loss: 0.1142815500497818
step: 900, Loss: 0.11374620348215103
step: 1000, Loss: 0.11389109492301941
step: 1100, Loss: 0.11428019404411316
step: 1200, Loss: 0.11447267234325409
step: 1300, Loss: 0.11621478199958801
step: 1400, Loss: 0.11379134654998779
step: 1500, Loss: 0.11352545022964478
step: 1600, Loss: 0.11507701873779297
step: 1700, Loss: 0.11483220010995865
step: 1800, Loss: 0.11409366875886917
step: 1900, Loss: 0.11481335759162903
step: 2000, Loss: 0.11367540806531906
step: 2100, Loss: 0.11399949342012405
step: 2200, Loss: 0.11540429294109344
step: 2300, Loss: 0.11417892575263977
step: 2400, Loss: 0.11518354713916779
step: 2500, Loss: 0.11439438164234161
step: 2600, Loss: 0.11454027146100998
step: 2700, Loss: 0.11607873439788818
step: 2800, Loss: 0.11352704465389252
step: 2900, Loss: 0.11482376605272293
step: 3000, Loss: 0.1143001988530159
step: 3100, Loss: 0.1135651245713234
step: 3200, Loss: 0.11433544754981995
step: 3300, Loss: 0.11343574523925781
step: 3400, Loss: 0.11492358148097992
step: 3500, Loss: 0.11553198099136353
step: 3600, Loss: 0.11544714868068695
step: 3700, Loss: 0.11586518585681915
step: 3800, Loss: 0.11455532908439636
step: 3900, Loss: 0.11362112313508987
step: 4000, Loss: 0.11468058079481125
step: 4100, Loss: 0.11307943612337112
step: 4200, Loss: 0.11461939662694931
step: 4300, Loss: 0.11462070047855377
step: 4400, Loss: 0.11624280363321304
step: 4500, Loss: 0.11398269236087799
step: 4600, Loss: 0.11678633093833923
step: 4700, Loss: 0.11433713138103485
step: 4800, Loss: 0.11515655368566513
step: 4900, Loss: 0.11665652692317963
step: 5000, Loss: 0.11614789068698883
step: 5100, Loss: 0.11491787433624268
step: 5200, Loss: 0.11388130486011505
step: 5300, Loss: 2.54624605178833
step: 5400, Loss: 0.14915379881858826
step: 5500, Loss: 0.13541299104690552
step: 5600, Loss: 0.13220199942588806
step: 5700, Loss: 0.1250288486480713
step: 5800, Loss: 0.11935622245073318
step: 5900, Loss: 0.12355644255876541
step: 6000, Loss: 0.12208734452724457
step: 6100, Loss: 0.12050613015890121
step: 6200, Loss: 0.1187693402171135
step: 6300, Loss: 0.12044260650873184
step: 6400, Loss: 0.1197376400232315
step: 6500, Loss: 0.12052805721759796
step: 6600, Loss: 0.11590739339590073
step: 6700, Loss: 0.1168411448597908
step: 6800, Loss: 0.11948073655366898
step: 6900, Loss: 0.11993680894374847
step: 7000, Loss: 0.11756251007318497
step: 7100, Loss: 0.11672833561897278
step: 7200, Loss: 0.11646105349063873
step: 7300, Loss: 0.11775270104408264
step: 7400, Loss: 0.11474911868572235
step: 7500, Loss: 0.11606451869010925
step: 7600, Loss: 0.11526679992675781
step: 7700, Loss: 0.11459293216466904
step: 7800, Loss: 0.11456573009490967
step: 7900, Loss: 0.11439217627048492
step: 8000, Loss: 0.11423882842063904
step: 8100, Loss: 0.11441358923912048
step: 8200, Loss: 0.11405469477176666
step: 8300, Loss: 0.11363372206687927
step: 8400, Loss: 0.11476145684719086
step: 8500, Loss: 0.11504077166318893
step: 8600, Loss: 0.11426202207803726
step: 8700, Loss: 0.11498399078845978
step: 8800, Loss: 0.11469186842441559
step: 8900, Loss: 0.11431489884853363
step: 9000, Loss: 0.11610814929008484
step: 9100, Loss: 0.115105539560318
step: 9200, Loss: 0.11381439119577408
step: 9300, Loss: 0.11659176647663116
step: 9400, Loss: 0.11331982910633087
step: 9500, Loss: 0.11530035734176636
step: 9600, Loss: 0.11262954026460648
step: 9700, Loss: 0.11515909433364868
step: 9800, Loss: 0.11441575735807419
step: 9900, Loss: 0.11342290043830872
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7142857142857143
recall: 0.8823529411764706
F_score: 0.7894736842105262
******fold 8******

Training... train_data length:281
step: 0, Loss: 1.3826658725738525
step: 100, Loss: 0.12527446448802948
step: 200, Loss: 0.11502043157815933
step: 300, Loss: 0.11533626914024353
step: 400, Loss: 0.11710669845342636
step: 500, Loss: 0.1148582249879837
step: 600, Loss: 0.11485666036605835
step: 700, Loss: 0.11553089320659637
step: 800, Loss: 0.11392740905284882
step: 900, Loss: 0.11400260031223297
step: 1000, Loss: 0.1144346296787262
step: 1100, Loss: 0.11448073387145996
step: 1200, Loss: 0.11483363062143326
step: 1300, Loss: 0.11416967958211899
step: 1400, Loss: 0.11420994997024536
step: 1500, Loss: 0.11481936275959015
step: 3800, Loss: 0.11650662124156952
step: 3900, Loss: 0.11473505198955536
step: 4000, Loss: 0.11572619527578354
step: 4100, Loss: 0.11524996906518936
step: 4200, Loss: 0.11527184396982193
step: 4300, Loss: 0.11416769027709961
step: 4400, Loss: 1.198099970817566
step: 4500, Loss: 1.2084195613861084
step: 4600, Loss: 0.14850984513759613
step: 4700, Loss: 0.1425001323223114
step: 4800, Loss: 0.14110389351844788
step: 4900, Loss: 0.12448486685752869
step: 5000, Loss: 0.14618149399757385
step: 5100, Loss: 0.12654916942119598
step: 5200, Loss: 0.12970419228076935
step: 5300, Loss: 0.19568201899528503
step: 5400, Loss: 0.13155066967010498
step: 5500, Loss: 0.11701894551515579
step: 5600, Loss: 0.12972229719161987
step: 5700, Loss: 0.12194861471652985
step: 5800, Loss: 0.11880341917276382
step: 5900, Loss: 0.11950213462114334
step: 6000, Loss: 0.12340734899044037
step: 6100, Loss: 0.12042300403118134
step: 6200, Loss: 0.11605387926101685
step: 6300, Loss: 0.1201791912317276
step: 6400, Loss: 0.11749099940061569
step: 6500, Loss: 0.11748988181352615
step: 6600, Loss: 0.1181931123137474
step: 6700, Loss: 0.1170584037899971
step: 6800, Loss: 0.11577225476503372
step: 6900, Loss: 0.1182992234826088
step: 7000, Loss: 0.11513003706932068
step: 7100, Loss: 0.11699414253234863
step: 7200, Loss: 0.19432783126831055
step: 7300, Loss: 0.1143166571855545
step: 7400, Loss: 0.11803211271762848
step: 7500, Loss: 0.118209607899189
step: 7600, Loss: 0.11718035489320755
step: 7700, Loss: 0.11588369309902191
step: 7800, Loss: 0.11870799958705902
step: 7900, Loss: 0.11514647305011749
step: 8000, Loss: 0.11508915573358536
step: 8100, Loss: 0.11417371779680252
step: 8200, Loss: 0.11488088965415955
step: 8300, Loss: 0.11511735618114471
step: 8400, Loss: 0.11463378369808197
step: 8500, Loss: 0.11517001688480377
step: 8600, Loss: 0.11589114367961884
step: 8700, Loss: 0.11343123018741608
step: 8800, Loss: 0.11557295173406601
step: 8900, Loss: 0.11396542191505432
step: 9000, Loss: 0.11456094682216644
step: 9100, Loss: 0.19161456823349
step: 9200, Loss: 0.1149197518825531
step: 9300, Loss: 0.11456075310707092
step: 9400, Loss: 0.115215964615345
step: 9500, Loss: 0.1145910918712616
step: 9600, Loss: 0.11491777002811432
step: 9700, Loss: 0.11380258202552795
step: 9800, Loss: 0.11339839547872543
step: 9900, Loss: 0.11412317305803299
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.925
recall: 0.925
F_score: 0.925
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.8188661336898804
step: 100, Loss: 0.12250369042158127
step: 200, Loss: 0.1172521635890007
step: 300, Loss: 0.11588507145643234
step: 400, Loss: 0.12365525960922241
step: 500, Loss: 0.11722566187381744
step: 600, Loss: 0.11622285097837448
step: 700, Loss: 0.11395092308521271
step: 800, Loss: 0.11601416021585464
step: 900, Loss: 0.11435030400753021
step: 1000, Loss: 0.11504962295293808
step: 1100, Loss: 0.11595269292593002
step: 1200, Loss: 0.11449051648378372
step: 1300, Loss: 0.11451442539691925
step: 1400, Loss: 0.11399976909160614
step: 1500, Loss: 0.1923266053199768
step: 1600, Loss: 0.11538106203079224
step: 1700, Loss: 0.11434896290302277
step: 1800, Loss: 0.11481746286153793
step: 1900, Loss: 0.11382092535495758
step: 2000, Loss: 0.11429888755083084
step: 2100, Loss: 0.1140303984284401
step: 2200, Loss: 0.11476939171552658
step: 2300, Loss: 0.11527734994888306
step: 2400, Loss: 0.11407110095024109
step: 2500, Loss: 0.11376498639583588
step: 2600, Loss: 0.11352119594812393
step: 2700, Loss: 0.11329852044582367
step: 2800, Loss: 0.11367736011743546
step: 2900, Loss: 0.11395779997110367
step: 3000, Loss: 0.11314519494771957
step: 3100, Loss: 0.11470146477222443
step: 3200, Loss: 0.11280173063278198
step: 3300, Loss: 0.11411355435848236
step: 3400, Loss: 0.1950055956840515
step: 3500, Loss: 0.1142062395811081
step: 3600, Loss: 0.11381414532661438
step: 3700, Loss: 0.11481781303882599
step: 3800, Loss: 0.11325333267450333
step: 3900, Loss: 0.11548944562673569
step: 4000, Loss: 0.11585614085197449
step: 4100, Loss: 0.1153671145439148
step: 4200, Loss: 2.444645881652832
step: 4300, Loss: 0.16422191262245178
step: 4400, Loss: 0.13622339069843292
step: 4500, Loss: 0.12532323598861694
step: 4600, Loss: 0.12370572984218597
step: 4700, Loss: 0.12268192321062088
step: 4800, Loss: 0.1252964735031128
step: 4900, Loss: 0.12163620442152023
step: 5000, Loss: 0.12091661989688873
step: 5100, Loss: 0.12055443972349167
step: 5200, Loss: 0.1215081661939621
step: 5300, Loss: 0.2069910764694214
step: 5400, Loss: 0.11892639100551605
step: 5500, Loss: 0.1178402379155159
step: 5600, Loss: 0.1293044537305832
step: 5700, Loss: 0.11919708549976349
step: 5800, Loss: 0.11808709055185318
step: 5900, Loss: 0.11930939555168152
step: 6000, Loss: 0.11624939739704132
step: 6100, Loss: 0.11817595362663269
step: 6200, Loss: 0.11733151972293854
step: 6300, Loss: 0.11938118189573288
step: 6400, Loss: 0.11387091875076294
step: 6500, Loss: 0.11805527657270432
step: 6600, Loss: 0.11693393439054489
step: 6700, Loss: 0.11567667871713638
step: 6800, Loss: 0.11481177061796188
step: 6900, Loss: 0.11653100699186325
step: 7000, Loss: 0.11389344185590744
step: 7100, Loss: 0.11544323712587357
step: 7200, Loss: 0.19775040447711945
step: 7300, Loss: 0.11536024510860443
step: 7400, Loss: 0.1152970939874649
step: 7500, Loss: 0.11631669104099274
step: 7600, Loss: 0.11526307463645935
step: 7700, Loss: 0.11443901807069778
step: 7800, Loss: 0.11529099941253662
step: 7900, Loss: 0.1149064302444458
step: 8000, Loss: 0.11468127369880676
step: 8100, Loss: 0.11496087908744812
step: 8200, Loss: 0.11449643224477768
step: 8300, Loss: 0.11485882103443146
step: 8400, Loss: 0.11336245387792587
step: 8500, Loss: 0.11449780315160751
step: 8600, Loss: 0.11552027612924576
step: 8700, Loss: 0.11394662410020828
step: 8800, Loss: 0.11370229721069336
step: 8900, Loss: 0.11486116051673889
step: 9000, Loss: 0.11376211047172546
step: 9100, Loss: 0.19250595569610596
step: 9200, Loss: 0.11291366815567017
step: 9300, Loss: 0.11333297193050385
step: 9400, Loss: 0.11429457366466522
step: 9500, Loss: 0.11336622387170792
step: 9600, Loss: 0.11354903876781464
step: 9700, Loss: 0.11443109810352325
step: 9800, Loss: 0.11314481496810913
step: 9900, Loss: 0.11429612338542938
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.211073100566864
step: 100, Loss: 0.1165742352604866
step: 200, Loss: 0.11526858806610107
step: 300, Loss: 0.11573582142591476
step: 400, Loss: 0.11483870446681976
step: 500, Loss: 0.11411646008491516
step: 600, Loss: 0.11406198143959045
step: 700, Loss: 0.1153343990445137
step: 800, Loss: 0.11291003227233887
step: 900, Loss: 0.11367736756801605
step: 1000, Loss: 0.11404237896203995
step: 1100, Loss: 0.1133885383605957
step: 1200, Loss: 0.11356665194034576
step: 1300, Loss: 0.11359409242868423
step: 1400, Loss: 0.3353567123413086
step: 1500, Loss: 0.21889440715312958
step: 1600, Loss: 0.13295462727546692
step: 1700, Loss: 0.12719111144542694
step: 1800, Loss: 0.13721665740013123
step: 1900, Loss: 0.12410499900579453
step: 2000, Loss: 0.12404743582010269
step: 2100, Loss: 0.12179013341665268
step: 2200, Loss: 0.1253538578748703
step: 2300, Loss: 0.12303639203310013
step: 2400, Loss: 0.1159861832857132
step: 2500, Loss: 0.1223587840795517
step: 2600, Loss: 0.11809758841991425
step: 2700, Loss: 0.11917170882225037
step: 2800, Loss: 0.11905505508184433
step: 2900, Loss: 0.11574691534042358
step: 3000, Loss: 0.11868874728679657
step: 3100, Loss: 0.12320369482040405
step: 3200, Loss: 0.11490645259618759
step: 3300, Loss: 0.11678342521190643
step: 3400, Loss: 0.1985037475824356
step: 3500, Loss: 0.11798126995563507
step: 3600, Loss: 0.11957499384880066
step: 3700, Loss: 0.11810458451509476
step: 3800, Loss: 0.11484293639659882
step: 3900, Loss: 0.11539299786090851
step: 4000, Loss: 0.11493133008480072
step: 4100, Loss: 0.11510556936264038
step: 4200, Loss: 0.11654067784547806
step: 4300, Loss: 0.1145186796784401
step: 4400, Loss: 0.11562603712081909
step: 1600, Loss: 0.11296206712722778
step: 1700, Loss: 0.11581537127494812
step: 1800, Loss: 0.11486221849918365
step: 1900, Loss: 0.11524619907140732
step: 2000, Loss: 0.11492357403039932
step: 2100, Loss: 0.11466403305530548
step: 2200, Loss: 0.11402423679828644
step: 2300, Loss: 0.11348529160022736
step: 2400, Loss: 0.11459611356258392
step: 2500, Loss: 0.11403290927410126
step: 2600, Loss: 0.1145516037940979
step: 2700, Loss: 0.11456199735403061
step: 2800, Loss: 0.11567455530166626
step: 2900, Loss: 0.11494471877813339
step: 3000, Loss: 0.11530401557683945
step: 3100, Loss: 0.11354995518922806
step: 3200, Loss: 0.11443568021059036
step: 3300, Loss: 0.11382467299699783
step: 3400, Loss: 0.11340269446372986
step: 3500, Loss: 0.11515137553215027
step: 3600, Loss: 0.11390386521816254
step: 3700, Loss: 0.11453184485435486
step: 3800, Loss: 0.1147250235080719
step: 3900, Loss: 0.11429324001073837
step: 4000, Loss: 0.11452415585517883
step: 4100, Loss: 0.11383116990327835
step: 4200, Loss: 0.1139889657497406
step: 4300, Loss: 0.11451849341392517
step: 4400, Loss: 0.11340067535638809
step: 4500, Loss: 0.11537039279937744
step: 4600, Loss: 0.11506582051515579
step: 4700, Loss: 0.11684505641460419
step: 4800, Loss: 0.11409342288970947
step: 4900, Loss: 0.1163223534822464
step: 5000, Loss: 0.114737868309021
step: 5100, Loss: 0.11488755792379379
step: 5200, Loss: 0.11494799703359604
step: 5300, Loss: 0.11560619622468948
step: 5400, Loss: 0.11404789239168167
step: 5500, Loss: 0.11601472645998001
step: 5600, Loss: 0.1140928715467453
step: 5700, Loss: 0.1165226548910141
step: 5800, Loss: 0.11418098211288452
step: 5900, Loss: 0.11417467892169952
step: 6000, Loss: 0.11515337973833084
step: 6100, Loss: 0.11960069835186005
step: 6200, Loss: 0.11545028537511826
step: 6300, Loss: 0.1136990338563919
step: 6400, Loss: 0.11329629272222519
step: 6500, Loss: 0.11602223664522171
step: 6600, Loss: 0.11435356736183167
step: 6700, Loss: 0.18144533038139343
step: 6800, Loss: 0.15759237110614777
step: 6900, Loss: 0.12495891749858856
step: 7000, Loss: 0.1330103576183319
step: 7100, Loss: 0.12111663073301315
step: 7200, Loss: 0.12764202058315277
step: 7300, Loss: 0.12242582440376282
step: 7400, Loss: 0.11868719756603241
step: 7500, Loss: 0.12134437263011932
step: 7600, Loss: 0.1200442984700203
step: 7700, Loss: 0.11474596709012985
step: 7800, Loss: 0.12008308619260788
step: 7900, Loss: 0.11705804616212845
step: 8000, Loss: 0.11779244244098663
step: 8100, Loss: 0.1171594113111496
step: 8200, Loss: 0.11934131383895874
step: 8300, Loss: 0.11480532586574554
step: 8400, Loss: 0.11903035640716553
step: 8500, Loss: 0.11645378172397614
step: 8600, Loss: 0.11764303594827652
step: 8700, Loss: 0.11504097282886505
step: 8800, Loss: 0.11558330059051514
step: 8900, Loss: 0.11437224596738815
step: 9000, Loss: 0.11774881929159164
step: 9100, Loss: 0.11334176361560822
step: 9200, Loss: 0.11570107936859131
step: 9300, Loss: 0.11719381809234619
step: 9400, Loss: 0.11467631161212921
step: 9500, Loss: 0.11508774012327194
step: 9600, Loss: 0.11367317289113998
step: 9700, Loss: 0.11452364176511765
step: 9800, Loss: 0.1148044541478157
step: 9900, Loss: 0.11342008411884308
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.6842105263157895
recall: 0.8666666666666667
F_score: 0.7647058823529413
******fold 9******

Training... train_data length:281
step: 0, Loss: 1.176517128944397
step: 100, Loss: 0.120884470641613
step: 200, Loss: 0.1148519515991211
step: 300, Loss: 0.11778958886861801
step: 400, Loss: 0.11483383178710938
step: 500, Loss: 0.1147829070687294
step: 600, Loss: 0.11598806828260422
step: 700, Loss: 0.11496257036924362
step: 800, Loss: 0.11414686590433121
step: 900, Loss: 0.11413659900426865
step: 1000, Loss: 0.11376409977674484
step: 1100, Loss: 0.11448206007480621
step: 1200, Loss: 0.11256308853626251
step: 1300, Loss: 0.11486908793449402
step: 1400, Loss: 0.11487848311662674
step: 1500, Loss: 0.11448542773723602
step: 1600, Loss: 0.11381031572818756
step: 1700, Loss: 0.11626176536083221
step: 1800, Loss: 0.11575755476951599
step: 1900, Loss: 0.11850768327713013
step: 2000, Loss: 0.11475371569395065
step: 2100, Loss: 0.11398573219776154
step: 2200, Loss: 0.11382143944501877
step: 2300, Loss: 0.11598507314920425
step: 2400, Loss: 0.11480239778757095
step: 2500, Loss: 0.11506174504756927
step: 2600, Loss: 0.1150064691901207
step: 2700, Loss: 0.1153472512960434
step: 2800, Loss: 0.11399408429861069
step: 2900, Loss: 0.11502266675233841
step: 3000, Loss: 0.11688286066055298
step: 3100, Loss: 0.1153235137462616
step: 3200, Loss: 0.11460109055042267
step: 3300, Loss: 0.11778092384338379
step: 3400, Loss: 0.11514127999544144
step: 3500, Loss: 0.11520178616046906
step: 3600, Loss: 0.11417090147733688
step: 3700, Loss: 0.11345559358596802
step: 3800, Loss: 0.11509155482053757
step: 3900, Loss: 0.11389890313148499
step: 4000, Loss: 0.113874651491642
step: 4100, Loss: 0.11457455158233643
step: 4200, Loss: 0.11673358827829361
step: 4300, Loss: 0.11433769017457962
step: 4400, Loss: 0.11425116658210754
step: 4500, Loss: 0.11423975229263306
step: 4600, Loss: 0.11726121604442596
step: 4700, Loss: 0.1171194314956665
step: 4800, Loss: 0.11351917684078217
step: 4900, Loss: 0.11551341414451599
step: 5000, Loss: 0.11396819353103638
step: 5100, Loss: 0.11428608000278473
step: 5200, Loss: 0.1137385368347168
step: 5300, Loss: 0.11532437801361084
step: 5400, Loss: 0.11383930593729019
step: 5500, Loss: 0.11586105823516846
step: 5600, Loss: 0.11464384198188782
step: 5700, Loss: 0.11689987778663635
step: 5800, Loss: 0.11287501454353333
step: 5900, Loss: 0.11870284378528595
step: 6000, Loss: 0.11578719317913055
step: 6100, Loss: 0.1151961162686348
step: 6200, Loss: 0.11371108889579773
step: 6300, Loss: 0.11723914742469788
step: 6400, Loss: 2.298858642578125
step: 6500, Loss: 0.1372525542974472
step: 6600, Loss: 0.13524532318115234
step: 6700, Loss: 0.1368226557970047
step: 6800, Loss: 0.12829943001270294
step: 6900, Loss: 0.11999298632144928
step: 7000, Loss: 0.12618650496006012
step: 7100, Loss: 0.12366005778312683
step: 7200, Loss: 0.12168806046247482
step: 7300, Loss: 0.11767541617155075
step: 7400, Loss: 0.11985571682453156
step: 7500, Loss: 0.11555063724517822
step: 7600, Loss: 0.1200622096657753
step: 7700, Loss: 0.11809618771076202
step: 7800, Loss: 0.11998304724693298
step: 7900, Loss: 0.11814559251070023
step: 8000, Loss: 0.1171872615814209
step: 8100, Loss: 0.11742570996284485
step: 8200, Loss: 0.11802969872951508
step: 8300, Loss: 0.1143140196800232
step: 8400, Loss: 0.11811394989490509
step: 8500, Loss: 0.11508753895759583
step: 8600, Loss: 0.11834415793418884
step: 8700, Loss: 0.11660405993461609
step: 8800, Loss: 0.1172168031334877
step: 8900, Loss: 0.114158496260643
step: 9000, Loss: 0.116524338722229
step: 9100, Loss: 0.1185324490070343
step: 9200, Loss: 0.11441744863986969
step: 9300, Loss: 0.11453002691268921
step: 9400, Loss: 0.11486634612083435
step: 9500, Loss: 0.1148434430360794
step: 9600, Loss: 0.11702142655849457
step: 9700, Loss: 0.11323603987693787
step: 9800, Loss: 0.11664870381355286
step: 9900, Loss: 0.11346900463104248
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7368421052631579
recall: 0.8235294117647058
F_score: 0.7777777777777778
******fold 10******

Training... train_data length:281
step: 0, Loss: 1.1350635290145874
step: 100, Loss: 0.11931614577770233
step: 200, Loss: 0.1164744421839714
step: 300, Loss: 0.11507043242454529
step: 400, Loss: 0.11605390161275864
step: 500, Loss: 0.11530369520187378
step: 600, Loss: 0.1148834154009819
step: 700, Loss: 0.1185578852891922
step: 800, Loss: 0.11549049615859985
step: 900, Loss: 0.11494306474924088
step: 1000, Loss: 0.11701811850070953
step: 1100, Loss: 0.11381809413433075
step: 1200, Loss: 0.1139020100235939
step: 1300, Loss: 0.11655417084693909
step: 1400, Loss: 0.1130511537194252
step: 1500, Loss: 0.11453483253717422
step: 1600, Loss: 0.11393646150827408
step: 1700, Loss: 0.11773434281349182
step: 1800, Loss: 0.11400560289621353
step: 1900, Loss: 0.11463792622089386
step: 2000, Loss: 0.11461538821458817
step: 4500, Loss: 0.11549543589353561
step: 4600, Loss: 0.11741632223129272
step: 4700, Loss: 0.11574263125658035
step: 4800, Loss: 0.11563749611377716
step: 4900, Loss: 0.11766698956489563
step: 5000, Loss: 0.11413761973381042
step: 5100, Loss: 0.11366033554077148
step: 5200, Loss: 0.11329597234725952
step: 5300, Loss: 0.19449880719184875
step: 5400, Loss: 0.11428069323301315
step: 5500, Loss: 0.11367329955101013
step: 5600, Loss: 0.11466982215642929
step: 5700, Loss: 0.11381030827760696
step: 5800, Loss: 0.11508985608816147
step: 5900, Loss: 0.11408402770757675
step: 6000, Loss: 0.11344532668590546
step: 6100, Loss: 0.11286310851573944
step: 6200, Loss: 0.11388496309518814
step: 6300, Loss: 0.11448082327842712
step: 6400, Loss: 0.11484546959400177
step: 6500, Loss: 0.11530432105064392
step: 6600, Loss: 0.11473900824785233
step: 6700, Loss: 0.11457321047782898
step: 6800, Loss: 0.1130577027797699
step: 6900, Loss: 0.11377394199371338
step: 7000, Loss: 0.11388795077800751
step: 7100, Loss: 0.1145334541797638
step: 7200, Loss: 0.19210778176784515
step: 7300, Loss: 0.1146514043211937
step: 7400, Loss: 0.1132572591304779
step: 7500, Loss: 0.11288788169622421
step: 7600, Loss: 0.11436156928539276
step: 7700, Loss: 0.11337295919656754
step: 7800, Loss: 0.11355669796466827
step: 7900, Loss: 0.11393997818231583
step: 8000, Loss: 0.11501295119524002
step: 8100, Loss: 0.11391378939151764
step: 8200, Loss: 0.11387023329734802
step: 8300, Loss: 0.11479204148054123
step: 8400, Loss: 0.11529862880706787
step: 8500, Loss: 0.11341996490955353
step: 8600, Loss: 0.11365494877099991
step: 8700, Loss: 0.11323521286249161
step: 8800, Loss: 0.11398322135210037
step: 8900, Loss: 0.11276277154684067
step: 9000, Loss: 0.11303648352622986
step: 9100, Loss: 0.19916217029094696
step: 9200, Loss: 0.11456906795501709
step: 9300, Loss: 0.11410564184188843
step: 9400, Loss: 0.11415345221757889
step: 9500, Loss: 0.11611361801624298
step: 9600, Loss: 0.11378788203001022
step: 9700, Loss: 0.11363798379898071
step: 9800, Loss: 0.11375650763511658
step: 9900, Loss: 0.11550165712833405
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 1.0
recall: 0.9032258064516129
F_score: 0.9491525423728813
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.21116672456264496
step: 100, Loss: 0.11615639179944992
step: 200, Loss: 0.11448056995868683
step: 300, Loss: 0.1150832399725914
step: 400, Loss: 0.11446011066436768
step: 500, Loss: 0.11469811946153641
step: 600, Loss: 0.1133987233042717
step: 700, Loss: 0.11386855691671371
step: 800, Loss: 0.11432711035013199
step: 900, Loss: 0.11424922943115234
step: 1000, Loss: 0.11391521245241165
step: 1100, Loss: 0.11329614371061325
step: 1200, Loss: 0.11425258964300156
step: 1300, Loss: 0.11412942409515381
step: 1400, Loss: 0.11381921172142029
step: 1500, Loss: 0.18987470865249634
step: 1600, Loss: 0.11409923434257507
step: 1700, Loss: 0.1127672791481018
step: 1800, Loss: 0.1142013669013977
step: 1900, Loss: 0.11461571604013443
step: 2000, Loss: 0.11390279233455658
step: 2100, Loss: 0.11357734352350235
step: 2200, Loss: 0.11424063891172409
step: 2300, Loss: 0.15429116785526276
step: 2400, Loss: 0.1379646211862564
step: 2500, Loss: 0.13176822662353516
step: 2600, Loss: 0.12150250375270844
step: 2700, Loss: 0.12760263681411743
step: 2800, Loss: 0.11930720508098602
step: 2900, Loss: 0.12725244462490082
step: 3000, Loss: 0.11836361885070801
step: 3100, Loss: 0.12085698544979095
step: 3200, Loss: 0.12032058835029602
step: 3300, Loss: 0.1213078498840332
step: 3400, Loss: 0.20139990746974945
step: 3500, Loss: 0.11810974031686783
step: 3600, Loss: 0.11494748294353485
step: 3700, Loss: 0.12257640063762665
step: 3800, Loss: 0.1183096170425415
step: 3900, Loss: 0.11675693094730377
step: 4000, Loss: 0.11532188951969147
step: 4100, Loss: 0.11615943163633347
step: 4200, Loss: 0.1150754764676094
step: 4300, Loss: 0.11594939231872559
step: 4400, Loss: 0.11887231469154358
step: 4500, Loss: 0.11637800186872482
step: 4600, Loss: 0.11877308785915375
step: 4700, Loss: 0.1160210445523262
step: 4800, Loss: 0.11787496507167816
step: 4900, Loss: 0.11344092339277267
step: 5000, Loss: 0.11685867607593536
step: 5100, Loss: 0.11506344377994537
step: 5200, Loss: 0.11316268891096115
step: 5300, Loss: 0.1953064352273941
step: 5400, Loss: 0.11626836657524109
step: 5500, Loss: 0.11541815102100372
step: 5600, Loss: 0.11426504701375961
step: 5700, Loss: 0.11480318754911423
step: 5800, Loss: 0.11326444894075394
step: 5900, Loss: 0.1156754121184349
step: 6000, Loss: 0.11419174075126648
step: 6100, Loss: 0.11437761783599854
step: 6200, Loss: 0.11336648464202881
step: 6300, Loss: 0.1139780730009079
step: 6400, Loss: 0.11387333273887634
step: 6500, Loss: 0.11429928243160248
step: 6600, Loss: 0.1130339652299881
step: 6700, Loss: 0.1129666343331337
step: 6800, Loss: 0.11553951352834702
step: 6900, Loss: 0.11358978599309921
step: 7000, Loss: 0.11252006143331528
step: 7100, Loss: 0.11424779146909714
step: 7200, Loss: 0.1914592981338501
step: 7300, Loss: 0.11313829571008682
step: 7400, Loss: 0.11367376148700714
step: 7500, Loss: 0.11384717375040054
step: 7600, Loss: 0.11319611966609955
step: 7700, Loss: 0.11362292617559433
step: 7800, Loss: 0.11317921429872513
step: 7900, Loss: 0.11357906460762024
step: 8000, Loss: 0.11342985928058624
step: 8100, Loss: 0.11380656808614731
step: 8200, Loss: 0.11315537244081497
step: 8300, Loss: 0.1137717068195343
step: 8400, Loss: 0.11381998658180237
step: 8500, Loss: 0.11377905309200287
step: 8600, Loss: 0.11358828097581863
step: 8700, Loss: 0.1128804013133049
step: 8800, Loss: 0.11418711394071579
step: 8900, Loss: 0.11353877186775208
step: 9000, Loss: 0.11310256272554398
step: 9100, Loss: 0.1920624077320099
step: 9200, Loss: 0.1139078363776207
step: 9300, Loss: 0.11414948850870132
step: 9400, Loss: 0.11387188732624054
step: 9500, Loss: 0.1129952222108841
step: 9600, Loss: 0.11465483903884888
step: 9700, Loss: 0.11583277583122253
step: 9800, Loss: 0.11359979212284088
step: 9900, Loss: 0.11381052434444427
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.925
recall: 1.0
F_score: 0.961038961038961
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.28896820545196533
step: 100, Loss: 0.11495199799537659
step: 200, Loss: 0.11610986292362213
step: 300, Loss: 0.11492560058832169
step: 400, Loss: 0.11324802041053772
step: 500, Loss: 0.11443916708230972
step: 600, Loss: 0.11282429099082947
step: 700, Loss: 0.11388880014419556
step: 800, Loss: 0.11431150883436203
step: 900, Loss: 0.1132110133767128
step: 1000, Loss: 0.11370504647493362
step: 1100, Loss: 0.115361787378788
step: 1200, Loss: 0.11401214450597763
step: 1300, Loss: 0.11285685747861862
step: 1400, Loss: 0.1146344318985939
step: 1500, Loss: 0.1925353854894638
step: 1600, Loss: 0.11451935768127441
step: 1700, Loss: 0.11427891254425049
step: 1800, Loss: 0.11400434374809265
step: 1900, Loss: 0.11291685700416565
step: 2000, Loss: 0.11407773196697235
step: 2100, Loss: 0.11386904120445251
step: 2200, Loss: 0.11346105486154556
step: 2300, Loss: 0.11356913298368454
step: 2400, Loss: 0.11589258909225464
step: 2500, Loss: 0.11448591947555542
step: 2600, Loss: 0.11413544416427612
step: 2700, Loss: 0.11503429710865021
step: 2800, Loss: 0.11430343240499496
step: 2900, Loss: 0.7571519613265991
step: 3000, Loss: 0.1450052261352539
step: 3100, Loss: 0.13053929805755615
step: 3200, Loss: 0.12560367584228516
step: 3300, Loss: 0.12311584502458572
step: 3400, Loss: 0.20978432893753052
step: 3500, Loss: 0.12131128460168839
step: 3600, Loss: 0.12356787919998169
step: 3700, Loss: 0.12252870947122574
step: 3800, Loss: 0.11908430606126785
step: 3900, Loss: 0.12250261753797531
step: 4000, Loss: 0.11709883064031601
step: 4100, Loss: 0.11795277148485184
step: 4200, Loss: 0.12074684351682663
step: 4300, Loss: 0.11940910667181015
step: 4400, Loss: 0.1177041232585907
step: 4500, Loss: 0.11682663857936859
step: 4600, Loss: 0.1167120486497879
step: 4700, Loss: 0.1148967444896698
step: 4800, Loss: 0.11422043293714523
step: 4900, Loss: 0.11663501709699631
step: 5000, Loss: 0.11604522168636322
step: 2100, Loss: 0.11350785195827484
step: 2200, Loss: 0.11460468173027039
step: 2300, Loss: 0.11277306824922562
step: 2400, Loss: 0.11490163952112198
step: 2500, Loss: 0.1167801097035408
step: 2600, Loss: 0.11366969347000122
step: 2700, Loss: 0.11424405872821808
step: 2800, Loss: 0.11304648220539093
step: 2900, Loss: 0.11515547335147858
step: 3000, Loss: 0.11418004333972931
step: 3100, Loss: 0.11619971692562103
step: 3200, Loss: 0.1162303239107132
step: 3300, Loss: 0.11461380124092102
step: 3400, Loss: 0.11431492865085602
step: 3500, Loss: 0.11318808794021606
step: 3600, Loss: 0.11426904797554016
step: 3700, Loss: 0.11452780663967133
step: 3800, Loss: 0.11325327306985855
step: 3900, Loss: 0.11576015502214432
step: 4000, Loss: 0.11559012532234192
step: 4100, Loss: 0.1147952526807785
step: 4200, Loss: 0.11351312696933746
step: 4300, Loss: 0.11527921259403229
step: 4400, Loss: 0.11412955075502396
step: 4500, Loss: 0.11393663287162781
step: 4600, Loss: 0.11709591746330261
step: 4700, Loss: 0.115030437707901
step: 4800, Loss: 0.11499586701393127
step: 4900, Loss: 0.11398511379957199
step: 5000, Loss: 0.11471421271562576
step: 5100, Loss: 0.11438712477684021
step: 5200, Loss: 0.11644747853279114
step: 5300, Loss: 0.11560630053281784
step: 5400, Loss: 0.11379484832286835
step: 5500, Loss: 0.11591699719429016
step: 5600, Loss: 0.11528034508228302
step: 5700, Loss: 0.11602664738893509
step: 5800, Loss: 0.1144251748919487
step: 5900, Loss: 0.11343682557344437
step: 6000, Loss: 0.1161041334271431
step: 6100, Loss: 0.11492804437875748
step: 6200, Loss: 0.6818063259124756
step: 6300, Loss: 0.1472771018743515
step: 6400, Loss: 0.1343650221824646
step: 6500, Loss: 0.12729325890541077
step: 6600, Loss: 0.12081924825906754
step: 6700, Loss: 0.12342146784067154
step: 6800, Loss: 0.12430457770824432
step: 6900, Loss: 0.12296006083488464
step: 7000, Loss: 0.12024141103029251
step: 7100, Loss: 0.1203891858458519
step: 7200, Loss: 0.11841630190610886
step: 7300, Loss: 0.12038169056177139
step: 7400, Loss: 0.11571656167507172
step: 7500, Loss: 0.11699351668357849
step: 7600, Loss: 0.11460726708173752
step: 7700, Loss: 0.11823944002389908
step: 7800, Loss: 0.11534879356622696
step: 7900, Loss: 0.11565641313791275
step: 8000, Loss: 0.11839240789413452
step: 8100, Loss: 0.11596394330263138
step: 8200, Loss: 0.11604820191860199
step: 8300, Loss: 0.1168171763420105
step: 8400, Loss: 0.1172306090593338
step: 8500, Loss: 0.1154886782169342
step: 8600, Loss: 0.11430884897708893
step: 8700, Loss: 0.11544359475374222
step: 8800, Loss: 0.11779391765594482
step: 8900, Loss: 0.1148597002029419
step: 9000, Loss: 0.11471649259328842
step: 9100, Loss: 0.1157853752374649
step: 9200, Loss: 0.11431173235177994
step: 9300, Loss: 0.11505627632141113
step: 9400, Loss: 0.11460639536380768
step: 9500, Loss: 0.11555006355047226
step: 9600, Loss: 0.11442015320062637
step: 9700, Loss: 0.11553347855806351
step: 9800, Loss: 0.11442123353481293
step: 9900, Loss: 0.11482374370098114
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.625
recall: 0.8333333333333334
F_score: 0.7142857142857143
subject 14 Avgacc: 0.7533333333333333 Avgfscore: 0.780896121303936 
 Max acc:0.84375, Max f score:0.8571428571428572
******** mix subject_15 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 26.625646591186523
step: 100, Loss: 0.46774357557296753
step: 200, Loss: 0.13911713659763336
step: 300, Loss: 0.1371133029460907
step: 400, Loss: 0.14051121473312378
step: 500, Loss: 0.1294603943824768
step: 600, Loss: 0.1273760199546814
step: 700, Loss: 0.12528395652770996
step: 800, Loss: 0.1275770217180252
step: 900, Loss: 0.11996524035930634
step: 1000, Loss: 0.11926408112049103
step: 1100, Loss: 0.11819536983966827
step: 1200, Loss: 0.11826322972774506
step: 1300, Loss: 0.12086135149002075
step: 1400, Loss: 0.11933910846710205
step: 1500, Loss: 0.12464589625597
step: 1600, Loss: 0.1182112842798233
step: 1700, Loss: 0.11724501103162766
step: 1800, Loss: 0.11505832523107529
step: 1900, Loss: 0.11950337886810303
step: 2000, Loss: 0.11532328277826309
step: 2100, Loss: 0.11712367832660675
step: 2200, Loss: 0.11998499929904938
step: 2300, Loss: 0.11868781596422195
step: 2400, Loss: 0.11852405965328217
step: 2500, Loss: 0.11434181034564972
step: 2600, Loss: 0.1178126111626625
step: 2700, Loss: 0.11678914725780487
step: 2800, Loss: 0.1155020222067833
step: 2900, Loss: 0.11697983741760254
step: 3000, Loss: 0.13181506097316742
step: 3100, Loss: 0.11637679487466812
step: 3200, Loss: 0.11623254418373108
step: 3300, Loss: 0.11919884383678436
step: 3400, Loss: 0.11560914665460587
step: 3500, Loss: 0.11597035080194473
step: 3600, Loss: 0.11956417560577393
step: 3700, Loss: 0.11545028537511826
step: 3800, Loss: 0.11671988666057587
step: 3900, Loss: 0.11431440711021423
step: 4000, Loss: 0.11708980053663254
step: 4100, Loss: 0.11820204555988312
step: 4200, Loss: 0.11633659899234772
step: 4300, Loss: 0.11562009155750275
step: 4400, Loss: 0.11476917564868927
step: 4500, Loss: 0.11571279168128967
step: 4600, Loss: 0.11568884551525116
step: 4700, Loss: 0.1153528243303299
step: 4800, Loss: 0.11433751881122589
step: 4900, Loss: 0.11452576518058777
step: 5000, Loss: 0.11571478843688965
step: 5100, Loss: 0.11485815048217773
step: 5200, Loss: 0.11588313430547714
step: 5300, Loss: 0.11738944053649902
step: 5400, Loss: 0.11379590630531311
step: 5500, Loss: 0.11851522326469421
step: 5600, Loss: 0.11548098176717758
step: 5700, Loss: 0.11365179717540741
step: 5800, Loss: 0.11621136218309402
step: 5900, Loss: 0.11630275100469589
step: 6000, Loss: 0.1144540011882782
step: 6100, Loss: 0.11532452702522278
step: 6200, Loss: 0.11493724584579468
step: 6300, Loss: 0.1147833988070488
step: 6400, Loss: 0.11564093828201294
step: 6500, Loss: 0.11710631847381592
step: 6600, Loss: 0.11630087345838547
step: 6700, Loss: 0.11338517814874649
step: 6800, Loss: 0.11559968441724777
step: 6900, Loss: 0.11439945548772812
step: 7000, Loss: 0.11467033624649048
step: 7100, Loss: 0.11746988445520401
step: 7200, Loss: 0.11474864929914474
step: 7300, Loss: 0.11998163908720016
step: 7400, Loss: 0.15705285966396332
step: 7500, Loss: 0.13869279623031616
step: 7600, Loss: 0.13402916491031647
step: 7700, Loss: 0.12494739145040512
step: 7800, Loss: 0.1314803808927536
step: 7900, Loss: 0.12376874685287476
step: 8000, Loss: 0.12046314775943756
step: 8100, Loss: 0.12089306116104126
step: 8200, Loss: 0.12109875679016113
step: 8300, Loss: 0.1271849423646927
step: 8400, Loss: 0.12191697955131531
step: 8500, Loss: 0.11922047287225723
step: 8600, Loss: 0.11725466698408127
step: 8700, Loss: 0.11746266484260559
step: 8800, Loss: 0.11754533648490906
step: 8900, Loss: 0.12353351712226868
step: 9000, Loss: 0.1162843108177185
step: 9100, Loss: 0.11851553618907928
step: 9200, Loss: 0.11632516235113144
step: 9300, Loss: 0.12536971271038055
step: 9400, Loss: 0.11495853960514069
step: 9500, Loss: 0.11823911964893341
step: 9600, Loss: 0.11679361760616302
step: 9700, Loss: 0.12021651118993759
step: 9800, Loss: 0.11686800420284271
step: 9900, Loss: 0.11686763912439346
training successfully ended.
validating...
validate data length:32
acc: 0.4375
precision: 0.375
recall: 0.75
F_score: 0.5
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.1506849229335785
step: 100, Loss: 0.12451190501451492
step: 200, Loss: 0.12228361517190933
step: 300, Loss: 0.12069650739431381
step: 400, Loss: 0.12034667283296585
step: 500, Loss: 0.11967332661151886
step: 600, Loss: 0.11777332425117493
step: 700, Loss: 0.11848703026771545
step: 800, Loss: 0.11665291339159012
step: 900, Loss: 0.12978708744049072
step: 1000, Loss: 0.11770884692668915
step: 1100, Loss: 0.11594740301370621
step: 1200, Loss: 0.11463005840778351
step: 1300, Loss: 0.115193210542202
step: 1400, Loss: 0.1157718300819397
step: 1500, Loss: 0.11584944278001785
step: 1600, Loss: 0.11502065509557724
step: 1700, Loss: 0.11474583297967911
step: 1800, Loss: 0.11832806468009949
step: 1900, Loss: 0.1172214150428772
step: 2000, Loss: 0.11470883339643478
step: 2100, Loss: 0.11507856100797653
step: 2200, Loss: 0.11839238554239273
step: 5100, Loss: 0.11489173024892807
step: 5200, Loss: 0.11418020725250244
step: 5300, Loss: 0.19492581486701965
step: 5400, Loss: 0.11586419492959976
step: 5500, Loss: 0.11525285989046097
step: 5600, Loss: 0.11420489847660065
step: 5700, Loss: 0.11479246616363525
step: 5800, Loss: 0.11376718431711197
step: 5900, Loss: 0.11314244568347931
step: 6000, Loss: 0.11655788123607635
step: 6100, Loss: 0.1144447922706604
step: 6200, Loss: 0.11453520506620407
step: 6300, Loss: 0.11343471705913544
step: 6400, Loss: 0.11408118158578873
step: 6500, Loss: 0.11531659960746765
step: 6600, Loss: 0.11412392556667328
step: 6700, Loss: 0.11359193176031113
step: 6800, Loss: 0.11347402632236481
step: 6900, Loss: 0.11312948167324066
step: 7000, Loss: 0.11355708539485931
step: 7100, Loss: 0.11329598724842072
step: 7200, Loss: 0.1918627768754959
step: 7300, Loss: 0.11322398483753204
step: 7400, Loss: 0.11342422664165497
step: 7500, Loss: 0.11508310586214066
step: 7600, Loss: 0.11445383727550507
step: 7700, Loss: 0.11385160684585571
step: 7800, Loss: 0.11463920772075653
step: 7900, Loss: 0.11342057585716248
step: 8000, Loss: 0.11509713530540466
step: 8100, Loss: 0.1131812185049057
step: 8200, Loss: 0.11434142291545868
step: 8300, Loss: 0.1139320358633995
step: 8400, Loss: 0.11356757581233978
step: 8500, Loss: 0.11463727802038193
step: 8600, Loss: 0.11324068158864975
step: 8700, Loss: 0.11354020982980728
step: 8800, Loss: 0.11336085200309753
step: 8900, Loss: 0.11501012742519379
step: 9000, Loss: 0.1143127828836441
step: 9100, Loss: 0.19921576976776123
step: 9200, Loss: 0.11305508762598038
step: 9300, Loss: 0.1134384348988533
step: 9400, Loss: 0.1131293848156929
step: 9500, Loss: 0.11429600417613983
step: 9600, Loss: 0.11331912875175476
step: 9700, Loss: 0.11373494565486908
step: 9800, Loss: 0.11341451853513718
step: 9900, Loss: 0.11378884315490723
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.967741935483871
F_score: 0.9836065573770492
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.2259024679660797
step: 100, Loss: 0.1152208000421524
step: 200, Loss: 0.1149555891752243
step: 300, Loss: 0.11492973566055298
step: 400, Loss: 0.11452752351760864
step: 500, Loss: 0.11373573541641235
step: 600, Loss: 0.11507616937160492
step: 700, Loss: 0.11305280774831772
step: 800, Loss: 0.1129390299320221
step: 900, Loss: 0.11387437582015991
step: 1000, Loss: 0.11397567391395569
step: 1100, Loss: 0.11289583146572113
step: 1200, Loss: 0.1144278272986412
step: 1300, Loss: 0.11444559693336487
step: 1400, Loss: 0.1133846566081047
step: 1500, Loss: 0.19516465067863464
step: 1600, Loss: 0.11380934715270996
step: 1700, Loss: 0.11410489678382874
step: 1800, Loss: 0.1136670783162117
step: 1900, Loss: 0.11417999863624573
step: 2000, Loss: 0.11459794640541077
step: 2100, Loss: 0.11355248093605042
step: 2200, Loss: 0.11341297626495361
step: 2300, Loss: 0.11377483606338501
step: 2400, Loss: 0.11366312205791473
step: 2500, Loss: 0.11524026095867157
step: 2600, Loss: 0.11313524097204208
step: 2700, Loss: 0.11342762410640717
step: 2800, Loss: 0.11495371162891388
step: 2900, Loss: 0.11466880142688751
step: 3000, Loss: 0.1139262393116951
step: 3100, Loss: 0.11347626894712448
step: 3200, Loss: 0.11323527991771698
step: 3300, Loss: 0.11287438124418259
step: 3400, Loss: 0.19275999069213867
step: 3500, Loss: 0.11320558935403824
step: 3600, Loss: 0.11296330392360687
step: 3700, Loss: 0.11462335288524628
step: 3800, Loss: 0.11679287254810333
step: 3900, Loss: 0.2471410036087036
step: 4000, Loss: 0.1436547040939331
step: 4100, Loss: 0.1277608424425125
step: 4200, Loss: 0.12418318539857864
step: 4300, Loss: 0.11797019094228745
step: 4400, Loss: 0.1220509484410286
step: 4500, Loss: 0.12033988535404205
step: 4600, Loss: 0.11845763772726059
step: 4700, Loss: 0.11837393045425415
step: 4800, Loss: 0.11786956340074539
step: 4900, Loss: 0.119052954018116
step: 5000, Loss: 0.1161758080124855
step: 5100, Loss: 0.11674344539642334
step: 5200, Loss: 0.11781597137451172
step: 5300, Loss: 0.19687610864639282
step: 5400, Loss: 0.11913464963436127
step: 5500, Loss: 0.11725333333015442
step: 5600, Loss: 0.11937014013528824
step: 5700, Loss: 0.11734931170940399
step: 5800, Loss: 0.11718837171792984
step: 5900, Loss: 0.1166592612862587
step: 6000, Loss: 0.11560173332691193
step: 6100, Loss: 0.1168816089630127
step: 6200, Loss: 0.11406482756137848
step: 6300, Loss: 0.11557076871395111
step: 6400, Loss: 0.11513258516788483
step: 6500, Loss: 0.11538037657737732
step: 6600, Loss: 0.11485008895397186
step: 6700, Loss: 0.1144419014453888
step: 6800, Loss: 0.11531876772642136
step: 6900, Loss: 0.11490891128778458
step: 7000, Loss: 0.11487212032079697
step: 7100, Loss: 0.11468145251274109
step: 7200, Loss: 0.19303585588932037
step: 7300, Loss: 0.11342471837997437
step: 7400, Loss: 0.11457450687885284
step: 7500, Loss: 0.11438189446926117
step: 7600, Loss: 0.11545757204294205
step: 7700, Loss: 0.11410876363515854
step: 7800, Loss: 0.11484241485595703
step: 7900, Loss: 0.11491593718528748
step: 8000, Loss: 0.11420630663633347
step: 8100, Loss: 0.11364459246397018
step: 8200, Loss: 0.11384564638137817
step: 8300, Loss: 0.11308519542217255
step: 8400, Loss: 0.11403057724237442
step: 8500, Loss: 0.11379662901163101
step: 8600, Loss: 0.11358577758073807
step: 8700, Loss: 0.11409638822078705
step: 8800, Loss: 0.1143835186958313
step: 8900, Loss: 0.11356453597545624
step: 9000, Loss: 0.11342719942331314
step: 9100, Loss: 0.1905832588672638
step: 9200, Loss: 0.11347108334302902
step: 9300, Loss: 0.11345108598470688
step: 9400, Loss: 0.11474548280239105
step: 9500, Loss: 0.11407928168773651
step: 9600, Loss: 0.11439841240644455
step: 9700, Loss: 0.11478763818740845
step: 9800, Loss: 0.11353123933076859
step: 9900, Loss: 0.1137794628739357
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9166666666666666
recall: 1.0
F_score: 0.9565217391304348
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.21113963425159454
step: 100, Loss: 0.11605473607778549
step: 200, Loss: 0.1176130622625351
step: 300, Loss: 0.11600697785615921
step: 400, Loss: 0.1155051440000534
step: 500, Loss: 0.1136198416352272
step: 600, Loss: 0.11371850967407227
step: 700, Loss: 0.11445310711860657
step: 800, Loss: 0.1144852340221405
step: 900, Loss: 0.11562033742666245
step: 1000, Loss: 0.11347855627536774
step: 1100, Loss: 0.11430060118436813
step: 1200, Loss: 0.11493838578462601
step: 1300, Loss: 0.11457125097513199
step: 1400, Loss: 0.11384597420692444
step: 1500, Loss: 0.19386442005634308
step: 1600, Loss: 0.11410104483366013
step: 1700, Loss: 0.11481063067913055
step: 1800, Loss: 0.11360038071870804
step: 1900, Loss: 0.11508644372224808
step: 2000, Loss: 0.11359144747257233
step: 2100, Loss: 0.11373000591993332
step: 2200, Loss: 0.11422080546617508
step: 2300, Loss: 0.11458596587181091
step: 2400, Loss: 0.11452330648899078
step: 2500, Loss: 0.11484874039888382
step: 2600, Loss: 0.11424364894628525
step: 2700, Loss: 0.46069565415382385
step: 2800, Loss: 0.13938942551612854
step: 2900, Loss: 0.1327349692583084
step: 3000, Loss: 0.13557541370391846
step: 3100, Loss: 0.12426090985536575
step: 3200, Loss: 0.12161911278963089
step: 3300, Loss: 0.1311231404542923
step: 3400, Loss: 0.21130529046058655
step: 3500, Loss: 0.12529510259628296
step: 3600, Loss: 0.12028206884860992
step: 3700, Loss: 0.12706606090068817
step: 3800, Loss: 0.11974729597568512
step: 3900, Loss: 0.12207608669996262
step: 4000, Loss: 0.11796954274177551
step: 4100, Loss: 0.1224789023399353
step: 4200, Loss: 0.12160468101501465
step: 4300, Loss: 0.11534696072340012
step: 4400, Loss: 0.11825183779001236
step: 4500, Loss: 0.11649014055728912
step: 4600, Loss: 0.11854992806911469
step: 4700, Loss: 0.11517936736345291
step: 4800, Loss: 0.11620193719863892
step: 4900, Loss: 0.11590657383203506
step: 5000, Loss: 0.11573921144008636
step: 5100, Loss: 0.11636196076869965
step: 5200, Loss: 0.11596021056175232
step: 5300, Loss: 0.19789017736911774
step: 5400, Loss: 0.11696268618106842
step: 5500, Loss: 0.11770305037498474
step: 2300, Loss: 0.11539444327354431
step: 2400, Loss: 0.1144571453332901
step: 2500, Loss: 0.11494581401348114
step: 2600, Loss: 0.1147117167711258
step: 2700, Loss: 0.11764075607061386
step: 2800, Loss: 0.1152779832482338
step: 2900, Loss: 0.11629454046487808
step: 3000, Loss: 0.11428843438625336
step: 3100, Loss: 0.1182229146361351
step: 3200, Loss: 0.113829106092453
step: 3300, Loss: 0.11429595202207565
step: 3400, Loss: 0.11580046266317368
step: 3500, Loss: 0.11508537828922272
step: 3600, Loss: 0.1137005016207695
step: 3700, Loss: 0.11535616219043732
step: 3800, Loss: 0.11466915905475616
step: 3900, Loss: 0.11537952721118927
step: 4000, Loss: 0.11671806871891022
step: 4100, Loss: 0.1164373904466629
step: 4200, Loss: 0.11704911291599274
step: 4300, Loss: 0.11452829837799072
step: 4400, Loss: 0.11570103466510773
step: 4500, Loss: 0.11753605306148529
step: 4600, Loss: 0.11556217819452286
step: 4700, Loss: 0.11464302241802216
step: 4800, Loss: 0.11361109465360641
step: 4900, Loss: 0.1159687414765358
step: 5000, Loss: 0.11523442715406418
step: 5100, Loss: 0.11747388541698456
step: 5200, Loss: 0.1147305890917778
step: 5300, Loss: 0.1162126213312149
step: 5400, Loss: 0.11499170958995819
step: 5500, Loss: 0.11533723771572113
step: 5600, Loss: 0.11534510552883148
step: 5700, Loss: 0.11559721827507019
step: 5800, Loss: 0.11437041312456131
step: 5900, Loss: 0.11401429027318954
step: 6000, Loss: 0.11382250487804413
step: 6100, Loss: 0.11525607109069824
step: 6200, Loss: 0.11375458538532257
step: 6300, Loss: 0.1157979816198349
step: 6400, Loss: 0.11428370326757431
step: 6500, Loss: 0.11668692529201508
step: 6600, Loss: 0.11407949030399323
step: 6700, Loss: 0.11393110454082489
step: 6800, Loss: 0.114299476146698
step: 6900, Loss: 0.1164238229393959
step: 7000, Loss: 0.11473431438207626
step: 7100, Loss: 0.11687423288822174
step: 7200, Loss: 0.11587047576904297
step: 7300, Loss: 0.11482956260442734
step: 7400, Loss: 0.1147012785077095
step: 7500, Loss: 0.11500605940818787
step: 7600, Loss: 0.11430199444293976
step: 7700, Loss: 0.11572303622961044
step: 7800, Loss: 0.1712304800748825
step: 7900, Loss: 0.14724940061569214
step: 8000, Loss: 0.13537348806858063
step: 8100, Loss: 0.1310589611530304
step: 8200, Loss: 0.12481461465358734
step: 8300, Loss: 0.12256073206663132
step: 8400, Loss: 0.12659844756126404
step: 8500, Loss: 0.12487906217575073
step: 8600, Loss: 0.12369394302368164
step: 8700, Loss: 0.12048137933015823
step: 8800, Loss: 0.12066632509231567
step: 8900, Loss: 0.11847273260354996
step: 9000, Loss: 0.12021584808826447
step: 9100, Loss: 0.1190694272518158
step: 9200, Loss: 0.11976708471775055
step: 9300, Loss: 0.11583134531974792
step: 9400, Loss: 0.1168113574385643
step: 9500, Loss: 0.11633378267288208
step: 9600, Loss: 0.11779072880744934
step: 9700, Loss: 0.11662688851356506
step: 9800, Loss: 0.11788727343082428
step: 9900, Loss: 0.11586552858352661
training successfully ended.
validating...
validate data length:32
acc: 0.75
precision: 0.75
recall: 0.75
F_score: 0.75
******fold 3******

Training... train_data length:281
step: 0, Loss: 0.2616894245147705
step: 100, Loss: 0.1220087856054306
step: 200, Loss: 0.12200447916984558
step: 300, Loss: 0.11855487525463104
step: 400, Loss: 0.11604712903499603
step: 500, Loss: 0.11760519444942474
step: 600, Loss: 0.11627977341413498
step: 700, Loss: 0.1161649078130722
step: 800, Loss: 0.11772117763757706
step: 900, Loss: 0.1166229099035263
step: 1000, Loss: 0.11575610935688019
step: 1100, Loss: 0.11437056213617325
step: 1200, Loss: 0.11727592349052429
step: 1300, Loss: 0.1150585189461708
step: 1400, Loss: 0.11693154275417328
step: 1500, Loss: 0.11434494704008102
step: 1600, Loss: 0.11524947732686996
step: 1700, Loss: 0.1180233508348465
step: 1800, Loss: 0.11617332696914673
step: 1900, Loss: 0.1149984672665596
step: 2000, Loss: 0.1143455058336258
step: 2100, Loss: 0.1127961054444313
step: 2200, Loss: 0.11550560593605042
step: 2300, Loss: 0.1167222112417221
step: 2400, Loss: 0.11564777791500092
step: 2500, Loss: 0.11431695520877838
step: 2600, Loss: 0.11713799834251404
step: 2700, Loss: 0.11364752054214478
step: 2800, Loss: 0.11709839105606079
step: 2900, Loss: 0.11453321576118469
step: 3000, Loss: 0.1157408356666565
step: 3100, Loss: 0.11721748113632202
step: 3200, Loss: 0.11597567796707153
step: 3300, Loss: 0.11820065230131149
step: 3400, Loss: 0.1137620210647583
step: 3500, Loss: 0.11548242717981339
step: 3600, Loss: 0.11357749998569489
step: 3700, Loss: 0.11535007506608963
step: 3800, Loss: 0.11425720900297165
step: 3900, Loss: 0.11635053157806396
step: 4000, Loss: 0.11396830528974533
step: 4100, Loss: 0.11520273983478546
step: 4200, Loss: 0.11450403928756714
step: 4300, Loss: 0.1140279471874237
step: 4400, Loss: 0.11537349224090576
step: 4500, Loss: 0.11467086523771286
step: 4600, Loss: 0.11408288776874542
step: 4700, Loss: 0.114983931183815
step: 4800, Loss: 0.11460594087839127
step: 4900, Loss: 0.1145012304186821
step: 5000, Loss: 0.11366100609302521
step: 5100, Loss: 0.1134987473487854
step: 5200, Loss: 0.1145884245634079
step: 5300, Loss: 0.11635074764490128
step: 5400, Loss: 0.11655325442552567
step: 5500, Loss: 0.11517839133739471
step: 5600, Loss: 0.11444141715765
step: 5700, Loss: 0.11659225821495056
step: 5800, Loss: 0.11401605606079102
step: 5900, Loss: 0.11573562026023865
step: 6000, Loss: 0.1154160425066948
step: 6100, Loss: 0.1148931011557579
step: 6200, Loss: 0.1145513653755188
step: 6300, Loss: 0.11498425900936127
step: 6400, Loss: 0.11424153298139572
step: 6500, Loss: 0.11796888709068298
step: 6600, Loss: 0.11574720591306686
step: 6700, Loss: 0.1142740398645401
step: 6800, Loss: 0.11757910251617432
step: 6900, Loss: 0.11428716778755188
step: 7000, Loss: 0.11405105888843536
step: 7100, Loss: 0.11524862051010132
step: 7200, Loss: 0.1162833720445633
step: 7300, Loss: 0.11533508449792862
step: 7400, Loss: 0.11524961143732071
step: 7500, Loss: 0.11848916113376617
step: 7600, Loss: 0.19034945964813232
step: 7700, Loss: 0.1362612098455429
step: 7800, Loss: 0.13804858922958374
step: 7900, Loss: 0.1233014240860939
step: 8000, Loss: 0.13370084762573242
step: 8100, Loss: 0.13153058290481567
step: 8200, Loss: 0.12606219947338104
step: 8300, Loss: 0.13317887485027313
step: 8400, Loss: 0.12687630951404572
step: 8500, Loss: 0.12003904581069946
step: 8600, Loss: 0.12101294845342636
step: 8700, Loss: 0.11864403635263443
step: 8800, Loss: 0.12174466252326965
step: 8900, Loss: 0.11552096903324127
step: 9000, Loss: 0.11961284279823303
step: 9100, Loss: 0.11737803369760513
step: 9200, Loss: 0.11828338354825974
step: 9300, Loss: 0.11709128320217133
step: 9400, Loss: 0.1163007840514183
step: 9500, Loss: 0.11839954555034637
step: 9600, Loss: 0.11873498558998108
step: 9700, Loss: 0.11653849482536316
step: 9800, Loss: 0.11568199098110199
step: 9900, Loss: 0.11530666053295135
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.6521739130434783
recall: 1.0
F_score: 0.7894736842105263
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.23349326848983765
step: 100, Loss: 0.1237061619758606
step: 200, Loss: 0.11765021085739136
step: 300, Loss: 0.11729410290718079
step: 400, Loss: 0.1166098564863205
step: 500, Loss: 0.11542123556137085
step: 600, Loss: 0.11528847366571426
step: 700, Loss: 0.11509552597999573
step: 800, Loss: 0.11843371391296387
step: 900, Loss: 0.11635801196098328
step: 1000, Loss: 0.11651051789522171
step: 1100, Loss: 0.11498759686946869
step: 1200, Loss: 0.11495756357908249
step: 1300, Loss: 0.11543774604797363
step: 1400, Loss: 0.11725819855928421
step: 1500, Loss: 0.11400788277387619
step: 1600, Loss: 0.11882573366165161
step: 1700, Loss: 0.11470286548137665
step: 1800, Loss: 0.11413418501615524
step: 1900, Loss: 0.11774108558893204
step: 2000, Loss: 0.1152934581041336
step: 2100, Loss: 0.11672449856996536
step: 2200, Loss: 0.11733698844909668
step: 2300, Loss: 0.1152985543012619
step: 2400, Loss: 0.115081787109375
step: 2500, Loss: 0.11711236834526062
step: 2600, Loss: 0.11559001356363297
step: 2700, Loss: 0.11519968509674072
step: 2800, Loss: 0.11494168639183044
step: 2900, Loss: 0.11556369066238403
step: 5600, Loss: 0.11476891487836838
step: 5700, Loss: 0.11404965817928314
step: 5800, Loss: 0.11556227505207062
step: 5900, Loss: 0.11362515389919281
step: 6000, Loss: 0.11465375125408173
step: 6100, Loss: 0.1154932826757431
step: 6200, Loss: 0.11377215385437012
step: 6300, Loss: 0.11466245353221893
step: 6400, Loss: 0.11455909162759781
step: 6500, Loss: 0.11482713371515274
step: 6600, Loss: 0.11599995195865631
step: 6700, Loss: 0.11409865319728851
step: 6800, Loss: 0.11497540771961212
step: 6900, Loss: 0.11319435387849808
step: 7000, Loss: 0.11458754539489746
step: 7100, Loss: 0.1144847571849823
step: 7200, Loss: 0.1952930986881256
step: 7300, Loss: 0.11502781510353088
step: 7400, Loss: 0.11441670358181
step: 7500, Loss: 0.11398958414793015
step: 7600, Loss: 0.11339481174945831
step: 7700, Loss: 0.11518782377243042
step: 7800, Loss: 0.11382300406694412
step: 7900, Loss: 0.11428644508123398
step: 8000, Loss: 0.11389864981174469
step: 8100, Loss: 0.11279711127281189
step: 8200, Loss: 0.11334232985973358
step: 8300, Loss: 0.1142580658197403
step: 8400, Loss: 0.11456163972616196
step: 8500, Loss: 0.11370972543954849
step: 8600, Loss: 0.11374053359031677
step: 8700, Loss: 0.11362893879413605
step: 8800, Loss: 0.11267100274562836
step: 8900, Loss: 0.11344980448484421
step: 9000, Loss: 0.11413857340812683
step: 9100, Loss: 0.19187334179878235
step: 9200, Loss: 0.11332585662603378
step: 9300, Loss: 0.11375786364078522
step: 9400, Loss: 0.11359182000160217
step: 9500, Loss: 0.11379578709602356
step: 9600, Loss: 0.11379501223564148
step: 9700, Loss: 0.11295626312494278
step: 9800, Loss: 0.11460672318935394
step: 9900, Loss: 0.11525652557611465
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9666666666666667
recall: 0.9666666666666667
F_score: 0.9666666666666667
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.24554020166397095
step: 100, Loss: 0.11699028313159943
step: 200, Loss: 0.11515459418296814
step: 300, Loss: 0.11621484905481339
step: 400, Loss: 0.11363983899354935
step: 500, Loss: 0.11507102102041245
step: 600, Loss: 0.11475539952516556
step: 700, Loss: 0.11557160317897797
step: 800, Loss: 0.11429896205663681
step: 900, Loss: 0.11438266187906265
step: 1000, Loss: 0.11561845242977142
step: 1100, Loss: 0.11384392529726028
step: 1200, Loss: 0.11374883353710175
step: 1300, Loss: 0.11283768713474274
step: 1400, Loss: 0.11335047334432602
step: 1500, Loss: 0.19388684630393982
step: 1600, Loss: 0.11376980692148209
step: 1700, Loss: 0.11448878049850464
step: 1800, Loss: 0.11474725604057312
step: 1900, Loss: 0.11474655568599701
step: 2000, Loss: 0.11495018005371094
step: 2100, Loss: 0.11445395648479462
step: 2200, Loss: 0.11366819590330124
step: 2300, Loss: 0.11371882259845734
step: 2400, Loss: 0.11379217356443405
step: 2500, Loss: 0.11409665644168854
step: 2600, Loss: 0.1144745945930481
step: 2700, Loss: 0.11365929245948792
step: 2800, Loss: 0.11333218961954117
step: 2900, Loss: 0.11367922276258469
step: 3000, Loss: 0.11452724039554596
step: 3100, Loss: 0.1133163645863533
step: 3200, Loss: 0.1172027513384819
step: 3300, Loss: 0.31993159651756287
step: 3400, Loss: 0.21650443971157074
step: 3500, Loss: 0.12664666771888733
step: 3600, Loss: 0.12790706753730774
step: 3700, Loss: 0.12847678363323212
step: 3800, Loss: 0.12316998839378357
step: 3900, Loss: 0.1323251575231552
step: 4000, Loss: 0.12174239009618759
step: 4100, Loss: 0.12051503360271454
step: 4200, Loss: 0.12364952266216278
step: 4300, Loss: 0.11947698146104813
step: 4400, Loss: 0.12032432109117508
step: 4500, Loss: 0.1171436458826065
step: 4600, Loss: 0.11906690895557404
step: 4700, Loss: 0.11700315028429031
step: 4800, Loss: 0.11809812486171722
step: 4900, Loss: 0.11589023470878601
step: 5000, Loss: 0.11733005195856094
step: 5100, Loss: 0.11448989808559418
step: 5200, Loss: 0.11761479824781418
step: 5300, Loss: 0.20108464360237122
step: 5400, Loss: 0.11588728427886963
step: 5500, Loss: 0.12029160559177399
step: 5600, Loss: 0.11717798560857773
step: 5700, Loss: 0.11597394198179245
step: 5800, Loss: 0.11563562601804733
step: 5900, Loss: 0.11454988270998001
step: 6000, Loss: 0.11451698839664459
step: 6100, Loss: 0.11603735387325287
step: 6200, Loss: 0.11764977872371674
step: 6300, Loss: 0.11454227566719055
step: 6400, Loss: 0.11414972692728043
step: 6500, Loss: 0.11668756604194641
step: 6600, Loss: 0.11414090543985367
step: 6700, Loss: 0.11489439010620117
step: 6800, Loss: 0.11383106559515
step: 6900, Loss: 0.11385053396224976
step: 7000, Loss: 0.11448442935943604
step: 7100, Loss: 0.11452260613441467
step: 7200, Loss: 0.19360829889774323
step: 7300, Loss: 0.11378000676631927
step: 7400, Loss: 0.11418313533067703
step: 7500, Loss: 0.11453877389431
step: 7600, Loss: 0.11365006119012833
step: 7700, Loss: 0.11460761725902557
step: 7800, Loss: 0.11289633810520172
step: 7900, Loss: 0.11382857710123062
step: 8000, Loss: 0.11529073864221573
step: 8100, Loss: 0.11339066922664642
step: 8200, Loss: 0.11469380557537079
step: 8300, Loss: 0.11437060683965683
step: 8400, Loss: 0.1132783442735672
step: 8500, Loss: 0.11435935646295547
step: 8600, Loss: 0.11382008343935013
step: 8700, Loss: 0.11357422173023224
step: 8800, Loss: 0.11282878369092941
step: 8900, Loss: 0.1134599894285202
step: 9000, Loss: 0.11417380720376968
step: 9100, Loss: 0.19727823138237
step: 9200, Loss: 0.11350217461585999
step: 9300, Loss: 0.11423798650503159
step: 9400, Loss: 0.11495468765497208
step: 9500, Loss: 0.11216914653778076
step: 9600, Loss: 0.11407721787691116
step: 9700, Loss: 0.11393317580223083
step: 9800, Loss: 0.11518490314483643
step: 9900, Loss: 0.11427564173936844
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9705882352941176
recall: 0.9428571428571428
F_score: 0.9565217391304348
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.2378019392490387
step: 100, Loss: 0.11380283534526825
step: 200, Loss: 0.11825992166996002
step: 300, Loss: 0.11563453823328018
step: 400, Loss: 0.11535227298736572
step: 500, Loss: 0.11390433460474014
step: 600, Loss: 0.11447292566299438
step: 700, Loss: 0.1137925237417221
step: 800, Loss: 0.11440812051296234
step: 900, Loss: 0.11307180672883987
step: 1000, Loss: 0.11348359286785126
step: 1100, Loss: 0.11322416365146637
step: 1200, Loss: 0.11546023190021515
step: 1300, Loss: 0.11437968164682388
step: 1400, Loss: 0.11443193256855011
step: 1500, Loss: 0.19319939613342285
step: 1600, Loss: 0.11318958550691605
step: 1700, Loss: 0.11268378049135208
step: 1800, Loss: 0.11394970118999481
step: 1900, Loss: 0.1159387156367302
step: 2000, Loss: 0.11553013324737549
step: 2100, Loss: 0.11369086056947708
step: 2200, Loss: 0.11446873098611832
step: 2300, Loss: 0.1137816533446312
step: 2400, Loss: 0.11495320498943329
step: 2500, Loss: 0.11473901569843292
step: 2600, Loss: 0.11523638665676117
step: 2700, Loss: 0.11473102122545242
step: 2800, Loss: 0.1152690127491951
step: 2900, Loss: 0.11489534378051758
step: 3000, Loss: 1.809760332107544
step: 3100, Loss: 0.21888019144535065
step: 3200, Loss: 0.12832003831863403
step: 3300, Loss: 0.12803006172180176
step: 3400, Loss: 0.2169010192155838
step: 3500, Loss: 0.12493174523115158
step: 3600, Loss: 0.11818433552980423
step: 3700, Loss: 0.1265772581100464
step: 3800, Loss: 0.123957559466362
step: 3900, Loss: 0.12234004586935043
step: 4000, Loss: 0.1180405393242836
step: 4100, Loss: 0.11920169740915298
step: 4200, Loss: 0.11915387958288193
step: 4300, Loss: 0.1151580959558487
step: 4400, Loss: 0.11747948080301285
step: 4500, Loss: 0.11881694197654724
step: 4600, Loss: 0.11688852310180664
step: 4700, Loss: 0.11777737736701965
step: 4800, Loss: 0.11580590903759003
step: 4900, Loss: 0.11501231044530869
step: 5000, Loss: 0.11569485068321228
step: 5100, Loss: 0.11504857242107391
step: 5200, Loss: 0.11435636132955551
step: 5300, Loss: 0.198783278465271
step: 5400, Loss: 0.11736099421977997
step: 5500, Loss: 0.11661595106124878
step: 5600, Loss: 0.11446557939052582
step: 5700, Loss: 0.11613921821117401
step: 5800, Loss: 0.11542406678199768
step: 5900, Loss: 0.11455545574426651
step: 6000, Loss: 0.11420976370573044
step: 3000, Loss: 0.11510821431875229
step: 3100, Loss: 0.11409834027290344
step: 3200, Loss: 0.11509242653846741
step: 3300, Loss: 0.11496303975582123
step: 3400, Loss: 0.11367017030715942
step: 3500, Loss: 0.11438123881816864
step: 3600, Loss: 0.11768823862075806
step: 3700, Loss: 0.11399341374635696
step: 3800, Loss: 0.11466499418020248
step: 3900, Loss: 0.11412148177623749
step: 4000, Loss: 0.11619393527507782
step: 4100, Loss: 0.1152024120092392
step: 4200, Loss: 0.11650864779949188
step: 4300, Loss: 0.11457464098930359
step: 4400, Loss: 0.11605004966259003
step: 4500, Loss: 0.11669768393039703
step: 4600, Loss: 0.11886116862297058
step: 4700, Loss: 0.11546891927719116
step: 4800, Loss: 0.11373808979988098
step: 4900, Loss: 0.11554726958274841
step: 5000, Loss: 0.11475737392902374
step: 5100, Loss: 0.11426082998514175
step: 5200, Loss: 0.1134653389453888
step: 5300, Loss: 0.11474072188138962
step: 5400, Loss: 0.1169210746884346
step: 5500, Loss: 0.11400400847196579
step: 5600, Loss: 0.11506153643131256
step: 5700, Loss: 0.11408375948667526
step: 5800, Loss: 0.11682409048080444
step: 5900, Loss: 0.11399580538272858
step: 6000, Loss: 0.11452661454677582
step: 6100, Loss: 0.11602184176445007
step: 6200, Loss: 0.11590398848056793
step: 6300, Loss: 0.11589708924293518
step: 6400, Loss: 0.11386312544345856
step: 6500, Loss: 0.11493653059005737
step: 6600, Loss: 0.1142277717590332
step: 6700, Loss: 0.11428509652614594
step: 6800, Loss: 0.11534830182790756
step: 6900, Loss: 0.11548871546983719
step: 7000, Loss: 0.11435244232416153
step: 7100, Loss: 0.11346018314361572
step: 7200, Loss: 0.11363210529088974
step: 7300, Loss: 0.11510422825813293
step: 7400, Loss: 0.11417155712842941
step: 7500, Loss: 0.11588384211063385
step: 7600, Loss: 0.11586257815361023
step: 7700, Loss: 0.20142875611782074
step: 7800, Loss: 0.14990486204624176
step: 7900, Loss: 0.12991271913051605
step: 8000, Loss: 0.14152780175209045
step: 8100, Loss: 0.12704984843730927
step: 8200, Loss: 0.13107672333717346
step: 8300, Loss: 0.12349675595760345
step: 8400, Loss: 0.12570291757583618
step: 8500, Loss: 0.11985379457473755
step: 8600, Loss: 0.1263577938079834
step: 8700, Loss: 0.11681520938873291
step: 8800, Loss: 0.12413117289543152
step: 8900, Loss: 0.12047205865383148
step: 9000, Loss: 0.12280313670635223
step: 9100, Loss: 0.11799341440200806
step: 9200, Loss: 0.1226147934794426
step: 9300, Loss: 0.11618034541606903
step: 9400, Loss: 0.12090028822422028
step: 9500, Loss: 0.11572017520666122
step: 9600, Loss: 0.12020402401685715
step: 9700, Loss: 0.11641446501016617
step: 9800, Loss: 0.11637939512729645
step: 9900, Loss: 0.1170680820941925
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.12218037992715836
step: 100, Loss: 0.12161357700824738
step: 200, Loss: 0.11953531205654144
step: 300, Loss: 0.12010890245437622
step: 400, Loss: 0.11842893809080124
step: 500, Loss: 0.11583418399095535
step: 600, Loss: 0.11589588224887848
step: 700, Loss: 0.11930425465106964
step: 800, Loss: 0.11528021842241287
step: 900, Loss: 0.118559330701828
step: 1000, Loss: 0.11360760778188705
step: 1100, Loss: 0.11481750011444092
step: 1200, Loss: 0.11558021605014801
step: 1300, Loss: 0.11607982218265533
step: 1400, Loss: 0.11424892395734787
step: 1500, Loss: 0.1141195148229599
step: 1600, Loss: 0.11451089382171631
step: 1700, Loss: 0.11549516767263412
step: 1800, Loss: 0.11483289301395416
step: 1900, Loss: 0.11511640250682831
step: 2000, Loss: 0.1144460141658783
step: 2100, Loss: 0.11479592323303223
step: 2200, Loss: 0.11473213881254196
step: 2300, Loss: 0.11874355375766754
step: 2400, Loss: 0.11628320813179016
step: 2500, Loss: 0.11523531377315521
step: 2600, Loss: 0.11832766234874725
step: 2700, Loss: 0.11550942808389664
step: 2800, Loss: 0.11394201964139938
step: 2900, Loss: 0.11702176183462143
step: 3000, Loss: 0.11430080980062485
step: 3100, Loss: 0.11856748163700104
step: 3200, Loss: 0.11507146060466766
step: 3300, Loss: 0.11438459157943726
step: 3400, Loss: 0.1173887699842453
step: 3500, Loss: 0.11486192047595978
step: 3600, Loss: 0.1143898144364357
step: 3700, Loss: 0.11522658914327621
step: 3800, Loss: 0.1138361245393753
step: 3900, Loss: 0.11550229787826538
step: 4000, Loss: 0.11416564881801605
step: 4100, Loss: 0.11580843478441238
step: 4200, Loss: 0.11414000391960144
step: 4300, Loss: 0.11401378363370895
step: 4400, Loss: 0.11426094174385071
step: 4500, Loss: 0.1177406907081604
step: 4600, Loss: 0.11599262803792953
step: 4700, Loss: 0.1151910126209259
step: 4800, Loss: 0.11572730541229248
step: 4900, Loss: 0.11411481350660324
step: 5000, Loss: 0.1145145446062088
step: 5100, Loss: 0.11495604366064072
step: 5200, Loss: 0.115431047976017
step: 5300, Loss: 0.11359211057424545
step: 5400, Loss: 0.11364603787660599
step: 5500, Loss: 0.1142454445362091
step: 5600, Loss: 0.11500665545463562
step: 5700, Loss: 0.11537431180477142
step: 5800, Loss: 0.1147109866142273
step: 5900, Loss: 0.11428132653236389
step: 6000, Loss: 0.11510089784860611
step: 6100, Loss: 0.11523094773292542
step: 6200, Loss: 0.11500097811222076
step: 6300, Loss: 0.11492520570755005
step: 6400, Loss: 0.1161244660615921
step: 6500, Loss: 0.1154811680316925
step: 6600, Loss: 0.11465278267860413
step: 6700, Loss: 0.11608298122882843
step: 6800, Loss: 0.115143321454525
step: 6900, Loss: 0.11639688909053802
step: 7000, Loss: 0.11357200145721436
step: 7100, Loss: 0.11495006829500198
step: 7200, Loss: 0.11403772979974747
step: 7300, Loss: 0.11542972922325134
step: 7400, Loss: 0.11405611038208008
step: 7500, Loss: 0.11693069338798523
step: 7600, Loss: 0.11392915993928909
step: 7700, Loss: 0.11649733781814575
step: 7800, Loss: 0.11455472558736801
step: 7900, Loss: 0.11495346575975418
step: 8000, Loss: 0.1262059211730957
step: 8100, Loss: 0.16220897436141968
step: 8200, Loss: 0.12915140390396118
step: 8300, Loss: 0.12858635187149048
step: 8400, Loss: 0.12409865856170654
step: 8500, Loss: 0.12019269913434982
step: 8600, Loss: 0.11992888897657394
step: 8700, Loss: 0.1216864213347435
step: 8800, Loss: 0.11942684650421143
step: 8900, Loss: 0.11888422071933746
step: 9000, Loss: 0.1181318610906601
step: 9100, Loss: 0.11467290669679642
step: 9200, Loss: 0.11747279763221741
step: 9300, Loss: 0.11929133534431458
step: 9400, Loss: 0.11595038324594498
step: 9500, Loss: 0.11755934357643127
step: 9600, Loss: 0.114494189620018
step: 9700, Loss: 0.11706699430942535
step: 9800, Loss: 0.11501625925302505
step: 9900, Loss: 0.11722801625728607
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.7142857142857143
recall: 0.8333333333333334
F_score: 0.7692307692307692
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.12790700793266296
step: 100, Loss: 0.12748895585536957
step: 200, Loss: 0.11738790571689606
step: 300, Loss: 0.11544369161128998
step: 400, Loss: 0.11900854855775833
step: 500, Loss: 0.11577355116605759
step: 600, Loss: 0.11513949185609818
step: 700, Loss: 0.11586825549602509
step: 800, Loss: 0.11508303135633469
step: 900, Loss: 0.11441695690155029
step: 1000, Loss: 0.1143576130270958
step: 1100, Loss: 0.11522437632083893
step: 1200, Loss: 0.11444909870624542
step: 1300, Loss: 0.11491762101650238
step: 1400, Loss: 0.11745724827051163
step: 1500, Loss: 0.11499279737472534
step: 1600, Loss: 0.1155627891421318
step: 1700, Loss: 0.1150803416967392
step: 1800, Loss: 0.1141413077712059
step: 1900, Loss: 0.11392439901828766
step: 2000, Loss: 0.11537648737430573
step: 2100, Loss: 0.11636794358491898
step: 2200, Loss: 0.11573343724012375
step: 2300, Loss: 0.11413216590881348
step: 2400, Loss: 0.11626829206943512
step: 2500, Loss: 0.11427576839923859
step: 2600, Loss: 0.11395732313394547
step: 2700, Loss: 0.11476383358240128
step: 2800, Loss: 0.11612729728221893
step: 2900, Loss: 0.11429928988218307
step: 3000, Loss: 0.11581288278102875
step: 3100, Loss: 0.11432678997516632
step: 3200, Loss: 0.11421553790569305
step: 3300, Loss: 0.11669465899467468
step: 3400, Loss: 0.11474210023880005
step: 6100, Loss: 0.11398347467184067
step: 6200, Loss: 0.11477702856063843
step: 6300, Loss: 0.11443731188774109
step: 6400, Loss: 0.11509647220373154
step: 6500, Loss: 0.11445514857769012
step: 6600, Loss: 0.11294860392808914
step: 6700, Loss: 0.1135425940155983
step: 6800, Loss: 0.11391989886760712
step: 6900, Loss: 0.11376643180847168
step: 7000, Loss: 0.1147582083940506
step: 7100, Loss: 0.11448493599891663
step: 7200, Loss: 0.1929728090763092
step: 7300, Loss: 0.11366014927625656
step: 7400, Loss: 0.11478322744369507
step: 7500, Loss: 0.1134374588727951
step: 7600, Loss: 0.11271817982196808
step: 7700, Loss: 0.11412297189235687
step: 7800, Loss: 0.11427082121372223
step: 7900, Loss: 0.11441457271575928
step: 8000, Loss: 0.11491682380437851
step: 8100, Loss: 0.1129886656999588
step: 8200, Loss: 0.11385145783424377
step: 8300, Loss: 0.11378379911184311
step: 8400, Loss: 0.11247123777866364
step: 8500, Loss: 0.11264334619045258
step: 8600, Loss: 0.1134805679321289
step: 8700, Loss: 0.11424706876277924
step: 8800, Loss: 0.11383356899023056
step: 8900, Loss: 0.11400295794010162
step: 9000, Loss: 0.1136208176612854
step: 9100, Loss: 0.19130158424377441
step: 9200, Loss: 0.1134648248553276
step: 9300, Loss: 0.11341037601232529
step: 9400, Loss: 0.11462553590536118
step: 9500, Loss: 0.11249727755784988
step: 9600, Loss: 0.11315783858299255
step: 9700, Loss: 0.11469867825508118
step: 9800, Loss: 0.11389132589101791
step: 9900, Loss: 0.11452135443687439
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9705882352941176
recall: 0.9705882352941176
F_score: 0.9705882352941176
subject 15 Avgacc: 0.951388888888889 Avgfscore: 0.9492625852775252 
 Max acc:1.0, Max f score:1.0
******** mix subject_16 ********

[304, 456]
******fold 1******

Training... train_data length:820
step: 0, Loss: 46.517311096191406
step: 100, Loss: 10.00311279296875
step: 200, Loss: 4.232840538024902
step: 300, Loss: 3.175689697265625
step: 400, Loss: 2.002183675765991
step: 500, Loss: 0.5765780210494995
step: 600, Loss: 1.6960861682891846
step: 700, Loss: 0.22017650306224823
step: 800, Loss: 3.9277544021606445
step: 900, Loss: 1.0073291063308716
step: 1000, Loss: 0.4129723906517029
step: 1100, Loss: 0.923164963722229
step: 1200, Loss: 0.24242836236953735
step: 1300, Loss: 0.2992044687271118
step: 1400, Loss: 0.17752112448215485
step: 1500, Loss: 0.1642109602689743
step: 1600, Loss: 0.1994245946407318
step: 1700, Loss: 0.1567402482032776
step: 1800, Loss: 0.1618041694164276
step: 1900, Loss: 0.15757068991661072
step: 2000, Loss: 0.5049479007720947
step: 2100, Loss: 0.15303972363471985
step: 2200, Loss: 0.14480498433113098
step: 2300, Loss: 0.14849111437797546
step: 2400, Loss: 0.14301764965057373
step: 2500, Loss: 0.13285492360591888
step: 2600, Loss: 0.14233441650867462
step: 2700, Loss: 0.15752874314785004
step: 2800, Loss: 0.13754142820835114
step: 2900, Loss: 0.133694127202034
step: 3000, Loss: 0.13180944323539734
step: 3100, Loss: 0.14243808388710022
step: 3200, Loss: 0.12745310366153717
step: 3300, Loss: 0.1304568648338318
step: 3400, Loss: 0.1233726441860199
step: 3500, Loss: 0.12586334347724915
step: 3600, Loss: 0.1297110617160797
step: 3700, Loss: 0.12647223472595215
step: 3800, Loss: 0.12490704655647278
step: 3900, Loss: 0.12724560499191284
step: 4000, Loss: 0.13521024584770203
step: 4100, Loss: 0.12158443033695221
step: 4200, Loss: 0.12214130163192749
step: 4300, Loss: 0.4391688406467438
step: 4400, Loss: 0.13492152094841003
step: 4500, Loss: 0.1224103793501854
step: 4600, Loss: 0.1279456913471222
step: 4700, Loss: 0.12702617049217224
step: 4800, Loss: 0.12157368659973145
step: 4900, Loss: 0.12276960909366608
step: 5000, Loss: 0.142459899187088
step: 5100, Loss: 0.11986730247735977
step: 5200, Loss: 0.12072953581809998
step: 5300, Loss: 0.12317642569541931
step: 5400, Loss: 0.1271107792854309
step: 5500, Loss: 0.11722345650196075
step: 5600, Loss: 0.1253819465637207
step: 5700, Loss: 0.11644994467496872
step: 5800, Loss: 0.1205926239490509
step: 5900, Loss: 0.12783625721931458
step: 6000, Loss: 0.11876626312732697
step: 6100, Loss: 0.18063423037528992
step: 6200, Loss: 1.789379358291626
step: 6300, Loss: 3.44649338722229
step: 6400, Loss: 0.19127897918224335
step: 6500, Loss: 0.1573808193206787
step: 6600, Loss: 0.5160006284713745
step: 6700, Loss: 0.14350706338882446
step: 6800, Loss: 0.14155322313308716
step: 6900, Loss: 0.14452816545963287
step: 7000, Loss: 0.13978075981140137
step: 7100, Loss: 0.13394735753536224
step: 7200, Loss: 0.12813860177993774
step: 7300, Loss: 0.14195884764194489
step: 7400, Loss: 0.1332269012928009
step: 7500, Loss: 0.13406747579574585
step: 7600, Loss: 0.12575620412826538
step: 7700, Loss: 0.13777358829975128
step: 7800, Loss: 0.13144060969352722
step: 7900, Loss: 0.12845364212989807
step: 8000, Loss: 0.12435297667980194
step: 8100, Loss: 0.11973303556442261
step: 8200, Loss: 0.13156834244728088
step: 8300, Loss: 0.12422581017017365
step: 8400, Loss: 0.12345593422651291
step: 8500, Loss: 0.12172439694404602
step: 8600, Loss: 0.12447391450405121
step: 8700, Loss: 0.12703412771224976
step: 8800, Loss: 0.11776624619960785
step: 8900, Loss: 0.43069761991500854
step: 9000, Loss: 0.12722718715667725
step: 9100, Loss: 0.12005437910556793
step: 9200, Loss: 0.11922166496515274
step: 9300, Loss: 0.12338107824325562
step: 9400, Loss: 0.1199839860200882
step: 9500, Loss: 0.1220729649066925
step: 9600, Loss: 0.12670180201530457
step: 9700, Loss: 0.11837213486433029
step: 9800, Loss: 0.1205698549747467
step: 9900, Loss: 0.11903341114521027
training successfully ended.
validating...
validate data length:92
acc: 0.7840909090909091
precision: 0.7719298245614035
recall: 0.88
F_score: 0.822429906542056
******fold 2******

Training... train_data length:820
step: 0, Loss: 0.12777522206306458
step: 100, Loss: 0.17549538612365723
step: 200, Loss: 0.15131749212741852
step: 300, Loss: 0.13977380096912384
step: 400, Loss: 0.13607370853424072
step: 500, Loss: 0.12107691913843155
step: 600, Loss: 0.12265245616436005
step: 700, Loss: 0.131215438246727
step: 800, Loss: 0.12658587098121643
step: 900, Loss: 0.12005841732025146
step: 1000, Loss: 0.12571997940540314
step: 1100, Loss: 0.12111067771911621
step: 1200, Loss: 0.11896710097789764
step: 1300, Loss: 0.1266174614429474
step: 1400, Loss: 0.12053520232439041
step: 1500, Loss: 0.12363114953041077
step: 1600, Loss: 0.11878318339586258
step: 1700, Loss: 0.1209540069103241
step: 1800, Loss: 0.1192791610956192
step: 1900, Loss: 0.11702004820108414
step: 2000, Loss: 0.3976874351501465
step: 2100, Loss: 0.1231064647436142
step: 2200, Loss: 0.11662188172340393
step: 2300, Loss: 0.11878570169210434
step: 2400, Loss: 0.12396225333213806
step: 2500, Loss: 0.11751670390367508
step: 2600, Loss: 0.11837122589349747
step: 2700, Loss: 0.1196267157793045
step: 2800, Loss: 0.11743348836898804
step: 2900, Loss: 0.12037591636180878
step: 3000, Loss: 6.2587890625
step: 3100, Loss: 0.4835461974143982
step: 3200, Loss: 0.14525571465492249
step: 3300, Loss: 0.16436174511909485
step: 3400, Loss: 0.14040273427963257
step: 3500, Loss: 0.13264897465705872
step: 3600, Loss: 0.12652941048145294
step: 3700, Loss: 0.12290450930595398
step: 3800, Loss: 0.13673412799835205
step: 3900, Loss: 0.1297818124294281
step: 4000, Loss: 0.1254139542579651
step: 4100, Loss: 0.12634848058223724
step: 4200, Loss: 0.12653256952762604
step: 4300, Loss: 0.42721131443977356
step: 4400, Loss: 0.13516084849834442
step: 4500, Loss: 0.1225450187921524
step: 4600, Loss: 0.12173262983560562
step: 4700, Loss: 0.12942202389240265
step: 4800, Loss: 0.12361009418964386
step: 4900, Loss: 0.12000773847103119
step: 5000, Loss: 0.12556415796279907
step: 5100, Loss: 0.12169462442398071
step: 5200, Loss: 0.11918874084949493
step: 5300, Loss: 0.13049879670143127
step: 5400, Loss: 0.1318250298500061
step: 5500, Loss: 0.12173673510551453
step: 5600, Loss: 0.12578485906124115
step: 5700, Loss: 0.11926065385341644
step: 5800, Loss: 0.11721132695674896
step: 5900, Loss: 0.1203613430261612
step: 6000, Loss: 0.11823666095733643
step: 6100, Loss: 0.11688036471605301
step: 6200, Loss: 0.11950764060020447
step: 3500, Loss: 0.11427736282348633
step: 3600, Loss: 0.11588947474956512
step: 3700, Loss: 0.11514735966920853
step: 3800, Loss: 0.11597756296396255
step: 3900, Loss: 0.11440318077802658
step: 4000, Loss: 0.11542940139770508
step: 4100, Loss: 0.1147746741771698
step: 4200, Loss: 0.11421718448400497
step: 4300, Loss: 0.11490745097398758
step: 4400, Loss: 0.11374961584806442
step: 4500, Loss: 0.11402493715286255
step: 4600, Loss: 0.11542518436908722
step: 4700, Loss: 0.11487972736358643
step: 4800, Loss: 0.11338584870100021
step: 4900, Loss: 0.116042859852314
step: 5000, Loss: 0.11419558525085449
step: 5100, Loss: 0.11308813095092773
step: 5200, Loss: 0.11373459547758102
step: 5300, Loss: 0.11656200140714645
step: 5400, Loss: 0.11406915634870529
step: 5500, Loss: 0.11579383909702301
step: 5600, Loss: 0.11496791988611221
step: 5700, Loss: 0.11502892524003983
step: 5800, Loss: 0.11434417217969894
step: 5900, Loss: 0.11349280178546906
step: 6000, Loss: 0.11559519916772842
step: 6100, Loss: 0.11517973244190216
step: 6200, Loss: 0.11415701359510422
step: 6300, Loss: 0.11558959633111954
step: 6400, Loss: 0.11619202792644501
step: 6500, Loss: 0.11337317526340485
step: 6600, Loss: 0.115425243973732
step: 6700, Loss: 0.11621309816837311
step: 6800, Loss: 0.11552859842777252
step: 6900, Loss: 0.11431534588336945
step: 7000, Loss: 0.11443142592906952
step: 7100, Loss: 0.11490219831466675
step: 7200, Loss: 0.2082696110010147
step: 7300, Loss: 0.12594331800937653
step: 7400, Loss: 0.1284782588481903
step: 7500, Loss: 0.12318622320890427
step: 7600, Loss: 0.12179675698280334
step: 7700, Loss: 0.11727097630500793
step: 7800, Loss: 0.11673891544342041
step: 7900, Loss: 0.11790865659713745
step: 8000, Loss: 0.1157815083861351
step: 8100, Loss: 0.1196175068616867
step: 8200, Loss: 0.11804057657718658
step: 8300, Loss: 0.1185818538069725
step: 8400, Loss: 0.11578340083360672
step: 8500, Loss: 0.11547300964593887
step: 8600, Loss: 0.11873602122068405
step: 8700, Loss: 0.11656267940998077
step: 8800, Loss: 0.1144956648349762
step: 8900, Loss: 0.11451884359121323
step: 9000, Loss: 0.11575471609830856
step: 9100, Loss: 0.1160566508769989
step: 9200, Loss: 0.11848647892475128
step: 9300, Loss: 0.11643014848232269
step: 9400, Loss: 0.1164165735244751
step: 9500, Loss: 0.11573126912117004
step: 9600, Loss: 0.11433859169483185
step: 9700, Loss: 0.11504079401493073
step: 9800, Loss: 0.11606040596961975
step: 9900, Loss: 0.11498838663101196
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8571428571428571
recall: 1.0
F_score: 0.923076923076923
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.12688511610031128
step: 100, Loss: 0.1168910562992096
step: 200, Loss: 0.11956407129764557
step: 300, Loss: 0.11837056279182434
step: 400, Loss: 0.11632800847291946
step: 500, Loss: 0.11475393176078796
step: 600, Loss: 0.11450032144784927
step: 700, Loss: 0.11547423899173737
step: 800, Loss: 0.11417124420404434
step: 900, Loss: 0.11558695137500763
step: 1000, Loss: 0.11448920518159866
step: 1100, Loss: 0.11440318077802658
step: 1200, Loss: 0.11499356478452682
step: 1300, Loss: 0.1153763085603714
step: 1400, Loss: 0.11562999337911606
step: 1500, Loss: 0.11463628709316254
step: 1600, Loss: 0.11472214758396149
step: 1700, Loss: 0.11584387719631195
step: 1800, Loss: 0.11510120332241058
step: 1900, Loss: 0.11514142155647278
step: 2000, Loss: 0.11463318765163422
step: 2100, Loss: 0.11377077549695969
step: 2200, Loss: 0.1175924688577652
step: 2300, Loss: 0.11354461312294006
step: 2400, Loss: 0.11446496844291687
step: 2500, Loss: 0.11718674004077911
step: 2600, Loss: 0.11571183055639267
step: 2700, Loss: 0.11656840145587921
step: 2800, Loss: 0.11426544189453125
step: 2900, Loss: 0.11427821964025497
step: 3000, Loss: 0.11352460831403732
step: 3100, Loss: 0.11812850832939148
step: 3200, Loss: 0.1154976636171341
step: 3300, Loss: 0.11513804644346237
step: 3400, Loss: 0.11729571223258972
step: 3500, Loss: 0.11542420834302902
step: 3600, Loss: 0.11662732064723969
step: 3700, Loss: 0.11680559813976288
step: 3800, Loss: 0.11369122564792633
step: 3900, Loss: 0.11388362944126129
step: 4000, Loss: 0.11554091423749924
step: 4100, Loss: 0.11472684144973755
step: 4200, Loss: 0.11641955375671387
step: 4300, Loss: 0.11521578580141068
step: 4400, Loss: 0.11472849547863007
step: 4500, Loss: 0.11432701349258423
step: 4600, Loss: 0.11748147010803223
step: 4700, Loss: 0.11390507221221924
step: 4800, Loss: 0.11466431617736816
step: 4900, Loss: 0.1152285486459732
step: 5000, Loss: 0.11409954726696014
step: 5100, Loss: 0.11545126140117645
step: 5200, Loss: 0.11622422933578491
step: 5300, Loss: 0.1138981506228447
step: 5400, Loss: 0.11583630740642548
step: 5500, Loss: 0.11563676595687866
step: 5600, Loss: 0.11535196006298065
step: 5700, Loss: 0.11570030450820923
step: 5800, Loss: 0.1148436963558197
step: 5900, Loss: 0.11439965665340424
step: 6000, Loss: 0.11617960780858994
step: 6100, Loss: 0.11571602523326874
step: 6200, Loss: 0.11652021110057831
step: 6300, Loss: 0.11369132250547409
step: 6400, Loss: 0.11622582376003265
step: 6500, Loss: 0.11431998759508133
step: 6600, Loss: 0.11802342534065247
step: 6700, Loss: 0.11578275263309479
step: 6800, Loss: 1.0759146213531494
step: 6900, Loss: 0.1401681900024414
step: 7000, Loss: 0.13880062103271484
step: 7100, Loss: 0.13394227623939514
step: 7200, Loss: 0.13322876393795013
step: 7300, Loss: 0.12308560311794281
step: 7400, Loss: 0.1267966628074646
step: 7500, Loss: 0.12277494370937347
step: 7600, Loss: 0.1297815442085266
step: 7700, Loss: 0.1209799200296402
step: 7800, Loss: 0.12962278723716736
step: 7900, Loss: 0.11703440546989441
step: 8000, Loss: 0.12283533811569214
step: 8100, Loss: 0.11810609698295593
step: 8200, Loss: 0.12067185342311859
step: 8300, Loss: 0.1192217469215393
step: 8400, Loss: 0.1202772706747055
step: 8500, Loss: 0.11720598489046097
step: 8600, Loss: 0.12175026535987854
step: 8700, Loss: 0.11919453740119934
step: 8800, Loss: 0.12342694401741028
step: 8900, Loss: 0.11692740768194199
step: 9000, Loss: 0.11926045268774033
step: 9100, Loss: 0.11572253704071045
step: 9200, Loss: 0.12082785367965698
step: 9300, Loss: 0.11715464293956757
step: 9400, Loss: 0.11790329217910767
step: 9500, Loss: 0.1154710054397583
step: 9600, Loss: 0.11640924215316772
step: 9700, Loss: 0.1152341440320015
step: 9800, Loss: 0.11712981015443802
step: 9900, Loss: 0.11432443559169769
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7142857142857143
recall: 0.8823529411764706
F_score: 0.7894736842105262
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.1287160962820053
step: 100, Loss: 0.12163835018873215
step: 200, Loss: 0.11515972763299942
step: 300, Loss: 0.11616189032793045
step: 400, Loss: 0.11580255627632141
step: 500, Loss: 0.1160159781575203
step: 600, Loss: 0.11527340859174728
step: 700, Loss: 0.11718511581420898
step: 800, Loss: 0.11421515792608261
step: 900, Loss: 0.11618532240390778
step: 1000, Loss: 0.11557037383317947
step: 1100, Loss: 0.11508817225694656
step: 1200, Loss: 0.11345791816711426
step: 1300, Loss: 0.1149119883775711
step: 1400, Loss: 0.11489368230104446
step: 1500, Loss: 0.11496412754058838
step: 1600, Loss: 0.11556266993284225
step: 1700, Loss: 0.11346320807933807
step: 1800, Loss: 0.11476878076791763
step: 1900, Loss: 0.1153889000415802
step: 2000, Loss: 0.11554375290870667
step: 2100, Loss: 0.11485332995653152
step: 2200, Loss: 0.11455606669187546
step: 2300, Loss: 0.11475148797035217
step: 2400, Loss: 0.11351634562015533
step: 2500, Loss: 0.11506757140159607
step: 2600, Loss: 0.11479905992746353
step: 2700, Loss: 0.11687595397233963
step: 2800, Loss: 0.11518874764442444
step: 2900, Loss: 0.11476042866706848
step: 3000, Loss: 0.11420796811580658
step: 3100, Loss: 0.11399099975824356
step: 3200, Loss: 0.11382625997066498
step: 3300, Loss: 0.11548429727554321
step: 3400, Loss: 0.11460675299167633
step: 3500, Loss: 0.11400291323661804
step: 3600, Loss: 0.11476807296276093
step: 3700, Loss: 0.11443230509757996
step: 3800, Loss: 0.11385456472635269
step: 3900, Loss: 0.11339810490608215
step: 6300, Loss: 0.11639942973852158
step: 6400, Loss: 0.11781682819128036
step: 6500, Loss: 0.11935023963451385
step: 6600, Loss: 0.4280158281326294
step: 6700, Loss: 0.12172529101371765
step: 6800, Loss: 0.11811479926109314
step: 6900, Loss: 0.11755315214395523
step: 7000, Loss: 0.11716412752866745
step: 7100, Loss: 0.11697792261838913
step: 7200, Loss: 0.11530749499797821
step: 7300, Loss: 0.12415867298841476
step: 7400, Loss: 0.11919987946748734
step: 7500, Loss: 0.11548789590597153
step: 7600, Loss: 0.11777733266353607
step: 7700, Loss: 0.11691582202911377
step: 7800, Loss: 0.11963599175214767
step: 7900, Loss: 0.11562690138816833
step: 8000, Loss: 8.143316268920898
step: 8100, Loss: 0.9326916933059692
step: 8200, Loss: 0.18548837304115295
step: 8300, Loss: 0.1372860223054886
step: 8400, Loss: 0.1348588615655899
step: 8500, Loss: 0.13064044713974
step: 8600, Loss: 0.12920571863651276
step: 8700, Loss: 0.13050810992717743
step: 8800, Loss: 0.13739308714866638
step: 8900, Loss: 0.42787161469459534
step: 9000, Loss: 0.14116451144218445
step: 9100, Loss: 0.12731210887432098
step: 9200, Loss: 0.13454991579055786
step: 9300, Loss: 0.1387902796268463
step: 9400, Loss: 0.12609760463237762
step: 9500, Loss: 0.12069868296384811
step: 9600, Loss: 0.1318885087966919
step: 9700, Loss: 0.1239398643374443
step: 9800, Loss: 0.12099005281925201
step: 9900, Loss: 0.1216927319765091
training successfully ended.
validating...
validate data length:92
acc: 0.8863636363636364
precision: 0.7954545454545454
recall: 0.9722222222222222
F_score: 0.875
******fold 3******

Training... train_data length:821
step: 0, Loss: 0.23400866985321045
step: 100, Loss: 0.14796753227710724
step: 200, Loss: 0.1329665631055832
step: 300, Loss: 0.12875792384147644
step: 400, Loss: 0.13132014870643616
step: 500, Loss: 0.13059969246387482
step: 600, Loss: 0.1282138228416443
step: 700, Loss: 0.1212259978055954
step: 800, Loss: 0.12109692394733429
step: 900, Loss: 0.12039685249328613
step: 1000, Loss: 0.11562565714120865
step: 1100, Loss: 0.11753182858228683
step: 1200, Loss: 1.7500861883163452
step: 1300, Loss: 0.18535955250263214
step: 1400, Loss: 0.13387273252010345
step: 1500, Loss: 0.14069198071956635
step: 1600, Loss: 0.14475993812084198
step: 1700, Loss: 0.1256672590970993
step: 1800, Loss: 0.12489132583141327
step: 1900, Loss: 0.13509371876716614
step: 2000, Loss: 0.36562466621398926
step: 2100, Loss: 0.13311247527599335
step: 2200, Loss: 0.13101999461650848
step: 2300, Loss: 0.11971841752529144
step: 2400, Loss: 0.12748244404792786
step: 2500, Loss: 0.1223101019859314
step: 2600, Loss: 0.12508748471736908
step: 2700, Loss: 0.1203451156616211
step: 2800, Loss: 0.12276308238506317
step: 2900, Loss: 0.11826777458190918
step: 3000, Loss: 0.12234393507242203
step: 3100, Loss: 0.12068580090999603
step: 3200, Loss: 0.12056153267621994
step: 3300, Loss: 0.11809183657169342
step: 3400, Loss: 0.11884018778800964
step: 3500, Loss: 0.1178000196814537
step: 3600, Loss: 0.12279224395751953
step: 3700, Loss: 0.11504621803760529
step: 3800, Loss: 0.11727690696716309
step: 3900, Loss: 0.11707164347171783
step: 4000, Loss: 0.11674553900957108
step: 4100, Loss: 0.11707966029644012
step: 4200, Loss: 0.12295369803905487
step: 4300, Loss: 0.36122870445251465
step: 4400, Loss: 0.11750651895999908
step: 4500, Loss: 0.11795298010110855
step: 4600, Loss: 0.11735717207193375
step: 4700, Loss: 0.11836168169975281
step: 4800, Loss: 0.1159520149230957
step: 4900, Loss: 0.11796218901872635
step: 5000, Loss: 0.11737608164548874
step: 5100, Loss: 0.11437001824378967
step: 5200, Loss: 0.11687655001878738
step: 5300, Loss: 0.11611349880695343
step: 5400, Loss: 0.11618194729089737
step: 5500, Loss: 0.11822211742401123
step: 5600, Loss: 0.11728320270776749
step: 5700, Loss: 3.3384346961975098
step: 5800, Loss: 0.32403290271759033
step: 5900, Loss: 0.1314566433429718
step: 6000, Loss: 0.13462723791599274
step: 6100, Loss: 0.14132572710514069
step: 6200, Loss: 0.13092060387134552
step: 6300, Loss: 0.12466195970773697
step: 6400, Loss: 0.12310589849948883
step: 6500, Loss: 0.13466784358024597
step: 6600, Loss: 0.37213143706321716
step: 6700, Loss: 0.13248327374458313
step: 6800, Loss: 0.12257001549005508
step: 6900, Loss: 0.12937235832214355
step: 7000, Loss: 0.12619277834892273
step: 7100, Loss: 0.12184358388185501
step: 7200, Loss: 0.1227327510714531
step: 7300, Loss: 0.12204836308956146
step: 7400, Loss: 0.123882956802845
step: 7500, Loss: 0.12000430375337601
step: 7600, Loss: 0.12097043544054031
step: 7700, Loss: 0.12186454981565475
step: 7800, Loss: 0.12246054410934448
step: 7900, Loss: 0.11863736808300018
step: 8000, Loss: 0.11878949403762817
step: 8100, Loss: 0.11984317004680634
step: 8200, Loss: 0.11633148789405823
step: 8300, Loss: 0.11677196621894836
step: 8400, Loss: 0.11940062791109085
step: 8500, Loss: 0.11928229033946991
step: 8600, Loss: 0.11506254971027374
step: 8700, Loss: 0.11764319241046906
step: 8800, Loss: 0.11575111746788025
step: 8900, Loss: 0.3681678771972656
step: 9000, Loss: 0.12043899297714233
step: 9100, Loss: 0.11654189229011536
step: 9200, Loss: 0.11806866526603699
step: 9300, Loss: 0.11752701550722122
step: 9400, Loss: 0.116982601583004
step: 9500, Loss: 0.11592420190572739
step: 9600, Loss: 0.11748476326465607
step: 9700, Loss: 0.11402511596679688
step: 9800, Loss: 0.11394049227237701
step: 9900, Loss: 0.11471213400363922
training successfully ended.
validating...
validate data length:91
acc: 0.9886363636363636
precision: 0.9782608695652174
recall: 1.0
F_score: 0.989010989010989
******fold 4******

Training... train_data length:821
step: 0, Loss: 0.11677531152963638
step: 100, Loss: 0.12835143506526947
step: 200, Loss: 0.12093881517648697
step: 300, Loss: 0.118794746696949
step: 400, Loss: 0.1163521334528923
step: 500, Loss: 0.11562924087047577
step: 600, Loss: 0.11679515242576599
step: 700, Loss: 0.11537060141563416
step: 800, Loss: 0.11885994672775269
step: 900, Loss: 0.11585558950901031
step: 1000, Loss: 0.11642833799123764
step: 1100, Loss: 0.11691893637180328
step: 1200, Loss: 0.11433082073926926
step: 1300, Loss: 0.11594828963279724
step: 1400, Loss: 0.11600403487682343
step: 1500, Loss: 0.11689665913581848
step: 1600, Loss: 0.11633498221635818
step: 1700, Loss: 0.11662423610687256
step: 1800, Loss: 0.11523197591304779
step: 1900, Loss: 0.11555317044258118
step: 2000, Loss: 0.38667091727256775
step: 2100, Loss: 0.11625945568084717
step: 2200, Loss: 0.11514493077993393
step: 2300, Loss: 0.11526265740394592
step: 2400, Loss: 0.11957848817110062
step: 2500, Loss: 0.11523032933473587
step: 2600, Loss: 0.11628672480583191
step: 2700, Loss: 0.11970783770084381
step: 2800, Loss: 0.11397050321102142
step: 2900, Loss: 0.11525478959083557
step: 3000, Loss: 0.11833158135414124
step: 3100, Loss: 0.11504055559635162
step: 3200, Loss: 0.12231944501399994
step: 3300, Loss: 0.12158358842134476
step: 3400, Loss: 0.4074549973011017
step: 3500, Loss: 0.14416134357452393
step: 3600, Loss: 0.1431432068347931
step: 3700, Loss: 0.13397283852100372
step: 3800, Loss: 0.1307486593723297
step: 3900, Loss: 0.12456914782524109
step: 4000, Loss: 0.12845967710018158
step: 4100, Loss: 0.11918105185031891
step: 4200, Loss: 0.1286395639181137
step: 4300, Loss: 0.35824552178382874
step: 4400, Loss: 0.12487402558326721
step: 4500, Loss: 0.12261888384819031
step: 4600, Loss: 0.12278001010417938
step: 4700, Loss: 0.12186872959136963
step: 4800, Loss: 0.11933984607458115
step: 4900, Loss: 0.12287773936986923
step: 5000, Loss: 0.12223448604345322
step: 5100, Loss: 0.12410721182823181
step: 5200, Loss: 0.1267038881778717
step: 5300, Loss: 0.12417355179786682
step: 5400, Loss: 0.11709993332624435
step: 5500, Loss: 0.12088728696107864
step: 5600, Loss: 0.11723487824201584
step: 5700, Loss: 0.11850368231534958
step: 5800, Loss: 0.11691571772098541
step: 5900, Loss: 0.11733074486255646
step: 6000, Loss: 0.11621500551700592
step: 6100, Loss: 0.11915503442287445
step: 6200, Loss: 0.1171083152294159
step: 6300, Loss: 0.11435961723327637
step: 6400, Loss: 0.1159188449382782
step: 6500, Loss: 0.11686636507511139
step: 6600, Loss: 0.3591582477092743
step: 6700, Loss: 0.11734813451766968
step: 4000, Loss: 0.11482786387205124
step: 4100, Loss: 0.11595311760902405
step: 4200, Loss: 0.11404188722372055
step: 4300, Loss: 0.11426295340061188
step: 4400, Loss: 0.11494862288236618
step: 4500, Loss: 0.11541379988193512
step: 4600, Loss: 0.11354683339595795
step: 4700, Loss: 0.11599704623222351
step: 4800, Loss: 0.11458149552345276
step: 4900, Loss: 0.1140124723315239
step: 5000, Loss: 0.11562664061784744
step: 5100, Loss: 0.11491845548152924
step: 5200, Loss: 0.11397245526313782
step: 5300, Loss: 0.11663125455379486
step: 5400, Loss: 0.11580191552639008
step: 5500, Loss: 0.11660241335630417
step: 5600, Loss: 0.11683729290962219
step: 5700, Loss: 0.11423217505216599
step: 5800, Loss: 0.1149926632642746
step: 5900, Loss: 0.11401666700839996
step: 6000, Loss: 0.11733803898096085
step: 6100, Loss: 0.11399741470813751
step: 6200, Loss: 0.11550264060497284
step: 6300, Loss: 0.11443141102790833
step: 6400, Loss: 0.11474992334842682
step: 6500, Loss: 0.11718252301216125
step: 6600, Loss: 0.11531028151512146
step: 6700, Loss: 0.11566150188446045
step: 6800, Loss: 0.11416980624198914
step: 6900, Loss: 0.11413776874542236
step: 7000, Loss: 0.11343549191951752
step: 7100, Loss: 0.1158641055226326
step: 7200, Loss: 0.11364404857158661
step: 7300, Loss: 0.11407396197319031
step: 7400, Loss: 0.11707387864589691
step: 7500, Loss: 0.35412660241127014
step: 7600, Loss: 0.13526156544685364
step: 7700, Loss: 0.12508898973464966
step: 7800, Loss: 0.12526360154151917
step: 7900, Loss: 0.12999805808067322
step: 8000, Loss: 0.12250478565692902
step: 8100, Loss: 0.1245402842760086
step: 8200, Loss: 0.1227390393614769
step: 8300, Loss: 0.11938376724720001
step: 8400, Loss: 0.12461232393980026
step: 8500, Loss: 0.11921828985214233
step: 8600, Loss: 0.11944059282541275
step: 8700, Loss: 0.11745282262563705
step: 8800, Loss: 0.11897697299718857
step: 8900, Loss: 0.11591731011867523
step: 9000, Loss: 0.11815918982028961
step: 9100, Loss: 0.11613431572914124
step: 9200, Loss: 0.11873465776443481
step: 9300, Loss: 0.11560755223035812
step: 9400, Loss: 0.11859005689620972
step: 9500, Loss: 0.11882823705673218
step: 9600, Loss: 0.11478219926357269
step: 9700, Loss: 0.11517059057950974
step: 9800, Loss: 0.11440388113260269
step: 9900, Loss: 0.11543245613574982
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8333333333333334
recall: 1.0
F_score: 0.9090909090909091
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.13405190408229828
step: 100, Loss: 0.11956512928009033
step: 200, Loss: 0.11676071584224701
step: 300, Loss: 0.11685620993375778
step: 400, Loss: 0.11658114194869995
step: 500, Loss: 0.11686140298843384
step: 600, Loss: 0.11462436616420746
step: 700, Loss: 0.11767089366912842
step: 800, Loss: 0.11508253961801529
step: 900, Loss: 0.11407434940338135
step: 1000, Loss: 0.11419730633497238
step: 1100, Loss: 0.11556293070316315
step: 1200, Loss: 0.1140279695391655
step: 1300, Loss: 0.11715736985206604
step: 1400, Loss: 0.11382707953453064
step: 1500, Loss: 0.11483676731586456
step: 1600, Loss: 0.11386249959468842
step: 1700, Loss: 0.11439502239227295
step: 1800, Loss: 0.11561987549066544
step: 1900, Loss: 0.11518549919128418
step: 2000, Loss: 0.11441013216972351
step: 2100, Loss: 0.11590823531150818
step: 2200, Loss: 0.113925039768219
step: 2300, Loss: 0.11377763748168945
step: 2400, Loss: 0.11459576338529587
step: 2500, Loss: 0.11751192808151245
step: 2600, Loss: 0.11404521018266678
step: 2700, Loss: 0.11430525779724121
step: 2800, Loss: 0.11472449451684952
step: 2900, Loss: 0.11765044927597046
step: 3000, Loss: 0.1148357167840004
step: 3100, Loss: 0.11443524062633514
step: 3200, Loss: 0.11647118628025055
step: 3300, Loss: 0.11364594101905823
step: 3400, Loss: 0.11501188576221466
step: 3500, Loss: 0.11507922410964966
step: 3600, Loss: 0.11461777985095978
step: 3700, Loss: 0.11506429314613342
step: 3800, Loss: 0.11588726937770844
step: 3900, Loss: 0.11583787202835083
step: 4000, Loss: 0.11444909125566483
step: 4100, Loss: 0.1141253337264061
step: 4200, Loss: 0.11453680694103241
step: 4300, Loss: 0.11456771194934845
step: 4400, Loss: 0.11388152092695236
step: 4500, Loss: 0.1146211177110672
step: 4600, Loss: 0.1133790984749794
step: 4700, Loss: 0.11544514447450638
step: 4800, Loss: 0.11519227921962738
step: 4900, Loss: 0.1146218553185463
step: 5000, Loss: 0.11591321229934692
step: 5100, Loss: 0.11699318140745163
step: 5200, Loss: 0.11398106813430786
step: 5300, Loss: 0.11413808166980743
step: 5400, Loss: 0.11489011347293854
step: 5500, Loss: 0.11464478820562363
step: 5600, Loss: 0.11412251740694046
step: 5700, Loss: 0.11429323256015778
step: 5800, Loss: 0.11367645859718323
step: 5900, Loss: 0.12763699889183044
step: 6000, Loss: 0.11460971087217331
step: 6100, Loss: 0.11798225343227386
step: 6200, Loss: 0.11459234356880188
step: 6300, Loss: 0.1142510399222374
step: 6400, Loss: 0.11480816453695297
step: 6500, Loss: 0.11406896263360977
step: 6600, Loss: 0.11732317507266998
step: 6700, Loss: 0.5904791951179504
step: 6800, Loss: 0.13839828968048096
step: 6900, Loss: 0.12506824731826782
step: 7000, Loss: 0.12465441226959229
step: 7100, Loss: 0.1212344616651535
step: 7200, Loss: 0.12492120265960693
step: 7300, Loss: 0.11860537528991699
step: 7400, Loss: 0.12407341599464417
step: 7500, Loss: 0.11837507784366608
step: 7600, Loss: 0.12119509279727936
step: 7700, Loss: 0.11756666749715805
step: 7800, Loss: 0.11746615916490555
step: 7900, Loss: 0.1183469146490097
step: 8000, Loss: 0.12005098164081573
step: 8100, Loss: 0.11492863297462463
step: 8200, Loss: 0.11650294810533524
step: 8300, Loss: 0.11587297171354294
step: 8400, Loss: 0.1174747496843338
step: 8500, Loss: 0.1159188374876976
step: 8600, Loss: 0.11562877148389816
step: 8700, Loss: 0.11589688807725906
step: 8800, Loss: 0.11491141468286514
step: 8900, Loss: 0.11704735457897186
step: 9000, Loss: 0.11557865887880325
step: 9100, Loss: 0.11660760641098022
step: 9200, Loss: 0.11508888006210327
step: 9300, Loss: 0.11407096683979034
step: 9400, Loss: 0.11476092040538788
step: 9500, Loss: 0.11658851057291031
step: 9600, Loss: 0.11580996960401535
step: 9700, Loss: 0.11489212512969971
step: 9800, Loss: 0.1162458211183548
step: 9900, Loss: 0.11599177122116089
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9444444444444444
recall: 1.0
F_score: 0.9714285714285714
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.3955800533294678
step: 100, Loss: 0.12064310908317566
step: 200, Loss: 0.11471699178218842
step: 300, Loss: 0.1168903335928917
step: 400, Loss: 0.11438972502946854
step: 500, Loss: 0.11668269336223602
step: 600, Loss: 0.1148669496178627
step: 700, Loss: 0.11787526309490204
step: 800, Loss: 0.11410467326641083
step: 900, Loss: 0.11490535736083984
step: 1000, Loss: 0.11511635780334473
step: 1100, Loss: 0.11661955714225769
step: 1200, Loss: 0.1140521913766861
step: 1300, Loss: 0.11680164933204651
step: 1400, Loss: 0.11507810652256012
step: 1500, Loss: 0.11436492204666138
step: 1600, Loss: 0.11368642002344131
step: 1700, Loss: 0.1142643615603447
step: 1800, Loss: 0.11458954960107803
step: 1900, Loss: 0.11803852021694183
step: 2000, Loss: 0.11399379372596741
step: 2100, Loss: 0.1147175282239914
step: 2200, Loss: 0.11357071250677109
step: 2300, Loss: 0.12005040049552917
step: 2400, Loss: 0.11474895477294922
step: 2500, Loss: 0.11384551972150803
step: 2600, Loss: 0.11517749726772308
step: 2700, Loss: 0.11384180933237076
step: 2800, Loss: 0.11469624936580658
step: 2900, Loss: 0.11589174717664719
step: 3000, Loss: 0.11508999764919281
step: 3100, Loss: 0.11431500315666199
step: 3200, Loss: 0.11555293202400208
step: 3300, Loss: 0.11455665528774261
step: 3400, Loss: 0.11584344506263733
step: 3500, Loss: 0.1151767373085022
step: 3600, Loss: 0.11749473959207535
step: 3700, Loss: 0.1164192259311676
step: 3800, Loss: 0.11460235714912415
step: 3900, Loss: 0.1145210713148117
step: 4000, Loss: 0.11538441479206085
step: 4100, Loss: 0.11537286639213562
step: 4200, Loss: 0.11305735260248184
step: 4300, Loss: 0.1156906932592392
step: 4400, Loss: 0.11428502947092056
step: 4500, Loss: 0.11477798968553543
step: 6800, Loss: 0.11600694060325623
step: 6900, Loss: 0.11537130922079086
step: 7000, Loss: 0.11500126123428345
step: 7100, Loss: 0.11581113934516907
step: 7200, Loss: 0.11426588147878647
step: 7300, Loss: 0.11377306282520294
step: 7400, Loss: 0.1140226274728775
step: 7500, Loss: 0.11475265771150589
step: 7600, Loss: 0.1143532320857048
step: 7700, Loss: 0.11428852379322052
step: 7800, Loss: 0.11542313545942307
step: 7900, Loss: 0.1143312156200409
step: 8000, Loss: 0.11444428563117981
step: 8100, Loss: 0.11848365515470505
step: 8200, Loss: 0.11472950130701065
step: 8300, Loss: 0.11476276814937592
step: 8400, Loss: 0.11506202071905136
step: 8500, Loss: 0.11449502408504486
step: 8600, Loss: 0.11442331969738007
step: 8700, Loss: 0.11518523842096329
step: 8800, Loss: 0.11690959334373474
step: 8900, Loss: 0.3767150640487671
step: 9000, Loss: 0.11974700540304184
step: 9100, Loss: 0.118177130818367
step: 9200, Loss: 0.11943424493074417
step: 9300, Loss: 3.3193631172180176
step: 9400, Loss: 1.07672917842865
step: 9500, Loss: 0.13017094135284424
step: 9600, Loss: 0.14290758967399597
step: 9700, Loss: 0.13548411428928375
step: 9800, Loss: 0.12514519691467285
step: 9900, Loss: 0.13164548575878143
training successfully ended.
validating...
validate data length:91
acc: 0.9772727272727273
precision: 0.9615384615384616
recall: 1.0
F_score: 0.9803921568627451
******fold 5******

Training... train_data length:821
step: 0, Loss: 0.11705983430147171
step: 100, Loss: 0.1293119490146637
step: 200, Loss: 0.12209485471248627
step: 300, Loss: 0.12182112038135529
step: 400, Loss: 0.1176111251115799
step: 500, Loss: 0.11475255340337753
step: 600, Loss: 0.11549633741378784
step: 700, Loss: 0.11582526564598083
step: 800, Loss: 0.116780586540699
step: 900, Loss: 0.11458363384008408
step: 1000, Loss: 0.11676091700792313
step: 1100, Loss: 0.11900864541530609
step: 1200, Loss: 0.11413653194904327
step: 1300, Loss: 0.11487233638763428
step: 1400, Loss: 0.11546355485916138
step: 1500, Loss: 0.11615976691246033
step: 1600, Loss: 0.11503191292285919
step: 1700, Loss: 0.11467087268829346
step: 1800, Loss: 0.11671952903270721
step: 1900, Loss: 0.12603212893009186
step: 2000, Loss: 0.3686402440071106
step: 2100, Loss: 0.11881984770298004
step: 2200, Loss: 0.11615470051765442
step: 2300, Loss: 0.11466576159000397
step: 2400, Loss: 0.1186574399471283
step: 2500, Loss: 0.11663409322500229
step: 2600, Loss: 0.1141093373298645
step: 2700, Loss: 0.11903353035449982
step: 2800, Loss: 0.11482314020395279
step: 2900, Loss: 0.11442176252603531
step: 3000, Loss: 0.12795177102088928
step: 3100, Loss: 0.8960868120193481
step: 3200, Loss: 0.16255013644695282
step: 3300, Loss: 0.13540750741958618
step: 3400, Loss: 0.13111312687397003
step: 3500, Loss: 0.13637501001358032
step: 3600, Loss: 0.12889464199543
step: 3700, Loss: 0.12229303270578384
step: 3800, Loss: 0.12612077593803406
step: 3900, Loss: 0.12227468937635422
step: 4000, Loss: 0.12159743905067444
step: 4100, Loss: 0.12144392728805542
step: 4200, Loss: 0.12335518002510071
step: 4300, Loss: 0.3744819164276123
step: 4400, Loss: 0.12536606192588806
step: 4500, Loss: 0.11777499318122864
step: 4600, Loss: 0.11914581805467606
step: 4700, Loss: 0.1251949667930603
step: 4800, Loss: 0.11922137439250946
step: 4900, Loss: 0.12278817594051361
step: 5000, Loss: 0.12147511541843414
step: 5100, Loss: 0.11654108762741089
step: 5200, Loss: 0.12166532874107361
step: 5300, Loss: 0.11581991612911224
step: 5400, Loss: 0.11828263849020004
step: 5500, Loss: 0.11669784039258957
step: 5600, Loss: 0.11719271540641785
step: 5700, Loss: 0.11621998250484467
step: 5800, Loss: 0.11473061144351959
step: 5900, Loss: 0.1173543855547905
step: 6000, Loss: 0.11584228277206421
step: 6100, Loss: 0.11529003083705902
step: 6200, Loss: 0.11496542394161224
step: 6300, Loss: 0.11445608735084534
step: 6400, Loss: 0.11384125053882599
step: 6500, Loss: 0.11716928333044052
step: 6600, Loss: 0.3668542504310608
step: 6700, Loss: 0.11597798764705658
step: 6800, Loss: 0.1145901083946228
step: 6900, Loss: 0.11408048868179321
step: 7000, Loss: 0.11642387509346008
step: 7100, Loss: 0.1140352413058281
step: 7200, Loss: 0.11499139666557312
step: 7300, Loss: 0.11833983659744263
step: 7400, Loss: 0.11390925943851471
step: 7500, Loss: 0.11419562995433807
step: 7600, Loss: 0.11428873240947723
step: 7700, Loss: 0.1153641939163208
step: 7800, Loss: 0.11399004608392715
step: 7900, Loss: 0.11396666616201401
step: 8000, Loss: 0.11416524648666382
step: 8100, Loss: 0.11430203914642334
step: 8200, Loss: 0.11835834383964539
step: 8300, Loss: 0.11563341319561005
step: 8400, Loss: 0.11758889257907867
step: 8500, Loss: 0.11563929170370102
step: 8600, Loss: 0.11461587995290756
step: 8700, Loss: 0.11637880653142929
step: 8800, Loss: 0.11576063185930252
step: 8900, Loss: 0.40577369928359985
step: 9000, Loss: 4.497588157653809
step: 9100, Loss: 0.161995530128479
step: 9200, Loss: 0.15856437385082245
step: 9300, Loss: 0.14211949706077576
step: 9400, Loss: 0.12671175599098206
step: 9500, Loss: 0.12772846221923828
step: 9600, Loss: 0.1430198848247528
step: 9700, Loss: 0.12032435834407806
step: 9800, Loss: 0.12505455315113068
step: 9900, Loss: 0.12619444727897644
training successfully ended.
validating...
validate data length:91
acc: 0.9431818181818182
precision: 0.9302325581395349
recall: 0.9523809523809523
F_score: 0.9411764705882352
******fold 6******

Training... train_data length:821
step: 0, Loss: 0.11673898994922638
step: 100, Loss: 0.1496773064136505
step: 200, Loss: 0.14552371203899384
step: 300, Loss: 0.12124338001012802
step: 400, Loss: 0.11727046966552734
step: 500, Loss: 0.11849429458379745
step: 600, Loss: 0.11660844832658768
step: 700, Loss: 0.1240808516740799
step: 800, Loss: 0.11725704371929169
step: 900, Loss: 0.11634724587202072
step: 1000, Loss: 0.11744000017642975
step: 1100, Loss: 0.11713121086359024
step: 1200, Loss: 0.11629629135131836
step: 1300, Loss: 0.11622792482376099
step: 1400, Loss: 0.1148570328950882
step: 1500, Loss: 0.11700238287448883
step: 1600, Loss: 0.11609601229429245
step: 1700, Loss: 0.11527732014656067
step: 1800, Loss: 0.12013977020978928
step: 1900, Loss: 0.11633507907390594
step: 2000, Loss: 0.3648131191730499
step: 2100, Loss: 0.12442293763160706
step: 2200, Loss: 0.11536972969770432
step: 2300, Loss: 0.11398621648550034
step: 2400, Loss: 0.11764258146286011
step: 2500, Loss: 0.11523716151714325
step: 2600, Loss: 0.11794432252645493
step: 2700, Loss: 0.11500278115272522
step: 2800, Loss: 0.11534559726715088
step: 2900, Loss: 0.11499223858118057
step: 3000, Loss: 0.11604912579059601
step: 3100, Loss: 0.11684749275445938
step: 3200, Loss: 0.11485257744789124
step: 3300, Loss: 0.11823035776615143
step: 3400, Loss: 0.11400893330574036
step: 3500, Loss: 0.11944475769996643
step: 3600, Loss: 0.11966752260923386
step: 3700, Loss: 1.6848958730697632
step: 3800, Loss: 0.27252936363220215
step: 3900, Loss: 0.14534571766853333
step: 4000, Loss: 0.13128936290740967
step: 4100, Loss: 0.13604094088077545
step: 4200, Loss: 0.14147831499576569
step: 4300, Loss: 0.4004083573818207
step: 4400, Loss: 0.1294114738702774
step: 4500, Loss: 0.12095526605844498
step: 4600, Loss: 0.12823505699634552
step: 4700, Loss: 0.13504137098789215
step: 4800, Loss: 0.1245274618268013
step: 4900, Loss: 0.11724519729614258
step: 5000, Loss: 0.12134785205125809
step: 5100, Loss: 0.12299104034900665
step: 5200, Loss: 0.12056287378072739
step: 5300, Loss: 0.12014911323785782
step: 5400, Loss: 0.12084706872701645
step: 5500, Loss: 0.12096913903951645
step: 5600, Loss: 0.1189175471663475
step: 5700, Loss: 0.11990539729595184
step: 5800, Loss: 0.11814837902784348
step: 5900, Loss: 0.11524917185306549
step: 6000, Loss: 0.11676998436450958
step: 6100, Loss: 0.11973796039819717
step: 6200, Loss: 0.11872292309999466
step: 6300, Loss: 0.11479602754116058
step: 6400, Loss: 0.11912630498409271
step: 6500, Loss: 0.11712861061096191
step: 6600, Loss: 0.35580679774284363
step: 6700, Loss: 0.11971776932477951
step: 6800, Loss: 0.11571168899536133
step: 6900, Loss: 0.11677654832601547
step: 7000, Loss: 0.1158672645688057
step: 7100, Loss: 0.11391769349575043
step: 7200, Loss: 0.11313757300376892
step: 4600, Loss: 0.11825001239776611
step: 4700, Loss: 0.1153542548418045
step: 4800, Loss: 0.11470700055360794
step: 4900, Loss: 0.11367854475975037
step: 5000, Loss: 0.11444270610809326
step: 5100, Loss: 0.1141929179430008
step: 5200, Loss: 0.1152745708823204
step: 5300, Loss: 0.1149921864271164
step: 5400, Loss: 0.11501669883728027
step: 5500, Loss: 0.11442175507545471
step: 5600, Loss: 0.11448895931243896
step: 5700, Loss: 0.11460454016923904
step: 5800, Loss: 0.11562492698431015
step: 5900, Loss: 0.11386062949895859
step: 6000, Loss: 0.11558994650840759
step: 6100, Loss: 0.11467636376619339
step: 6200, Loss: 0.11538573354482651
step: 6300, Loss: 0.11418778449296951
step: 6400, Loss: 0.11570873856544495
step: 6500, Loss: 0.11713381856679916
step: 6600, Loss: 0.11512532085180283
step: 6700, Loss: 0.11618876457214355
step: 6800, Loss: 0.11768947541713715
step: 6900, Loss: 0.11404053866863251
step: 7000, Loss: 0.1735793650150299
step: 7100, Loss: 0.13501864671707153
step: 7200, Loss: 0.12554390728473663
step: 7300, Loss: 0.1265314221382141
step: 7400, Loss: 0.1233755350112915
step: 7500, Loss: 0.12224932014942169
step: 7600, Loss: 0.12168779969215393
step: 7700, Loss: 0.11919847130775452
step: 7800, Loss: 0.12104202806949615
step: 7900, Loss: 0.1168140321969986
step: 8000, Loss: 0.1201145350933075
step: 8100, Loss: 0.1191241443157196
step: 8200, Loss: 0.11679690331220627
step: 8300, Loss: 0.11819619685411453
step: 8400, Loss: 0.11536858975887299
step: 8500, Loss: 0.11703264713287354
step: 8600, Loss: 0.11639276146888733
step: 8700, Loss: 0.1171119213104248
step: 8800, Loss: 0.1178799569606781
step: 8900, Loss: 0.11455285549163818
step: 9000, Loss: 0.11642856895923615
step: 9100, Loss: 0.11701267957687378
step: 9200, Loss: 0.11513727903366089
step: 9300, Loss: 0.11676517874002457
step: 9400, Loss: 0.11470291763544083
step: 9500, Loss: 0.11585656553506851
step: 9600, Loss: 0.1152212843298912
step: 9700, Loss: 0.11639884114265442
step: 9800, Loss: 0.11663636565208435
step: 9900, Loss: 0.11457563936710358
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.7857142857142857
recall: 0.9166666666666666
F_score: 0.8461538461538461
subject 15 Avgacc: 0.7820833333333332 Avgfscore: 0.8086638064821428 
 Max acc:0.9666666666666667, Max f score:0.9714285714285714
******** mix subject_16 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 19.78466033935547
step: 100, Loss: 0.6287117004394531
step: 200, Loss: 0.17483563721179962
step: 300, Loss: 0.1258532553911209
step: 400, Loss: 0.1295521855354309
step: 500, Loss: 0.123337522149086
step: 600, Loss: 0.12531346082687378
step: 700, Loss: 0.11919093132019043
step: 800, Loss: 0.11743228882551193
step: 900, Loss: 0.11685352027416229
step: 1000, Loss: 0.11638231575489044
step: 1100, Loss: 0.1166946068406105
step: 1200, Loss: 0.11717663705348969
step: 1300, Loss: 0.11583554744720459
step: 1400, Loss: 0.11646133661270142
step: 1500, Loss: 0.11634103953838348
step: 1600, Loss: 0.11575385183095932
step: 1700, Loss: 0.11528471112251282
step: 1800, Loss: 0.11403489112854004
step: 1900, Loss: 0.11469262838363647
step: 2000, Loss: 0.12015974521636963
step: 2100, Loss: 0.11739230155944824
step: 2200, Loss: 0.11463848501443863
step: 2300, Loss: 0.11596906930208206
step: 2400, Loss: 0.11482942849397659
step: 2500, Loss: 0.11377832293510437
step: 2600, Loss: 0.1163543164730072
step: 2700, Loss: 0.11543989181518555
step: 2800, Loss: 0.11472415924072266
step: 2900, Loss: 0.11296123266220093
step: 3000, Loss: 0.11447875946760178
step: 3100, Loss: 0.11691942811012268
step: 3200, Loss: 0.11939053982496262
step: 3300, Loss: 0.11555727571249008
step: 3400, Loss: 0.11642587184906006
step: 3500, Loss: 0.11458960175514221
step: 3600, Loss: 0.1181148961186409
step: 3700, Loss: 0.11377114057540894
step: 3800, Loss: 0.11511857807636261
step: 3900, Loss: 0.11581889539957047
step: 4000, Loss: 0.11493213474750519
step: 4100, Loss: 0.11407208442687988
step: 4200, Loss: 0.11746811866760254
step: 4300, Loss: 0.1149902194738388
step: 4400, Loss: 0.1164914071559906
step: 4500, Loss: 0.11449610441923141
step: 4600, Loss: 0.11614528298377991
step: 4700, Loss: 0.11412711441516876
step: 4800, Loss: 0.11641697585582733
step: 4900, Loss: 0.11537715047597885
step: 5000, Loss: 0.11479049175977707
step: 5100, Loss: 0.11553096026182175
step: 5200, Loss: 0.11460291594266891
step: 5300, Loss: 0.11706733703613281
step: 5400, Loss: 0.11429165303707123
step: 5500, Loss: 0.11850082874298096
step: 5600, Loss: 0.11450900137424469
step: 5700, Loss: 0.11607028543949127
step: 5800, Loss: 0.11693655699491501
step: 5900, Loss: 0.11567658185958862
step: 6000, Loss: 0.11514566093683243
step: 6100, Loss: 0.11441069096326828
step: 6200, Loss: 0.11572536826133728
step: 6300, Loss: 0.11626510322093964
step: 6400, Loss: 0.11498820781707764
step: 6500, Loss: 0.11520666629076004
step: 6600, Loss: 0.11444027721881866
step: 6700, Loss: 0.11547622084617615
step: 6800, Loss: 0.9578778743743896
step: 6900, Loss: 0.15314140915870667
step: 7000, Loss: 0.12573713064193726
step: 7100, Loss: 0.13114503026008606
step: 7200, Loss: 0.13067686557769775
step: 7300, Loss: 0.12486638873815536
step: 7400, Loss: 0.13302533328533173
step: 7500, Loss: 0.11742702126502991
step: 7600, Loss: 0.12051499634981155
step: 7700, Loss: 0.12282190471887589
step: 7800, Loss: 0.1188138797879219
step: 7900, Loss: 0.11777838319540024
step: 8000, Loss: 0.1229126900434494
step: 8100, Loss: 0.12242136895656586
step: 8200, Loss: 0.12037718296051025
step: 8300, Loss: 0.1160745695233345
step: 8400, Loss: 0.11680474877357483
step: 8500, Loss: 0.11470907926559448
step: 8600, Loss: 0.11586619913578033
step: 8700, Loss: 0.11699800193309784
step: 8800, Loss: 0.11584389209747314
step: 8900, Loss: 0.11514279991388321
step: 9000, Loss: 0.11719691753387451
step: 9100, Loss: 0.1169116348028183
step: 9200, Loss: 0.11452849209308624
step: 9300, Loss: 0.11544748395681381
step: 9400, Loss: 0.11628510057926178
step: 9500, Loss: 0.11532586067914963
step: 9600, Loss: 0.11512775719165802
step: 9700, Loss: 0.11599850654602051
step: 9800, Loss: 0.1153397411108017
step: 9900, Loss: 0.1167500764131546
training successfully ended.
validating...
validate data length:32
acc: 0.375
precision: 0.21428571428571427
recall: 0.25
F_score: 0.23076923076923075
******fold 2******

Training... train_data length:280
step: 0, Loss: 5.365670204162598
step: 100, Loss: 0.12710052728652954
step: 200, Loss: 0.12368056178092957
step: 300, Loss: 0.11979856342077255
step: 400, Loss: 0.11681543290615082
step: 500, Loss: 0.11865715682506561
step: 600, Loss: 0.1177063137292862
step: 700, Loss: 0.11554275453090668
step: 800, Loss: 0.11548575758934021
step: 900, Loss: 0.11556623876094818
step: 1000, Loss: 0.11525510251522064
step: 1100, Loss: 0.11539439111948013
step: 1200, Loss: 0.1160675585269928
step: 1300, Loss: 0.11618752777576447
step: 1400, Loss: 0.11721965670585632
step: 1500, Loss: 0.11439434438943863
step: 1600, Loss: 0.11439518630504608
step: 1700, Loss: 0.1155775934457779
step: 1800, Loss: 0.11401623487472534
step: 1900, Loss: 0.11487598717212677
step: 2000, Loss: 0.11393187195062637
step: 2100, Loss: 0.11391157656908035
step: 2200, Loss: 0.11573776602745056
step: 2300, Loss: 0.11398592591285706
step: 2400, Loss: 0.11501199007034302
step: 2500, Loss: 0.1131635382771492
step: 2600, Loss: 0.11459977179765701
step: 2700, Loss: 0.11418583244085312
step: 2800, Loss: 0.11569751799106598
step: 2900, Loss: 0.11452200263738632
step: 3000, Loss: 0.11433293670415878
step: 3100, Loss: 0.11400075256824493
step: 3200, Loss: 0.11543232202529907
step: 3300, Loss: 0.11522749066352844
step: 3400, Loss: 0.11408621072769165
step: 3500, Loss: 0.1155586838722229
step: 3600, Loss: 0.1140480637550354
step: 3700, Loss: 0.11378654837608337
step: 3800, Loss: 0.11388546973466873
step: 3900, Loss: 0.1136658787727356
step: 4000, Loss: 0.11303132027387619
step: 4100, Loss: 0.11390066891908646
step: 4200, Loss: 0.1139606386423111
step: 4300, Loss: 0.11424849927425385
step: 4400, Loss: 0.11478922516107559
step: 4500, Loss: 0.11583061516284943
step: 4600, Loss: 0.11333047598600388
step: 7300, Loss: 0.11524316668510437
step: 7400, Loss: 0.1146194189786911
step: 7500, Loss: 0.11516672372817993
step: 7600, Loss: 0.11453976482152939
step: 7700, Loss: 0.1161767989397049
step: 7800, Loss: 0.11527418345212936
step: 7900, Loss: 0.11570025235414505
step: 8000, Loss: 0.11413536220788956
step: 8100, Loss: 0.11410079896450043
step: 8200, Loss: 0.11731034517288208
step: 8300, Loss: 0.11380631476640701
step: 8400, Loss: 0.1169813945889473
step: 8500, Loss: 0.11499978601932526
step: 8600, Loss: 0.1146894246339798
step: 8700, Loss: 0.11620904505252838
step: 8800, Loss: 0.11627332866191864
step: 8900, Loss: 0.3871212601661682
step: 9000, Loss: 0.1159435585141182
step: 9100, Loss: 0.11487051844596863
step: 9200, Loss: 0.11472312361001968
step: 9300, Loss: 0.11695394665002823
step: 9400, Loss: 0.11414960026741028
step: 9500, Loss: 0.11605339497327805
step: 9600, Loss: 0.115320585668087
step: 9700, Loss: 6.89383602142334
step: 9800, Loss: 0.2606985569000244
step: 9900, Loss: 0.14268532395362854
training successfully ended.
validating...
validate data length:91
acc: 0.9204545454545454
precision: 0.8723404255319149
recall: 0.9761904761904762
F_score: 0.9213483146067415
******fold 7******

Training... train_data length:821
step: 0, Loss: 0.11626052111387253
step: 100, Loss: 0.12380565702915192
step: 200, Loss: 0.11826799809932709
step: 300, Loss: 0.1185750961303711
step: 400, Loss: 0.11838776618242264
step: 500, Loss: 0.11592386662960052
step: 600, Loss: 0.1156235933303833
step: 700, Loss: 0.114975705742836
step: 800, Loss: 0.11575876921415329
step: 900, Loss: 0.11553817987442017
step: 1000, Loss: 0.11512181162834167
step: 1100, Loss: 0.11541207134723663
step: 1200, Loss: 5.230092525482178
step: 1300, Loss: 0.25786733627319336
step: 1400, Loss: 0.13633212447166443
step: 1500, Loss: 0.14438316226005554
step: 1600, Loss: 0.13776402175426483
step: 1700, Loss: 0.12586736679077148
step: 1800, Loss: 0.12439475953578949
step: 1900, Loss: 0.14119058847427368
step: 2000, Loss: 0.4158409833908081
step: 2100, Loss: 0.1345663219690323
step: 2200, Loss: 0.12143489718437195
step: 2300, Loss: 0.12272138893604279
step: 2400, Loss: 0.1230415552854538
step: 2500, Loss: 0.11970006674528122
step: 2600, Loss: 0.12165768444538116
step: 2700, Loss: 0.12366004288196564
step: 2800, Loss: 0.12417270988225937
step: 2900, Loss: 0.11945529282093048
step: 3000, Loss: 0.12243896722793579
step: 3100, Loss: 0.12075802683830261
step: 3200, Loss: 0.11737874150276184
step: 3300, Loss: 0.11763209849596024
step: 3400, Loss: 0.116022028028965
step: 3500, Loss: 0.11702658981084824
step: 3600, Loss: 0.11800475418567657
step: 3700, Loss: 0.12112367153167725
step: 3800, Loss: 0.11840695142745972
step: 3900, Loss: 0.11475729197263718
step: 4000, Loss: 0.11582493782043457
step: 4100, Loss: 0.1162959560751915
step: 4200, Loss: 0.11780275404453278
step: 4300, Loss: 0.36721158027648926
step: 4400, Loss: 0.125858336687088
step: 4500, Loss: 0.1150379404425621
step: 4600, Loss: 0.11496201902627945
step: 4700, Loss: 0.11484986543655396
step: 4800, Loss: 0.11556380242109299
step: 4900, Loss: 0.11483093351125717
step: 5000, Loss: 0.11730901896953583
step: 5100, Loss: 0.11422476172447205
step: 5200, Loss: 0.11485296487808228
step: 5300, Loss: 0.11405116319656372
step: 5400, Loss: 0.1164146363735199
step: 5500, Loss: 0.11635574698448181
step: 5600, Loss: 0.11447128653526306
step: 5700, Loss: 0.11340855062007904
step: 5800, Loss: 0.11426515132188797
step: 5900, Loss: 0.1157265305519104
step: 6000, Loss: 0.11675256490707397
step: 6100, Loss: 0.11976105719804764
step: 6200, Loss: 0.11508065462112427
step: 6300, Loss: 0.120343416929245
step: 6400, Loss: 0.11390426009893417
step: 6500, Loss: 0.11546717584133148
step: 6600, Loss: 0.36589986085891724
step: 6700, Loss: 0.1190788745880127
step: 6800, Loss: 0.11454357206821442
step: 6900, Loss: 3.252856969833374
step: 7000, Loss: 0.9396387934684753
step: 7100, Loss: 0.1435915231704712
step: 7200, Loss: 0.13672557473182678
step: 7300, Loss: 0.13708002865314484
step: 7400, Loss: 0.13349013030529022
step: 7500, Loss: 0.13238579034805298
step: 7600, Loss: 0.12818744778633118
step: 7700, Loss: 0.13901928067207336
step: 7800, Loss: 0.12090697139501572
step: 7900, Loss: 0.15033799409866333
step: 8000, Loss: 0.12653280794620514
step: 8100, Loss: 0.1259041577577591
step: 8200, Loss: 0.12192617356777191
step: 8300, Loss: 0.12476392090320587
step: 8400, Loss: 0.12036111205816269
step: 8500, Loss: 0.12500935792922974
step: 8600, Loss: 0.12097130715847015
step: 8700, Loss: 0.12063047289848328
step: 8800, Loss: 0.12398247420787811
step: 8900, Loss: 0.37399086356163025
step: 9000, Loss: 0.12699326872825623
step: 9100, Loss: 0.11702607572078705
step: 9200, Loss: 0.11855762451887131
step: 9300, Loss: 0.11731913685798645
step: 9400, Loss: 0.11942914128303528
step: 9500, Loss: 0.1167035773396492
step: 9600, Loss: 0.11609235405921936
step: 9700, Loss: 0.11765493452548981
step: 9800, Loss: 0.11749931424856186
step: 9900, Loss: 0.11570407450199127
training successfully ended.
validating...
validate data length:91
acc: 0.9659090909090909
precision: 0.9487179487179487
recall: 0.9736842105263158
F_score: 0.9610389610389611
******fold 8******

Training... train_data length:821
step: 0, Loss: 0.11507034301757812
step: 100, Loss: 0.12346147000789642
step: 200, Loss: 0.12885357439517975
step: 300, Loss: 0.12344302237033844
step: 400, Loss: 0.1198401227593422
step: 500, Loss: 0.11789879202842712
step: 600, Loss: 0.11796530336141586
step: 700, Loss: 0.11785353720188141
step: 800, Loss: 0.1174682155251503
step: 900, Loss: 0.11767534911632538
step: 1000, Loss: 0.11777643859386444
step: 1100, Loss: 0.11692801117897034
step: 1200, Loss: 0.11502518504858017
step: 1300, Loss: 0.1171455979347229
step: 1400, Loss: 0.11485633254051208
step: 1500, Loss: 0.11585064977407455
step: 1600, Loss: 0.11522199958562851
step: 1700, Loss: 0.11447670310735703
step: 1800, Loss: 0.1152970939874649
step: 1900, Loss: 0.11595585942268372
step: 2000, Loss: 0.3790358603000641
step: 2100, Loss: 0.11933852732181549
step: 2200, Loss: 0.11482369899749756
step: 2300, Loss: 0.11844906210899353
step: 2400, Loss: 0.1220049187541008
step: 2500, Loss: 0.1153583824634552
step: 2600, Loss: 0.11412530392408371
step: 2700, Loss: 0.12150231748819351
step: 2800, Loss: 0.11525322496891022
step: 2900, Loss: 0.1175994724035263
step: 3000, Loss: 0.12255904078483582
step: 3100, Loss: 0.117105633020401
step: 3200, Loss: 0.11723356693983078
step: 3300, Loss: 0.12576523423194885
step: 3400, Loss: 1.036353349685669
step: 3500, Loss: 5.328329086303711
step: 3600, Loss: 0.12884904444217682
step: 3700, Loss: 0.13915909826755524
step: 3800, Loss: 0.13089245557785034
step: 3900, Loss: 0.13121767342090607
step: 4000, Loss: 0.12238086014986038
step: 4100, Loss: 0.12259882688522339
step: 4200, Loss: 0.13849957287311554
step: 4300, Loss: 0.38621145486831665
step: 4400, Loss: 0.12692943215370178
step: 4500, Loss: 0.12157739698886871
step: 4600, Loss: 0.11704015731811523
step: 4700, Loss: 0.12557101249694824
step: 4800, Loss: 0.11875060200691223
step: 4900, Loss: 0.12240596860647202
step: 5000, Loss: 0.12278009951114655
step: 5100, Loss: 0.11819596588611603
step: 5200, Loss: 0.11886971443891525
step: 5300, Loss: 0.11855664849281311
step: 5400, Loss: 0.1259697824716568
step: 5500, Loss: 0.11765913665294647
step: 5600, Loss: 0.12189941853284836
step: 5700, Loss: 0.11753155291080475
step: 5800, Loss: 0.12026859819889069
step: 5900, Loss: 0.12026091665029526
step: 6000, Loss: 0.11561772227287292
step: 6100, Loss: 0.11684047430753708
step: 6200, Loss: 0.11564009636640549
step: 6300, Loss: 0.1150272786617279
step: 6400, Loss: 0.11586886644363403
step: 6500, Loss: 0.1176089495420456
step: 6600, Loss: 0.35919618606567383
step: 6700, Loss: 0.11545014381408691
step: 6800, Loss: 0.11722810566425323
step: 6900, Loss: 0.1136302500963211
step: 7000, Loss: 0.11388953030109406
step: 7100, Loss: 0.11803548038005829
step: 7200, Loss: 0.11392810195684433
step: 7300, Loss: 0.11773684620857239
step: 7400, Loss: 0.1150030791759491
step: 7500, Loss: 0.11491072922945023
step: 7600, Loss: 0.11515705287456512
step: 7700, Loss: 0.11753873527050018
step: 4700, Loss: 0.11472021788358688
step: 4800, Loss: 0.11439938098192215
step: 4900, Loss: 0.11451031267642975
step: 5000, Loss: 0.11345746368169785
step: 5100, Loss: 0.11324387788772583
step: 5200, Loss: 0.11513157933950424
step: 5300, Loss: 0.11573720723390579
step: 5400, Loss: 0.11579295247793198
step: 5500, Loss: 0.11433157324790955
step: 5600, Loss: 0.11440536379814148
step: 5700, Loss: 0.11552345752716064
step: 5800, Loss: 0.11305675655603409
step: 5900, Loss: 0.11422256380319595
step: 6000, Loss: 0.1133982315659523
step: 6100, Loss: 0.11611295491456985
step: 6200, Loss: 0.11365198343992233
step: 6300, Loss: 0.11457403004169464
step: 6400, Loss: 0.11354348063468933
step: 6500, Loss: 0.11443749070167542
step: 6600, Loss: 0.11300671845674515
step: 6700, Loss: 0.11492329835891724
step: 6800, Loss: 0.11411155015230179
step: 6900, Loss: 0.11365941911935806
step: 7000, Loss: 0.11405734717845917
step: 7100, Loss: 0.1149560883641243
step: 7200, Loss: 0.11445748805999756
step: 7300, Loss: 0.11331517994403839
step: 7400, Loss: 0.11512012779712677
step: 7500, Loss: 0.11386822164058685
step: 7600, Loss: 0.11423565447330475
step: 7700, Loss: 0.11448140442371368
step: 7800, Loss: 0.11366663873195648
step: 7900, Loss: 0.11390955001115799
step: 8000, Loss: 0.1331397145986557
step: 8100, Loss: 0.17988727986812592
step: 8200, Loss: 0.13975660502910614
step: 8300, Loss: 0.1328197717666626
step: 8400, Loss: 0.1298510879278183
step: 8500, Loss: 0.1255757361650467
step: 8600, Loss: 0.12183010578155518
step: 8700, Loss: 0.11806604266166687
step: 8800, Loss: 0.11877402663230896
step: 8900, Loss: 0.12114882469177246
step: 9000, Loss: 0.12424873560667038
step: 9100, Loss: 0.1152968481183052
step: 9200, Loss: 0.1170479953289032
step: 9300, Loss: 0.11532346159219742
step: 9400, Loss: 0.11628317832946777
step: 9500, Loss: 0.11450270563364029
step: 9600, Loss: 0.11583553999662399
step: 9700, Loss: 0.11494635790586472
step: 9800, Loss: 0.11788962036371231
step: 9900, Loss: 0.1148848682641983
training successfully ended.
validating...
validate data length:32
acc: 0.6875
precision: 0.6666666666666666
recall: 0.75
F_score: 0.7058823529411765
******fold 3******

Training... train_data length:281
step: 0, Loss: 5.9140238761901855
step: 100, Loss: 0.12045162916183472
step: 200, Loss: 0.11990225315093994
step: 300, Loss: 0.11512639373540878
step: 400, Loss: 0.11701202392578125
step: 500, Loss: 0.11768484115600586
step: 600, Loss: 0.11583061516284943
step: 700, Loss: 0.117225781083107
step: 800, Loss: 0.11528666317462921
step: 900, Loss: 0.11622977256774902
step: 1000, Loss: 0.11490064859390259
step: 1100, Loss: 0.11499285697937012
step: 1200, Loss: 0.11626233905553818
step: 1300, Loss: 0.11379212141036987
step: 1400, Loss: 0.11490803956985474
step: 1500, Loss: 0.11617986857891083
step: 1600, Loss: 0.11467228829860687
step: 1700, Loss: 0.11408749222755432
step: 1800, Loss: 0.11544089019298553
step: 1900, Loss: 0.11442556977272034
step: 2000, Loss: 0.1138816699385643
step: 2100, Loss: 0.11525861918926239
step: 2200, Loss: 0.11382898688316345
step: 2300, Loss: 0.11441060900688171
step: 2400, Loss: 0.11311191320419312
step: 2500, Loss: 0.11290489882230759
step: 2600, Loss: 0.11491914838552475
step: 2700, Loss: 0.11490970104932785
step: 2800, Loss: 0.11418408900499344
step: 2900, Loss: 0.11434157937765121
step: 3000, Loss: 0.11325294524431229
step: 3100, Loss: 0.11420346051454544
step: 3200, Loss: 0.11380356550216675
step: 3300, Loss: 0.11547324061393738
step: 3400, Loss: 0.11489950865507126
step: 3500, Loss: 0.11482007056474686
step: 3600, Loss: 0.11359815299510956
step: 3700, Loss: 0.11325622349977493
step: 3800, Loss: 0.11496308445930481
step: 3900, Loss: 0.11478663980960846
step: 4000, Loss: 0.11384367942810059
step: 4100, Loss: 0.11394958198070526
step: 4200, Loss: 0.11419922113418579
step: 4300, Loss: 0.11452970653772354
step: 4400, Loss: 0.11493126302957535
step: 4500, Loss: 0.1138397753238678
step: 4600, Loss: 0.11374646425247192
step: 4700, Loss: 0.11500191688537598
step: 4800, Loss: 0.11354649811983109
step: 4900, Loss: 0.11344627290964127
step: 5000, Loss: 0.11388291418552399
step: 5100, Loss: 0.11645913869142532
step: 5200, Loss: 0.11482713371515274
step: 5300, Loss: 0.11484349519014359
step: 5400, Loss: 0.11367926001548767
step: 5500, Loss: 0.113181933760643
step: 5600, Loss: 0.1134059727191925
step: 5700, Loss: 0.11478941142559052
step: 5800, Loss: 0.11370629072189331
step: 5900, Loss: 0.11501292884349823
step: 6000, Loss: 0.11388607323169708
step: 6100, Loss: 0.11348707228899002
step: 6200, Loss: 0.11352406442165375
step: 6300, Loss: 0.11353055387735367
step: 6400, Loss: 0.113786481320858
step: 6500, Loss: 0.11443614959716797
step: 6600, Loss: 0.11455996334552765
step: 6700, Loss: 0.11365298926830292
step: 6800, Loss: 0.11320863664150238
step: 6900, Loss: 0.11345239728689194
step: 7000, Loss: 0.11407367140054703
step: 7100, Loss: 0.1135326623916626
step: 7200, Loss: 0.11350727826356888
step: 7300, Loss: 0.1145891547203064
step: 7400, Loss: 0.11554870009422302
step: 7500, Loss: 0.11481286585330963
step: 7600, Loss: 0.11529473960399628
step: 7700, Loss: 0.11438404023647308
step: 7800, Loss: 0.1125083789229393
step: 7900, Loss: 0.11423829197883606
step: 8000, Loss: 0.11291535943746567
step: 8100, Loss: 0.11370743811130524
step: 8200, Loss: 0.1161118596792221
step: 8300, Loss: 0.11289782077074051
step: 8400, Loss: 0.11306601762771606
step: 8500, Loss: 0.11534424871206284
step: 8600, Loss: 0.26223820447921753
step: 8700, Loss: 0.13545632362365723
step: 8800, Loss: 0.12930995225906372
step: 8900, Loss: 0.12265674769878387
step: 9000, Loss: 0.11951088905334473
step: 9100, Loss: 0.11723025143146515
step: 9200, Loss: 0.12133513391017914
step: 9300, Loss: 0.11446430534124374
step: 9400, Loss: 0.1198727935552597
step: 9500, Loss: 0.11593364179134369
step: 9600, Loss: 0.1167508065700531
step: 9700, Loss: 0.11595293879508972
step: 9800, Loss: 0.11561809480190277
step: 9900, Loss: 0.11603139340877533
training successfully ended.
validating...
validate data length:31
acc: 0.6333333333333333
precision: 0.625
recall: 0.6666666666666666
F_score: 0.6451612903225806
******fold 4******

Training... train_data length:281
step: 0, Loss: 5.8503570556640625
step: 100, Loss: 0.12177883088588715
step: 200, Loss: 0.11751465499401093
step: 300, Loss: 0.12209191918373108
step: 400, Loss: 0.11777063459157944
step: 500, Loss: 0.11544433236122131
step: 600, Loss: 0.1160571277141571
step: 700, Loss: 0.11610081791877747
step: 800, Loss: 0.1144372820854187
step: 900, Loss: 0.11380532383918762
step: 1000, Loss: 0.11437865346670151
step: 1100, Loss: 0.11408748477697372
step: 1200, Loss: 0.1136634424328804
step: 1300, Loss: 0.11386392265558243
step: 1400, Loss: 0.11410851031541824
step: 1500, Loss: 0.11425044387578964
step: 1600, Loss: 0.11624221503734589
step: 1700, Loss: 0.11437901854515076
step: 1800, Loss: 0.11434179544448853
step: 1900, Loss: 0.113957479596138
step: 2000, Loss: 0.11547285318374634
step: 2100, Loss: 0.11395611613988876
step: 2200, Loss: 0.11443984508514404
step: 2300, Loss: 0.11452297866344452
step: 2400, Loss: 0.11356576532125473
step: 2500, Loss: 0.11606889963150024
step: 2600, Loss: 0.11492074280977249
step: 2700, Loss: 0.11484822630882263
step: 2800, Loss: 0.11321970820426941
step: 2900, Loss: 0.1130654513835907
step: 3000, Loss: 0.11441683769226074
step: 3100, Loss: 0.11338619142770767
step: 3200, Loss: 0.11355491727590561
step: 3300, Loss: 0.1169871985912323
step: 3400, Loss: 0.11348451673984528
step: 3500, Loss: 0.11458315700292587
step: 3600, Loss: 0.11399561911821365
step: 3700, Loss: 0.11347296833992004
step: 3800, Loss: 0.11408717930316925
step: 3900, Loss: 0.11472930014133453
step: 4000, Loss: 0.1144157201051712
step: 4100, Loss: 0.11648581176996231
step: 4200, Loss: 0.11421629041433334
step: 4300, Loss: 0.11391858756542206
step: 4400, Loss: 0.11436790227890015
step: 4500, Loss: 0.11442311108112335
step: 4600, Loss: 0.1135190799832344
step: 4700, Loss: 0.11368162930011749
step: 4800, Loss: 0.11356547474861145
step: 4900, Loss: 0.11353453993797302
step: 5000, Loss: 0.11375443637371063
step: 5100, Loss: 0.11432018876075745
step: 5200, Loss: 0.11847437918186188
step: 7800, Loss: 0.1215600073337555
step: 7900, Loss: 0.11726269125938416
step: 8000, Loss: 0.11643335223197937
step: 8100, Loss: 0.11646485328674316
step: 8200, Loss: 0.11460348218679428
step: 8300, Loss: 0.11676734685897827
step: 8400, Loss: 0.11392318457365036
step: 8500, Loss: 0.11294066160917282
step: 8600, Loss: 0.11454254388809204
step: 8700, Loss: 0.11564354598522186
step: 8800, Loss: 0.1160750761628151
step: 8900, Loss: 0.3694046139717102
step: 9000, Loss: 0.11812277138233185
step: 9100, Loss: 0.1151360347867012
step: 9200, Loss: 0.11609561741352081
step: 9300, Loss: 0.11774212121963501
step: 9400, Loss: 2.471996784210205
step: 9500, Loss: 3.130051851272583
step: 9600, Loss: 0.1446278840303421
step: 9700, Loss: 0.13918808102607727
step: 9800, Loss: 0.1323295533657074
step: 9900, Loss: 0.12095758318901062
training successfully ended.
validating...
validate data length:91
acc: 0.9545454545454546
precision: 0.9411764705882353
recall: 0.9795918367346939
F_score: 0.96
******fold 9******

Training... train_data length:821
step: 0, Loss: 0.11565367132425308
step: 100, Loss: 0.1317327320575714
step: 200, Loss: 0.12230794131755829
step: 300, Loss: 0.12007318437099457
step: 400, Loss: 0.11695743352174759
step: 500, Loss: 0.12008741497993469
step: 600, Loss: 0.11462635546922684
step: 700, Loss: 0.11775680631399155
step: 800, Loss: 0.1154666543006897
step: 900, Loss: 0.11620762944221497
step: 1000, Loss: 0.11403282731771469
step: 1100, Loss: 0.11465433984994888
step: 1200, Loss: 0.11530852317810059
step: 1300, Loss: 0.11831530928611755
step: 1400, Loss: 0.11569441854953766
step: 1500, Loss: 0.1174079179763794
step: 1600, Loss: 0.11508266627788544
step: 1700, Loss: 0.11710838228464127
step: 1800, Loss: 0.11751638352870941
step: 1900, Loss: 0.11700775474309921
step: 2000, Loss: 0.4392574727535248
step: 2100, Loss: 0.11983271688222885
step: 2200, Loss: 0.11746060848236084
step: 2300, Loss: 0.11749613285064697
step: 2400, Loss: 0.1149798184633255
step: 2500, Loss: 0.11847252398729324
step: 2600, Loss: 0.11722144484519958
step: 2700, Loss: 0.11904019862413406
step: 2800, Loss: 2.452232599258423
step: 2900, Loss: 0.6067270040512085
step: 3000, Loss: 0.15831002593040466
step: 3100, Loss: 0.14246246218681335
step: 3200, Loss: 0.13053059577941895
step: 3300, Loss: 0.1294061541557312
step: 3400, Loss: 0.13181856274604797
step: 3500, Loss: 0.13362327218055725
step: 3600, Loss: 0.12793278694152832
step: 3700, Loss: 0.1222592443227768
step: 3800, Loss: 0.1323728710412979
step: 3900, Loss: 0.12225707620382309
step: 4000, Loss: 0.11840318143367767
step: 4100, Loss: 0.11942566931247711
step: 4200, Loss: 0.12792174518108368
step: 4300, Loss: 0.3665238618850708
step: 4400, Loss: 0.12691567838191986
step: 4500, Loss: 0.12058539688587189
step: 4600, Loss: 0.11689377576112747
step: 4700, Loss: 0.11954353004693985
step: 4800, Loss: 0.11714321374893188
step: 4900, Loss: 0.11698941886425018
step: 5000, Loss: 0.11985886096954346
step: 5100, Loss: 0.11444646120071411
step: 5200, Loss: 0.11940591037273407
step: 5300, Loss: 0.11551158875226974
step: 5400, Loss: 0.11902323365211487
step: 5500, Loss: 0.11636616289615631
step: 5600, Loss: 0.11698456853628159
step: 5700, Loss: 0.11486934870481491
step: 5800, Loss: 0.11825227737426758
step: 5900, Loss: 0.11448020488023758
step: 6000, Loss: 0.11514484137296677
step: 6100, Loss: 0.1177477017045021
step: 6200, Loss: 0.11587782949209213
step: 6300, Loss: 0.11344894021749496
step: 6400, Loss: 0.11643968522548676
step: 6500, Loss: 0.11519278585910797
step: 6600, Loss: 0.37503668665885925
step: 6700, Loss: 0.11881091445684433
step: 6800, Loss: 0.11477157473564148
step: 6900, Loss: 0.11556810885667801
step: 7000, Loss: 0.11595746129751205
step: 7100, Loss: 0.11361118406057358
step: 7200, Loss: 0.11330512166023254
step: 7300, Loss: 0.11592546850442886
step: 7400, Loss: 0.11556815356016159
step: 7500, Loss: 0.11414237320423126
step: 7600, Loss: 0.11505995690822601
step: 7700, Loss: 0.11452686786651611
step: 7800, Loss: 0.1145021840929985
step: 7900, Loss: 0.11273886263370514
step: 8000, Loss: 0.11338618397712708
step: 8100, Loss: 0.11534717679023743
step: 8200, Loss: 0.11619715392589569
step: 8300, Loss: 0.11461065709590912
step: 8400, Loss: 0.11603237688541412
step: 8500, Loss: 0.11371801048517227
step: 8600, Loss: 0.1155640259385109
step: 8700, Loss: 0.11846571415662766
step: 8800, Loss: 0.1164340227842331
step: 8900, Loss: 0.36000800132751465
step: 9000, Loss: 0.12100467085838318
step: 9100, Loss: 6.8288187980651855
step: 9200, Loss: 0.19761280715465546
step: 9300, Loss: 0.14982303977012634
step: 9400, Loss: 0.14133970439434052
step: 9500, Loss: 0.13775379955768585
step: 9600, Loss: 0.13329017162322998
step: 9700, Loss: 0.12503838539123535
step: 9800, Loss: 0.12888109683990479
step: 9900, Loss: 0.133724182844162
training successfully ended.
validating...
validate data length:91
acc: 0.9431818181818182
precision: 0.92
recall: 0.9787234042553191
F_score: 0.9484536082474226
******fold 10******

Training... train_data length:821
step: 0, Loss: 0.11526504158973694
step: 100, Loss: 0.12407216429710388
step: 200, Loss: 0.1243104636669159
step: 300, Loss: 0.12146233022212982
step: 400, Loss: 0.1162799745798111
step: 500, Loss: 0.11709876358509064
step: 600, Loss: 0.11507570743560791
step: 700, Loss: 0.11578185856342316
step: 800, Loss: 0.11585074663162231
step: 900, Loss: 0.11608365178108215
step: 1000, Loss: 0.11477990448474884
step: 1100, Loss: 0.11862602829933167
step: 1200, Loss: 0.1152549609541893
step: 1300, Loss: 0.11570878326892853
step: 1400, Loss: 0.11458670347929001
step: 1500, Loss: 0.11480467021465302
step: 1600, Loss: 0.11480600386857986
step: 1700, Loss: 0.11541582643985748
step: 1800, Loss: 0.11369199305772781
step: 1900, Loss: 0.11676180362701416
step: 2000, Loss: 0.37846678495407104
step: 2100, Loss: 0.11899992823600769
step: 2200, Loss: 0.11615034937858582
step: 2300, Loss: 0.11468658596277237
step: 2400, Loss: 0.12306097149848938
step: 2500, Loss: 2.186851978302002
step: 2600, Loss: 0.3805672526359558
step: 2700, Loss: 0.1680023968219757
step: 2800, Loss: 0.1438046395778656
step: 2900, Loss: 0.14391638338565826
step: 3000, Loss: 0.14141902327537537
step: 3100, Loss: 0.14191603660583496
step: 3200, Loss: 0.13888715207576752
step: 3300, Loss: 0.13180239498615265
step: 3400, Loss: 0.14084933698177338
step: 3500, Loss: 0.13271178305149078
step: 3600, Loss: 0.12425504624843597
step: 3700, Loss: 0.11792057752609253
step: 3800, Loss: 0.1235487088561058
step: 3900, Loss: 0.12384345382452011
step: 4000, Loss: 0.11991794407367706
step: 4100, Loss: 0.11911238729953766
step: 4200, Loss: 0.12873169779777527
step: 4300, Loss: 0.3750806450843811
step: 4400, Loss: 0.12478724867105484
step: 4500, Loss: 0.12483171373605728
step: 4600, Loss: 0.11593081057071686
step: 4700, Loss: 0.11889481544494629
step: 4800, Loss: 0.12333100289106369
step: 4900, Loss: 0.11546384543180466
step: 5000, Loss: 0.12171027064323425
step: 5100, Loss: 0.11927266418933868
step: 5200, Loss: 0.11747081577777863
step: 5300, Loss: 0.11782120168209076
step: 5400, Loss: 0.11844716966152191
step: 5500, Loss: 0.11857262253761292
step: 5600, Loss: 0.11722877621650696
step: 5700, Loss: 0.11847437918186188
step: 5800, Loss: 0.11626048386096954
step: 5900, Loss: 0.11634378880262375
step: 6000, Loss: 0.11551059782505035
step: 6100, Loss: 0.11852915585041046
step: 6200, Loss: 0.11575465649366379
step: 6300, Loss: 0.11564940214157104
step: 6400, Loss: 0.11751403659582138
step: 6500, Loss: 0.11727294325828552
step: 6600, Loss: 0.3562370836734772
step: 6700, Loss: 0.11719433963298798
step: 6800, Loss: 0.11583052575588226
step: 6900, Loss: 0.1149522140622139
step: 7000, Loss: 0.11447612941265106
step: 7100, Loss: 0.1162211075425148
step: 7200, Loss: 0.11389189213514328
step: 7300, Loss: 0.11778415739536285
step: 7400, Loss: 0.11439598351716995
step: 7500, Loss: 0.11777503788471222
step: 7600, Loss: 0.11569136381149292
step: 7700, Loss: 0.11518973112106323
step: 7800, Loss: 0.117533840239048
step: 7900, Loss: 0.11413884907960892
step: 8000, Loss: 0.11674773693084717
step: 8100, Loss: 0.11391787230968475
step: 8200, Loss: 0.11507971584796906
step: 8300, Loss: 0.1156054139137268
step: 5300, Loss: 4.897242546081543
step: 5400, Loss: 0.14849510788917542
step: 5500, Loss: 0.13258233666419983
step: 5600, Loss: 0.134089857339859
step: 5700, Loss: 0.12337431311607361
step: 5800, Loss: 0.12422561645507812
step: 5900, Loss: 0.12302820384502411
step: 6000, Loss: 0.11980724334716797
step: 6100, Loss: 0.12145698070526123
step: 6200, Loss: 0.12013908475637436
step: 6300, Loss: 0.11848516762256622
step: 6400, Loss: 0.11936577409505844
step: 6500, Loss: 0.11763106286525726
step: 6600, Loss: 0.11883070319890976
step: 6700, Loss: 0.11563414335250854
step: 6800, Loss: 0.11569512635469437
step: 6900, Loss: 0.11523490399122238
step: 7000, Loss: 0.11730276048183441
step: 7100, Loss: 0.1183343157172203
step: 7200, Loss: 0.1146709993481636
step: 7300, Loss: 0.11400879174470901
step: 7400, Loss: 0.11632686108350754
step: 7500, Loss: 0.11460528522729874
step: 7600, Loss: 0.11341207474470139
step: 7700, Loss: 0.11572866141796112
step: 7800, Loss: 0.11458998173475266
step: 7900, Loss: 0.11647384613752365
step: 8000, Loss: 0.11656096577644348
step: 8100, Loss: 0.11628083139657974
step: 8200, Loss: 0.1146383285522461
step: 8300, Loss: 0.11305622011423111
step: 8400, Loss: 0.11422973871231079
step: 8500, Loss: 0.11388806998729706
step: 8600, Loss: 0.11446371674537659
step: 8700, Loss: 0.11381794512271881
step: 8800, Loss: 0.11555789411067963
step: 8900, Loss: 0.11322414875030518
step: 9000, Loss: 0.11445112526416779
step: 9100, Loss: 0.11448884010314941
step: 9200, Loss: 0.11454262584447861
step: 9300, Loss: 0.11533823609352112
step: 9400, Loss: 0.11501937359571457
step: 9500, Loss: 0.11456121504306793
step: 9600, Loss: 0.11474637687206268
step: 9700, Loss: 0.11423633992671967
step: 9800, Loss: 0.11432990431785583
step: 9900, Loss: 0.11612123250961304
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8
recall: 0.8
F_score: 0.8000000000000002
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.11536510288715363
step: 100, Loss: 0.121161550283432
step: 200, Loss: 0.11457081139087677
step: 300, Loss: 0.11721649765968323
step: 400, Loss: 0.11451054364442825
step: 500, Loss: 0.11480142176151276
step: 600, Loss: 0.1172136515378952
step: 700, Loss: 0.11391434818506241
step: 800, Loss: 0.1153007447719574
step: 900, Loss: 0.11433634161949158
step: 1000, Loss: 0.11520962417125702
step: 1100, Loss: 0.11388760805130005
step: 1200, Loss: 0.11307425051927567
step: 1300, Loss: 0.114454485476017
step: 1400, Loss: 0.11323204636573792
step: 1500, Loss: 0.11455220729112625
step: 1600, Loss: 0.11497601866722107
step: 1700, Loss: 0.11343901604413986
step: 1800, Loss: 0.11321792006492615
step: 1900, Loss: 0.11358562856912613
step: 2000, Loss: 0.11316974461078644
step: 2100, Loss: 0.11382576078176498
step: 2200, Loss: 0.11322065442800522
step: 2300, Loss: 0.11362195014953613
step: 2400, Loss: 0.11507399380207062
step: 2500, Loss: 0.1126348152756691
step: 2600, Loss: 0.11450463533401489
step: 2700, Loss: 0.11341621726751328
step: 2800, Loss: 0.11452794075012207
step: 2900, Loss: 0.11374538391828537
step: 3000, Loss: 0.11328844726085663
step: 3100, Loss: 0.1145462766289711
step: 3200, Loss: 0.1135224848985672
step: 3300, Loss: 0.1142662912607193
step: 3400, Loss: 0.11320286989212036
step: 3500, Loss: 0.1159127801656723
step: 3600, Loss: 0.11446468532085419
step: 3700, Loss: 0.11360234022140503
step: 3800, Loss: 0.11363867670297623
step: 3900, Loss: 0.11299949139356613
step: 4000, Loss: 0.11356907337903976
step: 4100, Loss: 0.11471176892518997
step: 4200, Loss: 0.11412885785102844
step: 4300, Loss: 0.11286245286464691
step: 4400, Loss: 0.11373081803321838
step: 4500, Loss: 0.11356356739997864
step: 4600, Loss: 0.11263440549373627
step: 4700, Loss: 0.11336119472980499
step: 4800, Loss: 0.11347390711307526
step: 4900, Loss: 0.1135491281747818
step: 5000, Loss: 0.11424240469932556
step: 5100, Loss: 0.11349111795425415
step: 5200, Loss: 0.1140337735414505
step: 5300, Loss: 0.11350405216217041
step: 5400, Loss: 0.11341193318367004
step: 5500, Loss: 0.11328277736902237
step: 5600, Loss: 0.11474964022636414
step: 5700, Loss: 0.11361562460660934
step: 5800, Loss: 0.11375221610069275
step: 5900, Loss: 0.11303399503231049
step: 6000, Loss: 0.11341548711061478
step: 6100, Loss: 0.11315086483955383
step: 6200, Loss: 0.11492116004228592
step: 6300, Loss: 0.1138102188706398
step: 6400, Loss: 0.11334255337715149
step: 6500, Loss: 0.12235197424888611
step: 6600, Loss: 0.14745041728019714
step: 6700, Loss: 0.11893527209758759
step: 6800, Loss: 0.11823758482933044
step: 6900, Loss: 0.11991894245147705
step: 7000, Loss: 0.11676281690597534
step: 7100, Loss: 0.11499606817960739
step: 7200, Loss: 0.11583204567432404
step: 7300, Loss: 0.11662839353084564
step: 7400, Loss: 0.11501897871494293
step: 7500, Loss: 0.11782551556825638
step: 7600, Loss: 0.1146450936794281
step: 7700, Loss: 0.11398137360811234
step: 7800, Loss: 0.11564340442419052
step: 7900, Loss: 0.11333412677049637
step: 8000, Loss: 0.11454746127128601
step: 8100, Loss: 0.11391638964414597
step: 8200, Loss: 0.11633720993995667
step: 8300, Loss: 0.11377844959497452
step: 8400, Loss: 0.11410996317863464
step: 8500, Loss: 0.11339115351438522
step: 8600, Loss: 0.11478959769010544
step: 8700, Loss: 0.11333951354026794
step: 8800, Loss: 0.11459840089082718
step: 8900, Loss: 0.11573438346385956
step: 9000, Loss: 0.11454169452190399
step: 9100, Loss: 0.11372163891792297
step: 9200, Loss: 0.1134004220366478
step: 9300, Loss: 0.11324857920408249
step: 9400, Loss: 0.11311428248882294
step: 9500, Loss: 0.11447934806346893
step: 9600, Loss: 0.11329685151576996
step: 9700, Loss: 0.11299770325422287
step: 9800, Loss: 0.1148870438337326
step: 9900, Loss: 0.11349396407604218
training successfully ended.
validating...
validate data length:31
acc: 0.9333333333333333
precision: 0.9444444444444444
recall: 0.9444444444444444
F_score: 0.9444444444444444
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.258608877658844
step: 100, Loss: 0.11718565225601196
step: 200, Loss: 0.11513767391443253
step: 300, Loss: 0.11572445929050446
step: 400, Loss: 0.11473122239112854
step: 500, Loss: 0.11407534778118134
step: 600, Loss: 0.1143098771572113
step: 700, Loss: 0.11312098801136017
step: 800, Loss: 0.11409758031368256
step: 900, Loss: 0.11404094099998474
step: 1000, Loss: 0.11458943784236908
step: 1100, Loss: 0.11301302909851074
step: 1200, Loss: 0.11388025432825089
step: 1300, Loss: 0.11390885710716248
step: 1400, Loss: 0.11416333168745041
step: 1500, Loss: 0.11316660791635513
step: 1600, Loss: 0.11323094367980957
step: 1700, Loss: 0.11323222517967224
step: 1800, Loss: 0.11323115229606628
step: 1900, Loss: 0.1141745001077652
step: 2000, Loss: 0.11428936570882797
step: 2100, Loss: 0.11390616744756699
step: 2200, Loss: 0.11343245208263397
step: 2300, Loss: 0.11669162660837173
step: 2400, Loss: 0.11415600776672363
step: 2500, Loss: 0.11336653679609299
step: 2600, Loss: 0.11355561017990112
step: 2700, Loss: 0.11321324110031128
step: 2800, Loss: 0.1131988987326622
step: 2900, Loss: 0.1140952929854393
step: 3000, Loss: 0.1145310029387474
step: 3100, Loss: 0.11392802000045776
step: 3200, Loss: 0.11404100060462952
step: 3300, Loss: 0.11375866830348969
step: 3400, Loss: 0.11349743604660034
step: 3500, Loss: 0.11436442285776138
step: 3600, Loss: 0.11310437321662903
step: 3700, Loss: 0.11375026404857635
step: 3800, Loss: 0.11354576796293259
step: 3900, Loss: 0.1142117977142334
step: 4000, Loss: 0.11345645785331726
step: 4100, Loss: 3.767240047454834
step: 4200, Loss: 0.12622925639152527
step: 4300, Loss: 0.11790098249912262
step: 4400, Loss: 0.11758062243461609
step: 4500, Loss: 0.1143704503774643
step: 4600, Loss: 0.11683095246553421
step: 4700, Loss: 0.11680372059345245
step: 4800, Loss: 0.11636707931756973
step: 4900, Loss: 0.11555331200361252
step: 5000, Loss: 0.11461970210075378
step: 5100, Loss: 0.1141153946518898
step: 5200, Loss: 0.11385379731655121
step: 5300, Loss: 0.11758247017860413
step: 5400, Loss: 0.114528588950634
step: 5500, Loss: 0.11381834745407104
step: 5600, Loss: 0.11300533264875412
step: 5700, Loss: 0.11547975242137909
step: 5800, Loss: 0.11502326279878616
step: 8400, Loss: 0.11558855324983597
step: 8500, Loss: 0.11588362604379654
step: 8600, Loss: 5.87572717666626
step: 8700, Loss: 0.16722255945205688
step: 8800, Loss: 0.14477795362472534
step: 8900, Loss: 0.3740459978580475
step: 9000, Loss: 0.13640807569026947
step: 9100, Loss: 0.13581009209156036
step: 9200, Loss: 0.1294899731874466
step: 9300, Loss: 0.1267634928226471
step: 9400, Loss: 0.12343785911798477
step: 9500, Loss: 0.12621524930000305
step: 9600, Loss: 0.12226331979036331
step: 9700, Loss: 0.12328556180000305
step: 9800, Loss: 0.120277538895607
step: 9900, Loss: 0.12551656365394592
training successfully ended.
validating...
validate data length:91
acc: 0.9659090909090909
precision: 0.9285714285714286
recall: 1.0
F_score: 0.962962962962963
subject 16 Avgacc: 0.9329545454545455 Avgfscore: 0.9361813369860114 
 Max acc:0.9886363636363636, Max f score:0.989010989010989
******** mix subject_17 ********

[342, 418]
******fold 1******

Training... train_data length:684
step: 0, Loss: 45.66311264038086
step: 100, Loss: 3.7053961753845215
step: 200, Loss: 0.5697689056396484
step: 300, Loss: 0.5309351682662964
step: 400, Loss: 0.1778111755847931
step: 500, Loss: 0.17574292421340942
step: 600, Loss: 0.17470407485961914
step: 700, Loss: 0.14923012256622314
step: 800, Loss: 0.16065531969070435
step: 900, Loss: 0.14621707797050476
step: 1000, Loss: 0.14144743978977203
step: 1100, Loss: 0.1385170817375183
step: 1200, Loss: 0.1361171156167984
step: 1300, Loss: 0.14400321245193481
step: 1400, Loss: 0.14325016736984253
step: 1500, Loss: 0.20695507526397705
step: 1600, Loss: 0.13030919432640076
step: 1700, Loss: 0.13565251231193542
step: 1800, Loss: 0.12043196707963943
step: 1900, Loss: 0.1315261274576187
step: 2000, Loss: 0.1310466080904007
step: 2100, Loss: 0.12418004870414734
step: 2200, Loss: 0.11973927170038223
step: 2300, Loss: 0.11993245780467987
step: 2400, Loss: 0.12067826092243195
step: 2500, Loss: 0.1298888474702835
step: 2600, Loss: 0.1197500228881836
step: 2700, Loss: 0.12693580985069275
step: 2800, Loss: 0.11879618465900421
step: 2900, Loss: 0.11944988369941711
step: 3000, Loss: 0.12137048691511154
step: 3100, Loss: 0.11729643493890762
step: 3200, Loss: 0.12295570224523544
step: 3300, Loss: 0.12254834175109863
step: 3400, Loss: 0.19794553518295288
step: 3500, Loss: 0.11759757995605469
step: 3600, Loss: 0.11754976212978363
step: 3700, Loss: 0.11479444801807404
step: 3800, Loss: 0.11473853886127472
step: 3900, Loss: 0.11768972128629684
step: 4000, Loss: 0.11822463572025299
step: 4100, Loss: 0.11553113907575607
step: 4200, Loss: 0.11436617374420166
step: 4300, Loss: 0.11561767011880875
step: 4400, Loss: 0.11919599771499634
step: 4500, Loss: 0.11449115723371506
step: 4600, Loss: 0.11694638431072235
step: 4700, Loss: 0.1169556975364685
step: 4800, Loss: 0.1166531965136528
step: 4900, Loss: 0.1158066838979721
step: 5000, Loss: 0.11574897170066833
step: 5100, Loss: 0.11468257755041122
step: 5200, Loss: 0.11468994617462158
step: 5300, Loss: 0.1915239840745926
step: 5400, Loss: 0.11561994254589081
step: 5500, Loss: 0.11609325557947159
step: 5600, Loss: 0.1160615086555481
step: 5700, Loss: 0.11696527153253555
step: 5800, Loss: 0.11418052017688751
step: 5900, Loss: 0.4987567961215973
step: 6000, Loss: 0.27170056104660034
step: 6100, Loss: 0.17099854350090027
step: 6200, Loss: 0.14608177542686462
step: 6300, Loss: 0.14484216272830963
step: 6400, Loss: 0.13366249203681946
step: 6500, Loss: 0.12976230680942535
step: 6600, Loss: 0.13321903347969055
step: 6700, Loss: 0.13430021703243256
step: 6800, Loss: 0.12448090314865112
step: 6900, Loss: 0.13035285472869873
step: 7000, Loss: 0.12490594387054443
step: 7100, Loss: 0.12577372789382935
step: 7200, Loss: 0.20449912548065186
step: 7300, Loss: 0.11817944049835205
step: 7400, Loss: 0.1185363233089447
step: 7500, Loss: 0.11908043920993805
step: 7600, Loss: 0.11896733194589615
step: 7700, Loss: 0.12157841771841049
step: 7800, Loss: 0.12135607749223709
step: 7900, Loss: 0.11968892812728882
step: 8000, Loss: 0.12025509774684906
step: 8100, Loss: 0.1172255277633667
step: 8200, Loss: 0.11847557127475739
step: 8300, Loss: 0.11565190553665161
step: 8400, Loss: 0.11589021235704422
step: 8500, Loss: 0.11679383367300034
step: 8600, Loss: 0.11716088652610779
step: 8700, Loss: 0.11602915823459625
step: 8800, Loss: 0.11689817905426025
step: 8900, Loss: 0.11595805734395981
step: 9000, Loss: 0.11603403836488724
step: 9100, Loss: 0.1970798522233963
step: 9200, Loss: 0.1144227534532547
step: 9300, Loss: 0.1152195930480957
step: 9400, Loss: 0.11298810690641403
step: 9500, Loss: 0.11508593708276749
step: 9600, Loss: 0.1151062473654747
step: 9700, Loss: 0.11394519358873367
step: 9800, Loss: 0.11601454019546509
step: 9900, Loss: 0.1153845340013504
training successfully ended.
validating...
validate data length:76
acc: 0.8055555555555556
precision: 0.6363636363636364
recall: 0.9130434782608695
F_score: 0.75
******fold 2******

Training... train_data length:684
step: 0, Loss: 3.2653586864471436
step: 100, Loss: 0.12807780504226685
step: 200, Loss: 0.1307601034641266
step: 300, Loss: 0.12071938812732697
step: 400, Loss: 0.1246669813990593
step: 500, Loss: 0.11801761388778687
step: 600, Loss: 0.11863932758569717
step: 700, Loss: 0.11705915629863739
step: 800, Loss: 0.1156778484582901
step: 900, Loss: 0.11547715216875076
step: 1000, Loss: 0.1177426278591156
step: 1100, Loss: 0.11602570116519928
step: 1200, Loss: 0.11520874500274658
step: 1300, Loss: 0.11809390038251877
step: 1400, Loss: 0.11492609977722168
step: 1500, Loss: 0.19956985116004944
step: 1600, Loss: 0.1127726137638092
step: 1700, Loss: 0.11364158987998962
step: 1800, Loss: 0.11722442507743835
step: 1900, Loss: 0.11377047002315521
step: 2000, Loss: 0.11420521885156631
step: 2100, Loss: 0.11376060545444489
step: 2200, Loss: 0.11482837796211243
step: 2300, Loss: 0.11440522223711014
step: 2400, Loss: 0.11323171108961105
step: 2500, Loss: 0.11362980306148529
step: 2600, Loss: 0.11469411849975586
step: 2700, Loss: 0.11347894370555878
step: 2800, Loss: 0.11716872453689575
step: 2900, Loss: 0.11534275114536285
step: 3000, Loss: 0.11422598361968994
step: 3100, Loss: 0.11557529121637344
step: 3200, Loss: 0.11692094802856445
step: 3300, Loss: 0.11776404827833176
step: 3400, Loss: 0.1954507827758789
step: 3500, Loss: 0.11639177799224854
step: 3600, Loss: 0.11461830139160156
step: 3700, Loss: 0.11571893095970154
step: 3800, Loss: 0.11725546419620514
step: 3900, Loss: 0.11607923358678818
step: 4000, Loss: 0.11435836553573608
step: 4100, Loss: 0.11653245985507965
step: 4200, Loss: 0.12051732838153839
step: 4300, Loss: 0.11833779513835907
step: 4400, Loss: 0.11448856443166733
step: 4500, Loss: 0.1167118102312088
step: 4600, Loss: 0.11401494592428207
step: 4700, Loss: 0.11559633165597916
step: 4800, Loss: 0.1182745173573494
step: 4900, Loss: 0.7014611959457397
step: 5000, Loss: 0.19122374057769775
step: 5100, Loss: 0.15554970502853394
step: 5200, Loss: 0.1349254697561264
step: 5300, Loss: 0.2060367912054062
step: 5400, Loss: 0.13148713111877441
step: 5500, Loss: 0.12326685339212418
step: 5600, Loss: 0.13156312704086304
step: 5700, Loss: 0.13191206753253937
step: 5800, Loss: 0.12507304549217224
step: 5900, Loss: 0.123487189412117
step: 6000, Loss: 0.12059739232063293
step: 6100, Loss: 0.1268598884344101
step: 6200, Loss: 0.12147775292396545
step: 6300, Loss: 0.120506651699543
step: 6400, Loss: 0.11908018589019775
step: 6500, Loss: 0.12117727100849152
step: 6600, Loss: 0.11619599163532257
step: 6700, Loss: 0.12269449234008789
step: 6800, Loss: 0.11973847448825836
step: 6900, Loss: 0.11614871770143509
step: 7000, Loss: 0.11519018560647964
step: 7100, Loss: 0.12038552761077881
step: 7200, Loss: 0.1956494003534317
step: 7300, Loss: 0.1148848831653595
step: 7400, Loss: 0.11752520501613617
step: 7500, Loss: 0.11625661700963974
step: 7600, Loss: 0.11748804152011871
step: 7700, Loss: 0.11489278823137283
step: 7800, Loss: 0.1155536025762558
step: 7900, Loss: 0.11582162976264954
step: 8000, Loss: 0.11724364757537842
step: 8100, Loss: 0.11578112840652466
step: 8200, Loss: 0.1154264509677887
step: 8300, Loss: 0.1153096929192543
step: 8400, Loss: 0.11487241089344025
step: 5900, Loss: 0.11573013663291931
step: 6000, Loss: 0.11350782960653305
step: 6100, Loss: 0.11382526904344559
step: 6200, Loss: 0.11351417750120163
step: 6300, Loss: 0.114463210105896
step: 6400, Loss: 0.11353016644716263
step: 6500, Loss: 0.11318556219339371
step: 6600, Loss: 0.11390785127878189
step: 6700, Loss: 0.1145302951335907
step: 6800, Loss: 0.1144498661160469
step: 6900, Loss: 0.11355534940958023
step: 7000, Loss: 0.11312401294708252
step: 7100, Loss: 0.11352092772722244
step: 7200, Loss: 0.11508487164974213
step: 7300, Loss: 0.11418819427490234
step: 7400, Loss: 0.1133582592010498
step: 7500, Loss: 0.11399774253368378
step: 7600, Loss: 0.1137918084859848
step: 7700, Loss: 0.11343377828598022
step: 7800, Loss: 0.11378712207078934
step: 7900, Loss: 0.1132780909538269
step: 8000, Loss: 0.11378153413534164
step: 8100, Loss: 0.11502835899591446
step: 8200, Loss: 0.11295321583747864
step: 8300, Loss: 0.11322153359651566
step: 8400, Loss: 0.11421307921409607
step: 8500, Loss: 0.11292929947376251
step: 8600, Loss: 0.1132734939455986
step: 8700, Loss: 0.11313151568174362
step: 8800, Loss: 0.11435290426015854
step: 8900, Loss: 0.11324373632669449
step: 9000, Loss: 0.11254758387804031
step: 9100, Loss: 0.11365901678800583
step: 9200, Loss: 0.11325706541538239
step: 9300, Loss: 0.11372450739145279
step: 9400, Loss: 0.11312036961317062
step: 9500, Loss: 0.1131349429488182
step: 9600, Loss: 0.11350292712450027
step: 9700, Loss: 0.11385548859834671
step: 9800, Loss: 0.11304517835378647
step: 9900, Loss: 0.11385948210954666
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.48477303981781006
step: 100, Loss: 0.11498497426509857
step: 200, Loss: 0.11490215361118317
step: 300, Loss: 0.11363179981708527
step: 400, Loss: 0.11352090537548065
step: 500, Loss: 0.11472243815660477
step: 600, Loss: 0.11318020522594452
step: 700, Loss: 0.11367113143205643
step: 800, Loss: 0.11284267157316208
step: 900, Loss: 0.11382382363080978
step: 1000, Loss: 0.11355338990688324
step: 1100, Loss: 0.11372486501932144
step: 1200, Loss: 0.11417853832244873
step: 1300, Loss: 0.11408528685569763
step: 1400, Loss: 0.11486171931028366
step: 1500, Loss: 0.11247795820236206
step: 1600, Loss: 0.1138385757803917
step: 1700, Loss: 0.11333687603473663
step: 1800, Loss: 0.11350798606872559
step: 1900, Loss: 0.11376229673624039
step: 2000, Loss: 0.11289243400096893
step: 2100, Loss: 0.11336524784564972
step: 2200, Loss: 0.11376509070396423
step: 2300, Loss: 0.11444880068302155
step: 2400, Loss: 0.11371840536594391
step: 2500, Loss: 0.11344288289546967
step: 2600, Loss: 0.11479072272777557
step: 2700, Loss: 0.11340596526861191
step: 2800, Loss: 0.11341498792171478
step: 2900, Loss: 0.11401781439781189
step: 3000, Loss: 0.11388372629880905
step: 3100, Loss: 0.1134277731180191
step: 3200, Loss: 0.11327320337295532
step: 3300, Loss: 0.11277861893177032
step: 3400, Loss: 0.11378006637096405
step: 3500, Loss: 0.11375151574611664
step: 3600, Loss: 0.11379870772361755
step: 3700, Loss: 0.1133020743727684
step: 3800, Loss: 2.9528133869171143
step: 3900, Loss: 0.1239747554063797
step: 4000, Loss: 0.12285054475069046
step: 4100, Loss: 0.11960181593894958
step: 4200, Loss: 0.1188700944185257
step: 4300, Loss: 0.1168770045042038
step: 4400, Loss: 0.1189851313829422
step: 4500, Loss: 0.11882225424051285
step: 4600, Loss: 0.11863682419061661
step: 4700, Loss: 0.11570627987384796
step: 4800, Loss: 0.11377181112766266
step: 4900, Loss: 0.11451926827430725
step: 5000, Loss: 0.11561819911003113
step: 5100, Loss: 0.11416429281234741
step: 5200, Loss: 0.1158873438835144
step: 5300, Loss: 0.11454401910305023
step: 5400, Loss: 0.1154823750257492
step: 5500, Loss: 0.11429014801979065
step: 5600, Loss: 0.11717924475669861
step: 5700, Loss: 0.11367151886224747
step: 5800, Loss: 0.11699676513671875
step: 5900, Loss: 0.11485429108142853
step: 6000, Loss: 0.11378660798072815
step: 6100, Loss: 0.11374340206384659
step: 6200, Loss: 0.11811667680740356
step: 6300, Loss: 0.11400264501571655
step: 6400, Loss: 0.1154542788863182
step: 6500, Loss: 0.11428632587194443
step: 6600, Loss: 0.11673922091722488
step: 6700, Loss: 0.11387456953525543
step: 6800, Loss: 0.11892570555210114
step: 6900, Loss: 0.11390195041894913
step: 7000, Loss: 0.1143331527709961
step: 7100, Loss: 0.11350513994693756
step: 7200, Loss: 0.11366245150566101
step: 7300, Loss: 0.11324118077754974
step: 7400, Loss: 0.11369706690311432
step: 7500, Loss: 0.11274406313896179
step: 7600, Loss: 0.11382874846458435
step: 7700, Loss: 0.11431249976158142
step: 7800, Loss: 0.11369520425796509
step: 7900, Loss: 0.11410188674926758
step: 8000, Loss: 0.11532267928123474
step: 8100, Loss: 0.11354444921016693
step: 8200, Loss: 0.11314677447080612
step: 8300, Loss: 0.11422836780548096
step: 8400, Loss: 0.11368734389543533
step: 8500, Loss: 0.11446744948625565
step: 8600, Loss: 0.1136009618639946
step: 8700, Loss: 0.11332228779792786
step: 8800, Loss: 0.11333218961954117
step: 8900, Loss: 0.11457636952400208
step: 9000, Loss: 0.11380883306264877
step: 9100, Loss: 0.11328605562448502
step: 9200, Loss: 0.11331204324960709
step: 9300, Loss: 0.11377225071191788
step: 9400, Loss: 0.11309334635734558
step: 9500, Loss: 0.11362427473068237
step: 9600, Loss: 0.11466566473245621
step: 9700, Loss: 0.1135454773902893
step: 9800, Loss: 0.1133023127913475
step: 9900, Loss: 0.1137026697397232
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8823529411764706
recall: 0.8823529411764706
F_score: 0.8823529411764706
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.38980090618133545
step: 100, Loss: 0.11451093852519989
step: 200, Loss: 0.11355943977832794
step: 300, Loss: 0.11493085324764252
step: 400, Loss: 0.11312496662139893
step: 500, Loss: 0.11420992016792297
step: 600, Loss: 0.11320246011018753
step: 700, Loss: 0.113180011510849
step: 800, Loss: 0.11448434740304947
step: 900, Loss: 0.11410883814096451
step: 1000, Loss: 0.11430719494819641
step: 1100, Loss: 0.11280684918165207
step: 1200, Loss: 0.11354251950979233
step: 1300, Loss: 0.11314768344163895
step: 1400, Loss: 0.11397872120141983
step: 1500, Loss: 0.1135399118065834
step: 1600, Loss: 0.11374080926179886
step: 1700, Loss: 0.11463294923305511
step: 1800, Loss: 0.11331441253423691
step: 1900, Loss: 0.11299921572208405
step: 2000, Loss: 0.11366017907857895
step: 2100, Loss: 0.11387807130813599
step: 2200, Loss: 0.11334854364395142
step: 2300, Loss: 0.11404303461313248
step: 2400, Loss: 0.11377191543579102
step: 2500, Loss: 0.11410891264677048
step: 2600, Loss: 0.11407085508108139
step: 2700, Loss: 0.11283691227436066
step: 2800, Loss: 0.1139419823884964
step: 2900, Loss: 0.11399323493242264
step: 3000, Loss: 0.11297791451215744
step: 3100, Loss: 0.11367956548929214
step: 3200, Loss: 0.11329983174800873
step: 3300, Loss: 0.11431905627250671
step: 3400, Loss: 0.1134413629770279
step: 3500, Loss: 0.2632746696472168
step: 3600, Loss: 0.13161437213420868
step: 3700, Loss: 0.12010909616947174
step: 3800, Loss: 0.12095671892166138
step: 3900, Loss: 0.12348265945911407
step: 4000, Loss: 0.11589211970567703
step: 4100, Loss: 0.11533202975988388
step: 4200, Loss: 0.11444271355867386
step: 4300, Loss: 0.11841223388910294
step: 4400, Loss: 0.11333921551704407
step: 4500, Loss: 0.11591200530529022
step: 4600, Loss: 0.11556922644376755
step: 4700, Loss: 0.11479261517524719
step: 4800, Loss: 0.11622030287981033
step: 4900, Loss: 0.11526288092136383
step: 5000, Loss: 0.11497890949249268
step: 5100, Loss: 0.11537397652864456
step: 5200, Loss: 0.11441561579704285
step: 5300, Loss: 0.11425062268972397
step: 5400, Loss: 0.11394491791725159
step: 5500, Loss: 0.11491386592388153
step: 5600, Loss: 0.11444157361984253
step: 5700, Loss: 0.1136421486735344
step: 5800, Loss: 0.11568057537078857
step: 5900, Loss: 0.11496981978416443
step: 6000, Loss: 0.11482156068086624
step: 6100, Loss: 0.11317019909620285
step: 6200, Loss: 0.11490362882614136
step: 6300, Loss: 0.11370061337947845
step: 6400, Loss: 0.11477898061275482
step: 8500, Loss: 0.11340636014938354
step: 8600, Loss: 0.114675372838974
step: 8700, Loss: 0.11399257183074951
step: 8800, Loss: 0.11290809512138367
step: 8900, Loss: 0.11409303545951843
step: 9000, Loss: 0.11463279277086258
step: 9100, Loss: 0.19433337450027466
step: 9200, Loss: 0.11393991857767105
step: 9300, Loss: 0.11360850930213928
step: 9400, Loss: 0.11357465386390686
step: 9500, Loss: 0.1130104660987854
step: 9600, Loss: 0.11336466670036316
step: 9700, Loss: 0.11405492573976517
step: 9800, Loss: 0.11347933113574982
step: 9900, Loss: 0.11563694477081299
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.918918918918919
recall: 0.9444444444444444
F_score: 0.9315068493150684
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.7047004699707031
step: 100, Loss: 0.1276698112487793
step: 200, Loss: 0.11730922013521194
step: 300, Loss: 0.11806626617908478
step: 400, Loss: 0.11668306589126587
step: 500, Loss: 0.11758959293365479
step: 600, Loss: 0.11482366919517517
step: 700, Loss: 0.11720647662878036
step: 800, Loss: 0.11458580195903778
step: 900, Loss: 0.1140608936548233
step: 1000, Loss: 0.11440073698759079
step: 1100, Loss: 0.11580778658390045
step: 1200, Loss: 0.1131642535328865
step: 1300, Loss: 0.11397465318441391
step: 1400, Loss: 0.11355206370353699
step: 1500, Loss: 0.19090065360069275
step: 1600, Loss: 0.11468833684921265
step: 1700, Loss: 0.1140129491686821
step: 1800, Loss: 0.11340919137001038
step: 1900, Loss: 0.11716635525226593
step: 2000, Loss: 0.11283279210329056
step: 2100, Loss: 0.11343352496623993
step: 2200, Loss: 0.11448227614164352
step: 2300, Loss: 0.1136854737997055
step: 2400, Loss: 0.11348339170217514
step: 2500, Loss: 0.11449190974235535
step: 2600, Loss: 0.11586283892393112
step: 2700, Loss: 0.11672235280275345
step: 2800, Loss: 0.11409404128789902
step: 2900, Loss: 0.11453350633382797
step: 3000, Loss: 0.11606727540493011
step: 3100, Loss: 0.11494552344083786
step: 3200, Loss: 0.1152098998427391
step: 3300, Loss: 0.11542440205812454
step: 3400, Loss: 0.1949402093887329
step: 3500, Loss: 0.11410143971443176
step: 3600, Loss: 0.1138170138001442
step: 3700, Loss: 0.11548527330160141
step: 3800, Loss: 0.11811570823192596
step: 3900, Loss: 0.11587740480899811
step: 4000, Loss: 0.11436807364225388
step: 4100, Loss: 0.11559173464775085
step: 4200, Loss: 0.11536656320095062
step: 4300, Loss: 0.11883589625358582
step: 4400, Loss: 0.4472329020500183
step: 4500, Loss: 0.13404157757759094
step: 4600, Loss: 0.13844439387321472
step: 4700, Loss: 0.1369330883026123
step: 4800, Loss: 0.12552326917648315
step: 4900, Loss: 0.12539786100387573
step: 5000, Loss: 0.12585245072841644
step: 5100, Loss: 0.12973393499851227
step: 5200, Loss: 0.12265993654727936
step: 5300, Loss: 0.20246006548404694
step: 5400, Loss: 0.11951065063476562
step: 5500, Loss: 0.11563698947429657
step: 5600, Loss: 0.11686843633651733
step: 5700, Loss: 0.1164514422416687
step: 5800, Loss: 0.11811888217926025
step: 5900, Loss: 0.11562399566173553
step: 6000, Loss: 0.11875469982624054
step: 6100, Loss: 0.11827083677053452
step: 6200, Loss: 0.11607244610786438
step: 6300, Loss: 0.11584217846393585
step: 6400, Loss: 0.11530375480651855
step: 6500, Loss: 0.11654491722583771
step: 6600, Loss: 0.11776027828454971
step: 6700, Loss: 0.11822137236595154
step: 6800, Loss: 0.11624360829591751
step: 6900, Loss: 0.11587965488433838
step: 7000, Loss: 0.11860636621713638
step: 7100, Loss: 0.11625266820192337
step: 7200, Loss: 0.19561898708343506
step: 7300, Loss: 0.11426845192909241
step: 7400, Loss: 0.11355127394199371
step: 7500, Loss: 0.11447779834270477
step: 7600, Loss: 0.11423812806606293
step: 7700, Loss: 0.1156306192278862
step: 7800, Loss: 0.11504803597927094
step: 7900, Loss: 0.11515004187822342
step: 8000, Loss: 0.1152152270078659
step: 8100, Loss: 0.11354083567857742
step: 8200, Loss: 0.11308421939611435
step: 8300, Loss: 0.11398578435182571
step: 8400, Loss: 0.1133032888174057
step: 8500, Loss: 0.11470400542020798
step: 8600, Loss: 0.1152842789888382
step: 8700, Loss: 0.11374572664499283
step: 8800, Loss: 0.11417027562856674
step: 8900, Loss: 0.11402752250432968
step: 9000, Loss: 0.11514793336391449
step: 9100, Loss: 0.19277459383010864
step: 9200, Loss: 0.1134272888302803
step: 9300, Loss: 0.1134820207953453
step: 9400, Loss: 0.11295592039823532
step: 9500, Loss: 0.11466995626688004
step: 9600, Loss: 0.1157139390707016
step: 9700, Loss: 0.11390021443367004
step: 9800, Loss: 0.11457628011703491
step: 9900, Loss: 0.11503499746322632
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.975
recall: 0.975
F_score: 0.975
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.11589361727237701
step: 100, Loss: 0.12266051769256592
step: 200, Loss: 0.11742851138114929
step: 300, Loss: 0.11403405666351318
step: 400, Loss: 0.11521684378385544
step: 500, Loss: 0.1149078905582428
step: 600, Loss: 0.11481159925460815
step: 700, Loss: 0.11480957269668579
step: 800, Loss: 0.115175262093544
step: 900, Loss: 0.11511804163455963
step: 1000, Loss: 0.11806120723485947
step: 1100, Loss: 0.11403955519199371
step: 1200, Loss: 0.1136816143989563
step: 1300, Loss: 0.11295104026794434
step: 1400, Loss: 0.11395630240440369
step: 1500, Loss: 0.1913132667541504
step: 1600, Loss: 0.11404454708099365
step: 1700, Loss: 0.11284227669239044
step: 1800, Loss: 0.11243054270744324
step: 1900, Loss: 0.11495032906532288
step: 2000, Loss: 0.1144239604473114
step: 2100, Loss: 0.11245232075452805
step: 2200, Loss: 0.11438087373971939
step: 2300, Loss: 0.11323206126689911
step: 2400, Loss: 0.11486026644706726
step: 2500, Loss: 0.1133236363530159
step: 2600, Loss: 0.11317548155784607
step: 2700, Loss: 0.11406544595956802
step: 2800, Loss: 0.11643238365650177
step: 2900, Loss: 0.11480452865362167
step: 3000, Loss: 0.11565686762332916
step: 3100, Loss: 0.11337240040302277
step: 3200, Loss: 0.11461783200502396
step: 3300, Loss: 0.11639729142189026
step: 3400, Loss: 0.1924045979976654
step: 3500, Loss: 0.11371199041604996
step: 3600, Loss: 0.11311256885528564
step: 3700, Loss: 0.12075736373662949
step: 3800, Loss: 1.0012190341949463
step: 3900, Loss: 0.14153526723384857
step: 4000, Loss: 0.13576316833496094
step: 4100, Loss: 0.1283515989780426
step: 4200, Loss: 0.12421771883964539
step: 4300, Loss: 0.11951905488967896
step: 4400, Loss: 0.12490533292293549
step: 4500, Loss: 0.12081333249807358
step: 4600, Loss: 0.11832945048809052
step: 4700, Loss: 0.1187523603439331
step: 4800, Loss: 0.11555777490139008
step: 4900, Loss: 0.11783367395401001
step: 5000, Loss: 0.11856167018413544
step: 5100, Loss: 0.11642112582921982
step: 5200, Loss: 0.11867707222700119
step: 5300, Loss: 0.198365718126297
step: 5400, Loss: 0.1185411661863327
step: 5500, Loss: 0.115555539727211
step: 5600, Loss: 0.11799340695142746
step: 5700, Loss: 0.11572122573852539
step: 5800, Loss: 0.115163654088974
step: 5900, Loss: 0.1188536137342453
step: 6000, Loss: 0.11606065928936005
step: 6100, Loss: 0.11552464962005615
step: 6200, Loss: 0.11471852660179138
step: 6300, Loss: 0.1170658990740776
step: 6400, Loss: 0.11526020616292953
step: 6500, Loss: 0.11499755084514618
step: 6600, Loss: 0.11594543606042862
step: 6700, Loss: 0.11328752338886261
step: 6800, Loss: 0.11431719362735748
step: 6900, Loss: 0.11458761990070343
step: 7000, Loss: 0.1134452149271965
step: 7100, Loss: 0.11425382643938065
step: 7200, Loss: 0.19256636500358582
step: 7300, Loss: 0.11365866661071777
step: 7400, Loss: 0.11497848480939865
step: 7500, Loss: 0.11384359747171402
step: 7600, Loss: 0.11360201984643936
step: 7700, Loss: 0.1150970384478569
step: 7800, Loss: 0.11471664905548096
step: 7900, Loss: 0.11293652653694153
step: 8000, Loss: 0.11378054320812225
step: 8100, Loss: 0.11285625398159027
step: 8200, Loss: 0.11366844177246094
step: 8300, Loss: 0.1146279126405716
step: 8400, Loss: 0.11386795341968536
step: 8500, Loss: 0.11353883892297745
step: 8600, Loss: 0.11596782505512238
step: 8700, Loss: 0.11412941664457321
step: 8800, Loss: 0.11454784870147705
step: 8900, Loss: 0.1126859188079834
step: 9000, Loss: 0.11401038616895676
step: 6500, Loss: 0.11380553245544434
step: 6600, Loss: 0.11498835682868958
step: 6700, Loss: 0.11523456126451492
step: 6800, Loss: 0.11325687170028687
step: 6900, Loss: 0.11403141915798187
step: 7000, Loss: 0.11413366347551346
step: 7100, Loss: 0.11356722563505173
step: 7200, Loss: 0.11454011499881744
step: 7300, Loss: 0.11275646090507507
step: 7400, Loss: 0.11439639329910278
step: 7500, Loss: 0.11458539962768555
step: 7600, Loss: 0.11365128308534622
step: 7700, Loss: 0.11376440525054932
step: 7800, Loss: 0.11305859684944153
step: 7900, Loss: 0.11478158086538315
step: 8000, Loss: 0.11361422389745712
step: 8100, Loss: 0.11266137659549713
step: 8200, Loss: 0.1133451983332634
step: 8300, Loss: 0.11319024115800858
step: 8400, Loss: 0.11645010113716125
step: 8500, Loss: 0.11275888234376907
step: 8600, Loss: 0.11365456134080887
step: 8700, Loss: 0.11416441947221756
step: 8800, Loss: 0.11295399069786072
step: 8900, Loss: 0.11438041925430298
step: 9000, Loss: 0.11521904915571213
step: 9100, Loss: 0.11269209533929825
step: 9200, Loss: 0.11344616860151291
step: 9300, Loss: 0.1134747564792633
step: 9400, Loss: 0.11278551816940308
step: 9500, Loss: 0.11429055780172348
step: 9600, Loss: 0.11404621601104736
step: 9700, Loss: 0.114155612885952
step: 9800, Loss: 0.11661282181739807
step: 9900, Loss: 0.11412926018238068
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8235294117647058
recall: 0.9333333333333333
F_score: 0.8749999999999999
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.3139876425266266
step: 100, Loss: 0.11702527105808258
step: 200, Loss: 0.11614146828651428
step: 300, Loss: 0.11350418627262115
step: 400, Loss: 0.11364948004484177
step: 500, Loss: 0.11437606811523438
step: 600, Loss: 0.11495773494243622
step: 700, Loss: 0.1133614033460617
step: 800, Loss: 0.11324582993984222
step: 900, Loss: 0.1134454682469368
step: 1000, Loss: 0.11323662847280502
step: 1100, Loss: 0.11611762642860413
step: 1200, Loss: 0.1128862127661705
step: 1300, Loss: 0.11335153877735138
step: 1400, Loss: 0.11300890892744064
step: 1500, Loss: 0.1134566068649292
step: 1600, Loss: 0.113242506980896
step: 1700, Loss: 0.11450614780187607
step: 1800, Loss: 0.11447747051715851
step: 1900, Loss: 0.11381001770496368
step: 2000, Loss: 0.1135084480047226
step: 2100, Loss: 0.11331094056367874
step: 2200, Loss: 0.11294560134410858
step: 2300, Loss: 0.11672458052635193
step: 2400, Loss: 0.11393401026725769
step: 2500, Loss: 0.11249838769435883
step: 2600, Loss: 0.11443090438842773
step: 2700, Loss: 1.5697273015975952
step: 2800, Loss: 0.15092362463474274
step: 2900, Loss: 0.12821397185325623
step: 3000, Loss: 0.1272420734167099
step: 3100, Loss: 0.12328988313674927
step: 3200, Loss: 0.11904337257146835
step: 3300, Loss: 0.1200554221868515
step: 3400, Loss: 0.11611521244049072
step: 3500, Loss: 0.11640841513872147
step: 3600, Loss: 0.11810170114040375
step: 3700, Loss: 0.117306649684906
step: 3800, Loss: 0.11541488021612167
step: 3900, Loss: 0.11707883328199387
step: 4000, Loss: 0.11617244780063629
step: 4100, Loss: 0.11370711028575897
step: 4200, Loss: 0.11554527282714844
step: 4300, Loss: 0.1158197894692421
step: 4400, Loss: 0.11455409228801727
step: 4500, Loss: 0.11509428918361664
step: 4600, Loss: 0.11475242674350739
step: 4700, Loss: 0.11427588760852814
step: 4800, Loss: 0.11504495143890381
step: 4900, Loss: 0.1136343777179718
step: 5000, Loss: 0.11363355070352554
step: 5100, Loss: 0.11510511487722397
step: 5200, Loss: 0.11655104905366898
step: 5300, Loss: 0.11435804516077042
step: 5400, Loss: 0.11384057998657227
step: 5500, Loss: 0.11645859479904175
step: 5600, Loss: 0.11363564431667328
step: 5700, Loss: 0.1165279820561409
step: 5800, Loss: 0.11310605704784393
step: 5900, Loss: 0.1128830760717392
step: 6000, Loss: 0.11441308259963989
step: 6100, Loss: 0.11503490805625916
step: 6200, Loss: 0.11397426575422287
step: 6300, Loss: 0.1166309267282486
step: 6400, Loss: 0.11392233520746231
step: 6500, Loss: 0.11304015666246414
step: 6600, Loss: 0.11418943852186203
step: 6700, Loss: 0.11500092595815659
step: 6800, Loss: 0.1150909960269928
step: 6900, Loss: 0.11407767981290817
step: 7000, Loss: 0.11290260404348373
step: 7100, Loss: 0.11590883135795593
step: 7200, Loss: 0.11304391920566559
step: 7300, Loss: 0.11272577196359634
step: 7400, Loss: 0.11396527290344238
step: 7500, Loss: 0.11355599761009216
step: 7600, Loss: 0.11359530687332153
step: 7700, Loss: 0.11606401950120926
step: 7800, Loss: 0.11333578824996948
step: 7900, Loss: 0.11363033950328827
step: 8000, Loss: 0.11496336758136749
step: 8100, Loss: 0.1138746589422226
step: 8200, Loss: 0.1137014850974083
step: 8300, Loss: 0.11425355821847916
step: 8400, Loss: 0.11471680551767349
step: 8500, Loss: 0.11352471262216568
step: 8600, Loss: 0.11268855631351471
step: 8700, Loss: 0.11523126065731049
step: 8800, Loss: 0.11426740884780884
step: 8900, Loss: 0.11336703598499298
step: 9000, Loss: 0.1135011613368988
step: 9100, Loss: 0.11408281326293945
step: 9200, Loss: 0.11280691623687744
step: 9300, Loss: 0.11307186633348465
step: 9400, Loss: 0.11419045180082321
step: 9500, Loss: 0.1141178235411644
step: 9600, Loss: 0.11360768228769302
step: 9700, Loss: 0.11493757367134094
step: 9800, Loss: 0.11284740269184113
step: 9900, Loss: 0.11392243206501007
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8888888888888888
recall: 0.9411764705882353
F_score: 0.9142857142857143
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.37633776664733887
step: 100, Loss: 0.11365751922130585
step: 200, Loss: 0.11320192366838455
step: 300, Loss: 0.11528293788433075
step: 400, Loss: 0.1138308197259903
step: 500, Loss: 0.11327976733446121
step: 600, Loss: 0.11325337737798691
step: 700, Loss: 0.11372929066419601
step: 800, Loss: 0.11527509242296219
step: 900, Loss: 0.11251869797706604
step: 1000, Loss: 0.11380445957183838
step: 1100, Loss: 0.11293083429336548
step: 1200, Loss: 0.11391698569059372
step: 1300, Loss: 0.11439943313598633
step: 1400, Loss: 0.11369156837463379
step: 1500, Loss: 0.11297821253538132
step: 1600, Loss: 0.11412864923477173
step: 1700, Loss: 0.11361488699913025
step: 1800, Loss: 0.11303643882274628
step: 1900, Loss: 0.11285749077796936
step: 2000, Loss: 0.11340239644050598
step: 2100, Loss: 0.11378154158592224
step: 2200, Loss: 0.11367721110582352
step: 2300, Loss: 0.11413131654262543
step: 2400, Loss: 0.11316361278295517
step: 2500, Loss: 0.1131259948015213
step: 2600, Loss: 0.11281237006187439
step: 2700, Loss: 0.11457870900630951
step: 2800, Loss: 0.11373236775398254
step: 2900, Loss: 0.11354334652423859
step: 3000, Loss: 0.11316772550344467
step: 3100, Loss: 0.11490066349506378
step: 3200, Loss: 0.11280298233032227
step: 3300, Loss: 0.11457647383213043
step: 3400, Loss: 0.16175569593906403
step: 3500, Loss: 0.13049939274787903
step: 3600, Loss: 0.12110196053981781
step: 3700, Loss: 0.11814932525157928
step: 3800, Loss: 0.12236205488443375
step: 3900, Loss: 0.1193200945854187
step: 4000, Loss: 0.11622532457113266
step: 4100, Loss: 0.11712578684091568
step: 4200, Loss: 0.11443325132131577
step: 4300, Loss: 0.11589276045560837
step: 4400, Loss: 0.11716274917125702
step: 4500, Loss: 0.11903855204582214
step: 4600, Loss: 0.11539939790964127
step: 4700, Loss: 0.11542265862226486
step: 4800, Loss: 0.11451566219329834
step: 4900, Loss: 0.11383738368749619
step: 5000, Loss: 0.11367405951023102
step: 5100, Loss: 0.11469975858926773
step: 5200, Loss: 0.1182149201631546
step: 5300, Loss: 0.11390330642461777
step: 5400, Loss: 0.11419501900672913
step: 5500, Loss: 0.11645875126123428
step: 5600, Loss: 0.11565645784139633
step: 5700, Loss: 0.11536400020122528
step: 5800, Loss: 0.1143394485116005
step: 5900, Loss: 0.11392934620380402
step: 6000, Loss: 0.1140357181429863
step: 6100, Loss: 0.11503858864307404
step: 6200, Loss: 0.11355661600828171
step: 6300, Loss: 0.11459799110889435
step: 6400, Loss: 0.11422915011644363
step: 6500, Loss: 0.11626662313938141
step: 6600, Loss: 0.11364791542291641
step: 6700, Loss: 0.1143885999917984
step: 6800, Loss: 0.11417263001203537
step: 6900, Loss: 0.11413000524044037
step: 9100, Loss: 0.19267940521240234
step: 9200, Loss: 0.11334224790334702
step: 9300, Loss: 0.11322440952062607
step: 9400, Loss: 0.11334751546382904
step: 9500, Loss: 0.11359226703643799
step: 9600, Loss: 0.11406996101140976
step: 9700, Loss: 0.11588890850543976
step: 9800, Loss: 0.11334717273712158
step: 9900, Loss: 0.11366530507802963
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9696969696969697
recall: 1.0
F_score: 0.9846153846153847
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.18040838837623596
step: 100, Loss: 0.12025673687458038
step: 200, Loss: 0.11763065308332443
step: 300, Loss: 0.1150042712688446
step: 400, Loss: 0.11592010408639908
step: 500, Loss: 0.11345718055963516
step: 600, Loss: 0.11575040221214294
step: 700, Loss: 0.11528760939836502
step: 800, Loss: 0.11456191539764404
step: 900, Loss: 0.11288763582706451
step: 1000, Loss: 0.11310730129480362
step: 1100, Loss: 0.11343592405319214
step: 1200, Loss: 0.1141267642378807
step: 1300, Loss: 0.11293159425258636
step: 1400, Loss: 0.1134365126490593
step: 1500, Loss: 0.19178958237171173
step: 1600, Loss: 0.11312651634216309
step: 1700, Loss: 0.11353910714387894
step: 1800, Loss: 0.11425802111625671
step: 1900, Loss: 0.11432459950447083
step: 2000, Loss: 0.11390506476163864
step: 2100, Loss: 0.11369198560714722
step: 2200, Loss: 0.11510278284549713
step: 2300, Loss: 0.11352350562810898
step: 2400, Loss: 0.11448200047016144
step: 2500, Loss: 0.11340510100126266
step: 2600, Loss: 0.11410046368837357
step: 2700, Loss: 0.11429807543754578
step: 2800, Loss: 0.11466936767101288
step: 2900, Loss: 0.11368853598833084
step: 3000, Loss: 0.11331631988286972
step: 3100, Loss: 0.11399322003126144
step: 3200, Loss: 0.11406926810741425
step: 3300, Loss: 0.11345252394676208
step: 3400, Loss: 0.1947653889656067
step: 3500, Loss: 0.11356381326913834
step: 3600, Loss: 0.11524265259504318
step: 3700, Loss: 0.113886758685112
step: 3800, Loss: 0.11434599757194519
step: 3900, Loss: 0.11473334580659866
step: 4000, Loss: 0.11560140550136566
step: 4100, Loss: 0.11362749338150024
step: 4200, Loss: 1.674991250038147
step: 4300, Loss: 0.21675582230091095
step: 4400, Loss: 0.14387740194797516
step: 4500, Loss: 0.14294542372226715
step: 4600, Loss: 0.127482071518898
step: 4700, Loss: 0.12432460486888885
step: 4800, Loss: 0.13163767755031586
step: 4900, Loss: 0.12234710156917572
step: 5000, Loss: 0.12157896906137466
step: 5100, Loss: 0.1246212050318718
step: 5200, Loss: 0.12094268202781677
step: 5300, Loss: 0.20922712981700897
step: 5400, Loss: 0.12001575529575348
step: 5500, Loss: 0.11704269796609879
step: 5600, Loss: 0.11909503489732742
step: 5700, Loss: 0.11951164156198502
step: 5800, Loss: 0.11655986309051514
step: 5900, Loss: 0.1165715828537941
step: 6000, Loss: 0.11783856153488159
step: 6100, Loss: 0.11768356710672379
step: 6200, Loss: 0.11582108587026596
step: 6300, Loss: 0.11525431275367737
step: 6400, Loss: 0.11422940343618393
step: 6500, Loss: 0.11623033136129379
step: 6600, Loss: 0.11673127114772797
step: 6700, Loss: 0.11681496351957321
step: 6800, Loss: 0.11387109011411667
step: 6900, Loss: 0.11446667462587357
step: 7000, Loss: 0.1133568212389946
step: 7100, Loss: 0.11517229676246643
step: 7200, Loss: 0.1944439709186554
step: 7300, Loss: 0.11438722163438797
step: 7400, Loss: 0.11370376497507095
step: 7500, Loss: 0.11487285792827606
step: 7600, Loss: 0.11656247079372406
step: 7700, Loss: 0.11506016552448273
step: 7800, Loss: 0.11437278985977173
step: 7900, Loss: 0.11398133635520935
step: 8000, Loss: 0.11426085233688354
step: 8100, Loss: 0.11475725471973419
step: 8200, Loss: 0.11473800241947174
step: 8300, Loss: 0.11455722898244858
step: 8400, Loss: 0.11411221325397491
step: 8500, Loss: 0.11452769488096237
step: 8600, Loss: 0.11367865651845932
step: 8700, Loss: 0.11426088213920593
step: 8800, Loss: 0.11487812548875809
step: 8900, Loss: 0.11439565569162369
step: 9000, Loss: 0.11407356709241867
step: 9100, Loss: 0.19488921761512756
step: 9200, Loss: 0.11434228718280792
step: 9300, Loss: 0.11385288089513779
step: 9400, Loss: 0.11575208604335785
step: 9500, Loss: 0.11318057775497437
step: 9600, Loss: 0.1139627993106842
step: 9700, Loss: 0.11557496339082718
step: 9800, Loss: 0.11320963501930237
step: 9900, Loss: 0.1139717698097229
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.8857142857142857
recall: 1.0
F_score: 0.9393939393939393
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.11758497357368469
step: 100, Loss: 0.11678983271121979
step: 200, Loss: 0.11584536731243134
step: 300, Loss: 0.11420033872127533
step: 400, Loss: 0.11462253332138062
step: 500, Loss: 0.11464192718267441
step: 600, Loss: 0.11464837938547134
step: 700, Loss: 0.1137731671333313
step: 800, Loss: 0.1136133149266243
step: 900, Loss: 0.11389198899269104
step: 1000, Loss: 0.1140669733285904
step: 1100, Loss: 0.11417315900325775
step: 1200, Loss: 0.11373547464609146
step: 1300, Loss: 0.1147201731801033
step: 1400, Loss: 0.11446992307901382
step: 1500, Loss: 0.19098171591758728
step: 1600, Loss: 0.11264428496360779
step: 1700, Loss: 0.11454712599515915
step: 1800, Loss: 0.1138467937707901
step: 1900, Loss: 0.11457851529121399
step: 2000, Loss: 0.11287088692188263
step: 2100, Loss: 0.11417002975940704
step: 2200, Loss: 0.11284975707530975
step: 2300, Loss: 0.11406966298818588
step: 2400, Loss: 0.11318203806877136
step: 2500, Loss: 0.11378379166126251
step: 2600, Loss: 0.11321429908275604
step: 2700, Loss: 0.11333440244197845
step: 2800, Loss: 0.11293558031320572
step: 2900, Loss: 0.11353176832199097
step: 3000, Loss: 0.11451005935668945
step: 3100, Loss: 0.11430223286151886
step: 3200, Loss: 0.11386154592037201
step: 3300, Loss: 0.11396396160125732
step: 3400, Loss: 0.19258032739162445
step: 3500, Loss: 0.11218570172786713
step: 3600, Loss: 0.11513097584247589
step: 3700, Loss: 0.11674705147743225
step: 3800, Loss: 0.11530116200447083
step: 3900, Loss: 0.11616675555706024
step: 4000, Loss: 4.64483642578125
step: 4100, Loss: 0.1562041938304901
step: 4200, Loss: 0.14671337604522705
step: 4300, Loss: 0.12493856251239777
step: 4400, Loss: 0.12698400020599365
step: 4500, Loss: 0.12424419075250626
step: 4600, Loss: 0.1256517916917801
step: 4700, Loss: 0.11757928133010864
step: 4800, Loss: 0.11763429641723633
step: 4900, Loss: 0.12115099281072617
step: 5000, Loss: 0.1169997900724411
step: 5100, Loss: 0.1191020980477333
step: 5200, Loss: 0.11625459790229797
step: 5300, Loss: 0.20621588826179504
step: 5400, Loss: 0.11804249882698059
step: 5500, Loss: 0.11590293049812317
step: 5600, Loss: 0.11904104053974152
step: 5700, Loss: 0.11792159080505371
step: 5800, Loss: 0.1200341135263443
step: 5900, Loss: 0.11728809028863907
step: 6000, Loss: 0.11680394411087036
step: 6100, Loss: 0.11580632627010345
step: 6200, Loss: 0.11540625989437103
step: 6300, Loss: 0.11631688475608826
step: 6400, Loss: 0.11476458609104156
step: 6500, Loss: 0.11541412025690079
step: 6600, Loss: 0.11521885544061661
step: 6700, Loss: 0.11587511748075485
step: 6800, Loss: 0.11583946645259857
step: 6900, Loss: 0.11406812816858292
step: 7000, Loss: 0.11566942930221558
step: 7100, Loss: 0.11305170506238937
step: 7200, Loss: 0.19627892971038818
step: 7300, Loss: 0.1140778660774231
step: 7400, Loss: 0.11400734633207321
step: 7500, Loss: 0.1150914877653122
step: 7600, Loss: 0.11332927644252777
step: 7700, Loss: 0.11371619999408722
step: 7800, Loss: 0.11383243650197983
step: 7900, Loss: 0.1144186481833458
step: 8000, Loss: 0.11402120441198349
step: 8100, Loss: 0.11839345097541809
step: 8200, Loss: 0.11282595992088318
step: 8300, Loss: 0.11393186450004578
step: 8400, Loss: 0.1129402220249176
step: 8500, Loss: 0.11331004649400711
step: 8600, Loss: 0.11346478015184402
step: 8700, Loss: 0.11372674256563187
step: 8800, Loss: 0.11473618447780609
step: 8900, Loss: 0.1129186749458313
step: 9000, Loss: 0.11444062739610672
step: 9100, Loss: 0.19460166990756989
step: 9200, Loss: 0.11479337513446808
step: 9300, Loss: 0.11305509507656097
step: 9400, Loss: 0.11533210426568985
step: 9500, Loss: 0.11358228325843811
step: 7000, Loss: 0.11367054283618927
step: 7100, Loss: 0.11334048956632614
step: 7200, Loss: 0.11303158849477768
step: 7300, Loss: 0.11401690542697906
step: 7400, Loss: 0.11352350562810898
step: 7500, Loss: 0.11428280174732208
step: 7600, Loss: 0.11271696537733078
step: 7700, Loss: 0.1163889467716217
step: 7800, Loss: 0.11377912014722824
step: 7900, Loss: 0.11313316226005554
step: 8000, Loss: 0.11501029133796692
step: 8100, Loss: 0.11344943195581436
step: 8200, Loss: 0.11306871473789215
step: 8300, Loss: 0.112785205245018
step: 8400, Loss: 0.11387532949447632
step: 8500, Loss: 0.11335013806819916
step: 8600, Loss: 0.11390342563390732
step: 8700, Loss: 0.11368747800588608
step: 8800, Loss: 0.11350837349891663
step: 8900, Loss: 0.1136946976184845
step: 9000, Loss: 0.11276093870401382
step: 9100, Loss: 0.11368659138679504
step: 9200, Loss: 0.1129099428653717
step: 9300, Loss: 0.11329584568738937
step: 9400, Loss: 0.11426351964473724
step: 9500, Loss: 0.11343394219875336
step: 9600, Loss: 0.11445407569408417
step: 9700, Loss: 0.11270368099212646
step: 9800, Loss: 0.11362622678279877
step: 9900, Loss: 0.11304263025522232
training successfully ended.
validating...
validate data length:31
acc: 0.9
precision: 0.8461538461538461
recall: 0.9166666666666666
F_score: 0.8799999999999999
subject 16 Avgacc: 0.7962500000000001 Avgfscore: 0.7877895973939617 
 Max acc:1.0, Max f score:1.0
******** mix subject_17 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 71.4986801147461
step: 100, Loss: 2.735304355621338
step: 200, Loss: 0.13469676673412323
step: 300, Loss: 0.2664426565170288
step: 400, Loss: 0.11984708160161972
step: 500, Loss: 0.15161636471748352
step: 600, Loss: 0.11968447268009186
step: 700, Loss: 0.21945348381996155
step: 800, Loss: 0.11540888249874115
step: 900, Loss: 0.11713438481092453
step: 1000, Loss: 0.11741834133863449
step: 1100, Loss: 0.11900801956653595
step: 1200, Loss: 0.11568088084459305
step: 1300, Loss: 0.11806263029575348
step: 1400, Loss: 0.11523416638374329
step: 1500, Loss: 0.11764269322156906
step: 1600, Loss: 0.11608610302209854
step: 1700, Loss: 0.11569984257221222
step: 1800, Loss: 0.11399468034505844
step: 1900, Loss: 0.11757810413837433
step: 2000, Loss: 0.1138048768043518
step: 2100, Loss: 0.11482136696577072
step: 2200, Loss: 0.11496837437152863
step: 2300, Loss: 0.1161758154630661
step: 2400, Loss: 0.11403684318065643
step: 2500, Loss: 0.11624029278755188
step: 2600, Loss: 0.11331638693809509
step: 2700, Loss: 0.20684191584587097
step: 2800, Loss: 0.11348764598369598
step: 2900, Loss: 0.11695756763219833
step: 3000, Loss: 0.11365703493356705
step: 3100, Loss: 0.11628711223602295
step: 3200, Loss: 0.11439114063978195
step: 3300, Loss: 0.1160319373011589
step: 3400, Loss: 0.11529739946126938
step: 3500, Loss: 0.1157158613204956
step: 3600, Loss: 0.11508268117904663
step: 3700, Loss: 0.11655797809362411
step: 3800, Loss: 0.11526156216859818
step: 3900, Loss: 0.11395756155252457
step: 4000, Loss: 0.11450913548469543
step: 4100, Loss: 0.11665693670511246
step: 4200, Loss: 0.11360001564025879
step: 4300, Loss: 0.1162770465016365
step: 4400, Loss: 0.11569508910179138
step: 4500, Loss: 0.11360882222652435
step: 4600, Loss: 0.11477044224739075
step: 4700, Loss: 0.11465732008218765
step: 4800, Loss: 0.11530061811208725
step: 4900, Loss: 0.4256305396556854
step: 5000, Loss: 0.128077894449234
step: 5100, Loss: 0.13226726651191711
step: 5200, Loss: 0.12774311006069183
step: 5300, Loss: 0.12228100001811981
step: 5400, Loss: 0.11914421617984772
step: 5500, Loss: 0.11861361563205719
step: 5600, Loss: 0.11926751583814621
step: 5700, Loss: 0.18237997591495514
step: 5800, Loss: 0.11783045530319214
step: 5900, Loss: 0.1197972223162651
step: 6000, Loss: 0.11803251504898071
step: 6100, Loss: 0.119320347905159
step: 6200, Loss: 0.11480094492435455
step: 6300, Loss: 0.11472104489803314
step: 6400, Loss: 0.11633388698101044
step: 6500, Loss: 0.1627548784017563
step: 6600, Loss: 0.1185123473405838
step: 6700, Loss: 0.11592291295528412
step: 6800, Loss: 0.11669619381427765
step: 6900, Loss: 0.13537487387657166
step: 7000, Loss: 0.11658602952957153
step: 7100, Loss: 0.11538098752498627
step: 7200, Loss: 0.11510781943798065
step: 7300, Loss: 0.14442098140716553
step: 7400, Loss: 0.11459686607122421
step: 7500, Loss: 0.11546431481838226
step: 7600, Loss: 0.11350078135728836
step: 7700, Loss: 0.11555477976799011
step: 7800, Loss: 0.11491740494966507
step: 7900, Loss: 0.11584123969078064
step: 8000, Loss: 0.11592172086238861
step: 8100, Loss: 0.11421471834182739
step: 8200, Loss: 0.1138799786567688
step: 8300, Loss: 0.11429286003112793
step: 8400, Loss: 0.11538581550121307
step: 8500, Loss: 0.11392759531736374
step: 8600, Loss: 0.11351661384105682
step: 8700, Loss: 0.11443545669317245
step: 8800, Loss: 0.11438601464033127
step: 8900, Loss: 0.11589930951595306
step: 9000, Loss: 0.11426600813865662
step: 9100, Loss: 0.1127026230096817
step: 9200, Loss: 0.11396113783121109
step: 9300, Loss: 0.11427179723978043
step: 9400, Loss: 0.11404421180486679
step: 9500, Loss: 0.11424729973077774
step: 9600, Loss: 0.11328109353780746
step: 9700, Loss: 0.11467212438583374
step: 9800, Loss: 0.11364907026290894
step: 9900, Loss: 0.13794079422950745
training successfully ended.
validating...
validate data length:32
acc: 0.3125
precision: 0.25
recall: 0.4166666666666667
F_score: 0.3125
******fold 2******

Training... train_data length:280
step: 0, Loss: 1.8753612041473389
step: 100, Loss: 0.12409089505672455
step: 200, Loss: 0.12248381972312927
step: 300, Loss: 0.11478688567876816
step: 400, Loss: 0.11566083878278732
step: 500, Loss: 0.11550267040729523
step: 600, Loss: 0.11497349292039871
step: 700, Loss: 0.11674439162015915
step: 800, Loss: 0.11438586562871933
step: 900, Loss: 0.11407402157783508
step: 1000, Loss: 0.1146925613284111
step: 1100, Loss: 0.11459891498088837
step: 1200, Loss: 0.11542384326457977
step: 1300, Loss: 0.1174217164516449
step: 1400, Loss: 0.11288748681545258
step: 1500, Loss: 0.11498117446899414
step: 1600, Loss: 0.11329831182956696
step: 1700, Loss: 0.11400757730007172
step: 1800, Loss: 0.11451517045497894
step: 1900, Loss: 0.1142754852771759
step: 2000, Loss: 0.11413250863552094
step: 2100, Loss: 0.11423280090093613
step: 2200, Loss: 0.11344271898269653
step: 2300, Loss: 0.11340511590242386
step: 2400, Loss: 0.11402533203363419
step: 2500, Loss: 0.11404182016849518
step: 2600, Loss: 0.11403767764568329
step: 2700, Loss: 0.1138685792684555
step: 2800, Loss: 0.11307797580957413
step: 2900, Loss: 0.1137404590845108
step: 3000, Loss: 0.11350013315677643
step: 3100, Loss: 0.11365688592195511
step: 3200, Loss: 0.11481358110904694
step: 3300, Loss: 0.11667834967374802
step: 3400, Loss: 0.11361531168222427
step: 3500, Loss: 0.11578387022018433
step: 3600, Loss: 0.11282207816839218
step: 3700, Loss: 0.11447617411613464
step: 3800, Loss: 0.11376498639583588
step: 3900, Loss: 0.11396552622318268
step: 4000, Loss: 0.11318890750408173
step: 4100, Loss: 0.11353409290313721
step: 4200, Loss: 0.1137595847249031
step: 4300, Loss: 0.1134827733039856
step: 4400, Loss: 0.11319024860858917
step: 4500, Loss: 0.11416707187891006
step: 4600, Loss: 0.11357368528842926
step: 4700, Loss: 0.11429646611213684
step: 4800, Loss: 0.11467275768518448
step: 4900, Loss: 0.11376520246267319
step: 5000, Loss: 0.11425905674695969
step: 5100, Loss: 0.11324098706245422
step: 5200, Loss: 0.11487185209989548
step: 5300, Loss: 0.11429443955421448
step: 5400, Loss: 0.11267140507698059
step: 5500, Loss: 0.11281716823577881
step: 5600, Loss: 0.11445742845535278
step: 5700, Loss: 0.11357950419187546
step: 5800, Loss: 0.11520343273878098
step: 5900, Loss: 0.11460724472999573
step: 6000, Loss: 0.11431819200515747
step: 6100, Loss: 0.11396624892950058
step: 6200, Loss: 0.11384156346321106
step: 6300, Loss: 0.11371000856161118
step: 6400, Loss: 0.11552487313747406
step: 6500, Loss: 0.1147039458155632
step: 6600, Loss: 0.1134166419506073
step: 6700, Loss: 0.1144961416721344
step: 6800, Loss: 0.11453532427549362
step: 6900, Loss: 0.1144716739654541
step: 7000, Loss: 0.11386001110076904
step: 7100, Loss: 0.11410868167877197
step: 9600, Loss: 0.11346325278282166
step: 9700, Loss: 0.11262454837560654
step: 9800, Loss: 0.11451312899589539
step: 9900, Loss: 0.11548572778701782
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11814682185649872
step: 100, Loss: 0.11954690515995026
step: 200, Loss: 0.11695827543735504
step: 300, Loss: 0.11414332687854767
step: 400, Loss: 0.1148848608136177
step: 500, Loss: 0.11447961628437042
step: 600, Loss: 0.11540210992097855
step: 700, Loss: 0.11323459446430206
step: 800, Loss: 0.11333421617746353
step: 900, Loss: 0.11380772292613983
step: 1000, Loss: 0.11385556310415268
step: 1100, Loss: 0.11462666094303131
step: 1200, Loss: 0.11408781260251999
step: 1300, Loss: 0.11499067395925522
step: 1400, Loss: 0.11510086059570312
step: 1500, Loss: 0.193955659866333
step: 1600, Loss: 0.11353939026594162
step: 1700, Loss: 0.11358405649662018
step: 1800, Loss: 0.1157415360212326
step: 1900, Loss: 0.11482036858797073
step: 2000, Loss: 0.11450064927339554
step: 2100, Loss: 0.11484012752771378
step: 2200, Loss: 0.11342576146125793
step: 2300, Loss: 0.11602932214736938
step: 2400, Loss: 0.15972787141799927
step: 2500, Loss: 0.13509151339530945
step: 2600, Loss: 0.1301054060459137
step: 2700, Loss: 0.12617608904838562
step: 2800, Loss: 0.12976452708244324
step: 2900, Loss: 0.13036911189556122
step: 3000, Loss: 0.12322045117616653
step: 3100, Loss: 0.12095349282026291
step: 3200, Loss: 0.11753999441862106
step: 3300, Loss: 0.1184592992067337
step: 3400, Loss: 0.205694317817688
step: 3500, Loss: 0.1201983094215393
step: 3600, Loss: 0.11698177456855774
step: 3700, Loss: 0.11729951947927475
step: 3800, Loss: 0.1175018772482872
step: 3900, Loss: 0.12100884318351746
step: 4000, Loss: 0.11533206701278687
step: 4100, Loss: 0.11539806425571442
step: 4200, Loss: 0.11516152322292328
step: 4300, Loss: 0.11739574372768402
step: 4400, Loss: 0.11675567924976349
step: 4500, Loss: 0.11431532353162766
step: 4600, Loss: 0.11481916904449463
step: 4700, Loss: 0.1164165735244751
step: 4800, Loss: 0.11405771970748901
step: 4900, Loss: 0.1150381788611412
step: 5000, Loss: 0.11494099348783493
step: 5100, Loss: 0.11341846734285355
step: 5200, Loss: 0.1136518269777298
step: 5300, Loss: 0.1996832638978958
step: 5400, Loss: 0.11525095254182816
step: 5500, Loss: 0.11533698439598083
step: 5600, Loss: 0.11466763913631439
step: 5700, Loss: 0.11372923105955124
step: 5800, Loss: 0.11466121673583984
step: 5900, Loss: 0.11407121270895004
step: 6000, Loss: 0.11342564225196838
step: 6100, Loss: 0.11532668024301529
step: 6200, Loss: 0.11503729224205017
step: 6300, Loss: 0.11326269060373306
step: 6400, Loss: 0.11495977640151978
step: 6500, Loss: 0.11340869218111038
step: 6600, Loss: 0.11351924389600754
step: 6700, Loss: 0.11403973400592804
step: 6800, Loss: 0.11390215903520584
step: 6900, Loss: 0.11402315646409988
step: 7000, Loss: 0.11411410570144653
step: 7100, Loss: 0.11410709470510483
step: 7200, Loss: 0.19072416424751282
step: 7300, Loss: 0.11317639797925949
step: 7400, Loss: 0.11286608129739761
step: 7500, Loss: 0.11362619698047638
step: 7600, Loss: 0.11328001320362091
step: 7700, Loss: 0.11462501436471939
step: 7800, Loss: 0.11340175569057465
step: 7900, Loss: 0.11359803378582001
step: 8000, Loss: 0.11383745074272156
step: 8100, Loss: 0.11350683122873306
step: 8200, Loss: 0.11348219215869904
step: 8300, Loss: 0.11279310286045074
step: 8400, Loss: 0.11301224678754807
step: 8500, Loss: 0.11483421176671982
step: 8600, Loss: 0.11360931396484375
step: 8700, Loss: 0.11277440935373306
step: 8800, Loss: 0.1132194846868515
step: 8900, Loss: 0.1149631217122078
step: 9000, Loss: 0.11367511749267578
step: 9100, Loss: 0.19652853906154633
step: 9200, Loss: 0.1131930947303772
step: 9300, Loss: 0.11410227417945862
step: 9400, Loss: 0.11471080034971237
step: 9500, Loss: 0.11333610117435455
step: 9600, Loss: 0.11491139978170395
step: 9700, Loss: 0.1143588274717331
step: 9800, Loss: 0.11380614340305328
step: 9900, Loss: 0.11398473381996155
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11908913403749466
step: 100, Loss: 0.11761307716369629
step: 200, Loss: 0.11612015962600708
step: 300, Loss: 0.11633855104446411
step: 400, Loss: 0.11425617337226868
step: 500, Loss: 0.1149226501584053
step: 600, Loss: 0.11446346342563629
step: 700, Loss: 0.11346061527729034
step: 800, Loss: 0.11460214853286743
step: 900, Loss: 0.1136917993426323
step: 1000, Loss: 0.11512816697359085
step: 1100, Loss: 0.11421677470207214
step: 1200, Loss: 0.11366242170333862
step: 1300, Loss: 0.11383642256259918
step: 1400, Loss: 0.11414337158203125
step: 1500, Loss: 0.191805899143219
step: 1600, Loss: 0.11445990204811096
step: 1700, Loss: 0.11514954268932343
step: 1800, Loss: 0.1129269152879715
step: 1900, Loss: 0.11583302915096283
step: 2000, Loss: 0.11436578631401062
step: 2100, Loss: 0.11327402293682098
step: 2200, Loss: 0.11500155925750732
step: 2300, Loss: 0.11463089287281036
step: 2400, Loss: 0.11393770575523376
step: 2500, Loss: 0.129915252327919
step: 2600, Loss: 0.20365744829177856
step: 2700, Loss: 0.12878301739692688
step: 2800, Loss: 0.1346733272075653
step: 2900, Loss: 0.12620797753334045
step: 3000, Loss: 0.12750288844108582
step: 3100, Loss: 0.12334612011909485
step: 3200, Loss: 0.12101463973522186
step: 3300, Loss: 0.12309307605028152
step: 3400, Loss: 0.2057453989982605
step: 3500, Loss: 0.12004321068525314
step: 3600, Loss: 0.11604319512844086
step: 3700, Loss: 0.11618600785732269
step: 3800, Loss: 0.11992788314819336
step: 3900, Loss: 0.11647457629442215
step: 4000, Loss: 0.1168217584490776
step: 4100, Loss: 0.11596746742725372
step: 4200, Loss: 0.11886590719223022
step: 4300, Loss: 0.11765825748443604
step: 4400, Loss: 0.11824087798595428
step: 4500, Loss: 0.11745935678482056
step: 4600, Loss: 0.11516918241977692
step: 4700, Loss: 0.11693736910820007
step: 4800, Loss: 0.11610172688961029
step: 4900, Loss: 0.11560629308223724
step: 5000, Loss: 0.11639061570167542
step: 5100, Loss: 0.11743690818548203
step: 5200, Loss: 0.11479747295379639
step: 5300, Loss: 0.1981094777584076
step: 5400, Loss: 0.1149841845035553
step: 5500, Loss: 0.11365249007940292
step: 5600, Loss: 0.11245796829462051
step: 5700, Loss: 0.11444075405597687
step: 5800, Loss: 0.11398668587207794
step: 5900, Loss: 0.11434914916753769
step: 6000, Loss: 0.11488056182861328
step: 6100, Loss: 0.11477725952863693
step: 6200, Loss: 0.11447429656982422
step: 6300, Loss: 0.11369723081588745
step: 6400, Loss: 0.11411363631486893
step: 6500, Loss: 0.11448390781879425
step: 6600, Loss: 0.11462877690792084
step: 6700, Loss: 0.11377552896738052
step: 6800, Loss: 0.11383017152547836
step: 6900, Loss: 0.11365468055009842
step: 7000, Loss: 0.11418206244707108
step: 7100, Loss: 0.11344107985496521
step: 7200, Loss: 0.1934950053691864
step: 7300, Loss: 0.11484399437904358
step: 7400, Loss: 0.11300267279148102
step: 7500, Loss: 0.11359924077987671
step: 7600, Loss: 0.11315200477838516
step: 7700, Loss: 0.1128869280219078
step: 7800, Loss: 0.11413798481225967
step: 7900, Loss: 0.11543218791484833
step: 8000, Loss: 0.1142217293381691
step: 8100, Loss: 0.1141190230846405
step: 8200, Loss: 0.1136326864361763
step: 8300, Loss: 0.11314233392477036
step: 8400, Loss: 0.11414297670125961
step: 8500, Loss: 0.11436857283115387
step: 8600, Loss: 0.11428678780794144
step: 8700, Loss: 0.11499194800853729
step: 8800, Loss: 0.11390706896781921
step: 8900, Loss: 0.11406829953193665
step: 9000, Loss: 0.11384090781211853
step: 9100, Loss: 0.19222880899906158
step: 9200, Loss: 0.11346670240163803
step: 9300, Loss: 0.11414076387882233
step: 9400, Loss: 0.11376617848873138
step: 9500, Loss: 0.11375735700130463
step: 9600, Loss: 0.11370741575956345
step: 9700, Loss: 0.11369480937719345
step: 9800, Loss: 0.11535356193780899
step: 9900, Loss: 0.1133354976773262
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
step: 7200, Loss: 4.856155872344971
step: 7300, Loss: 0.15152069926261902
step: 7400, Loss: 0.13454580307006836
step: 7500, Loss: 0.13367225229740143
step: 7600, Loss: 0.12315969169139862
step: 7700, Loss: 0.12537771463394165
step: 7800, Loss: 0.11656796932220459
step: 7900, Loss: 0.12129807472229004
step: 8000, Loss: 0.1152052953839302
step: 8100, Loss: 0.11926119774580002
step: 8200, Loss: 0.11690463870763779
step: 8300, Loss: 0.11630256474018097
step: 8400, Loss: 0.11372999101877213
step: 8500, Loss: 0.11799812316894531
step: 8600, Loss: 0.11414925754070282
step: 8700, Loss: 0.11805351078510284
step: 8800, Loss: 0.11640123277902603
step: 8900, Loss: 0.11657843738794327
step: 9000, Loss: 0.11433681100606918
step: 9100, Loss: 0.11797571182250977
step: 9200, Loss: 0.11446663737297058
step: 9300, Loss: 0.1142909899353981
step: 9400, Loss: 0.1155887097120285
step: 9500, Loss: 0.11456780135631561
step: 9600, Loss: 0.11729399859905243
step: 9700, Loss: 0.11522418260574341
step: 9800, Loss: 0.11497557163238525
step: 9900, Loss: 0.11593320220708847
training successfully ended.
validating...
validate data length:32
acc: 0.75
precision: 0.7222222222222222
recall: 0.8125
F_score: 0.7647058823529411
******fold 3******

Training... train_data length:281
step: 0, Loss: 2.440934896469116
step: 100, Loss: 0.12051606178283691
step: 200, Loss: 0.11665519326925278
step: 300, Loss: 0.1148693859577179
step: 400, Loss: 0.11569829285144806
step: 500, Loss: 0.11382095515727997
step: 600, Loss: 0.11490843445062637
step: 700, Loss: 0.11497721076011658
step: 800, Loss: 0.11640454828739166
step: 900, Loss: 0.11389356106519699
step: 1000, Loss: 0.11437346041202545
step: 1100, Loss: 0.11314082890748978
step: 1200, Loss: 0.1159181296825409
step: 1300, Loss: 0.11368534713983536
step: 1400, Loss: 0.11545037478208542
step: 1500, Loss: 0.11311690509319305
step: 1600, Loss: 0.11385110765695572
step: 1700, Loss: 0.11367613822221756
step: 1800, Loss: 0.11371185630559921
step: 1900, Loss: 0.113497294485569
step: 2000, Loss: 0.11265869438648224
step: 2100, Loss: 0.11549614369869232
step: 2200, Loss: 0.1133810505270958
step: 2300, Loss: 0.1146019771695137
step: 2400, Loss: 0.1130913645029068
step: 2500, Loss: 0.11460172384977341
step: 2600, Loss: 0.11373160779476166
step: 2700, Loss: 0.11330719292163849
step: 2800, Loss: 0.11432357132434845
step: 2900, Loss: 0.11401310563087463
step: 3000, Loss: 0.11313212662935257
step: 3100, Loss: 0.11445873975753784
step: 3200, Loss: 0.11414511501789093
step: 3300, Loss: 0.11329805850982666
step: 3400, Loss: 0.11373792588710785
step: 3500, Loss: 0.11449987441301346
step: 3600, Loss: 0.11356709897518158
step: 3700, Loss: 0.1146637499332428
step: 3800, Loss: 0.11314361542463303
step: 3900, Loss: 0.11491893231868744
step: 4000, Loss: 0.11365188658237457
step: 4100, Loss: 0.11263510584831238
step: 4200, Loss: 0.11371644586324692
step: 4300, Loss: 0.11339730769395828
step: 4400, Loss: 0.11365870386362076
step: 4500, Loss: 0.11419108510017395
step: 4600, Loss: 0.11343268305063248
step: 4700, Loss: 0.11454692482948303
step: 4800, Loss: 0.11336999386548996
step: 4900, Loss: 0.11421628296375275
step: 5000, Loss: 0.11455878615379333
step: 5100, Loss: 0.11477061361074448
step: 5200, Loss: 0.11336539685726166
step: 5300, Loss: 0.11922281980514526
step: 5400, Loss: 0.11358801275491714
step: 5500, Loss: 0.11414125561714172
step: 5600, Loss: 0.11431671679019928
step: 5700, Loss: 0.11465922743082047
step: 5800, Loss: 0.11320431530475616
step: 5900, Loss: 0.11408486217260361
step: 6000, Loss: 0.11360444128513336
step: 6100, Loss: 0.11358752101659775
step: 6200, Loss: 0.11439143866300583
step: 6300, Loss: 0.11684875190258026
step: 6400, Loss: 0.7558078765869141
step: 6500, Loss: 0.1398523598909378
step: 6600, Loss: 0.131972998380661
step: 6700, Loss: 0.13662949204444885
step: 6800, Loss: 0.12233494222164154
step: 6900, Loss: 0.12012909352779388
step: 7000, Loss: 0.1264127790927887
step: 7100, Loss: 0.11964597553014755
step: 7200, Loss: 0.12058338522911072
step: 7300, Loss: 0.11725600063800812
step: 7400, Loss: 0.11872626841068268
step: 7500, Loss: 0.11761779338121414
step: 7600, Loss: 0.11554748564958572
step: 7700, Loss: 0.11649855226278305
step: 7800, Loss: 0.11382067203521729
step: 7900, Loss: 0.11476779729127884
step: 8000, Loss: 0.11617964506149292
step: 8100, Loss: 0.11539361625909805
step: 8200, Loss: 0.1157035380601883
step: 8300, Loss: 0.11553016304969788
step: 8400, Loss: 0.1133510172367096
step: 8500, Loss: 0.115839883685112
step: 8600, Loss: 0.11412490904331207
step: 8700, Loss: 0.11451947689056396
step: 8800, Loss: 0.11455833911895752
step: 8900, Loss: 0.11496235430240631
step: 9000, Loss: 0.11416350305080414
step: 9100, Loss: 0.11654489487409592
step: 9200, Loss: 0.11380313336849213
step: 9300, Loss: 0.11369713395833969
step: 9400, Loss: 0.11622992157936096
step: 9500, Loss: 0.11375753581523895
step: 9600, Loss: 0.11375115811824799
step: 9700, Loss: 0.11356794834136963
step: 9800, Loss: 0.11429470777511597
step: 9900, Loss: 0.11429785192012787
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.9090909090909091
recall: 0.6666666666666666
F_score: 0.7692307692307692
******fold 4******

Training... train_data length:281
step: 0, Loss: 1.0319229364395142
step: 100, Loss: 0.11578583717346191
step: 200, Loss: 0.11656777560710907
step: 300, Loss: 0.11906915158033371
step: 400, Loss: 0.11555155366659164
step: 500, Loss: 0.11703754961490631
step: 600, Loss: 0.11526588350534439
step: 700, Loss: 0.11471414566040039
step: 800, Loss: 0.11436203122138977
step: 900, Loss: 0.11350500583648682
step: 1000, Loss: 0.11321745067834854
step: 1100, Loss: 0.1163032129406929
step: 1200, Loss: 0.11464999616146088
step: 1300, Loss: 0.11411444842815399
step: 1400, Loss: 0.1135917454957962
step: 1500, Loss: 0.1126188188791275
step: 1600, Loss: 0.11344487220048904
step: 1700, Loss: 0.11459363996982574
step: 1800, Loss: 0.11400247365236282
step: 1900, Loss: 0.11527691781520844
step: 2000, Loss: 0.11420276015996933
step: 2100, Loss: 0.11352849751710892
step: 2200, Loss: 0.11353030800819397
step: 2300, Loss: 0.11421523988246918
step: 2400, Loss: 0.1144968569278717
step: 2500, Loss: 0.11481636762619019
step: 2600, Loss: 0.11396338045597076
step: 2700, Loss: 0.9597989916801453
step: 2800, Loss: 0.12324485182762146
step: 2900, Loss: 0.12072122097015381
step: 3000, Loss: 0.12161345034837723
step: 3100, Loss: 0.11773961782455444
step: 3200, Loss: 0.12227757275104523
step: 3300, Loss: 0.11834286898374557
step: 3400, Loss: 0.11713938415050507
step: 3500, Loss: 0.11684656143188477
step: 3600, Loss: 0.11674006283283234
step: 3700, Loss: 0.11544442176818848
step: 3800, Loss: 0.11651725322008133
step: 3900, Loss: 0.11593632400035858
step: 4000, Loss: 0.1140221580862999
step: 4100, Loss: 0.11486873030662537
step: 4200, Loss: 0.11519075930118561
step: 4300, Loss: 0.11546453833580017
step: 4400, Loss: 0.11433999985456467
step: 4500, Loss: 0.11851866543292999
step: 4600, Loss: 0.11555472761392593
step: 4700, Loss: 0.11362005770206451
step: 4800, Loss: 0.11485797166824341
step: 4900, Loss: 0.11304081231355667
step: 5000, Loss: 0.11433006823062897
step: 5100, Loss: 0.1150398775935173
step: 5200, Loss: 0.11481153964996338
step: 5300, Loss: 0.11414926499128342
step: 5400, Loss: 0.11419648677110672
step: 5500, Loss: 0.11411689966917038
step: 5600, Loss: 0.11435018479824066
step: 5700, Loss: 0.11462037265300751
step: 5800, Loss: 0.1138397678732872
step: 5900, Loss: 0.11578967422246933
step: 6000, Loss: 0.11455938220024109
step: 6100, Loss: 0.116265669465065
step: 6200, Loss: 0.11282762885093689
step: 6300, Loss: 0.11354464292526245
step: 6400, Loss: 0.11357318609952927
step: 6500, Loss: 0.11522693932056427
step: 6600, Loss: 0.1142018735408783
step: 6700, Loss: 0.11403064429759979
step: 6800, Loss: 0.11380966752767563
step: 6900, Loss: 0.11461544036865234
step: 7000, Loss: 0.11334450542926788
step: 7100, Loss: 0.11367339640855789
step: 7200, Loss: 0.11341685056686401
step: 7300, Loss: 0.11419697105884552
step: 7400, Loss: 0.11323603242635727
step: 7500, Loss: 0.11307203769683838
step: 7600, Loss: 0.11323302984237671
step: 7700, Loss: 0.11353720724582672
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11845805495977402
step: 100, Loss: 0.11686913669109344
step: 200, Loss: 0.11832751333713531
step: 300, Loss: 0.1162986159324646
step: 400, Loss: 0.11435447633266449
step: 500, Loss: 0.1139187440276146
step: 600, Loss: 0.11451265215873718
step: 700, Loss: 0.11426839977502823
step: 800, Loss: 0.11382460594177246
step: 900, Loss: 0.1142101138830185
step: 1000, Loss: 0.11316417902708054
step: 1100, Loss: 0.11281364411115646
step: 1200, Loss: 0.11458374559879303
step: 1300, Loss: 0.11439594626426697
step: 1400, Loss: 0.11459428071975708
step: 1500, Loss: 0.1915990114212036
step: 1600, Loss: 0.11400196701288223
step: 1700, Loss: 0.11253622174263
step: 1800, Loss: 0.11435255408287048
step: 1900, Loss: 0.11424317955970764
step: 2000, Loss: 0.11306055635213852
step: 2100, Loss: 0.1133052259683609
step: 2200, Loss: 0.11348038166761398
step: 2300, Loss: 0.11420398205518723
step: 2400, Loss: 0.1145465150475502
step: 2500, Loss: 0.114928238093853
step: 2600, Loss: 0.1134788766503334
step: 2700, Loss: 0.11336904019117355
step: 2800, Loss: 0.11917568743228912
step: 2900, Loss: 0.11366371065378189
step: 3000, Loss: 0.11608073860406876
step: 3100, Loss: 0.11743985861539841
step: 3200, Loss: 0.11454834789037704
step: 3300, Loss: 0.11362065374851227
step: 3400, Loss: 0.19612747430801392
step: 3500, Loss: 0.11439438164234161
step: 3600, Loss: 0.11523294448852539
step: 3700, Loss: 0.11494053900241852
step: 3800, Loss: 0.11395986378192902
step: 3900, Loss: 0.11455187201499939
step: 4000, Loss: 2.444831132888794
step: 4100, Loss: 0.1426757276058197
step: 4200, Loss: 0.12763255834579468
step: 4300, Loss: 0.13055220246315002
step: 4400, Loss: 0.12426817417144775
step: 4500, Loss: 0.12033742666244507
step: 4600, Loss: 0.12161412090063095
step: 4700, Loss: 0.1192563846707344
step: 4800, Loss: 0.11696989834308624
step: 4900, Loss: 0.12043319642543793
step: 5000, Loss: 0.1194770485162735
step: 5100, Loss: 0.11624616384506226
step: 5200, Loss: 0.11795933544635773
step: 5300, Loss: 0.2032819390296936
step: 5400, Loss: 0.1158108115196228
step: 5500, Loss: 0.11573411524295807
step: 5600, Loss: 0.1171489804983139
step: 5700, Loss: 0.11601654440164566
step: 5800, Loss: 0.11507182568311691
step: 5900, Loss: 0.11837081611156464
step: 6000, Loss: 0.11479956656694412
step: 6100, Loss: 0.11605918407440186
step: 6200, Loss: 0.11600420624017715
step: 6300, Loss: 0.11386062204837799
step: 6400, Loss: 0.11388476192951202
step: 6500, Loss: 0.11529935896396637
step: 6600, Loss: 0.11526410281658173
step: 6700, Loss: 0.11397462338209152
step: 6800, Loss: 0.11440791189670563
step: 6900, Loss: 0.1154627725481987
step: 7000, Loss: 0.1133558452129364
step: 7100, Loss: 0.11406373232603073
step: 7200, Loss: 0.1969076246023178
step: 7300, Loss: 0.11407817900180817
step: 7400, Loss: 0.11336582154035568
step: 7500, Loss: 0.11315108835697174
step: 7600, Loss: 0.11332035064697266
step: 7700, Loss: 0.11476107686758041
step: 7800, Loss: 0.11439196765422821
step: 7900, Loss: 0.1139073371887207
step: 8000, Loss: 0.11302246898412704
step: 8100, Loss: 0.11507797241210938
step: 8200, Loss: 0.11324980109930038
step: 8300, Loss: 0.11380711942911148
step: 8400, Loss: 0.11392229795455933
step: 8500, Loss: 0.11372165381908417
step: 8600, Loss: 0.11250633001327515
step: 8700, Loss: 0.11318662762641907
step: 8800, Loss: 0.11479412019252777
step: 8900, Loss: 0.11259341239929199
step: 9000, Loss: 0.11371023207902908
step: 9100, Loss: 0.19267284870147705
step: 9200, Loss: 0.11558730900287628
step: 9300, Loss: 0.11301229149103165
step: 9400, Loss: 0.11406916379928589
step: 9500, Loss: 0.11342836916446686
step: 9600, Loss: 0.11351912468671799
step: 9700, Loss: 0.11261192709207535
step: 9800, Loss: 0.1130610853433609
step: 9900, Loss: 0.1139838770031929
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9736842105263158
recall: 1.0
F_score: 0.9866666666666666
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11769822984933853
step: 100, Loss: 0.12119868397712708
step: 200, Loss: 0.11841908097267151
step: 300, Loss: 0.11650963872671127
step: 400, Loss: 0.11475129425525665
step: 500, Loss: 0.11317048966884613
step: 600, Loss: 0.11253100633621216
step: 700, Loss: 0.11456985771656036
step: 800, Loss: 0.11517801880836487
step: 900, Loss: 0.11435677111148834
step: 1000, Loss: 0.11410582065582275
step: 1100, Loss: 0.11330652236938477
step: 1200, Loss: 0.11318475008010864
step: 1300, Loss: 0.11390134692192078
step: 1400, Loss: 0.1144338846206665
step: 1500, Loss: 0.19540828466415405
step: 1600, Loss: 0.11438178271055222
step: 1700, Loss: 0.11346104741096497
step: 1800, Loss: 0.11519093811511993
step: 1900, Loss: 0.11489908397197723
step: 2000, Loss: 0.11320263147354126
step: 2100, Loss: 0.1147303357720375
step: 2200, Loss: 0.11454927176237106
step: 2300, Loss: 0.11424747109413147
step: 2400, Loss: 0.11279720813035965
step: 2500, Loss: 0.11414744704961777
step: 2600, Loss: 0.11411672830581665
step: 2700, Loss: 0.1131034642457962
step: 2800, Loss: 0.11544603854417801
step: 2900, Loss: 0.1143098771572113
step: 3000, Loss: 0.11322882026433945
step: 3100, Loss: 0.11423306167125702
step: 3200, Loss: 0.11412272602319717
step: 3300, Loss: 0.11368148028850555
step: 3400, Loss: 0.19740590453147888
step: 3500, Loss: 0.1143447682261467
step: 3600, Loss: 0.11301455646753311
step: 3700, Loss: 0.11393450945615768
step: 3800, Loss: 0.11485973000526428
step: 3900, Loss: 0.11302372813224792
step: 4000, Loss: 0.11692194640636444
step: 4100, Loss: 0.11469988524913788
step: 4200, Loss: 0.11414583027362823
step: 4300, Loss: 0.1142095997929573
step: 4400, Loss: 3.9534711837768555
step: 4500, Loss: 0.15583261847496033
step: 4600, Loss: 0.13288895785808563
step: 4700, Loss: 0.13369980454444885
step: 4800, Loss: 0.12828776240348816
step: 4900, Loss: 0.121697798371315
step: 5000, Loss: 0.12431706488132477
step: 5100, Loss: 0.12015098333358765
step: 5200, Loss: 0.12541745603084564
step: 5300, Loss: 0.21088919043540955
step: 5400, Loss: 0.11996342241764069
step: 5500, Loss: 0.11971086263656616
step: 5600, Loss: 0.11774412542581558
step: 5700, Loss: 0.11944432556629181
step: 5800, Loss: 0.11955825239419937
step: 5900, Loss: 0.12012685090303421
step: 6000, Loss: 0.11581173539161682
step: 6100, Loss: 0.11689308285713196
step: 6200, Loss: 0.11530406773090363
step: 6300, Loss: 0.11851239204406738
step: 6400, Loss: 0.11651261150836945
step: 6500, Loss: 0.11425124853849411
step: 6600, Loss: 0.11713824421167374
step: 6700, Loss: 0.1146126240491867
step: 6800, Loss: 0.11443067342042923
step: 6900, Loss: 0.1154303327202797
step: 7000, Loss: 0.11494749784469604
step: 7100, Loss: 0.11471576988697052
step: 7200, Loss: 0.20146402716636658
step: 7300, Loss: 0.11585607379674911
step: 7400, Loss: 0.11437512189149857
step: 7500, Loss: 0.11448175460100174
step: 7600, Loss: 0.11411648243665695
step: 7700, Loss: 0.11380087584257126
step: 7800, Loss: 0.11338113248348236
step: 7900, Loss: 0.11456914991140366
step: 8000, Loss: 0.1150340735912323
step: 8100, Loss: 0.11361107975244522
step: 8200, Loss: 0.11464781314134598
step: 8300, Loss: 0.11490090191364288
step: 8400, Loss: 0.1134234070777893
step: 8500, Loss: 0.11487792432308197
step: 8600, Loss: 0.11386996507644653
step: 8700, Loss: 0.11337500065565109
step: 8800, Loss: 0.11408988386392593
step: 8900, Loss: 0.11349823325872421
step: 9000, Loss: 0.11457626521587372
step: 9100, Loss: 0.19497089087963104
step: 9200, Loss: 0.113913893699646
step: 9300, Loss: 0.11342919617891312
step: 9400, Loss: 0.11441624164581299
step: 9500, Loss: 0.11441868543624878
step: 9600, Loss: 0.1143568605184555
step: 9700, Loss: 0.1145077794790268
step: 9800, Loss: 0.11383824795484543
step: 9900, Loss: 0.113544762134552
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9655172413793104
recall: 0.9655172413793104
F_score: 0.9655172413793104
subject 17 Avgacc: 0.9569444444444445 Avgfscore: 0.950044201685424 
 Max acc:1.0, Max f score:1.0
******** mix subject_18 ********

[380, 380]
******fold 1******

Training... train_data length:684
step: 7800, Loss: 0.11350823938846588
step: 7900, Loss: 0.11337471008300781
step: 8000, Loss: 0.11288287490606308
step: 8100, Loss: 0.11346547305583954
step: 8200, Loss: 0.11330555379390717
step: 8300, Loss: 0.11398972570896149
step: 8400, Loss: 0.11297444999217987
step: 8500, Loss: 0.11273201555013657
step: 8600, Loss: 0.11384440958499908
step: 8700, Loss: 0.11338227987289429
step: 8800, Loss: 0.11339239776134491
step: 8900, Loss: 0.11415498703718185
step: 9000, Loss: 0.11430847644805908
step: 9100, Loss: 0.11334580928087234
step: 9200, Loss: 0.11344600468873978
step: 9300, Loss: 0.11320468038320541
step: 9400, Loss: 0.11354769766330719
step: 9500, Loss: 0.11393995583057404
step: 9600, Loss: 0.1131826862692833
step: 9700, Loss: 0.11330150067806244
step: 9800, Loss: 0.11475230008363724
step: 9900, Loss: 0.11410524696111679
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.16493448615074158
step: 100, Loss: 0.11825516074895859
step: 200, Loss: 0.11337524652481079
step: 300, Loss: 0.11360006034374237
step: 400, Loss: 0.11334836483001709
step: 500, Loss: 0.11392410099506378
step: 600, Loss: 0.11362282186746597
step: 700, Loss: 0.11349646747112274
step: 800, Loss: 0.11428514122962952
step: 900, Loss: 0.11345845460891724
step: 1000, Loss: 0.11320572346448898
step: 1100, Loss: 0.11441336572170258
step: 1200, Loss: 0.11326322704553604
step: 1300, Loss: 0.11290747672319412
step: 1400, Loss: 0.11621833592653275
step: 1500, Loss: 0.11390338838100433
step: 1600, Loss: 0.11293911188840866
step: 1700, Loss: 0.11279003322124481
step: 1800, Loss: 0.11401563137769699
step: 1900, Loss: 0.11410815268754959
step: 2000, Loss: 0.11362174898386002
step: 2100, Loss: 0.1163046658039093
step: 2200, Loss: 0.11293841898441315
step: 2300, Loss: 0.11328400671482086
step: 2400, Loss: 0.1145830899477005
step: 2500, Loss: 0.11308207362890244
step: 2600, Loss: 0.11379655450582504
step: 2700, Loss: 0.11348641663789749
step: 2800, Loss: 0.11385749280452728
step: 2900, Loss: 0.1131928414106369
step: 3000, Loss: 0.11341127753257751
step: 3100, Loss: 0.1135675311088562
step: 3200, Loss: 0.11510448902845383
step: 3300, Loss: 0.11303520947694778
step: 3400, Loss: 0.1131676435470581
step: 3500, Loss: 0.11299879103899002
step: 3600, Loss: 0.11451123654842377
step: 3700, Loss: 0.11423162370920181
step: 3800, Loss: 0.11369755119085312
step: 3900, Loss: 0.11343245953321457
step: 4000, Loss: 0.113337941467762
step: 4100, Loss: 0.11356063932180405
step: 4200, Loss: 0.11334658414125443
step: 4300, Loss: 0.11336161941289902
step: 4400, Loss: 0.11381548643112183
step: 4500, Loss: 0.1133914515376091
step: 4600, Loss: 0.1130145937204361
step: 4700, Loss: 0.11423337459564209
step: 4800, Loss: 0.1134788915514946
step: 4900, Loss: 0.11303198337554932
step: 5000, Loss: 0.11253451555967331
step: 5100, Loss: 0.11347915977239609
step: 5200, Loss: 0.11356086283922195
step: 5300, Loss: 0.11363255977630615
step: 5400, Loss: 0.1142411082983017
step: 5500, Loss: 0.11288944631814957
step: 5600, Loss: 0.11406650394201279
step: 5700, Loss: 0.11307776719331741
step: 5800, Loss: 0.11536945402622223
step: 5900, Loss: 0.11351188272237778
step: 6000, Loss: 0.11282819509506226
step: 6100, Loss: 0.11393561959266663
step: 6200, Loss: 0.11379532516002655
step: 6300, Loss: 0.11402956396341324
step: 6400, Loss: 0.11434625089168549
step: 6500, Loss: 0.11358776688575745
step: 6600, Loss: 0.11392703652381897
step: 6700, Loss: 0.5778308510780334
step: 6800, Loss: 0.13177353143692017
step: 6900, Loss: 0.12218176573514938
step: 7000, Loss: 0.12161871790885925
step: 7100, Loss: 0.11756014823913574
step: 7200, Loss: 0.1189941018819809
step: 7300, Loss: 0.11537743359804153
step: 7400, Loss: 0.11563398689031601
step: 7500, Loss: 0.11711617559194565
step: 7600, Loss: 0.11537104099988937
step: 7700, Loss: 0.11492264270782471
step: 7800, Loss: 0.11359266936779022
step: 7900, Loss: 0.11417307704687119
step: 8000, Loss: 0.12134946882724762
step: 8100, Loss: 0.11429333686828613
step: 8200, Loss: 0.11665607243776321
step: 8300, Loss: 0.11492478847503662
step: 8400, Loss: 0.11496810615062714
step: 8500, Loss: 0.11378323286771774
step: 8600, Loss: 0.11416106671094894
step: 8700, Loss: 0.11492694914340973
step: 8800, Loss: 0.11462628841400146
step: 8900, Loss: 0.11399722099304199
step: 9000, Loss: 0.1151638776063919
step: 9100, Loss: 0.11428696662187576
step: 9200, Loss: 0.11593570560216904
step: 9300, Loss: 0.1139088049530983
step: 9400, Loss: 0.11419032514095306
step: 9500, Loss: 0.11456365883350372
step: 9600, Loss: 0.11383827775716782
step: 9700, Loss: 0.11515847593545914
step: 9800, Loss: 0.11548487097024918
step: 9900, Loss: 0.11412759870290756
training successfully ended.
validating...
validate data length:31
acc: 0.6666666666666666
precision: 0.75
recall: 0.6666666666666666
F_score: 0.7058823529411765
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.14982053637504578
step: 100, Loss: 0.11343127489089966
step: 200, Loss: 0.11501897126436234
step: 300, Loss: 0.11347942054271698
step: 400, Loss: 0.11353576183319092
step: 500, Loss: 0.11409807205200195
step: 600, Loss: 0.11446435004472733
step: 700, Loss: 0.11413316428661346
step: 800, Loss: 0.11375431716442108
step: 900, Loss: 0.11354400217533112
step: 1000, Loss: 0.11430791020393372
step: 1100, Loss: 0.11400183290243149
step: 1200, Loss: 0.11422161757946014
step: 1300, Loss: 0.11361082643270493
step: 1400, Loss: 0.11237331479787827
step: 1500, Loss: 0.11468509584665298
step: 1600, Loss: 0.11251987516880035
step: 1700, Loss: 0.11412850022315979
step: 1800, Loss: 0.11428236961364746
step: 1900, Loss: 0.11377989500761032
step: 2000, Loss: 0.11416972428560257
step: 2100, Loss: 0.1127389520406723
step: 2200, Loss: 0.11333175003528595
step: 2300, Loss: 0.11269880831241608
step: 2400, Loss: 0.11300039291381836
step: 2500, Loss: 0.11284688115119934
step: 2600, Loss: 0.1139330193400383
step: 2700, Loss: 0.11281563341617584
step: 2800, Loss: 0.11490082740783691
step: 2900, Loss: 0.1130942851305008
step: 3000, Loss: 0.11300316452980042
step: 3100, Loss: 0.11426869034767151
step: 3200, Loss: 0.11428244411945343
step: 3300, Loss: 0.11341354995965958
step: 3400, Loss: 0.11404838413000107
step: 3500, Loss: 0.11294819414615631
step: 3600, Loss: 0.11306950449943542
step: 3700, Loss: 0.11335623264312744
step: 3800, Loss: 0.11415952444076538
step: 3900, Loss: 0.11337055265903473
step: 4000, Loss: 0.11388920992612839
step: 4100, Loss: 0.11354757100343704
step: 4200, Loss: 0.1138225570321083
step: 4300, Loss: 0.11358749121427536
step: 4400, Loss: 0.11376134306192398
step: 4500, Loss: 0.11354022473096848
step: 4600, Loss: 0.1145666167140007
step: 4700, Loss: 0.11292310059070587
step: 4800, Loss: 0.11275840550661087
step: 4900, Loss: 0.11512114107608795
step: 5000, Loss: 0.11304962635040283
step: 5100, Loss: 0.11396944522857666
step: 5200, Loss: 0.11364796757698059
step: 5300, Loss: 0.11308617889881134
step: 5400, Loss: 0.11463778465986252
step: 5500, Loss: 0.1144615039229393
step: 5600, Loss: 0.11352425813674927
step: 5700, Loss: 0.11384201049804688
step: 5800, Loss: 0.11370972543954849
step: 5900, Loss: 0.11373300850391388
step: 6000, Loss: 0.11463838815689087
step: 6100, Loss: 0.11334766447544098
step: 6200, Loss: 0.11353810876607895
step: 6300, Loss: 0.11441457271575928
step: 6400, Loss: 0.11453398317098618
step: 6500, Loss: 0.11333176493644714
step: 6600, Loss: 0.113680899143219
step: 6700, Loss: 0.11385025829076767
step: 6800, Loss: 0.1133599653840065
step: 6900, Loss: 0.11409148573875427
step: 7000, Loss: 0.11493828892707825
step: 7100, Loss: 0.13211935758590698
step: 7200, Loss: 0.1231999397277832
step: 7300, Loss: 0.11603425443172455
step: 7400, Loss: 0.11851701140403748
step: 7500, Loss: 0.11510015279054642
step: 7600, Loss: 0.1180972158908844
step: 7700, Loss: 0.11530672013759613
step: 7800, Loss: 0.11462056636810303
step: 7900, Loss: 0.11475765705108643
step: 8000, Loss: 0.11489379405975342
step: 8100, Loss: 0.11295989155769348
step: 8200, Loss: 0.11688406765460968
step: 0, Loss: 43.32686996459961
step: 100, Loss: 4.433475017547607
step: 200, Loss: 0.7939434051513672
step: 300, Loss: 0.9726293087005615
step: 400, Loss: 0.1618737280368805
step: 500, Loss: 0.16750116646289825
step: 600, Loss: 0.15895439684391022
step: 700, Loss: 0.15172842144966125
step: 800, Loss: 0.14266283810138702
step: 900, Loss: 0.14013652503490448
step: 1000, Loss: 0.149058535695076
step: 1100, Loss: 0.12399375438690186
step: 1200, Loss: 0.13733667135238647
step: 1300, Loss: 0.13281992077827454
step: 1400, Loss: 0.13047845661640167
step: 1500, Loss: 0.20806865394115448
step: 1600, Loss: 0.1239168643951416
step: 1700, Loss: 0.15304547548294067
step: 1800, Loss: 0.1332128345966339
step: 1900, Loss: 0.12174257636070251
step: 2000, Loss: 0.11829891800880432
step: 2100, Loss: 0.12731125950813293
step: 2200, Loss: 0.11974361538887024
step: 2300, Loss: 0.12285502254962921
step: 2400, Loss: 0.11873731017112732
step: 2500, Loss: 0.12362036854028702
step: 2600, Loss: 0.11920537799596786
step: 2700, Loss: 0.12336596101522446
step: 2800, Loss: 0.11985132098197937
step: 2900, Loss: 0.1252761036157608
step: 3000, Loss: 0.11615841835737228
step: 3100, Loss: 0.11737439036369324
step: 3200, Loss: 0.11974052339792252
step: 3300, Loss: 0.11683681607246399
step: 3400, Loss: 0.1985151767730713
step: 3500, Loss: 0.1183401569724083
step: 3600, Loss: 0.12440741807222366
step: 3700, Loss: 0.11511386185884476
step: 3800, Loss: 0.11434976756572723
step: 3900, Loss: 0.11491125822067261
step: 4000, Loss: 0.11838138848543167
step: 4100, Loss: 0.11531718075275421
step: 4200, Loss: 0.11456508189439774
step: 4300, Loss: 0.1164538636803627
step: 4400, Loss: 0.1139918714761734
step: 4500, Loss: 0.11476591229438782
step: 4600, Loss: 0.11622404307126999
step: 4700, Loss: 0.11456170678138733
step: 4800, Loss: 0.11660051345825195
step: 4900, Loss: 0.11666862666606903
step: 5000, Loss: 0.11815622448921204
step: 5100, Loss: 0.11815768480300903
step: 5200, Loss: 0.11516007035970688
step: 5300, Loss: 0.19404377043247223
step: 5400, Loss: 0.11754356324672699
step: 5500, Loss: 0.12513229250907898
step: 5600, Loss: 8.864898681640625
step: 5700, Loss: 1.0507211685180664
step: 5800, Loss: 0.15835891664028168
step: 5900, Loss: 0.13736531138420105
step: 6000, Loss: 0.13622406125068665
step: 6100, Loss: 0.1376926302909851
step: 6200, Loss: 0.13574308156967163
step: 6300, Loss: 0.12906484305858612
step: 6400, Loss: 0.12723106145858765
step: 6500, Loss: 0.13077029585838318
step: 6600, Loss: 0.127325177192688
step: 6700, Loss: 0.13826417922973633
step: 6800, Loss: 0.11963484436273575
step: 6900, Loss: 0.1228792816400528
step: 7000, Loss: 0.12188059836626053
step: 7100, Loss: 0.12428711354732513
step: 7200, Loss: 0.20559661090373993
step: 7300, Loss: 0.12360796332359314
step: 7400, Loss: 0.12292403727769852
step: 7500, Loss: 0.12200000882148743
step: 7600, Loss: 0.12220151722431183
step: 7700, Loss: 0.11972381174564362
step: 7800, Loss: 0.11623746901750565
step: 7900, Loss: 0.11443912982940674
step: 8000, Loss: 0.12260997295379639
step: 8100, Loss: 0.12030810862779617
step: 8200, Loss: 0.11676705628633499
step: 8300, Loss: 0.11847370862960815
step: 8400, Loss: 0.11892268806695938
step: 8500, Loss: 0.11894061416387558
step: 8600, Loss: 0.11871703714132309
step: 8700, Loss: 0.1158444806933403
step: 8800, Loss: 0.11649975925683975
step: 8900, Loss: 0.11726512014865875
step: 9000, Loss: 0.11678192764520645
step: 9100, Loss: 0.19859816133975983
step: 9200, Loss: 0.1169634610414505
step: 9300, Loss: 0.11888687312602997
step: 9400, Loss: 0.11401872336864471
step: 9500, Loss: 0.11456689983606339
step: 9600, Loss: 0.11557228863239288
step: 9700, Loss: 0.11388054490089417
step: 9800, Loss: 0.11460065096616745
step: 9900, Loss: 0.11691003292798996
training successfully ended.
validating...
validate data length:76
acc: 0.7777777777777778
precision: 0.7021276595744681
recall: 0.9428571428571428
F_score: 0.8048780487804879
******fold 2******

Training... train_data length:684
step: 0, Loss: 0.13306519389152527
step: 100, Loss: 0.13123676180839539
step: 200, Loss: 0.13473400473594666
step: 300, Loss: 0.12334270030260086
step: 400, Loss: 0.12625755369663239
step: 500, Loss: 0.11975421756505966
step: 600, Loss: 0.12011359632015228
step: 700, Loss: 0.11569483578205109
step: 800, Loss: 0.11795014142990112
step: 900, Loss: 0.11907991766929626
step: 1000, Loss: 0.1163531169295311
step: 1100, Loss: 0.11788894236087799
step: 1200, Loss: 0.11613066494464874
step: 1300, Loss: 0.11639686673879623
step: 1400, Loss: 0.1171204000711441
step: 1500, Loss: 0.19894945621490479
step: 1600, Loss: 0.11540111899375916
step: 1700, Loss: 0.11446943134069443
step: 1800, Loss: 0.11600804328918457
step: 1900, Loss: 0.11458642035722733
step: 2000, Loss: 0.11497194319963455
step: 2100, Loss: 0.11412075161933899
step: 2200, Loss: 0.11695007234811783
step: 2300, Loss: 0.11529640853404999
step: 2400, Loss: 0.11600220948457718
step: 2500, Loss: 0.1154581680893898
step: 2600, Loss: 0.11555421352386475
step: 2700, Loss: 0.11470970511436462
step: 2800, Loss: 0.11577670276165009
step: 2900, Loss: 0.11479205638170242
step: 3000, Loss: 0.11599523574113846
step: 3100, Loss: 0.11737370491027832
step: 3200, Loss: 0.11801870167255402
step: 3300, Loss: 0.2674487829208374
step: 3400, Loss: 0.24934715032577515
step: 3500, Loss: 0.13550196588039398
step: 3600, Loss: 0.1509142816066742
step: 3700, Loss: 0.13651716709136963
step: 3800, Loss: 0.12743724882602692
step: 3900, Loss: 0.13032697141170502
step: 4000, Loss: 0.1289685219526291
step: 4100, Loss: 0.12649737298488617
step: 4200, Loss: 0.1309320628643036
step: 4300, Loss: 0.12653203308582306
step: 4400, Loss: 0.12815937399864197
step: 4500, Loss: 0.12351429462432861
step: 4600, Loss: 0.12216262519359589
step: 4700, Loss: 0.12524549663066864
step: 4800, Loss: 0.12008008360862732
step: 4900, Loss: 0.11858430504798889
step: 5000, Loss: 0.12021860480308533
step: 5100, Loss: 0.12186819314956665
step: 5200, Loss: 0.11786294728517532
step: 5300, Loss: 0.20517447590827942
step: 5400, Loss: 0.1171419769525528
step: 5500, Loss: 0.12039888650178909
step: 5600, Loss: 0.12006564438343048
step: 5700, Loss: 0.11711050570011139
step: 5800, Loss: 0.11841185390949249
step: 5900, Loss: 0.11812478303909302
step: 6000, Loss: 0.11424635350704193
step: 6100, Loss: 0.11784020066261292
step: 6200, Loss: 0.11553294956684113
step: 6300, Loss: 0.11913782358169556
step: 6400, Loss: 0.11609426885843277
step: 6500, Loss: 0.11526820808649063
step: 6600, Loss: 0.1185392290353775
step: 6700, Loss: 0.11498220264911652
step: 6800, Loss: 0.11418488621711731
step: 6900, Loss: 0.11375342309474945
step: 7000, Loss: 0.11697635054588318
step: 7100, Loss: 0.11449702084064484
step: 7200, Loss: 0.19641529023647308
step: 7300, Loss: 0.11370164155960083
step: 7400, Loss: 0.11568070948123932
step: 7500, Loss: 0.11347620189189911
step: 7600, Loss: 0.11438925564289093
step: 7700, Loss: 0.11341433227062225
step: 7800, Loss: 0.11502588540315628
step: 7900, Loss: 0.11467334628105164
step: 8000, Loss: 0.11332090944051743
step: 8100, Loss: 0.11522047221660614
step: 8200, Loss: 0.11460186541080475
step: 8300, Loss: 0.11362440139055252
step: 8400, Loss: 0.11440983414649963
step: 8500, Loss: 0.11394348740577698
step: 8600, Loss: 0.11329081654548645
step: 8700, Loss: 0.1143714189529419
step: 8800, Loss: 0.11351755261421204
step: 8900, Loss: 0.11486406624317169
step: 9000, Loss: 0.11492584645748138
step: 9100, Loss: 0.1944725066423416
step: 9200, Loss: 0.11404398083686829
step: 9300, Loss: 0.11645615100860596
step: 9400, Loss: 0.11770886927843094
step: 9500, Loss: 0.11599786579608917
step: 9600, Loss: 0.11666005104780197
step: 9700, Loss: 0.11349718272686005
step: 9800, Loss: 0.11464215815067291
step: 9900, Loss: 0.11912056803703308
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.9069767441860465
recall: 1.0
F_score: 0.951219512195122
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.35210976004600525
step: 100, Loss: 0.12582170963287354
step: 200, Loss: 0.12236885726451874
step: 300, Loss: 0.11937472224235535
step: 400, Loss: 0.11618755757808685
step: 8300, Loss: 0.11697699874639511
step: 8400, Loss: 0.11522097140550613
step: 8500, Loss: 0.11383737623691559
step: 8600, Loss: 0.1153000146150589
step: 8700, Loss: 0.11375675350427628
step: 8800, Loss: 0.11385655403137207
step: 8900, Loss: 0.11487065255641937
step: 9000, Loss: 0.1139926165342331
step: 9100, Loss: 0.11387541145086288
step: 9200, Loss: 0.1146441400051117
step: 9300, Loss: 0.11367184668779373
step: 9400, Loss: 0.11472196877002716
step: 9500, Loss: 0.11438295245170593
step: 9600, Loss: 0.11342287063598633
step: 9700, Loss: 0.1134570986032486
step: 9800, Loss: 0.1145707219839096
step: 9900, Loss: 0.11376643180847168
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.85
recall: 0.9444444444444444
F_score: 0.8947368421052632
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.35127657651901245
step: 100, Loss: 0.11808562278747559
step: 200, Loss: 0.11534999310970306
step: 300, Loss: 0.1147402748465538
step: 400, Loss: 0.11402884870767593
step: 500, Loss: 0.11466974020004272
step: 600, Loss: 0.11322222650051117
step: 700, Loss: 0.11423631012439728
step: 800, Loss: 0.11457259953022003
step: 900, Loss: 0.11539534479379654
step: 1000, Loss: 0.11327128112316132
step: 1100, Loss: 0.11466509848833084
step: 1200, Loss: 0.11349371820688248
step: 1300, Loss: 0.11274712532758713
step: 1400, Loss: 0.1128886416554451
step: 1500, Loss: 0.1130591481924057
step: 1600, Loss: 0.11444291472434998
step: 1700, Loss: 0.11421317607164383
step: 1800, Loss: 0.1126537024974823
step: 1900, Loss: 0.11599129438400269
step: 2000, Loss: 0.11323585361242294
step: 2100, Loss: 0.114580437541008
step: 2200, Loss: 0.11377831548452377
step: 2300, Loss: 0.11429698765277863
step: 2400, Loss: 0.11421002447605133
step: 2500, Loss: 0.11337609589099884
step: 2600, Loss: 0.11517557501792908
step: 2700, Loss: 0.11300984025001526
step: 2800, Loss: 0.11408794671297073
step: 2900, Loss: 0.1137629896402359
step: 3000, Loss: 0.11368520557880402
step: 3100, Loss: 0.11334426701068878
step: 3200, Loss: 0.1131591796875
step: 3300, Loss: 0.1136896163225174
step: 3400, Loss: 0.11337285488843918
step: 3500, Loss: 0.11465983837842941
step: 3600, Loss: 0.11309947818517685
step: 3700, Loss: 0.1143292486667633
step: 3800, Loss: 0.1137104332447052
step: 3900, Loss: 0.11354037374258041
step: 4000, Loss: 0.11365725845098495
step: 4100, Loss: 0.11279645562171936
step: 4200, Loss: 0.11485737562179565
step: 4300, Loss: 0.11397787928581238
step: 4400, Loss: 0.1131996214389801
step: 4500, Loss: 0.11298985779285431
step: 4600, Loss: 0.11358703672885895
step: 4700, Loss: 0.11368536204099655
step: 4800, Loss: 0.11328013241291046
step: 4900, Loss: 0.1124088317155838
step: 5000, Loss: 0.11580261588096619
step: 5100, Loss: 0.11470306664705276
step: 5200, Loss: 0.11345505714416504
step: 5300, Loss: 0.11507365107536316
step: 5400, Loss: 0.11631748825311661
step: 5500, Loss: 0.11640225350856781
step: 5600, Loss: 0.11338023841381073
step: 5700, Loss: 0.11309109628200531
step: 5800, Loss: 0.1125578060746193
step: 5900, Loss: 0.11354757845401764
step: 6000, Loss: 0.1130129024386406
step: 6100, Loss: 0.11337555199861526
step: 6200, Loss: 0.11311549693346024
step: 6300, Loss: 0.11293795704841614
step: 6400, Loss: 0.11306022852659225
step: 6500, Loss: 0.11454906314611435
step: 6600, Loss: 0.1126275509595871
step: 6700, Loss: 0.11425010859966278
step: 6800, Loss: 0.11470583081245422
step: 6900, Loss: 0.11288975179195404
step: 7000, Loss: 0.11345936357975006
step: 7100, Loss: 0.11305101960897446
step: 7200, Loss: 0.11335094273090363
step: 7300, Loss: 1.0250650644302368
step: 7400, Loss: 0.12277066707611084
step: 7500, Loss: 0.11771948635578156
step: 7600, Loss: 0.11500880867242813
step: 7700, Loss: 0.11500895023345947
step: 7800, Loss: 0.11568145453929901
step: 7900, Loss: 0.11505533009767532
step: 8000, Loss: 0.11571155488491058
step: 8100, Loss: 0.11540722101926804
step: 8200, Loss: 0.11362522095441818
step: 8300, Loss: 0.11463810503482819
step: 8400, Loss: 0.1144394800066948
step: 8500, Loss: 0.11496175825595856
step: 8600, Loss: 0.1164836585521698
step: 8700, Loss: 0.1134902685880661
step: 8800, Loss: 0.11361996829509735
step: 8900, Loss: 0.11338440328836441
step: 9000, Loss: 0.11488571763038635
step: 9100, Loss: 0.11460298299789429
step: 9200, Loss: 0.11534900218248367
step: 9300, Loss: 0.11373984813690186
step: 9400, Loss: 0.11358900368213654
step: 9500, Loss: 0.11354313045740128
step: 9600, Loss: 0.11536867916584015
step: 9700, Loss: 0.11304160952568054
step: 9800, Loss: 0.11340059340000153
step: 9900, Loss: 0.1128353700041771
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8333333333333334
recall: 0.8823529411764706
F_score: 0.8571428571428571
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.3154589533805847
step: 100, Loss: 0.11790424585342407
step: 200, Loss: 0.11478292942047119
step: 300, Loss: 0.11333859711885452
step: 400, Loss: 0.11455392092466354
step: 500, Loss: 0.1145249456167221
step: 600, Loss: 0.11491860449314117
step: 700, Loss: 0.11352363228797913
step: 800, Loss: 0.11389945447444916
step: 900, Loss: 0.11288106441497803
step: 1000, Loss: 0.1134936511516571
step: 1100, Loss: 0.11299576610326767
step: 1200, Loss: 0.11305937170982361
step: 1300, Loss: 0.11320159584283829
step: 1400, Loss: 0.11331556737422943
step: 1500, Loss: 0.11330091208219528
step: 1600, Loss: 0.11389417201280594
step: 1700, Loss: 0.11531032621860504
step: 1800, Loss: 0.11346497386693954
step: 1900, Loss: 0.11396963149309158
step: 2000, Loss: 0.11405089497566223
step: 2100, Loss: 0.11478894203901291
step: 2200, Loss: 0.11544382572174072
step: 2300, Loss: 0.11317813396453857
step: 2400, Loss: 0.11307183653116226
step: 2500, Loss: 0.11389575898647308
step: 2600, Loss: 0.11367321014404297
step: 2700, Loss: 0.11398511379957199
step: 2800, Loss: 0.11290739476680756
step: 2900, Loss: 0.11282461881637573
step: 3000, Loss: 0.11373161524534225
step: 3100, Loss: 0.11278361082077026
step: 3200, Loss: 0.11302216351032257
step: 3300, Loss: 0.11276423931121826
step: 3400, Loss: 0.11444542557001114
step: 3500, Loss: 0.11264146864414215
step: 3600, Loss: 0.11494170129299164
step: 3700, Loss: 0.11248083412647247
step: 3800, Loss: 0.11277764290571213
step: 3900, Loss: 0.11312325298786163
step: 4000, Loss: 0.11335121840238571
step: 4100, Loss: 0.11318221688270569
step: 4200, Loss: 0.11470044404268265
step: 4300, Loss: 0.11365090310573578
step: 4400, Loss: 0.11550898849964142
step: 4500, Loss: 0.11435756087303162
step: 4600, Loss: 0.11300346255302429
step: 4700, Loss: 0.11411957442760468
step: 4800, Loss: 0.11417794227600098
step: 4900, Loss: 0.11451850086450577
step: 5000, Loss: 0.11376889050006866
step: 5100, Loss: 0.11328329145908356
step: 5200, Loss: 0.11700005829334259
step: 5300, Loss: 0.11322375386953354
step: 5400, Loss: 0.11383496224880219
step: 5500, Loss: 0.11786200851202011
step: 5600, Loss: 0.11454480141401291
step: 5700, Loss: 0.11354734003543854
step: 5800, Loss: 0.11341936886310577
step: 5900, Loss: 0.11321946978569031
step: 6000, Loss: 0.1146550178527832
step: 6100, Loss: 0.11228936165571213
step: 6200, Loss: 0.11461704969406128
step: 6300, Loss: 0.17305885255336761
step: 6400, Loss: 0.1262780874967575
step: 6500, Loss: 0.1271132379770279
step: 6600, Loss: 0.12063506990671158
step: 6700, Loss: 0.11680543422698975
step: 6800, Loss: 0.11782145500183105
step: 6900, Loss: 0.11638382077217102
step: 7000, Loss: 0.11824993789196014
step: 7100, Loss: 0.1159791648387909
step: 7200, Loss: 0.11455278843641281
step: 7300, Loss: 0.11506877839565277
step: 7400, Loss: 0.11654897034168243
step: 7500, Loss: 0.11312442272901535
step: 7600, Loss: 0.1149861291050911
step: 7700, Loss: 0.1139800027012825
step: 7800, Loss: 0.11484716087579727
step: 7900, Loss: 0.11454371362924576
step: 8000, Loss: 0.1150166243314743
step: 8100, Loss: 0.11455050855875015
step: 8200, Loss: 0.11431972682476044
step: 8300, Loss: 0.11467339098453522
step: 8400, Loss: 0.1134556382894516
step: 8500, Loss: 0.11406245827674866
step: 8600, Loss: 0.11421134322881699
step: 8700, Loss: 0.11604427546262741
step: 500, Loss: 0.11734840273857117
step: 600, Loss: 0.1171221062541008
step: 700, Loss: 0.11608763784170151
step: 800, Loss: 0.11499089747667313
step: 900, Loss: 0.11395865678787231
step: 1000, Loss: 0.11494801193475723
step: 1100, Loss: 0.11540502309799194
step: 1200, Loss: 0.11511441320180893
step: 1300, Loss: 0.11522271484136581
step: 1400, Loss: 0.11449005454778671
step: 1500, Loss: 0.19420087337493896
step: 1600, Loss: 0.11664612591266632
step: 1700, Loss: 0.1135372668504715
step: 1800, Loss: 0.11395217478275299
step: 1900, Loss: 0.1136959046125412
step: 2000, Loss: 0.11644947528839111
step: 2100, Loss: 0.11366928368806839
step: 2200, Loss: 0.11315996199846268
step: 2300, Loss: 0.11694630980491638
step: 2400, Loss: 0.1138429194688797
step: 2500, Loss: 0.11370144784450531
step: 2600, Loss: 0.11412213742733002
step: 2700, Loss: 0.11332613974809647
step: 2800, Loss: 0.1148696094751358
step: 2900, Loss: 0.11406785994768143
step: 3000, Loss: 0.11439672857522964
step: 3100, Loss: 0.11393320560455322
step: 3200, Loss: 0.11327605694532394
step: 3300, Loss: 0.11388882994651794
step: 3400, Loss: 0.19090145826339722
step: 3500, Loss: 0.11348052322864532
step: 3600, Loss: 0.11501124501228333
step: 3700, Loss: 0.11581765860319138
step: 3800, Loss: 0.11568784713745117
step: 3900, Loss: 0.11827528476715088
step: 4000, Loss: 0.11457289010286331
step: 4100, Loss: 4.979695796966553
step: 4200, Loss: 0.20065946877002716
step: 4300, Loss: 0.16599687933921814
step: 4400, Loss: 0.1517607569694519
step: 4500, Loss: 0.13667702674865723
step: 4600, Loss: 0.13322430849075317
step: 4700, Loss: 0.1243349090218544
step: 4800, Loss: 0.12531191110610962
step: 4900, Loss: 0.1221601814031601
step: 5000, Loss: 0.12168221920728683
step: 5100, Loss: 0.12070855498313904
step: 5200, Loss: 0.12053391337394714
step: 5300, Loss: 0.20779971778392792
step: 5400, Loss: 0.11845472455024719
step: 5500, Loss: 0.12020444124937057
step: 5600, Loss: 0.12071964144706726
step: 5700, Loss: 0.11996515840291977
step: 5800, Loss: 0.11523903161287308
step: 5900, Loss: 0.11711657047271729
step: 6000, Loss: 0.11548315733671188
step: 6100, Loss: 0.11774888634681702
step: 6200, Loss: 0.11732480674982071
step: 6300, Loss: 0.11586026847362518
step: 6400, Loss: 0.11821412295103073
step: 6500, Loss: 0.11633743345737457
step: 6600, Loss: 0.1166234165430069
step: 6700, Loss: 0.11756689101457596
step: 6800, Loss: 0.11468194425106049
step: 6900, Loss: 0.11607056856155396
step: 7000, Loss: 0.11358505487442017
step: 7100, Loss: 0.11402403563261032
step: 7200, Loss: 0.19777216017246246
step: 7300, Loss: 0.11385058611631393
step: 7400, Loss: 0.11470703780651093
step: 7500, Loss: 0.11411996185779572
step: 7600, Loss: 0.11454842984676361
step: 7700, Loss: 0.11324204504489899
step: 7800, Loss: 0.11531330645084381
step: 7900, Loss: 0.11455442011356354
step: 8000, Loss: 0.11349507421255112
step: 8100, Loss: 0.11349289864301682
step: 8200, Loss: 0.1148725301027298
step: 8300, Loss: 0.11431294679641724
step: 8400, Loss: 0.1130332425236702
step: 8500, Loss: 0.11413083970546722
step: 8600, Loss: 0.1143842414021492
step: 8700, Loss: 0.1132429838180542
step: 8800, Loss: 0.11487457901239395
step: 8900, Loss: 0.1134304404258728
step: 9000, Loss: 0.11310920864343643
step: 9100, Loss: 0.19533255696296692
step: 9200, Loss: 0.11429159343242645
step: 9300, Loss: 0.11445920914411545
step: 9400, Loss: 0.11439482122659683
step: 9500, Loss: 0.11392413824796677
step: 9600, Loss: 0.11350475251674652
step: 9700, Loss: 0.11463694274425507
step: 9800, Loss: 0.1137458086013794
step: 9900, Loss: 0.11580917239189148
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9487179487179487
recall: 0.9736842105263158
F_score: 0.9610389610389611
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.14657513797283173
step: 100, Loss: 0.12128914147615433
step: 200, Loss: 0.11636658757925034
step: 300, Loss: 0.1155276969075203
step: 400, Loss: 0.1146313026547432
step: 500, Loss: 0.11330747604370117
step: 600, Loss: 0.1145033985376358
step: 700, Loss: 0.11748123168945312
step: 800, Loss: 0.117154061794281
step: 900, Loss: 0.11394038051366806
step: 1000, Loss: 0.11427675187587738
step: 1100, Loss: 0.11571186780929565
step: 1200, Loss: 0.1143283098936081
step: 1300, Loss: 0.11289948970079422
step: 1400, Loss: 0.11277779936790466
step: 1500, Loss: 0.19239218533039093
step: 1600, Loss: 0.11371245235204697
step: 1700, Loss: 0.1138809472322464
step: 1800, Loss: 0.1141362264752388
step: 1900, Loss: 0.11346958577632904
step: 2000, Loss: 0.11386358737945557
step: 2100, Loss: 0.114165298640728
step: 2200, Loss: 0.11473256349563599
step: 2300, Loss: 0.11543169617652893
step: 2400, Loss: 0.11428254097700119
step: 2500, Loss: 0.11366349458694458
step: 2600, Loss: 0.11402910947799683
step: 2700, Loss: 0.11513981223106384
step: 2800, Loss: 0.11490960419178009
step: 2900, Loss: 0.1156727522611618
step: 3000, Loss: 0.12007187306880951
step: 3100, Loss: 0.11581925302743912
step: 3200, Loss: 0.11373095959424973
step: 3300, Loss: 0.11445388942956924
step: 3400, Loss: 0.19487592577934265
step: 3500, Loss: 0.11410392075777054
step: 3600, Loss: 0.11430458724498749
step: 3700, Loss: 0.11434188485145569
step: 3800, Loss: 0.11474183201789856
step: 3900, Loss: 0.11458390951156616
step: 4000, Loss: 0.11385655403137207
step: 4100, Loss: 0.11434608697891235
step: 4200, Loss: 0.1148831844329834
step: 4300, Loss: 0.7879767417907715
step: 4400, Loss: 0.8939543962478638
step: 4500, Loss: 0.13871902227401733
step: 4600, Loss: 0.1341649442911148
step: 4700, Loss: 0.13197444379329681
step: 4800, Loss: 0.1282975822687149
step: 4900, Loss: 0.12462034076452255
step: 5000, Loss: 0.12515988945960999
step: 5100, Loss: 0.12411821633577347
step: 5200, Loss: 0.12243383377790451
step: 5300, Loss: 0.19993801414966583
step: 5400, Loss: 0.1246447041630745
step: 5500, Loss: 0.12613549828529358
step: 5600, Loss: 0.1215173676609993
step: 5700, Loss: 0.12051337957382202
step: 5800, Loss: 0.1214236170053482
step: 5900, Loss: 0.11676735430955887
step: 6000, Loss: 0.1185227483510971
step: 6100, Loss: 0.12175342440605164
step: 6200, Loss: 0.11450903117656708
step: 6300, Loss: 0.11668185144662857
step: 6400, Loss: 0.11870677769184113
step: 6500, Loss: 0.1149306520819664
step: 6600, Loss: 0.11963576078414917
step: 6700, Loss: 0.11749834567308426
step: 6800, Loss: 0.11691629886627197
step: 6900, Loss: 0.11663877964019775
step: 7000, Loss: 0.11535115540027618
step: 7100, Loss: 0.11431455612182617
step: 7200, Loss: 0.19700799882411957
step: 7300, Loss: 0.11493772268295288
step: 7400, Loss: 0.11650911718606949
step: 7500, Loss: 0.11446379125118256
step: 7600, Loss: 0.11405232548713684
step: 7700, Loss: 0.11529412120580673
step: 7800, Loss: 0.11672662198543549
step: 7900, Loss: 0.11614260822534561
step: 8000, Loss: 0.11361619085073471
step: 8100, Loss: 0.11460089683532715
step: 8200, Loss: 0.11432522535324097
step: 8300, Loss: 0.11337798833847046
step: 8400, Loss: 0.11394559592008591
step: 8500, Loss: 0.11538463085889816
step: 8600, Loss: 0.11556665599346161
step: 8700, Loss: 0.11440008878707886
step: 8800, Loss: 0.11510913819074631
step: 8900, Loss: 0.11322973668575287
step: 9000, Loss: 0.11441021412611008
step: 9100, Loss: 0.19470341503620148
step: 9200, Loss: 0.11314661800861359
step: 9300, Loss: 0.11264445632696152
step: 9400, Loss: 0.1134830117225647
step: 9500, Loss: 0.1131306141614914
step: 9600, Loss: 0.11521806567907333
step: 9700, Loss: 0.11405472457408905
step: 9800, Loss: 0.1137005016207695
step: 9900, Loss: 0.11439844965934753
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.8974358974358975
recall: 1.0
F_score: 0.945945945945946
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.15525510907173157
step: 100, Loss: 0.11774816364049911
step: 200, Loss: 0.11646133661270142
step: 300, Loss: 0.11704099923372269
step: 400, Loss: 0.11432904750108719
step: 500, Loss: 0.11605676263570786
step: 600, Loss: 0.11409853398799896
step: 700, Loss: 0.11443700641393661
step: 800, Loss: 0.1145368292927742
step: 900, Loss: 0.11409533023834229
step: 8800, Loss: 0.11297953873872757
step: 8900, Loss: 0.11377300322055817
step: 9000, Loss: 0.1131780669093132
step: 9100, Loss: 0.11484681814908981
step: 9200, Loss: 0.11425040662288666
step: 9300, Loss: 0.11580299586057663
step: 9400, Loss: 0.11428581178188324
step: 9500, Loss: 0.11305060982704163
step: 9600, Loss: 0.11393769085407257
step: 9700, Loss: 0.11441852152347565
step: 9800, Loss: 0.11338725686073303
step: 9900, Loss: 0.11381280422210693
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9230769230769231
recall: 0.8
F_score: 0.8571428571428571
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.26937609910964966
step: 100, Loss: 0.11714518070220947
step: 200, Loss: 0.11396659165620804
step: 300, Loss: 0.11378028243780136
step: 400, Loss: 0.11526449024677277
step: 500, Loss: 0.11358154565095901
step: 600, Loss: 0.11688549816608429
step: 700, Loss: 0.1137772798538208
step: 800, Loss: 0.11419319361448288
step: 900, Loss: 0.11410865932703018
step: 1000, Loss: 0.11307500302791595
step: 1100, Loss: 0.11410483717918396
step: 1200, Loss: 0.11371010541915894
step: 1300, Loss: 0.11388340592384338
step: 1400, Loss: 0.11407684534788132
step: 1500, Loss: 0.11358293890953064
step: 1600, Loss: 0.11322563141584396
step: 1700, Loss: 0.1143343448638916
step: 1800, Loss: 0.11374662071466446
step: 1900, Loss: 0.11378829926252365
step: 2000, Loss: 0.11531218141317368
step: 2100, Loss: 0.11352281272411346
step: 2200, Loss: 0.11356302350759506
step: 2300, Loss: 0.11695963144302368
step: 2400, Loss: 0.11346367746591568
step: 2500, Loss: 0.11339262872934341
step: 2600, Loss: 0.12005775421857834
step: 2700, Loss: 0.11313845217227936
step: 2800, Loss: 0.11435167491436005
step: 2900, Loss: 0.11294331401586533
step: 3000, Loss: 0.1148715615272522
step: 3100, Loss: 0.11411266773939133
step: 3200, Loss: 0.11371861398220062
step: 3300, Loss: 0.11499461531639099
step: 3400, Loss: 0.11287865042686462
step: 3500, Loss: 0.11346083879470825
step: 3600, Loss: 0.11311645805835724
step: 3700, Loss: 0.11378426104784012
step: 3800, Loss: 0.11509332060813904
step: 3900, Loss: 0.11348039656877518
step: 4000, Loss: 0.11297810822725296
step: 4100, Loss: 0.11465705186128616
step: 4200, Loss: 0.11465167999267578
step: 4300, Loss: 0.11345481872558594
step: 4400, Loss: 0.1140732541680336
step: 4500, Loss: 0.11343231052160263
step: 4600, Loss: 0.1137780249118805
step: 4700, Loss: 0.11371368914842606
step: 4800, Loss: 0.11517088115215302
step: 4900, Loss: 0.1133527010679245
step: 5000, Loss: 0.11285056918859482
step: 5100, Loss: 0.11312872916460037
step: 5200, Loss: 0.11271697282791138
step: 5300, Loss: 0.11434227228164673
step: 5400, Loss: 0.11465074867010117
step: 5500, Loss: 0.11339394003152847
step: 5600, Loss: 0.11388921737670898
step: 5700, Loss: 0.11260857433080673
step: 5800, Loss: 0.11304248869419098
step: 5900, Loss: 0.11454207450151443
step: 6000, Loss: 0.11329512298107147
step: 6100, Loss: 0.11341452598571777
step: 6200, Loss: 0.11319375038146973
step: 6300, Loss: 0.11293923109769821
step: 6400, Loss: 0.11408080160617828
step: 6500, Loss: 0.11266064643859863
step: 6600, Loss: 0.11409008502960205
step: 6700, Loss: 1.1402209997177124
step: 6800, Loss: 0.1331537514925003
step: 6900, Loss: 0.11667823791503906
step: 7000, Loss: 0.11917639523744583
step: 7100, Loss: 0.11826788634061813
step: 7200, Loss: 0.11833272129297256
step: 7300, Loss: 0.1158929318189621
step: 7400, Loss: 0.11583417654037476
step: 7500, Loss: 0.11557817459106445
step: 7600, Loss: 0.11570480465888977
step: 7700, Loss: 0.11532165855169296
step: 7800, Loss: 0.11737313121557236
step: 7900, Loss: 0.11539320647716522
step: 8000, Loss: 0.11436359584331512
step: 8100, Loss: 0.11474735289812088
step: 8200, Loss: 0.11310829222202301
step: 8300, Loss: 0.11492863297462463
step: 8400, Loss: 0.11450054496526718
step: 8500, Loss: 0.11354470998048782
step: 8600, Loss: 0.11305549740791321
step: 8700, Loss: 0.1149391233921051
step: 8800, Loss: 0.11529592424631119
step: 8900, Loss: 0.11451698839664459
step: 9000, Loss: 0.11586633324623108
step: 9100, Loss: 0.11565050482749939
step: 9200, Loss: 0.11298669129610062
step: 9300, Loss: 0.11397138983011246
step: 9400, Loss: 0.11947594583034515
step: 9500, Loss: 0.11443016678094864
step: 9600, Loss: 0.1146937683224678
step: 9700, Loss: 0.11367572844028473
step: 9800, Loss: 0.11603409051895142
step: 9900, Loss: 0.11399704962968826
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.7222222222222222
recall: 0.7647058823529411
F_score: 0.7428571428571428
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.33905813097953796
step: 100, Loss: 0.11934049427509308
step: 200, Loss: 0.11721614003181458
step: 300, Loss: 0.11485953629016876
step: 400, Loss: 0.11487016826868057
step: 500, Loss: 0.11314011365175247
step: 600, Loss: 0.11312633007764816
step: 700, Loss: 0.1134907677769661
step: 800, Loss: 0.11361759155988693
step: 900, Loss: 0.11546336114406586
step: 1000, Loss: 0.1134909838438034
step: 1100, Loss: 0.11344953626394272
step: 1200, Loss: 0.11464335024356842
step: 1300, Loss: 0.11324093490839005
step: 1400, Loss: 0.11397533863782883
step: 1500, Loss: 0.11287175863981247
step: 1600, Loss: 0.112706758081913
step: 1700, Loss: 0.1138589158654213
step: 1800, Loss: 0.11260119825601578
step: 1900, Loss: 0.11352574080228806
step: 2000, Loss: 0.11437603086233139
step: 2100, Loss: 0.11294122040271759
step: 2200, Loss: 0.11357536911964417
step: 2300, Loss: 0.11300436407327652
step: 2400, Loss: 0.11382871866226196
step: 2500, Loss: 0.1133449450135231
step: 2600, Loss: 0.11362246423959732
step: 2700, Loss: 0.1143568605184555
step: 2800, Loss: 0.11350622028112411
step: 2900, Loss: 0.11364535987377167
step: 3000, Loss: 0.1128024086356163
step: 3100, Loss: 0.11285699158906937
step: 3200, Loss: 0.1131671667098999
step: 3300, Loss: 0.11288933455944061
step: 3400, Loss: 0.11383480578660965
step: 3500, Loss: 0.11410154402256012
step: 3600, Loss: 0.11337782442569733
step: 3700, Loss: 0.11281836032867432
step: 3800, Loss: 0.11304369568824768
step: 3900, Loss: 0.1137351244688034
step: 4000, Loss: 0.11398972570896149
step: 4100, Loss: 0.11412540078163147
step: 4200, Loss: 0.11522196978330612
step: 4300, Loss: 0.11344970762729645
step: 4400, Loss: 0.1149359717965126
step: 4500, Loss: 0.11345024406909943
step: 4600, Loss: 0.11324718594551086
step: 4700, Loss: 0.11325154453516006
step: 4800, Loss: 0.11356266587972641
step: 4900, Loss: 0.11324775218963623
step: 5000, Loss: 0.11376357823610306
step: 5100, Loss: 0.11369951069355011
step: 5200, Loss: 0.11254522949457169
step: 5300, Loss: 0.1167413741350174
step: 5400, Loss: 0.11506728082895279
step: 5500, Loss: 0.11326804757118225
step: 5600, Loss: 0.11394459009170532
step: 5700, Loss: 0.11435927450656891
step: 5800, Loss: 0.11506876349449158
step: 5900, Loss: 0.11344876140356064
step: 6000, Loss: 0.11418972909450531
step: 6100, Loss: 0.11353942006826401
step: 6200, Loss: 0.1137392520904541
step: 6300, Loss: 0.1132940948009491
step: 6400, Loss: 0.11453405767679214
step: 6500, Loss: 0.11507394164800644
step: 6600, Loss: 0.11448530852794647
step: 6700, Loss: 0.11295229196548462
step: 6800, Loss: 0.11483347415924072
step: 6900, Loss: 0.3374018371105194
step: 7000, Loss: 0.13048426806926727
step: 7100, Loss: 0.118336521089077
step: 7200, Loss: 0.11969497799873352
step: 7300, Loss: 0.1177297830581665
step: 7400, Loss: 0.11865125596523285
step: 7500, Loss: 0.11370641738176346
step: 7600, Loss: 0.11607110500335693
step: 7700, Loss: 0.11583707481622696
step: 7800, Loss: 0.11500121653079987
step: 7900, Loss: 0.11415192484855652
step: 8000, Loss: 0.11600508540868759
step: 8100, Loss: 0.11511874198913574
step: 8200, Loss: 0.11416158080101013
step: 8300, Loss: 0.113758385181427
step: 8400, Loss: 0.11613167822360992
step: 8500, Loss: 0.11389981210231781
step: 8600, Loss: 0.11425641924142838
step: 8700, Loss: 0.1148330345749855
step: 8800, Loss: 0.11596287786960602
step: 8900, Loss: 0.11322452872991562
step: 9000, Loss: 0.11571644991636276
step: 9100, Loss: 0.11451075971126556
step: 9200, Loss: 0.11333910375833511
step: 1000, Loss: 0.1137591227889061
step: 1100, Loss: 0.11404506862163544
step: 1200, Loss: 0.11522970348596573
step: 1300, Loss: 0.11327441781759262
step: 1400, Loss: 0.11290451884269714
step: 1500, Loss: 0.1923564225435257
step: 1600, Loss: 0.1130906417965889
step: 1700, Loss: 0.1130858063697815
step: 1800, Loss: 0.11378089338541031
step: 1900, Loss: 0.11338350176811218
step: 2000, Loss: 0.11357071250677109
step: 2100, Loss: 0.11298596113920212
step: 2200, Loss: 0.11392403393983841
step: 2300, Loss: 0.11500852555036545
step: 2400, Loss: 0.11447156965732574
step: 2500, Loss: 0.11440463364124298
step: 2600, Loss: 0.11391698569059372
step: 2700, Loss: 0.1156715676188469
step: 2800, Loss: 0.11482782661914825
step: 2900, Loss: 0.11493341624736786
step: 3000, Loss: 0.11655925214290619
step: 3100, Loss: 0.11378871649503708
step: 3200, Loss: 0.11372281610965729
step: 3300, Loss: 1.7994970083236694
step: 3400, Loss: 0.22903317213058472
step: 3500, Loss: 0.12554985284805298
step: 3600, Loss: 0.14332373440265656
step: 3700, Loss: 0.12771430611610413
step: 3800, Loss: 0.12230361998081207
step: 3900, Loss: 0.12673825025558472
step: 4000, Loss: 0.1243617832660675
step: 4100, Loss: 0.12022686004638672
step: 4200, Loss: 0.1193152666091919
step: 4300, Loss: 0.11871244013309479
step: 4400, Loss: 0.12446398288011551
step: 4500, Loss: 0.11855277419090271
step: 4600, Loss: 0.11939212679862976
step: 4700, Loss: 0.1188853308558464
step: 4800, Loss: 0.12419197708368301
step: 4900, Loss: 0.12229317426681519
step: 5000, Loss: 0.11946271359920502
step: 5100, Loss: 0.11646828800439835
step: 5200, Loss: 0.11643895506858826
step: 5300, Loss: 0.20071126520633698
step: 5400, Loss: 0.11598727107048035
step: 5500, Loss: 0.11801829189062119
step: 5600, Loss: 0.12056105583906174
step: 5700, Loss: 0.11516264081001282
step: 5800, Loss: 0.11572983115911484
step: 5900, Loss: 0.11430199444293976
step: 6000, Loss: 0.11467181146144867
step: 6100, Loss: 0.1150003969669342
step: 6200, Loss: 0.11448715627193451
step: 6300, Loss: 0.11720254272222519
step: 6400, Loss: 0.11389727890491486
step: 6500, Loss: 0.11631552129983902
step: 6600, Loss: 0.1159413754940033
step: 6700, Loss: 0.11694763600826263
step: 6800, Loss: 0.11421416699886322
step: 6900, Loss: 0.11449773609638214
step: 7000, Loss: 0.11554522067308426
step: 7100, Loss: 0.11528793722391129
step: 7200, Loss: 0.19407570362091064
step: 7300, Loss: 0.11462681740522385
step: 7400, Loss: 0.11502569168806076
step: 7500, Loss: 0.11440970748662949
step: 7600, Loss: 0.11399693042039871
step: 7700, Loss: 0.11362406611442566
step: 7800, Loss: 0.1134808287024498
step: 7900, Loss: 0.11373066902160645
step: 8000, Loss: 0.11419772356748581
step: 8100, Loss: 0.11449002474546432
step: 8200, Loss: 0.11443641781806946
step: 8300, Loss: 0.11439800262451172
step: 8400, Loss: 0.11323478817939758
step: 8500, Loss: 0.11338816583156586
step: 8600, Loss: 0.114391028881073
step: 8700, Loss: 0.11423979699611664
step: 8800, Loss: 0.11299872398376465
step: 8900, Loss: 0.11347091943025589
step: 9000, Loss: 0.11308464407920837
step: 9100, Loss: 0.1937783658504486
step: 9200, Loss: 0.11472935974597931
step: 9300, Loss: 0.11360825598239899
step: 9400, Loss: 0.1152837947010994
step: 9500, Loss: 0.11396638303995132
step: 9600, Loss: 0.114316925406456
step: 9700, Loss: 0.11410950869321823
step: 9800, Loss: 0.11525870859622955
step: 9900, Loss: 0.11432197690010071
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.96875
recall: 0.9393939393939394
F_score: 0.9538461538461539
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.14308567345142365
step: 100, Loss: 0.1192936897277832
step: 200, Loss: 0.11644440144300461
step: 300, Loss: 0.1143408715724945
step: 400, Loss: 0.11469715088605881
step: 500, Loss: 0.11626212298870087
step: 600, Loss: 0.1164718046784401
step: 700, Loss: 0.1143369972705841
step: 800, Loss: 0.11536207795143127
step: 900, Loss: 0.1151384636759758
step: 1000, Loss: 0.11349260061979294
step: 1100, Loss: 0.11369156837463379
step: 1200, Loss: 0.11342757940292358
step: 1300, Loss: 0.11343380063772202
step: 1400, Loss: 0.1160028800368309
step: 1500, Loss: 0.19487035274505615
step: 1600, Loss: 0.11405956745147705
step: 1700, Loss: 0.1128728985786438
step: 1800, Loss: 0.11441172659397125
step: 1900, Loss: 0.11356384307146072
step: 2000, Loss: 0.11611858010292053
step: 2100, Loss: 0.11549705266952515
step: 2200, Loss: 0.11287912726402283
step: 2300, Loss: 0.1137344092130661
step: 2400, Loss: 0.1131356880068779
step: 2500, Loss: 0.11470074951648712
step: 2600, Loss: 0.11347628384828568
step: 2700, Loss: 0.11425384879112244
step: 2800, Loss: 0.11631954461336136
step: 2900, Loss: 0.1151997447013855
step: 3000, Loss: 0.11494922637939453
step: 3100, Loss: 0.11581436544656754
step: 3200, Loss: 0.11569047719240189
step: 3300, Loss: 0.1139708161354065
step: 3400, Loss: 0.19203214347362518
step: 3500, Loss: 0.1138073280453682
step: 3600, Loss: 0.11536671966314316
step: 3700, Loss: 0.11400087177753448
step: 3800, Loss: 0.11613153666257858
step: 3900, Loss: 0.11643484234809875
step: 4000, Loss: 1.8331178426742554
step: 4100, Loss: 0.14879778027534485
step: 4200, Loss: 0.14828148484230042
step: 4300, Loss: 0.13012608885765076
step: 4400, Loss: 0.1280219405889511
step: 4500, Loss: 0.12850478291511536
step: 4600, Loss: 0.12338227778673172
step: 4700, Loss: 0.12923233211040497
step: 4800, Loss: 0.13197635114192963
step: 4900, Loss: 0.1216464713215828
step: 5000, Loss: 0.12116441875696182
step: 5100, Loss: 0.12273199111223221
step: 5200, Loss: 0.12482628226280212
step: 5300, Loss: 0.19926294684410095
step: 5400, Loss: 0.11712959408760071
step: 5500, Loss: 0.120285764336586
step: 5600, Loss: 0.12184856832027435
step: 5700, Loss: 0.12165752053260803
step: 5800, Loss: 0.12410531938076019
step: 5900, Loss: 0.11738934367895126
step: 6000, Loss: 0.11627599596977234
step: 6100, Loss: 0.11804348230361938
step: 6200, Loss: 0.11913250386714935
step: 6300, Loss: 0.11799542605876923
step: 6400, Loss: 0.11875356733798981
step: 6500, Loss: 0.11641897261142731
step: 6600, Loss: 0.1160903126001358
step: 6700, Loss: 0.1204453632235527
step: 6800, Loss: 0.11451522260904312
step: 6900, Loss: 0.11741580814123154
step: 7000, Loss: 0.1157311499118805
step: 7100, Loss: 0.11605265736579895
step: 7200, Loss: 0.19615888595581055
step: 7300, Loss: 0.11526191234588623
step: 7400, Loss: 0.11791965365409851
step: 7500, Loss: 0.11470407992601395
step: 7600, Loss: 0.11576275527477264
step: 7700, Loss: 0.11467723548412323
step: 7800, Loss: 0.11386480927467346
step: 7900, Loss: 0.11533429473638535
step: 8000, Loss: 0.11329212784767151
step: 8100, Loss: 0.11470600962638855
step: 8200, Loss: 0.11496777832508087
step: 8300, Loss: 0.11347657442092896
step: 8400, Loss: 0.11360640078783035
step: 8500, Loss: 0.11415402591228485
step: 8600, Loss: 0.11529761552810669
step: 8700, Loss: 0.11561916023492813
step: 8800, Loss: 0.11524271219968796
step: 8900, Loss: 0.11512946337461472
step: 9000, Loss: 0.11375226080417633
step: 9100, Loss: 0.19443580508232117
step: 9200, Loss: 0.1147054135799408
step: 9300, Loss: 0.11428233236074448
step: 9400, Loss: 0.11442379653453827
step: 9500, Loss: 0.11452294141054153
step: 9600, Loss: 0.1129603162407875
step: 9700, Loss: 0.11441013216972351
step: 9800, Loss: 0.11435352265834808
step: 9900, Loss: 0.11363248527050018
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9736842105263158
recall: 1.0
F_score: 0.9866666666666666
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11871302127838135
step: 100, Loss: 0.13526496291160583
step: 200, Loss: 0.12369319796562195
step: 300, Loss: 0.11806482821702957
step: 400, Loss: 0.11530548334121704
step: 500, Loss: 0.11776931583881378
step: 600, Loss: 0.11511837691068649
step: 700, Loss: 0.1171676516532898
step: 800, Loss: 0.11478833109140396
step: 900, Loss: 0.11613455414772034
step: 1000, Loss: 0.11317870020866394
step: 1100, Loss: 0.11399977654218674
step: 1200, Loss: 0.11630866676568985
step: 1300, Loss: 0.11349780857563019
step: 1400, Loss: 0.1136457622051239
step: 9300, Loss: 0.11486148834228516
step: 9400, Loss: 0.11478075385093689
step: 9500, Loss: 0.11415930837392807
step: 9600, Loss: 0.11437168717384338
step: 9700, Loss: 0.11366152763366699
step: 9800, Loss: 0.11335856467485428
step: 9900, Loss: 0.11480721831321716
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8181818181818182
recall: 0.75
F_score: 0.7826086956521738
subject 17 Avgacc: 0.7462500000000001 Avgfscore: 0.7525517076844536 
 Max acc:0.8666666666666667, Max f score:0.8947368421052632
******** mix subject_18 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 46.594905853271484
step: 100, Loss: 4.161192417144775
step: 200, Loss: 0.13750435411930084
step: 300, Loss: 0.1334286332130432
step: 400, Loss: 0.12708419561386108
step: 500, Loss: 0.12613369524478912
step: 600, Loss: 0.12397854030132294
step: 700, Loss: 0.125888854265213
step: 800, Loss: 0.1199910119175911
step: 900, Loss: 0.1359824538230896
step: 1000, Loss: 0.12187149375677109
step: 1100, Loss: 0.11640450358390808
step: 1200, Loss: 0.12033451348543167
step: 1300, Loss: 0.11930286884307861
step: 1400, Loss: 0.11657674610614777
step: 1500, Loss: 0.12753964960575104
step: 1600, Loss: 0.11751118302345276
step: 1700, Loss: 0.11801405996084213
step: 1800, Loss: 0.117156982421875
step: 1900, Loss: 0.11839455366134644
step: 2000, Loss: 0.1164025217294693
step: 2100, Loss: 0.12007397413253784
step: 2200, Loss: 0.11626160144805908
step: 2300, Loss: 0.11729200184345245
step: 2400, Loss: 0.11800869554281235
step: 2500, Loss: 0.12007695436477661
step: 2600, Loss: 0.1194617748260498
step: 2700, Loss: 0.11615452170372009
step: 2800, Loss: 0.11601685732603073
step: 2900, Loss: 0.11703582108020782
step: 3000, Loss: 0.11600789427757263
step: 3100, Loss: 0.11631752550601959
step: 3200, Loss: 0.11572374403476715
step: 3300, Loss: 0.11732262372970581
step: 3400, Loss: 0.11631396412849426
step: 3500, Loss: 0.11482203751802444
step: 3600, Loss: 0.11492593586444855
step: 3700, Loss: 0.11738206446170807
step: 3800, Loss: 0.11686485260725021
step: 3900, Loss: 0.11755117774009705
step: 4000, Loss: 0.11845172941684723
step: 4100, Loss: 0.11732777953147888
step: 4200, Loss: 0.11710983514785767
step: 4300, Loss: 0.11729981750249863
step: 4400, Loss: 0.1149430125951767
step: 4500, Loss: 0.11677064746618271
step: 4600, Loss: 0.1148560419678688
step: 4700, Loss: 0.11507882922887802
step: 4800, Loss: 0.11616549640893936
step: 4900, Loss: 0.11418621242046356
step: 5000, Loss: 0.11679352074861526
step: 5100, Loss: 0.11820631474256516
step: 5200, Loss: 1.575674057006836
step: 5300, Loss: 0.1736597716808319
step: 5400, Loss: 0.14113424718379974
step: 5500, Loss: 0.12503588199615479
step: 5600, Loss: 0.12267427146434784
step: 5700, Loss: 0.11926915496587753
step: 5800, Loss: 0.12311349809169769
step: 5900, Loss: 0.11919672042131424
step: 6000, Loss: 0.12351920455694199
step: 6100, Loss: 0.12010753899812698
step: 6200, Loss: 0.11813623458147049
step: 6300, Loss: 0.11650287359952927
step: 6400, Loss: 0.11702530831098557
step: 6500, Loss: 0.11605033278465271
step: 6600, Loss: 0.11693108081817627
step: 6700, Loss: 0.11526302248239517
step: 6800, Loss: 0.11733642220497131
step: 6900, Loss: 0.11465108394622803
step: 7000, Loss: 0.11597136408090591
step: 7100, Loss: 0.1198275089263916
step: 7200, Loss: 0.11488085985183716
step: 7300, Loss: 0.11594823002815247
step: 7400, Loss: 0.11837108433246613
step: 7500, Loss: 0.11496754735708237
step: 7600, Loss: 0.11539018154144287
step: 7700, Loss: 0.11464732885360718
step: 7800, Loss: 0.11580368876457214
step: 7900, Loss: 0.11491205543279648
step: 8000, Loss: 0.1180315837264061
step: 8100, Loss: 0.11719153821468353
step: 8200, Loss: 0.11297105252742767
step: 8300, Loss: 0.11366929113864899
step: 8400, Loss: 0.11526401340961456
step: 8500, Loss: 0.11771953105926514
step: 8600, Loss: 0.11446592211723328
step: 8700, Loss: 0.11542993038892746
step: 8800, Loss: 0.1151294931769371
step: 8900, Loss: 0.11635282635688782
step: 9000, Loss: 0.11433824896812439
step: 9100, Loss: 0.11426407098770142
step: 9200, Loss: 0.11808545887470245
step: 9300, Loss: 0.11355729401111603
step: 9400, Loss: 0.11359741538763046
step: 9500, Loss: 0.11383046209812164
step: 9600, Loss: 0.11364790052175522
step: 9700, Loss: 0.11821997910737991
step: 9800, Loss: 0.11583321541547775
step: 9900, Loss: 0.11579447984695435
training successfully ended.
validating...
validate data length:32
acc: 0.53125
precision: 0.42857142857142855
recall: 0.75
F_score: 0.5454545454545454
******fold 2******

Training... train_data length:280
step: 0, Loss: 0.2887587249279022
step: 100, Loss: 0.12145934998989105
step: 200, Loss: 0.11701364070177078
step: 300, Loss: 0.11819732189178467
step: 400, Loss: 0.1145222932100296
step: 500, Loss: 0.1158813089132309
step: 600, Loss: 0.11588302999734879
step: 700, Loss: 0.11456738412380219
step: 800, Loss: 0.11447979509830475
step: 900, Loss: 0.11555434763431549
step: 1000, Loss: 0.11536890268325806
step: 1100, Loss: 0.11444232612848282
step: 1200, Loss: 0.11698123812675476
step: 1300, Loss: 0.11354215443134308
step: 1400, Loss: 0.11490695178508759
step: 1500, Loss: 0.1159045472741127
step: 1600, Loss: 0.11592963337898254
step: 1700, Loss: 0.11522810906171799
step: 1800, Loss: 0.11422485113143921
step: 1900, Loss: 0.11579804867506027
step: 2000, Loss: 0.11447754502296448
step: 2100, Loss: 0.11659377813339233
step: 2200, Loss: 0.1149478405714035
step: 2300, Loss: 0.11531941592693329
step: 2400, Loss: 0.11357691884040833
step: 2500, Loss: 0.11465470492839813
step: 2600, Loss: 0.11513488739728928
step: 2700, Loss: 0.11736138164997101
step: 2800, Loss: 0.11471027880907059
step: 2900, Loss: 0.11353239417076111
step: 3000, Loss: 0.11372418701648712
step: 3100, Loss: 0.11496920883655548
step: 3200, Loss: 0.11467250436544418
step: 3300, Loss: 0.11861570179462433
step: 3400, Loss: 0.11464095115661621
step: 3500, Loss: 0.11621025949716568
step: 3600, Loss: 0.11471787095069885
step: 3700, Loss: 0.11583953350782394
step: 3800, Loss: 0.11417841911315918
step: 3900, Loss: 0.11584797501564026
step: 4000, Loss: 0.11599951982498169
step: 4100, Loss: 0.11587417125701904
step: 4200, Loss: 0.11416225135326385
step: 4300, Loss: 0.11507195234298706
step: 4400, Loss: 0.11535519361495972
step: 4500, Loss: 0.11790984869003296
step: 4600, Loss: 0.22276419401168823
step: 4700, Loss: 2.6468026638031006
step: 4800, Loss: 0.15080486238002777
step: 4900, Loss: 0.13170617818832397
step: 5000, Loss: 0.13002905249595642
step: 5100, Loss: 0.127303346991539
step: 5200, Loss: 0.12400730699300766
step: 5300, Loss: 0.12537924945354462
step: 5400, Loss: 0.12954741716384888
step: 5500, Loss: 0.12466377019882202
step: 5600, Loss: 0.12251508235931396
step: 5700, Loss: 0.12028945237398148
step: 5800, Loss: 0.11710567772388458
step: 5900, Loss: 0.11634224653244019
step: 6000, Loss: 0.12172876298427582
step: 6100, Loss: 0.12542858719825745
step: 6200, Loss: 0.12221777439117432
step: 6300, Loss: 0.12286543846130371
step: 6400, Loss: 0.11585176736116409
step: 6500, Loss: 0.1165638267993927
step: 6600, Loss: 0.11626925319433212
step: 6700, Loss: 0.11862878501415253
step: 6800, Loss: 0.11799986660480499
step: 6900, Loss: 0.11838673055171967
step: 7000, Loss: 0.11733874678611755
step: 7100, Loss: 0.11903843283653259
step: 7200, Loss: 0.11639063060283661
step: 7300, Loss: 0.11431723088026047
step: 7400, Loss: 0.11490261554718018
step: 7500, Loss: 0.11817816644906998
step: 7600, Loss: 0.11683951318264008
step: 7700, Loss: 0.11672692745923996
step: 7800, Loss: 0.1182745099067688
step: 7900, Loss: 0.11678046733140945
step: 8000, Loss: 0.12775635719299316
step: 8100, Loss: 0.1146852970123291
step: 8200, Loss: 0.1146371141076088
step: 8300, Loss: 0.11560657620429993
step: 8400, Loss: 0.11456076055765152
step: 8500, Loss: 0.11388959735631943
step: 8600, Loss: 0.11408846080303192
step: 8700, Loss: 0.11613436043262482
step: 8800, Loss: 0.11571688205003738
step: 8900, Loss: 0.11326412856578827
step: 9000, Loss: 0.1142469048500061
step: 9100, Loss: 0.11603736132383347
step: 9200, Loss: 0.11601833254098892
step: 9300, Loss: 0.11650489270687103
step: 1500, Loss: 0.1918390393257141
step: 1600, Loss: 0.11478165537118912
step: 1700, Loss: 0.11516009271144867
step: 1800, Loss: 0.11422229558229446
step: 1900, Loss: 0.11388476192951202
step: 2000, Loss: 0.11315471678972244
step: 2100, Loss: 0.11341634392738342
step: 2200, Loss: 0.11389317363500595
step: 2300, Loss: 0.114315927028656
step: 2400, Loss: 0.11391150206327438
step: 2500, Loss: 0.11279028654098511
step: 2600, Loss: 0.1133580133318901
step: 2700, Loss: 0.11562873423099518
step: 2800, Loss: 0.11388800293207169
step: 2900, Loss: 0.11566667258739471
step: 3000, Loss: 0.11317531019449234
step: 3100, Loss: 0.11343428492546082
step: 3200, Loss: 0.11402950435876846
step: 3300, Loss: 0.11533285677433014
step: 3400, Loss: 0.19379130005836487
step: 3500, Loss: 0.11491382867097855
step: 3600, Loss: 0.11454963684082031
step: 3700, Loss: 0.1138104647397995
step: 3800, Loss: 0.1133100837469101
step: 3900, Loss: 0.11482743918895721
step: 4000, Loss: 0.11361178010702133
step: 4100, Loss: 0.11440428346395493
step: 4200, Loss: 0.11400242149829865
step: 4300, Loss: 0.11539563536643982
step: 4400, Loss: 0.11443070322275162
step: 4500, Loss: 0.11567940562963486
step: 4600, Loss: 0.11351704597473145
step: 4700, Loss: 0.1665959358215332
step: 4800, Loss: 0.13984133303165436
step: 4900, Loss: 0.1312417834997177
step: 5000, Loss: 0.12552332878112793
step: 5100, Loss: 0.12371120601892471
step: 5200, Loss: 0.12220614403486252
step: 5300, Loss: 0.2033746987581253
step: 5400, Loss: 0.11790657043457031
step: 5500, Loss: 0.12096845358610153
step: 5600, Loss: 0.12044773995876312
step: 5700, Loss: 0.11778624355792999
step: 5800, Loss: 0.11819691210985184
step: 5900, Loss: 0.11792685091495514
step: 6000, Loss: 0.11591260135173798
step: 6100, Loss: 0.11628662049770355
step: 6200, Loss: 0.11470000445842743
step: 6300, Loss: 0.11558069288730621
step: 6400, Loss: 0.11838173121213913
step: 6500, Loss: 0.11455002427101135
step: 6600, Loss: 0.11670000106096268
step: 6700, Loss: 0.11559886485338211
step: 6800, Loss: 0.1164427325129509
step: 6900, Loss: 0.11503608524799347
step: 7000, Loss: 0.11570516228675842
step: 7100, Loss: 0.11457666754722595
step: 7200, Loss: 0.19524776935577393
step: 7300, Loss: 0.11427094787359238
step: 7400, Loss: 0.11408141255378723
step: 7500, Loss: 0.11525242030620575
step: 7600, Loss: 0.1152031198143959
step: 7700, Loss: 0.11400607973337173
step: 7800, Loss: 0.11334722489118576
step: 7900, Loss: 0.11487042158842087
step: 8000, Loss: 0.11615683138370514
step: 8100, Loss: 0.11353369057178497
step: 8200, Loss: 0.11492259800434113
step: 8300, Loss: 0.11376592516899109
step: 8400, Loss: 0.11404607445001602
step: 8500, Loss: 0.11487258225679398
step: 8600, Loss: 0.11376962810754776
step: 8700, Loss: 0.11342492699623108
step: 8800, Loss: 0.11423663049936295
step: 8900, Loss: 0.11473832279443741
step: 9000, Loss: 0.11379808187484741
step: 9100, Loss: 0.1909540295600891
step: 9200, Loss: 0.11331114172935486
step: 9300, Loss: 0.1135876476764679
step: 9400, Loss: 0.1131143718957901
step: 9500, Loss: 0.11305946111679077
step: 9600, Loss: 0.11295844614505768
step: 9700, Loss: 0.11478090286254883
step: 9800, Loss: 0.11492946743965149
step: 9900, Loss: 0.11434417963027954
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.12133043259382248
step: 100, Loss: 0.1192563846707344
step: 200, Loss: 0.11750563234090805
step: 300, Loss: 0.11643130332231522
step: 400, Loss: 0.11469754576683044
step: 500, Loss: 0.11435557156801224
step: 600, Loss: 0.11296435445547104
step: 700, Loss: 0.11599303781986237
step: 800, Loss: 0.11402732878923416
step: 900, Loss: 0.11402665078639984
step: 1000, Loss: 0.1137491762638092
step: 1100, Loss: 0.11425530165433884
step: 1200, Loss: 0.11443193256855011
step: 1300, Loss: 0.11318652331829071
step: 1400, Loss: 0.1143019050359726
step: 1500, Loss: 0.1920955926179886
step: 1600, Loss: 0.11484204232692719
step: 1700, Loss: 0.11419457197189331
step: 1800, Loss: 0.11414983868598938
step: 1900, Loss: 0.11376936733722687
step: 2000, Loss: 0.11297400295734406
step: 2100, Loss: 0.1142214834690094
step: 2200, Loss: 0.11269137263298035
step: 2300, Loss: 0.11484314501285553
step: 2400, Loss: 0.11448288708925247
step: 2500, Loss: 0.11341360956430435
step: 2600, Loss: 0.11495433747768402
step: 2700, Loss: 0.11422988027334213
step: 2800, Loss: 0.11451678723096848
step: 2900, Loss: 0.11475688219070435
step: 3000, Loss: 0.11440084874629974
step: 3100, Loss: 0.11397244781255722
step: 3200, Loss: 0.11408849060535431
step: 3300, Loss: 0.11490286141633987
step: 3400, Loss: 0.19625860452651978
step: 3500, Loss: 0.11500073969364166
step: 3600, Loss: 0.11436782032251358
step: 3700, Loss: 0.11371511965990067
step: 3800, Loss: 0.11575427651405334
step: 3900, Loss: 0.7247376441955566
step: 4000, Loss: 0.13164854049682617
step: 4100, Loss: 0.12559355795383453
step: 4200, Loss: 0.13147979974746704
step: 4300, Loss: 0.12703818082809448
step: 4400, Loss: 0.12597772479057312
step: 4500, Loss: 0.11987834423780441
step: 4600, Loss: 0.11583030968904495
step: 4700, Loss: 0.12260284274816513
step: 4800, Loss: 0.11876815557479858
step: 4900, Loss: 0.12174272537231445
step: 5000, Loss: 0.11679290980100632
step: 5100, Loss: 0.12253791838884354
step: 5200, Loss: 0.1151757687330246
step: 5300, Loss: 0.1957949548959732
step: 5400, Loss: 0.11668848246335983
step: 5500, Loss: 0.11960088461637497
step: 5600, Loss: 0.11742457747459412
step: 5700, Loss: 0.11542880535125732
step: 5800, Loss: 0.11578866094350815
step: 5900, Loss: 0.11545542627573013
step: 6000, Loss: 0.11551263183355331
step: 6100, Loss: 0.11885952949523926
step: 6200, Loss: 0.11628694832324982
step: 6300, Loss: 0.1164190024137497
step: 6400, Loss: 0.11444815993309021
step: 6500, Loss: 0.1148395761847496
step: 6600, Loss: 0.11397725343704224
step: 6700, Loss: 0.11494763195514679
step: 6800, Loss: 0.11422771960496902
step: 6900, Loss: 0.1149521917104721
step: 7000, Loss: 0.11417679488658905
step: 7100, Loss: 0.11553913354873657
step: 7200, Loss: 0.1942882090806961
step: 7300, Loss: 0.11462698876857758
step: 7400, Loss: 0.11491330713033676
step: 7500, Loss: 0.11394337564706802
step: 7600, Loss: 0.11466719210147858
step: 7700, Loss: 0.11416185647249222
step: 7800, Loss: 0.11388052999973297
step: 7900, Loss: 0.11308105289936066
step: 8000, Loss: 0.11467652767896652
step: 8100, Loss: 0.11429379135370255
step: 8200, Loss: 0.11367429047822952
step: 8300, Loss: 0.11427720636129379
step: 8400, Loss: 0.11366455256938934
step: 8500, Loss: 0.11306187510490417
step: 8600, Loss: 0.11481276154518127
step: 8700, Loss: 0.11451337486505508
step: 8800, Loss: 0.11370059102773666
step: 8900, Loss: 0.11386126279830933
step: 9000, Loss: 0.11437232047319412
step: 9100, Loss: 0.19362872838974
step: 9200, Loss: 0.11424543708562851
step: 9300, Loss: 0.11320779472589493
step: 9400, Loss: 0.11368771642446518
step: 9500, Loss: 0.11425786465406418
step: 9600, Loss: 0.11371947824954987
step: 9700, Loss: 0.11368243396282196
step: 9800, Loss: 0.11298325657844543
step: 9900, Loss: 0.11348673701286316
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11904916912317276
step: 100, Loss: 0.11786056309938431
step: 200, Loss: 0.11546879261732101
step: 300, Loss: 0.11483899503946304
step: 400, Loss: 0.11519315093755722
step: 500, Loss: 0.11479684710502625
step: 600, Loss: 0.11479105055332184
step: 700, Loss: 0.11445347964763641
step: 800, Loss: 0.1131708025932312
step: 900, Loss: 0.113119937479496
step: 1000, Loss: 0.11397642642259598
step: 1100, Loss: 0.11513374000787735
step: 1200, Loss: 0.11343766748905182
step: 1300, Loss: 0.11412888020277023
step: 1400, Loss: 0.11308945715427399
step: 1500, Loss: 0.19373443722724915
step: 1600, Loss: 0.1138143315911293
step: 1700, Loss: 0.11366073042154312
step: 1800, Loss: 0.11510875076055527
step: 1900, Loss: 0.11398962140083313
step: 2000, Loss: 0.11350148171186447
step: 2100, Loss: 0.1135723888874054
step: 2200, Loss: 0.11395448446273804
step: 9400, Loss: 0.11399681121110916
step: 9500, Loss: 0.11498186737298965
step: 9600, Loss: 0.11344942450523376
step: 9700, Loss: 0.11380405724048615
step: 9800, Loss: 0.11501617729663849
step: 9900, Loss: 0.11462896317243576
training successfully ended.
validating...
validate data length:32
acc: 0.65625
precision: 0.631578947368421
recall: 0.75
F_score: 0.6857142857142857
******fold 3******

Training... train_data length:281
step: 0, Loss: 1.2914868593215942
step: 100, Loss: 0.11948907375335693
step: 200, Loss: 0.11626894772052765
step: 300, Loss: 0.11589713394641876
step: 400, Loss: 0.11582881212234497
step: 500, Loss: 0.11753414571285248
step: 600, Loss: 0.1197628602385521
step: 700, Loss: 0.11778391897678375
step: 800, Loss: 0.113520547747612
step: 900, Loss: 0.11497338861227036
step: 1000, Loss: 0.1146344393491745
step: 1100, Loss: 0.11572781950235367
step: 1200, Loss: 0.11482664942741394
step: 1300, Loss: 0.1148267537355423
step: 1400, Loss: 0.11504186689853668
step: 1500, Loss: 0.11557498574256897
step: 1600, Loss: 0.1138235554099083
step: 1700, Loss: 0.11392257362604141
step: 1800, Loss: 0.11589846014976501
step: 1900, Loss: 0.11440044641494751
step: 2000, Loss: 0.11394345015287399
step: 2100, Loss: 0.11537886410951614
step: 2200, Loss: 0.11422008275985718
step: 2300, Loss: 0.11582758277654648
step: 2400, Loss: 0.11576832830905914
step: 2500, Loss: 0.11402244120836258
step: 2600, Loss: 0.1139412671327591
step: 2700, Loss: 0.11406486481428146
step: 2800, Loss: 0.11456077545881271
step: 2900, Loss: 0.11447489261627197
step: 3000, Loss: 0.11380476504564285
step: 3100, Loss: 0.11425504088401794
step: 3200, Loss: 0.11317326873540878
step: 3300, Loss: 0.1143864095211029
step: 3400, Loss: 0.11578071117401123
step: 3500, Loss: 0.11454061418771744
step: 3600, Loss: 0.11356101185083389
step: 3700, Loss: 0.11367771774530411
step: 3800, Loss: 0.1137625202536583
step: 3900, Loss: 0.11460147053003311
step: 4000, Loss: 0.11384265124797821
step: 4100, Loss: 0.1183772161602974
step: 4200, Loss: 0.11463859677314758
step: 4300, Loss: 0.11360935866832733
step: 4400, Loss: 0.11495039612054825
step: 4500, Loss: 0.11415141820907593
step: 4600, Loss: 0.11552728712558746
step: 4700, Loss: 0.11461273580789566
step: 4800, Loss: 0.11604622006416321
step: 4900, Loss: 0.11418615281581879
step: 5000, Loss: 0.11394773423671722
step: 5100, Loss: 0.11759419739246368
step: 5200, Loss: 0.11480346322059631
step: 5300, Loss: 0.11424905061721802
step: 5400, Loss: 0.1935352385044098
step: 5500, Loss: 0.14464683830738068
step: 5600, Loss: 0.13179761171340942
step: 5700, Loss: 0.1254728138446808
step: 5800, Loss: 0.12869982421398163
step: 5900, Loss: 0.12224361300468445
step: 6000, Loss: 0.12276884913444519
step: 6100, Loss: 0.1204974502325058
step: 6200, Loss: 0.11830990016460419
step: 6300, Loss: 0.11728297919034958
step: 6400, Loss: 0.12363061308860779
step: 6500, Loss: 0.11698457598686218
step: 6600, Loss: 0.11575696617364883
step: 6700, Loss: 0.11698747426271439
step: 6800, Loss: 0.1169620007276535
step: 6900, Loss: 0.11679670214653015
step: 7000, Loss: 0.12148058414459229
step: 7100, Loss: 0.1351526528596878
step: 7200, Loss: 0.11599444597959518
step: 7300, Loss: 0.11892984807491302
step: 7400, Loss: 0.11546936631202698
step: 7500, Loss: 0.1166887879371643
step: 7600, Loss: 0.11783629655838013
step: 7700, Loss: 0.115198515355587
step: 7800, Loss: 0.11995589733123779
step: 7900, Loss: 0.11414715647697449
step: 8000, Loss: 0.11532489210367203
step: 8100, Loss: 0.1189345121383667
step: 8200, Loss: 0.12292918562889099
step: 8300, Loss: 0.11372929811477661
step: 8400, Loss: 0.1136881560087204
step: 8500, Loss: 0.115306556224823
step: 8600, Loss: 0.11405085772275925
step: 8700, Loss: 0.11593765020370483
step: 8800, Loss: 0.11378473043441772
step: 8900, Loss: 0.1162879690527916
step: 9000, Loss: 0.1154906153678894
step: 9100, Loss: 0.11694273352622986
step: 9200, Loss: 0.11399870365858078
step: 9300, Loss: 0.11560896039009094
step: 9400, Loss: 0.11540676653385162
step: 9500, Loss: 0.11475124955177307
step: 9600, Loss: 0.11709952354431152
step: 9700, Loss: 0.11520421504974365
step: 9800, Loss: 0.11372971534729004
step: 9900, Loss: 0.11451553553342819
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8235294117647058
recall: 0.9333333333333333
F_score: 0.8749999999999999
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.21234112977981567
step: 100, Loss: 0.11695582419633865
step: 200, Loss: 0.11690139770507812
step: 300, Loss: 0.11572527885437012
step: 400, Loss: 0.1162722036242485
step: 500, Loss: 0.11691078543663025
step: 600, Loss: 0.11487168818712234
step: 700, Loss: 0.11523093283176422
step: 800, Loss: 0.114034503698349
step: 900, Loss: 0.11415042728185654
step: 1000, Loss: 0.11452434957027435
step: 1100, Loss: 0.114614337682724
step: 1200, Loss: 0.11570870131254196
step: 1300, Loss: 0.11565260589122772
step: 1400, Loss: 0.1147899255156517
step: 1500, Loss: 0.11437676846981049
step: 1600, Loss: 0.11476952582597733
step: 1700, Loss: 0.1140534058213234
step: 1800, Loss: 0.1135363057255745
step: 1900, Loss: 0.11444328725337982
step: 2000, Loss: 0.11375416070222855
step: 2100, Loss: 0.11375585198402405
step: 2200, Loss: 0.11688309907913208
step: 2300, Loss: 0.11379364132881165
step: 2400, Loss: 0.1150127500295639
step: 2500, Loss: 0.11386612057685852
step: 2600, Loss: 0.1160847395658493
step: 2700, Loss: 0.11536157876253128
step: 2800, Loss: 0.1154954805970192
step: 2900, Loss: 0.11442536860704422
step: 3000, Loss: 0.11369509994983673
step: 3100, Loss: 0.11416718363761902
step: 3200, Loss: 0.11319039762020111
step: 3300, Loss: 0.11441464722156525
step: 3400, Loss: 0.11499620229005814
step: 3500, Loss: 0.11409103870391846
step: 3600, Loss: 0.11391773074865341
step: 3700, Loss: 0.11400673538446426
step: 3800, Loss: 0.11518026143312454
step: 3900, Loss: 0.11453548818826675
step: 4000, Loss: 0.1152348667383194
step: 4100, Loss: 0.1146726906299591
step: 4200, Loss: 0.11328426003456116
step: 4300, Loss: 0.11385059356689453
step: 4400, Loss: 0.11567385494709015
step: 4500, Loss: 0.11460970342159271
step: 4600, Loss: 0.11396101862192154
step: 4700, Loss: 0.11320946365594864
step: 4800, Loss: 0.11372561752796173
step: 4900, Loss: 0.11436259746551514
step: 5000, Loss: 0.11367277055978775
step: 5100, Loss: 0.11435525864362717
step: 5200, Loss: 0.1162184625864029
step: 5300, Loss: 0.11385469138622284
step: 5400, Loss: 0.114005446434021
step: 5500, Loss: 0.11471385508775711
step: 5600, Loss: 0.11293558776378632
step: 5700, Loss: 1.1630640029907227
step: 5800, Loss: 0.13190573453903198
step: 5900, Loss: 0.13444960117340088
step: 6000, Loss: 0.11893123388290405
step: 6100, Loss: 0.12178288400173187
step: 6200, Loss: 0.11756233870983124
step: 6300, Loss: 0.11703026294708252
step: 6400, Loss: 0.12124143540859222
step: 6500, Loss: 0.11652792990207672
step: 6600, Loss: 0.11951850354671478
step: 6700, Loss: 0.11744517832994461
step: 6800, Loss: 0.11852008104324341
step: 6900, Loss: 0.1146920770406723
step: 7000, Loss: 0.14039596915245056
step: 7100, Loss: 0.1152481734752655
step: 7200, Loss: 0.11916688084602356
step: 7300, Loss: 0.117203488945961
step: 7400, Loss: 0.11642993986606598
step: 7500, Loss: 0.11526994407176971
step: 7600, Loss: 0.11409994214773178
step: 7700, Loss: 0.11428610980510712
step: 7800, Loss: 0.11506010591983795
step: 7900, Loss: 0.11548729240894318
step: 8000, Loss: 0.11532382667064667
step: 8100, Loss: 0.11365974694490433
step: 8200, Loss: 0.11596766114234924
step: 8300, Loss: 0.11377095431089401
step: 8400, Loss: 0.11472851783037186
step: 8500, Loss: 0.11468209326267242
step: 8600, Loss: 0.11491094529628754
step: 8700, Loss: 0.11554215848445892
step: 8800, Loss: 0.11467809975147247
step: 8900, Loss: 0.11751322448253632
step: 9000, Loss: 0.1160559132695198
step: 9100, Loss: 0.11366818100214005
step: 9200, Loss: 0.11352954804897308
step: 9300, Loss: 0.1134834885597229
step: 9400, Loss: 0.11910541355609894
step: 9500, Loss: 0.11715760827064514
step: 9600, Loss: 0.11409702152013779
step: 9700, Loss: 0.11328957974910736
step: 9800, Loss: 0.11438053101301193
step: 9900, Loss: 0.11348215490579605
step: 2300, Loss: 0.11648400127887726
step: 2400, Loss: 0.11463145911693573
step: 2500, Loss: 0.11467625200748444
step: 2600, Loss: 0.11369958519935608
step: 2700, Loss: 0.11519081145524979
step: 2800, Loss: 0.11460362374782562
step: 2900, Loss: 0.11608537286520004
step: 3000, Loss: 0.11437741667032242
step: 3100, Loss: 0.11472414433956146
step: 3200, Loss: 0.11352631449699402
step: 3300, Loss: 0.11452130228281021
step: 3400, Loss: 1.5437747240066528
step: 3500, Loss: 0.13844700157642365
step: 3600, Loss: 0.1430835872888565
step: 3700, Loss: 0.12867459654808044
step: 3800, Loss: 0.12785808742046356
step: 3900, Loss: 0.12412004172801971
step: 4000, Loss: 0.11864645034074783
step: 4100, Loss: 0.12088257819414139
step: 4200, Loss: 0.12399886548519135
step: 4300, Loss: 0.12339504063129425
step: 4400, Loss: 0.11884064972400665
step: 4500, Loss: 0.11807467043399811
step: 4600, Loss: 0.12280867993831635
step: 4700, Loss: 0.119449183344841
step: 4800, Loss: 0.11872625350952148
step: 4900, Loss: 0.12013264000415802
step: 5000, Loss: 0.11670990288257599
step: 5100, Loss: 0.11585777997970581
step: 5200, Loss: 0.11601020395755768
step: 5300, Loss: 0.19942757487297058
step: 5400, Loss: 0.11665794998407364
step: 5500, Loss: 0.1198260486125946
step: 5600, Loss: 0.11488398164510727
step: 5700, Loss: 0.11466088891029358
step: 5800, Loss: 0.11580168455839157
step: 5900, Loss: 0.11535795032978058
step: 6000, Loss: 0.11594394594430923
step: 6100, Loss: 0.11772991716861725
step: 6200, Loss: 0.11452396959066391
step: 6300, Loss: 0.11467095464468002
step: 6400, Loss: 0.11527358740568161
step: 6500, Loss: 0.11564857512712479
step: 6600, Loss: 0.11636792123317719
step: 6700, Loss: 0.11603832989931107
step: 6800, Loss: 0.11476650089025497
step: 6900, Loss: 0.11437248438596725
step: 7000, Loss: 0.11430606991052628
step: 7100, Loss: 0.1144176721572876
step: 7200, Loss: 0.19431450963020325
step: 7300, Loss: 0.11503134667873383
step: 7400, Loss: 0.1138959676027298
step: 7500, Loss: 0.11600114405155182
step: 7600, Loss: 0.11388927698135376
step: 7700, Loss: 0.11579418182373047
step: 7800, Loss: 0.11537910252809525
step: 7900, Loss: 0.11352097988128662
step: 8000, Loss: 0.11372078210115433
step: 8100, Loss: 0.11494921147823334
step: 8200, Loss: 0.11496008932590485
step: 8300, Loss: 0.11451543867588043
step: 8400, Loss: 0.11427846550941467
step: 8500, Loss: 0.11501605808734894
step: 8600, Loss: 0.11511068046092987
step: 8700, Loss: 0.11462992429733276
step: 8800, Loss: 0.11558499932289124
step: 8900, Loss: 0.11524752527475357
step: 9000, Loss: 0.11390139162540436
step: 9100, Loss: 0.1921575516462326
step: 9200, Loss: 0.11433079838752747
step: 9300, Loss: 0.11398361623287201
step: 9400, Loss: 0.11409226059913635
step: 9500, Loss: 0.1145629733800888
step: 9600, Loss: 0.11364562064409256
step: 9700, Loss: 0.11290943622589111
step: 9800, Loss: 0.11362124234437943
step: 9900, Loss: 0.11461390554904938
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.12241780757904053
step: 100, Loss: 0.12189182639122009
step: 200, Loss: 0.1172381043434143
step: 300, Loss: 0.11483399569988251
step: 400, Loss: 0.11381147801876068
step: 500, Loss: 0.11541545391082764
step: 600, Loss: 0.11475694179534912
step: 700, Loss: 0.11419656127691269
step: 800, Loss: 0.11312499642372131
step: 900, Loss: 0.11322076618671417
step: 1000, Loss: 0.11272777616977692
step: 1100, Loss: 0.11494608223438263
step: 1200, Loss: 0.11417441070079803
step: 1300, Loss: 0.11539269983768463
step: 1400, Loss: 0.11311136931180954
step: 1500, Loss: 0.1954910308122635
step: 1600, Loss: 0.11417742073535919
step: 1700, Loss: 0.11356393992900848
step: 1800, Loss: 0.11466240882873535
step: 1900, Loss: 0.1131104901432991
step: 2000, Loss: 0.11389048397541046
step: 2100, Loss: 0.11438976228237152
step: 2200, Loss: 0.11302486062049866
step: 2300, Loss: 0.11431297659873962
step: 2400, Loss: 0.11295448243618011
step: 2500, Loss: 0.11369837820529938
step: 2600, Loss: 0.11398328840732574
step: 2700, Loss: 0.11328914761543274
step: 2800, Loss: 0.11438728868961334
step: 2900, Loss: 0.11307252943515778
step: 3000, Loss: 0.11482584476470947
step: 3100, Loss: 0.11369028687477112
step: 3200, Loss: 0.11556766927242279
step: 3300, Loss: 0.11300334334373474
step: 3400, Loss: 1.1352488994598389
step: 3500, Loss: 0.17619620263576508
step: 3600, Loss: 0.13116416335105896
step: 3700, Loss: 0.13153831660747528
step: 3800, Loss: 0.13543765246868134
step: 3900, Loss: 0.1313306838274002
step: 4000, Loss: 0.12206591665744781
step: 4100, Loss: 0.12225064635276794
step: 4200, Loss: 0.12522366642951965
step: 4300, Loss: 0.1172984391450882
step: 4400, Loss: 0.12022584676742554
step: 4500, Loss: 0.12109668552875519
step: 4600, Loss: 0.11780007183551788
step: 4700, Loss: 0.12037523835897446
step: 4800, Loss: 0.11917953193187714
step: 4900, Loss: 0.11760719120502472
step: 5000, Loss: 0.11868948489427567
step: 5100, Loss: 0.11662781238555908
step: 5200, Loss: 0.11930301785469055
step: 5300, Loss: 0.20502355694770813
step: 5400, Loss: 0.11555778980255127
step: 5500, Loss: 0.11726563423871994
step: 5600, Loss: 0.11460564285516739
step: 5700, Loss: 0.11558625847101212
step: 5800, Loss: 0.11687852442264557
step: 5900, Loss: 0.11675779521465302
step: 6000, Loss: 0.11531972885131836
step: 6100, Loss: 0.11823566257953644
step: 6200, Loss: 0.11485766619443893
step: 6300, Loss: 0.11637355387210846
step: 6400, Loss: 0.11449109017848969
step: 6500, Loss: 0.11358487606048584
step: 6600, Loss: 0.11446240544319153
step: 6700, Loss: 0.11431426554918289
step: 6800, Loss: 0.11455217748880386
step: 6900, Loss: 0.1177908331155777
step: 7000, Loss: 0.11420783400535583
step: 7100, Loss: 0.11582602560520172
step: 7200, Loss: 0.19515174627304077
step: 7300, Loss: 0.11459320038557053
step: 7400, Loss: 0.11470728367567062
step: 7500, Loss: 0.11518922448158264
step: 7600, Loss: 0.11413143575191498
step: 7700, Loss: 0.11376471072435379
step: 7800, Loss: 0.11418942362070084
step: 7900, Loss: 0.11392609775066376
step: 8000, Loss: 0.11485128104686737
step: 8100, Loss: 0.11405827850103378
step: 8200, Loss: 0.1132805347442627
step: 8300, Loss: 0.11415459215641022
step: 8400, Loss: 0.11582012474536896
step: 8500, Loss: 0.1139703094959259
step: 8600, Loss: 0.11370160430669785
step: 8700, Loss: 0.11394286155700684
step: 8800, Loss: 0.1135844737291336
step: 8900, Loss: 0.1150846779346466
step: 9000, Loss: 0.11358873546123505
step: 9100, Loss: 0.19569936394691467
step: 9200, Loss: 0.11334839463233948
step: 9300, Loss: 0.11377854645252228
step: 9400, Loss: 0.11331410706043243
step: 9500, Loss: 0.11213618516921997
step: 9600, Loss: 0.11403906345367432
step: 9700, Loss: 0.11356325447559357
step: 9800, Loss: 0.11388331651687622
step: 9900, Loss: 0.11352202296257019
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 18 Avgacc: 0.9569444444444445 Avgfscore: 0.9603595288473338 
 Max acc:1.0, Max f score:1.0
******** mix subject_19 ********

[323, 437]
******fold 1******

Training... train_data length:786
step: 0, Loss: 42.1820068359375
step: 100, Loss: 4.15476131439209
step: 200, Loss: 1.6325792074203491
step: 300, Loss: 0.3040037155151367
step: 400, Loss: 0.25652849674224854
step: 500, Loss: 0.17129452526569366
step: 600, Loss: 0.15974442660808563
step: 700, Loss: 0.13974329829216003
step: 800, Loss: 0.14147254824638367
step: 900, Loss: 0.13360388576984406
step: 1000, Loss: 0.13767322897911072
step: 1100, Loss: 0.13716661930084229
step: 1200, Loss: 0.14676393568515778
step: 1300, Loss: 0.13529106974601746
step: 1400, Loss: 0.14424455165863037
step: 1500, Loss: 0.13907253742218018
step: 1600, Loss: 0.12996985018253326
step: 1700, Loss: 0.12921327352523804
step: 1800, Loss: 0.12436521053314209
step: 1900, Loss: 0.12200014293193817
step: 2000, Loss: 0.12540742754936218
step: 2100, Loss: 0.12269183993339539
step: 2200, Loss: 0.12241707742214203
step: 2300, Loss: 0.12643741071224213
step: 2400, Loss: 0.12007927894592285
step: 2500, Loss: 0.13001888990402222
step: 2600, Loss: 0.12481293082237244
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9230769230769231
recall: 0.8
F_score: 0.8571428571428571
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.21352848410606384
step: 100, Loss: 0.1201755627989769
step: 200, Loss: 0.11739693582057953
step: 300, Loss: 0.11701977252960205
step: 400, Loss: 0.11475463211536407
step: 500, Loss: 0.11708538234233856
step: 600, Loss: 0.11525376886129379
step: 700, Loss: 0.11574353277683258
step: 800, Loss: 0.11414644867181778
step: 900, Loss: 0.11721205711364746
step: 1000, Loss: 0.11285986006259918
step: 1100, Loss: 0.11429128050804138
step: 1200, Loss: 0.11491574347019196
step: 1300, Loss: 0.1146945208311081
step: 1400, Loss: 0.11402644962072372
step: 1500, Loss: 0.11585047096014023
step: 1600, Loss: 0.11441554129123688
step: 1700, Loss: 0.1141498014330864
step: 1800, Loss: 0.11519792675971985
step: 1900, Loss: 0.11384712159633636
step: 2000, Loss: 0.11479245126247406
step: 2100, Loss: 0.11419099569320679
step: 2200, Loss: 0.11452799290418625
step: 2300, Loss: 0.11476322263479233
step: 2400, Loss: 0.11243654787540436
step: 2500, Loss: 0.11392858624458313
step: 2600, Loss: 0.11647875607013702
step: 2700, Loss: 0.11430643498897552
step: 2800, Loss: 0.11536715924739838
step: 2900, Loss: 0.1148020550608635
step: 3000, Loss: 0.11516039073467255
step: 3100, Loss: 0.11485511064529419
step: 3200, Loss: 0.11346513032913208
step: 3300, Loss: 0.11444781720638275
step: 3400, Loss: 0.11339662224054337
step: 3500, Loss: 0.11403128504753113
step: 3600, Loss: 0.11396542191505432
step: 3700, Loss: 0.11291559040546417
step: 3800, Loss: 0.11498387902975082
step: 3900, Loss: 0.11799407005310059
step: 4000, Loss: 0.11318369209766388
step: 4100, Loss: 0.11376406252384186
step: 4200, Loss: 0.113894984126091
step: 4300, Loss: 0.11406585574150085
step: 4400, Loss: 0.11414270848035812
step: 4500, Loss: 0.11419668793678284
step: 4600, Loss: 0.11309801787137985
step: 4700, Loss: 0.11354799568653107
step: 4800, Loss: 0.11455094814300537
step: 4900, Loss: 0.12152078002691269
step: 5000, Loss: 0.11702129989862442
step: 5100, Loss: 0.11575952917337418
step: 5200, Loss: 0.11324116587638855
step: 5300, Loss: 0.1139598861336708
step: 5400, Loss: 0.11356386542320251
step: 5500, Loss: 0.11441981792449951
step: 5600, Loss: 0.11561426520347595
step: 5700, Loss: 0.11520589143037796
step: 5800, Loss: 0.11618971824645996
step: 5900, Loss: 0.11334694176912308
step: 6000, Loss: 0.11321715265512466
step: 6100, Loss: 0.11555065214633942
step: 6200, Loss: 4.045280456542969
step: 6300, Loss: 0.14041519165039062
step: 6400, Loss: 0.12719637155532837
step: 6500, Loss: 0.13170160353183746
step: 6600, Loss: 0.12724030017852783
step: 6700, Loss: 0.1219547837972641
step: 6800, Loss: 0.12367446720600128
step: 6900, Loss: 0.1182197704911232
step: 7000, Loss: 0.12563370168209076
step: 7100, Loss: 0.11508717387914658
step: 7200, Loss: 0.11742724478244781
step: 7300, Loss: 0.11728009581565857
step: 7400, Loss: 0.11658146977424622
step: 7500, Loss: 0.1152116060256958
step: 7600, Loss: 0.11567308008670807
step: 7700, Loss: 0.11586827039718628
step: 7800, Loss: 0.11469949781894684
step: 7900, Loss: 0.11585298180580139
step: 8000, Loss: 0.11595919728279114
step: 8100, Loss: 0.11500638723373413
step: 8200, Loss: 0.11389748007059097
step: 8300, Loss: 0.11397982388734818
step: 8400, Loss: 0.11436204612255096
step: 8500, Loss: 0.11687716841697693
step: 8600, Loss: 0.11739970743656158
step: 8700, Loss: 0.11756807565689087
step: 8800, Loss: 0.11414249241352081
step: 8900, Loss: 0.11772554367780685
step: 9000, Loss: 0.11592332273721695
step: 9100, Loss: 0.1147473007440567
step: 9200, Loss: 0.11318924278020859
step: 9300, Loss: 0.11499263346195221
step: 9400, Loss: 0.1136687695980072
step: 9500, Loss: 0.11409179866313934
step: 9600, Loss: 0.1135285496711731
step: 9700, Loss: 0.11496278643608093
step: 9800, Loss: 0.11563116312026978
step: 9900, Loss: 0.1141139566898346
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8333333333333334
recall: 0.8333333333333334
F_score: 0.8333333333333334
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.3847689628601074
step: 100, Loss: 0.1154356300830841
step: 200, Loss: 0.11429760605096817
step: 300, Loss: 0.11487583816051483
step: 400, Loss: 0.11530555784702301
step: 500, Loss: 0.11423353850841522
step: 600, Loss: 0.11390278488397598
step: 700, Loss: 0.11390344798564911
step: 800, Loss: 0.11471860110759735
step: 900, Loss: 0.11347483843564987
step: 1000, Loss: 0.11727974563837051
step: 1100, Loss: 0.11521658301353455
step: 1200, Loss: 0.11400757730007172
step: 1300, Loss: 0.11315762996673584
step: 1400, Loss: 0.11444950848817825
step: 1500, Loss: 0.11417170614004135
step: 1600, Loss: 0.11508079618215561
step: 1700, Loss: 0.11345543712377548
step: 1800, Loss: 0.1145324558019638
step: 1900, Loss: 0.11340268701314926
step: 2000, Loss: 0.11357267200946808
step: 2100, Loss: 0.11710195243358612
step: 2200, Loss: 0.11384445428848267
step: 2300, Loss: 0.11423633992671967
step: 2400, Loss: 0.11437977105379105
step: 2500, Loss: 0.11466747522354126
step: 2600, Loss: 0.11296281218528748
step: 2700, Loss: 0.11397895216941833
step: 2800, Loss: 0.11446049809455872
step: 2900, Loss: 0.11410098522901535
step: 3000, Loss: 0.11400257050991058
step: 3100, Loss: 0.11527681350708008
step: 3200, Loss: 0.1132999062538147
step: 3300, Loss: 0.11389808356761932
step: 3400, Loss: 0.11323017627000809
step: 3500, Loss: 0.11418154835700989
step: 3600, Loss: 0.11489660292863846
step: 3700, Loss: 0.1146625205874443
step: 3800, Loss: 0.11369990557432175
step: 3900, Loss: 0.11376133561134338
step: 4000, Loss: 0.11489040404558182
step: 4100, Loss: 0.11293184757232666
step: 4200, Loss: 0.11332691460847855
step: 4300, Loss: 0.11385815590620041
step: 4400, Loss: 0.11504514515399933
step: 4500, Loss: 0.11589035391807556
step: 4600, Loss: 0.1144014522433281
step: 4700, Loss: 0.11450786143541336
step: 4800, Loss: 0.1157209500670433
step: 4900, Loss: 0.11440438777208328
step: 5000, Loss: 0.1141563355922699
step: 5100, Loss: 0.11489532887935638
step: 5200, Loss: 0.11369319260120392
step: 5300, Loss: 0.11443061381578445
step: 5400, Loss: 0.11445536464452744
step: 5500, Loss: 0.11386772990226746
step: 5600, Loss: 0.11694313585758209
step: 5700, Loss: 0.11431363224983215
step: 5800, Loss: 2.1218295097351074
step: 5900, Loss: 0.16884773969650269
step: 6000, Loss: 0.12915930151939392
step: 6100, Loss: 0.1270771622657776
step: 6200, Loss: 0.12427382171154022
step: 6300, Loss: 0.1201791763305664
step: 6400, Loss: 0.11977168917655945
step: 6500, Loss: 0.11675594747066498
step: 6600, Loss: 0.11588849127292633
step: 6700, Loss: 0.12184250354766846
step: 6800, Loss: 0.11597160995006561
step: 6900, Loss: 0.11600194871425629
step: 7000, Loss: 0.11447931826114655
step: 7100, Loss: 0.1155920922756195
step: 7200, Loss: 0.11558423936367035
step: 7300, Loss: 0.11454164981842041
step: 7400, Loss: 0.11426635086536407
step: 7500, Loss: 0.11553549021482468
step: 7600, Loss: 0.11710561066865921
step: 7700, Loss: 0.1174253299832344
step: 7800, Loss: 0.11748762428760529
step: 7900, Loss: 0.11432123184204102
step: 8000, Loss: 0.11628979444503784
step: 8100, Loss: 0.11567667871713638
step: 8200, Loss: 0.11513806134462357
step: 8300, Loss: 0.11548260599374771
step: 8400, Loss: 0.1148315817117691
step: 8500, Loss: 0.11513251811265945
step: 8600, Loss: 0.11462679505348206
step: 8700, Loss: 0.11377810686826706
step: 8800, Loss: 0.11525574326515198
step: 8900, Loss: 0.11644317209720612
step: 9000, Loss: 0.11477818340063095
step: 9100, Loss: 0.11319410800933838
step: 9200, Loss: 0.1146746426820755
step: 9300, Loss: 0.11410610377788544
step: 9400, Loss: 0.11285758018493652
step: 9500, Loss: 0.11338421702384949
step: 9600, Loss: 0.11531099677085876
step: 9700, Loss: 0.1142314225435257
step: 9800, Loss: 0.11345329880714417
step: 9900, Loss: 0.11343203485012054
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.8
recall: 0.6666666666666666
F_score: 0.7272727272727272
******fold 7******

Training... train_data length:281
step: 2700, Loss: 0.12087079137563705
step: 2800, Loss: 0.12011336535215378
step: 2900, Loss: 0.12066033482551575
step: 3000, Loss: 0.11812476813793182
step: 3100, Loss: 0.116366907954216
step: 3200, Loss: 0.11767521500587463
step: 3300, Loss: 0.11744605004787445
step: 3400, Loss: 0.11735225468873978
step: 3500, Loss: 0.11518865823745728
step: 3600, Loss: 0.12359447032213211
step: 3700, Loss: 0.1183592826128006
step: 3800, Loss: 0.11528268456459045
step: 3900, Loss: 0.11571688950061798
step: 4000, Loss: 0.11579577624797821
step: 4100, Loss: 0.11510951817035675
step: 4200, Loss: 0.11457569152116776
step: 4300, Loss: 0.11510809510946274
step: 4400, Loss: 0.11524796485900879
step: 4500, Loss: 0.11757716536521912
step: 4600, Loss: 0.11397730559110641
step: 4700, Loss: 0.11923669278621674
step: 4800, Loss: 0.11512249708175659
step: 4900, Loss: 0.11564462631940842
step: 5000, Loss: 0.11488250643014908
step: 5100, Loss: 0.1157282143831253
step: 5200, Loss: 0.11374519765377045
step: 5300, Loss: 0.12458746135234833
step: 5400, Loss: 0.11738982796669006
step: 5500, Loss: 0.11355696618556976
step: 5600, Loss: 0.114500492811203
step: 5700, Loss: 0.1170734316110611
step: 5800, Loss: 0.2681567072868347
step: 5900, Loss: 4.357631683349609
step: 6000, Loss: 0.1508406102657318
step: 6100, Loss: 0.16198104619979858
step: 6200, Loss: 0.14278161525726318
step: 6300, Loss: 0.1404445916414261
step: 6400, Loss: 0.12314324080944061
step: 6500, Loss: 0.13070730865001678
step: 6600, Loss: 0.12825798988342285
step: 6700, Loss: 0.12643195688724518
step: 6800, Loss: 0.1223808079957962
step: 6900, Loss: 0.13044241070747375
step: 7000, Loss: 0.1264551281929016
step: 7100, Loss: 0.12161318212747574
step: 7200, Loss: 0.1316460818052292
step: 7300, Loss: 0.1283954232931137
step: 7400, Loss: 0.1259845644235611
step: 7500, Loss: 0.11567593365907669
step: 7600, Loss: 0.12039867043495178
step: 7700, Loss: 0.11721771210432053
step: 7800, Loss: 0.1207338199019432
step: 7900, Loss: 0.11705022305250168
step: 8000, Loss: 0.1234666258096695
step: 8100, Loss: 0.12354288250207901
step: 8200, Loss: 0.1157960519194603
step: 8300, Loss: 0.1266709566116333
step: 8400, Loss: 0.119719959795475
step: 8500, Loss: 0.12014766037464142
step: 8600, Loss: 0.11668157577514648
step: 8700, Loss: 0.11498988419771194
step: 8800, Loss: 0.11380806565284729
step: 8900, Loss: 0.11665956676006317
step: 9000, Loss: 0.11466643959283829
step: 9100, Loss: 0.11848482489585876
step: 9200, Loss: 0.11445152759552002
step: 9300, Loss: 0.11507077515125275
step: 9400, Loss: 0.11964286863803864
step: 9500, Loss: 0.11649447679519653
step: 9600, Loss: 0.11590608954429626
step: 9700, Loss: 0.11461900174617767
step: 9800, Loss: 0.1154395341873169
step: 9900, Loss: 0.11348306387662888
training successfully ended.
validating...
validate data length:88
acc: 0.875
precision: 0.875
recall: 0.8936170212765957
F_score: 0.8842105263157894
******fold 2******

Training... train_data length:786
step: 0, Loss: 0.11639948934316635
step: 100, Loss: 0.1304592788219452
step: 200, Loss: 0.12265197932720184
step: 300, Loss: 0.12721911072731018
step: 400, Loss: 0.11836181581020355
step: 500, Loss: 0.12388115376234055
step: 600, Loss: 0.12325020134449005
step: 700, Loss: 0.11749797314405441
step: 800, Loss: 0.11950606107711792
step: 900, Loss: 0.11440715193748474
step: 1000, Loss: 0.11622621864080429
step: 1100, Loss: 0.11486061662435532
step: 1200, Loss: 0.11625856161117554
step: 1300, Loss: 0.11726005375385284
step: 1400, Loss: 0.11829587072134018
step: 1500, Loss: 0.11549758166074753
step: 1600, Loss: 0.116457998752594
step: 1700, Loss: 0.11780387908220291
step: 1800, Loss: 0.11357446759939194
step: 1900, Loss: 0.11356955766677856
step: 2000, Loss: 0.11526656150817871
step: 2100, Loss: 0.11574075371026993
step: 2200, Loss: 0.11602946370840073
step: 2300, Loss: 0.11398877203464508
step: 2400, Loss: 0.11449139565229416
step: 2500, Loss: 0.1164727658033371
step: 2600, Loss: 0.11555358022451401
step: 2700, Loss: 0.1138777807354927
step: 2800, Loss: 0.11413417756557465
step: 2900, Loss: 0.11434395611286163
step: 3000, Loss: 0.11387676745653152
step: 3100, Loss: 0.11450321227312088
step: 3200, Loss: 0.1155725046992302
step: 3300, Loss: 0.11338502913713455
step: 3400, Loss: 0.11476456373929977
step: 3500, Loss: 0.11704693734645844
step: 3600, Loss: 0.1144067570567131
step: 3700, Loss: 0.11420769989490509
step: 3800, Loss: 0.11530213057994843
step: 3900, Loss: 0.11356204003095627
step: 4000, Loss: 0.11875766515731812
step: 4100, Loss: 0.1175759881734848
step: 4200, Loss: 0.114064060151577
step: 4300, Loss: 0.6821633577346802
step: 4400, Loss: 0.16445767879486084
step: 4500, Loss: 0.1557675451040268
step: 4600, Loss: 0.1334788203239441
step: 4700, Loss: 0.1335708051919937
step: 4800, Loss: 0.13487064838409424
step: 4900, Loss: 0.12740463018417358
step: 5000, Loss: 0.12810079753398895
step: 5100, Loss: 0.12239306420087814
step: 5200, Loss: 0.11950061470270157
step: 5300, Loss: 0.11726006865501404
step: 5400, Loss: 0.12574584782123566
step: 5500, Loss: 0.1257215291261673
step: 5600, Loss: 0.12645816802978516
step: 5700, Loss: 0.12108488380908966
step: 5800, Loss: 0.12018438428640366
step: 5900, Loss: 0.11633946001529694
step: 6000, Loss: 0.1178593784570694
step: 6100, Loss: 0.11703962832689285
step: 6200, Loss: 0.11869396269321442
step: 6300, Loss: 0.1174270510673523
step: 6400, Loss: 0.1155441552400589
step: 6500, Loss: 0.12160027772188187
step: 6600, Loss: 0.11673647910356522
step: 6700, Loss: 0.11900550872087479
step: 6800, Loss: 0.11661139130592346
step: 6900, Loss: 0.116457499563694
step: 7000, Loss: 0.11475224047899246
step: 7100, Loss: 0.11559242755174637
step: 7200, Loss: 0.11548872292041779
step: 7300, Loss: 0.11538583040237427
step: 7400, Loss: 0.1165575459599495
step: 7500, Loss: 0.11424446105957031
step: 7600, Loss: 0.115981325507164
step: 7700, Loss: 0.1143379956483841
step: 7800, Loss: 0.11370212584733963
step: 7900, Loss: 0.11472932249307632
step: 8000, Loss: 0.1146848052740097
step: 8100, Loss: 0.11405527591705322
step: 8200, Loss: 0.11458006501197815
step: 8300, Loss: 0.11435704678297043
step: 8400, Loss: 0.11450902372598648
step: 8500, Loss: 0.1133168488740921
step: 8600, Loss: 0.11616479605436325
step: 8700, Loss: 0.11458663642406464
step: 8800, Loss: 0.11455018818378448
step: 8900, Loss: 0.11362729966640472
step: 9000, Loss: 0.1134691834449768
step: 9100, Loss: 0.11387306451797485
step: 9200, Loss: 0.11382333934307098
step: 9300, Loss: 0.11336053162813187
step: 9400, Loss: 0.11443319916725159
step: 9500, Loss: 0.11464862525463104
step: 9600, Loss: 0.1126817986369133
step: 9700, Loss: 0.11309777200222015
step: 9800, Loss: 0.11408600211143494
step: 9900, Loss: 0.11530689150094986
training successfully ended.
validating...
validate data length:88
acc: 0.9659090909090909
precision: 0.9347826086956522
recall: 1.0
F_score: 0.9662921348314606
******fold 3******

Training... train_data length:786
step: 0, Loss: 0.1809181571006775
step: 100, Loss: 0.12703421711921692
step: 200, Loss: 0.11951044201850891
step: 300, Loss: 0.12201549112796783
step: 400, Loss: 0.11533662676811218
step: 500, Loss: 0.11780036985874176
step: 600, Loss: 0.11554715782403946
step: 700, Loss: 0.11521822214126587
step: 800, Loss: 0.11508291214704514
step: 900, Loss: 0.11439475417137146
step: 1000, Loss: 0.11436211317777634
step: 1100, Loss: 0.11441662907600403
step: 1200, Loss: 0.11361366510391235
step: 1300, Loss: 0.11483190953731537
step: 1400, Loss: 0.11537475883960724
step: 1500, Loss: 0.11293313652276993
step: 1600, Loss: 0.11345577985048294
step: 1700, Loss: 0.11422179639339447
step: 1800, Loss: 0.11517193913459778
step: 1900, Loss: 0.11271576583385468
step: 2000, Loss: 0.11422305554151535
step: 2100, Loss: 0.11387989670038223
step: 2200, Loss: 0.11336036771535873
step: 2300, Loss: 0.11460193246603012
step: 2400, Loss: 0.11474661529064178
step: 2500, Loss: 0.1171240285038948
step: 2600, Loss: 0.11468540132045746
step: 2700, Loss: 0.11398844420909882
step: 2800, Loss: 0.1167505756020546
step: 2900, Loss: 0.11650954186916351
step: 3000, Loss: 0.1154564768075943
step: 3100, Loss: 0.11772856116294861
step: 3200, Loss: 0.11322076618671417
step: 0, Loss: 0.18025818467140198
step: 100, Loss: 0.11784623563289642
step: 200, Loss: 0.11792441457509995
step: 300, Loss: 0.11589224636554718
step: 400, Loss: 0.11841366440057755
step: 500, Loss: 0.11630988866090775
step: 600, Loss: 0.11430057883262634
step: 700, Loss: 0.11574426293373108
step: 800, Loss: 0.11326418071985245
step: 900, Loss: 0.11401202529668808
step: 1000, Loss: 0.1131187304854393
step: 1100, Loss: 0.1146450787782669
step: 1200, Loss: 0.12794680893421173
step: 1300, Loss: 0.11358559876680374
step: 1400, Loss: 0.11445029079914093
step: 1500, Loss: 0.11454746127128601
step: 1600, Loss: 0.11479474604129791
step: 1700, Loss: 0.11332289129495621
step: 1800, Loss: 0.1157674789428711
step: 1900, Loss: 0.11367476731538773
step: 2000, Loss: 0.11816540360450745
step: 2100, Loss: 0.11414909362792969
step: 2200, Loss: 0.11317683756351471
step: 2300, Loss: 0.11387471109628677
step: 2400, Loss: 0.11736950278282166
step: 2500, Loss: 0.11345843225717545
step: 2600, Loss: 0.11418399214744568
step: 2700, Loss: 0.11377828568220139
step: 2800, Loss: 0.11446838080883026
step: 2900, Loss: 0.11578062176704407
step: 3000, Loss: 0.11386875808238983
step: 3100, Loss: 0.11556176841259003
step: 3200, Loss: 0.11497582495212555
step: 3300, Loss: 0.11634831130504608
step: 3400, Loss: 0.11437112092971802
step: 3500, Loss: 0.11399045586585999
step: 3600, Loss: 0.11401258409023285
step: 3700, Loss: 0.11401012539863586
step: 3800, Loss: 0.11482051014900208
step: 3900, Loss: 0.11373776942491531
step: 4000, Loss: 0.11370378732681274
step: 4100, Loss: 0.11499407887458801
step: 4200, Loss: 0.11388038098812103
step: 4300, Loss: 0.11424398422241211
step: 4400, Loss: 0.1138562485575676
step: 4500, Loss: 0.11514373868703842
step: 4600, Loss: 0.11572200804948807
step: 4700, Loss: 0.11425136029720306
step: 4800, Loss: 0.1151861697435379
step: 4900, Loss: 0.11466354131698608
step: 5000, Loss: 0.11424439400434494
step: 5100, Loss: 0.11403317749500275
step: 5200, Loss: 0.11520622670650482
step: 5300, Loss: 0.116753488779068
step: 5400, Loss: 0.11438806354999542
step: 5500, Loss: 0.11452265828847885
step: 5600, Loss: 0.1137600690126419
step: 5700, Loss: 0.11441432684659958
step: 5800, Loss: 0.23671603202819824
step: 5900, Loss: 0.12480179965496063
step: 6000, Loss: 0.12462237477302551
step: 6100, Loss: 0.11952786147594452
step: 6200, Loss: 0.11905595660209656
step: 6300, Loss: 0.11875363439321518
step: 6400, Loss: 0.11603394150733948
step: 6500, Loss: 0.12060616165399551
step: 6600, Loss: 0.11935485899448395
step: 6700, Loss: 0.11480867862701416
step: 6800, Loss: 0.1192002147436142
step: 6900, Loss: 0.11921556293964386
step: 7000, Loss: 0.11663205921649933
step: 7100, Loss: 0.11781813204288483
step: 7200, Loss: 0.1170259639620781
step: 7300, Loss: 0.11440856009721756
step: 7400, Loss: 0.11386992782354355
step: 7500, Loss: 0.11571045964956284
step: 7600, Loss: 0.11534278094768524
step: 7700, Loss: 0.11592461168766022
step: 7800, Loss: 0.11462806910276413
step: 7900, Loss: 0.115918830037117
step: 8000, Loss: 0.1143418401479721
step: 8100, Loss: 0.11490420997142792
step: 8200, Loss: 0.11670027673244476
step: 8300, Loss: 0.11356464773416519
step: 8400, Loss: 0.11424180120229721
step: 8500, Loss: 0.1143498346209526
step: 8600, Loss: 0.11373892426490784
step: 8700, Loss: 0.11291244626045227
step: 8800, Loss: 0.11505746841430664
step: 8900, Loss: 0.11409326642751694
step: 9000, Loss: 0.11458203941583633
step: 9100, Loss: 0.11584286391735077
step: 9200, Loss: 0.11413736641407013
step: 9300, Loss: 0.115975022315979
step: 9400, Loss: 0.11442834883928299
step: 9500, Loss: 0.11641889810562134
step: 9600, Loss: 0.1146160215139389
step: 9700, Loss: 0.11534901708364487
step: 9800, Loss: 0.11427400261163712
step: 9900, Loss: 0.11401835829019547
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.9333333333333333
recall: 0.8235294117647058
F_score: 0.8749999999999999
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.2338753342628479
step: 100, Loss: 0.12060736119747162
step: 200, Loss: 0.1192859560251236
step: 300, Loss: 0.11777181923389435
step: 400, Loss: 0.11676964163780212
step: 500, Loss: 0.11489705741405487
step: 600, Loss: 0.11512834578752518
step: 700, Loss: 0.11708731204271317
step: 800, Loss: 0.11328913271427155
step: 900, Loss: 0.11513465642929077
step: 1000, Loss: 0.11318615078926086
step: 1100, Loss: 0.11330342292785645
step: 1200, Loss: 0.11527403444051743
step: 1300, Loss: 0.11447074264287949
step: 1400, Loss: 0.11418063193559647
step: 1500, Loss: 0.11505685746669769
step: 1600, Loss: 0.11409430205821991
step: 1700, Loss: 0.11436949670314789
step: 1800, Loss: 0.1146966964006424
step: 1900, Loss: 0.11367368698120117
step: 2000, Loss: 0.11304043233394623
step: 2100, Loss: 0.1142190545797348
step: 2200, Loss: 0.11431910842657089
step: 2300, Loss: 0.11630707234144211
step: 2400, Loss: 0.11538529396057129
step: 2500, Loss: 0.11412418633699417
step: 2600, Loss: 0.1137162297964096
step: 2700, Loss: 0.113274484872818
step: 2800, Loss: 0.11452089995145798
step: 2900, Loss: 0.11397726833820343
step: 3000, Loss: 0.113713338971138
step: 3100, Loss: 0.11613491177558899
step: 3200, Loss: 0.11579128354787827
step: 3300, Loss: 0.11435125023126602
step: 3400, Loss: 0.1153806820511818
step: 3500, Loss: 0.1150914877653122
step: 3600, Loss: 0.11383012682199478
step: 3700, Loss: 0.11524288356304169
step: 3800, Loss: 0.11345688253641129
step: 3900, Loss: 0.11323697119951248
step: 4000, Loss: 0.11509953439235687
step: 4100, Loss: 0.11590152233839035
step: 4200, Loss: 0.11282853782176971
step: 4300, Loss: 0.11380982398986816
step: 4400, Loss: 0.11383655667304993
step: 4500, Loss: 0.11511040478944778
step: 4600, Loss: 0.1149134486913681
step: 4700, Loss: 0.11457015573978424
step: 4800, Loss: 0.11470023542642593
step: 4900, Loss: 0.11474429070949554
step: 5000, Loss: 0.11287609487771988
step: 5100, Loss: 0.11443561315536499
step: 5200, Loss: 0.11499394476413727
step: 5300, Loss: 0.11369480192661285
step: 5400, Loss: 0.1147400438785553
step: 5500, Loss: 0.25711649656295776
step: 5600, Loss: 0.12926068902015686
step: 5700, Loss: 0.1342766433954239
step: 5800, Loss: 0.12504048645496368
step: 5900, Loss: 0.12138941884040833
step: 6000, Loss: 0.12267852574586868
step: 6100, Loss: 0.12127140164375305
step: 6200, Loss: 0.1192445456981659
step: 6300, Loss: 0.1201336607336998
step: 6400, Loss: 0.12324400246143341
step: 6500, Loss: 0.12496660649776459
step: 6600, Loss: 0.11797338724136353
step: 6700, Loss: 0.11850513517856598
step: 6800, Loss: 0.11493487656116486
step: 6900, Loss: 0.11673060059547424
step: 7000, Loss: 0.12058227509260178
step: 7100, Loss: 0.1143060177564621
step: 7200, Loss: 0.1165013313293457
step: 7300, Loss: 0.11781950294971466
step: 7400, Loss: 0.11816774308681488
step: 7500, Loss: 0.11461754143238068
step: 7600, Loss: 0.11674052476882935
step: 7700, Loss: 0.11380282789468765
step: 7800, Loss: 0.1145961806178093
step: 7900, Loss: 0.11602319777011871
step: 8000, Loss: 0.12834537029266357
step: 8100, Loss: 0.1155136376619339
step: 8200, Loss: 0.1147974506020546
step: 8300, Loss: 0.11387263983488083
step: 8400, Loss: 0.11537618935108185
step: 8500, Loss: 0.1150103434920311
step: 8600, Loss: 0.11454297602176666
step: 8700, Loss: 0.11545215547084808
step: 8800, Loss: 0.12203580141067505
step: 8900, Loss: 0.11679334193468094
step: 9000, Loss: 0.11465049535036087
step: 9100, Loss: 0.11477380245923996
step: 9200, Loss: 0.1155259907245636
step: 9300, Loss: 0.11416362226009369
step: 9400, Loss: 0.1154327243566513
step: 9500, Loss: 0.1164098009467125
step: 9600, Loss: 0.11459310352802277
step: 9700, Loss: 0.11613750457763672
step: 9800, Loss: 0.11502291262149811
step: 9900, Loss: 0.11513759195804596
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.75
recall: 0.8
F_score: 0.7741935483870969
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.23325097560882568
step: 100, Loss: 0.11758357286453247
step: 200, Loss: 0.11759551614522934
step: 300, Loss: 0.1162588968873024
step: 400, Loss: 0.11708754301071167
step: 500, Loss: 0.11711039394140244
step: 600, Loss: 0.1143161877989769
step: 700, Loss: 0.11483078449964523
step: 800, Loss: 0.11498861759901047
step: 900, Loss: 0.11635094881057739
step: 1000, Loss: 0.1150423139333725
step: 1100, Loss: 0.1154608428478241
step: 1200, Loss: 0.11540023982524872
step: 1300, Loss: 0.11397859454154968
step: 1400, Loss: 0.1145796999335289
step: 1500, Loss: 0.11373907327651978
step: 1600, Loss: 0.11349116265773773
step: 1700, Loss: 0.11308300495147705
step: 1800, Loss: 0.11691565066576004
step: 1900, Loss: 0.11248456686735153
step: 2000, Loss: 0.11343901604413986
step: 2100, Loss: 0.11305046826601028
step: 2200, Loss: 0.11283408105373383
step: 2300, Loss: 0.11375832557678223
step: 2400, Loss: 0.11450043320655823
step: 2500, Loss: 0.1126248687505722
step: 2600, Loss: 0.11678014695644379
step: 2700, Loss: 0.11420812457799911
step: 2800, Loss: 0.11354444921016693
step: 2900, Loss: 0.11415603011846542
step: 3000, Loss: 0.11510356515645981
step: 3100, Loss: 0.11444500833749771
step: 3200, Loss: 0.11425900459289551
step: 3300, Loss: 0.11365805566310883
step: 3400, Loss: 0.11584781855344772
step: 3500, Loss: 0.11329509317874908
step: 3600, Loss: 0.1134912371635437
step: 3700, Loss: 0.11366124451160431
step: 3800, Loss: 0.11383465677499771
step: 3900, Loss: 0.11451655626296997
step: 4000, Loss: 0.11500058323144913
step: 4100, Loss: 0.11424872279167175
step: 4200, Loss: 0.11497604846954346
step: 4300, Loss: 0.11429867148399353
step: 4400, Loss: 0.11523072421550751
step: 4500, Loss: 0.11567522585391998
step: 4600, Loss: 0.11421991884708405
step: 4700, Loss: 0.1146220713853836
step: 4800, Loss: 0.11459280550479889
step: 4900, Loss: 0.11564040184020996
step: 5000, Loss: 0.11366789788007736
step: 5100, Loss: 0.11422479152679443
step: 5200, Loss: 0.11503416299819946
step: 5300, Loss: 0.11596973240375519
step: 5400, Loss: 0.11481615900993347
step: 5500, Loss: 0.1169130727648735
step: 5600, Loss: 0.12823070585727692
step: 5700, Loss: 0.12383407354354858
step: 5800, Loss: 0.12066551297903061
step: 5900, Loss: 0.12156307697296143
step: 6000, Loss: 0.12236440926790237
step: 6100, Loss: 0.11832291632890701
step: 6200, Loss: 0.11853504180908203
step: 6300, Loss: 0.11908659338951111
step: 6400, Loss: 0.11759906262159348
step: 6500, Loss: 0.11770818382501602
step: 6600, Loss: 0.11525864899158478
step: 6700, Loss: 0.1195027306675911
step: 6800, Loss: 0.11562702059745789
step: 6900, Loss: 0.11689001321792603
step: 7000, Loss: 0.11731889843940735
step: 7100, Loss: 0.11474768817424774
step: 7200, Loss: 0.11544493585824966
step: 7300, Loss: 0.1150524914264679
step: 7400, Loss: 0.1160876527428627
step: 7500, Loss: 0.11633636057376862
step: 7600, Loss: 0.11456132680177689
step: 7700, Loss: 0.11776629090309143
step: 7800, Loss: 0.11657730489969254
step: 7900, Loss: 0.11418524384498596
step: 8000, Loss: 0.11558699607849121
step: 8100, Loss: 0.11439232528209686
step: 8200, Loss: 0.11524882167577744
step: 8300, Loss: 0.1139967069029808
step: 8400, Loss: 0.11338569223880768
step: 8500, Loss: 0.1153189092874527
step: 8600, Loss: 0.11520763486623764
step: 8700, Loss: 0.11471382528543472
step: 8800, Loss: 0.11599776893854141
step: 8900, Loss: 0.11458823084831238
step: 9000, Loss: 0.11684824526309967
step: 9100, Loss: 0.11408641934394836
step: 9200, Loss: 0.11403056234121323
step: 9300, Loss: 0.11434668302536011
step: 9400, Loss: 0.11381523311138153
step: 9500, Loss: 0.11635860800743103
step: 9600, Loss: 0.11629106104373932
step: 9700, Loss: 0.11579189449548721
step: 9800, Loss: 0.11354847252368927
step: 9900, Loss: 0.11377213150262833
training successfully ended.
validating...
validate data length:31
acc: 0.7333333333333333
precision: 0.7647058823529411
recall: 0.7647058823529411
F_score: 0.7647058823529412
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.19632551074028015
step: 100, Loss: 0.12536224722862244
step: 200, Loss: 0.11503171920776367
step: 300, Loss: 0.11573612689971924
step: 400, Loss: 0.11505044996738434
step: 500, Loss: 0.11405667662620544
step: 600, Loss: 0.11499219387769699
step: 700, Loss: 0.11382405459880829
step: 800, Loss: 0.113531693816185
step: 900, Loss: 0.11470726132392883
step: 1000, Loss: 0.11392012238502502
step: 1100, Loss: 0.11455371975898743
step: 1200, Loss: 0.11639681458473206
step: 1300, Loss: 0.11630965769290924
step: 1400, Loss: 0.11459283530712128
step: 1500, Loss: 0.11454276740550995
step: 1600, Loss: 0.11347860097885132
step: 1700, Loss: 0.11343745142221451
step: 1800, Loss: 0.11434775590896606
step: 1900, Loss: 0.11389191448688507
step: 2000, Loss: 0.11915967613458633
step: 2100, Loss: 0.1136920377612114
step: 2200, Loss: 0.11412130296230316
step: 2300, Loss: 0.11429915577173233
step: 2400, Loss: 0.11235184222459793
step: 2500, Loss: 0.11423344910144806
step: 2600, Loss: 0.11551796644926071
step: 2700, Loss: 0.11437344551086426
step: 2800, Loss: 0.1145387589931488
step: 2900, Loss: 0.11449671536684036
step: 3000, Loss: 0.1136942058801651
step: 3100, Loss: 0.11590598523616791
step: 3200, Loss: 0.11427769809961319
step: 3300, Loss: 0.11349764466285706
step: 3400, Loss: 0.11411549150943756
step: 3500, Loss: 0.11464877426624298
step: 3600, Loss: 0.11413997411727905
step: 3700, Loss: 0.11475953459739685
step: 3800, Loss: 0.11379515379667282
step: 3900, Loss: 0.11354674398899078
step: 4000, Loss: 0.11392100155353546
step: 4100, Loss: 0.11471349745988846
step: 4200, Loss: 0.11600370705127716
step: 4300, Loss: 0.11507139354944229
step: 4400, Loss: 0.11408395320177078
step: 4500, Loss: 4.639608860015869
step: 4600, Loss: 0.13212773203849792
step: 4700, Loss: 0.13019110262393951
step: 4800, Loss: 0.11984717100858688
step: 4900, Loss: 0.12285814434289932
step: 5000, Loss: 0.11865806579589844
step: 5100, Loss: 0.11860974133014679
step: 5200, Loss: 0.11749504506587982
step: 5300, Loss: 0.11664485931396484
step: 5400, Loss: 0.11760953068733215
step: 5500, Loss: 0.11692623049020767
step: 5600, Loss: 0.11701594293117523
step: 5700, Loss: 0.11966308951377869
step: 5800, Loss: 0.11577115207910538
step: 5900, Loss: 0.11544442176818848
step: 6000, Loss: 0.11549422889947891
step: 6100, Loss: 0.1144818440079689
step: 6200, Loss: 0.12089625000953674
step: 6300, Loss: 0.11627727001905441
step: 6400, Loss: 0.11386331170797348
step: 6500, Loss: 0.11568990349769592
step: 6600, Loss: 0.11748690903186798
step: 6700, Loss: 0.11422157287597656
step: 6800, Loss: 0.11515015363693237
step: 6900, Loss: 0.11574257910251617
step: 7000, Loss: 0.11697670072317123
step: 7100, Loss: 0.11665914952754974
step: 7200, Loss: 0.11364519596099854
step: 7300, Loss: 0.11444704979658127
step: 7400, Loss: 0.11670809239149094
step: 7500, Loss: 0.12441742420196533
step: 7600, Loss: 0.11520543694496155
step: 7700, Loss: 0.11283621191978455
step: 7800, Loss: 0.11485694348812103
step: 7900, Loss: 0.11427631229162216
step: 8000, Loss: 0.11446534097194672
step: 8100, Loss: 0.11459190398454666
step: 8200, Loss: 0.11602724343538284
step: 8300, Loss: 0.11309117078781128
step: 8400, Loss: 0.11523298174142838
step: 8500, Loss: 0.11324434727430344
step: 8600, Loss: 0.11350829899311066
step: 8700, Loss: 0.11369237303733826
step: 8800, Loss: 0.1146572083234787
step: 8900, Loss: 0.11384822428226471
step: 9000, Loss: 0.11601932346820831
step: 9100, Loss: 0.11288236081600189
step: 9200, Loss: 0.11350802332162857
step: 9300, Loss: 0.11453276872634888
step: 9400, Loss: 0.11367304623126984
step: 9500, Loss: 0.11255309730768204
step: 9600, Loss: 0.11357609182596207
step: 9700, Loss: 0.1154165118932724
step: 9800, Loss: 0.11683429032564163
step: 9900, Loss: 0.11385038495063782
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.7333333333333333
recall: 0.9166666666666666
F_score: 0.8148148148148148
subject 18 Avgacc: 0.7620833333333333 Avgfscore: 0.7752631994472602 
 Max acc:0.8666666666666667, Max f score:0.8749999999999999
******** mix subject_19 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 46.74139404296875
step: 100, Loss: 1.284582257270813
step: 200, Loss: 0.15214210748672485
step: 300, Loss: 0.13558177649974823
step: 400, Loss: 0.16606637835502625
step: 500, Loss: 0.12481760233640671
step: 3300, Loss: 0.11377733945846558
step: 3400, Loss: 0.11454442143440247
step: 3500, Loss: 0.1150190532207489
step: 3600, Loss: 0.11704957485198975
step: 3700, Loss: 0.11424219608306885
step: 3800, Loss: 0.11488917469978333
step: 3900, Loss: 0.11716659367084503
step: 4000, Loss: 0.11399756371974945
step: 4100, Loss: 0.11740345507860184
step: 4200, Loss: 0.11385554820299149
step: 4300, Loss: 0.11485783010721207
step: 4400, Loss: 0.11513488739728928
step: 4500, Loss: 0.11375535279512405
step: 4600, Loss: 0.11555822938680649
step: 4700, Loss: 1.144063115119934
step: 4800, Loss: 0.19778528809547424
step: 4900, Loss: 0.145326167345047
step: 5000, Loss: 0.13561496138572693
step: 5100, Loss: 0.13714717328548431
step: 5200, Loss: 0.13753598928451538
step: 5300, Loss: 0.1213894635438919
step: 5400, Loss: 0.13284932076931
step: 5500, Loss: 0.12350912392139435
step: 5600, Loss: 0.13083745539188385
step: 5700, Loss: 0.12384597957134247
step: 5800, Loss: 0.13369187712669373
step: 5900, Loss: 0.11923158168792725
step: 6000, Loss: 0.11853978037834167
step: 6100, Loss: 0.1250072717666626
step: 6200, Loss: 0.11817748099565506
step: 6300, Loss: 0.12414108216762543
step: 6400, Loss: 0.11650276184082031
step: 6500, Loss: 0.12287632375955582
step: 6600, Loss: 0.11914161592721939
step: 6700, Loss: 0.12272053956985474
step: 6800, Loss: 0.11568690091371536
step: 6900, Loss: 0.12372736632823944
step: 7000, Loss: 0.11604577302932739
step: 7100, Loss: 0.11459315568208694
step: 7200, Loss: 0.1168861985206604
step: 7300, Loss: 0.11593309044837952
step: 7400, Loss: 0.11965440958738327
step: 7500, Loss: 0.11680220067501068
step: 7600, Loss: 0.11726722121238708
step: 7700, Loss: 0.1159842386841774
step: 7800, Loss: 0.11615179479122162
step: 7900, Loss: 0.11580497026443481
step: 8000, Loss: 0.12087135761976242
step: 8100, Loss: 0.11448639631271362
step: 8200, Loss: 0.11548567563295364
step: 8300, Loss: 0.11503534764051437
step: 8400, Loss: 0.11611761152744293
step: 8500, Loss: 0.11647914350032806
step: 8600, Loss: 0.11375285685062408
step: 8700, Loss: 0.11579647660255432
step: 8800, Loss: 0.11435317993164062
step: 8900, Loss: 0.11373569071292877
step: 9000, Loss: 0.11436129361391068
step: 9100, Loss: 0.11479832977056503
step: 9200, Loss: 0.11311078071594238
step: 9300, Loss: 0.1132388785481453
step: 9400, Loss: 0.11469699442386627
step: 9500, Loss: 0.11589403450489044
step: 9600, Loss: 0.11336588114500046
step: 9700, Loss: 0.11427520215511322
step: 9800, Loss: 0.1161164864897728
step: 9900, Loss: 0.11337006092071533
training successfully ended.
validating...
validate data length:88
acc: 0.9431818181818182
precision: 0.9130434782608695
recall: 0.9767441860465116
F_score: 0.9438202247191011
******fold 4******

Training... train_data length:786
step: 0, Loss: 0.15090683102607727
step: 100, Loss: 0.1258029192686081
step: 200, Loss: 0.11700445413589478
step: 300, Loss: 0.11810868978500366
step: 400, Loss: 0.11665257811546326
step: 500, Loss: 0.1160426214337349
step: 600, Loss: 0.11646367609500885
step: 700, Loss: 0.11444781720638275
step: 800, Loss: 0.1152242124080658
step: 900, Loss: 0.11329420655965805
step: 1000, Loss: 0.1152617335319519
step: 1100, Loss: 0.11436457931995392
step: 1200, Loss: 0.11554304510354996
step: 1300, Loss: 0.11447247862815857
step: 1400, Loss: 0.11518890410661697
step: 1500, Loss: 0.11483027786016464
step: 1600, Loss: 0.11383969336748123
step: 1700, Loss: 0.11320188641548157
step: 1800, Loss: 0.11423036456108093
step: 1900, Loss: 0.1137465387582779
step: 2000, Loss: 0.11436952650547028
step: 2100, Loss: 0.1139989122748375
step: 2200, Loss: 0.11432942003011703
step: 2300, Loss: 0.11363184452056885
step: 2400, Loss: 0.11472386121749878
step: 2500, Loss: 0.11429035663604736
step: 2600, Loss: 0.11293669044971466
step: 2700, Loss: 0.11469660699367523
step: 2800, Loss: 0.11666610836982727
step: 2900, Loss: 0.11521696299314499
step: 3000, Loss: 0.11613701283931732
step: 3100, Loss: 0.11402146518230438
step: 3200, Loss: 0.11526698619127274
step: 3300, Loss: 0.1143825575709343
step: 3400, Loss: 0.11717461049556732
step: 3500, Loss: 0.11621874570846558
step: 3600, Loss: 0.1177753210067749
step: 3700, Loss: 0.11509682983160019
step: 3800, Loss: 0.11366134136915207
step: 3900, Loss: 0.11536695808172226
step: 4000, Loss: 0.1165539026260376
step: 4100, Loss: 0.11550900340080261
step: 4200, Loss: 0.11485441029071808
step: 4300, Loss: 0.11450104415416718
step: 4400, Loss: 0.11451859027147293
step: 4500, Loss: 0.11580942571163177
step: 4600, Loss: 0.1170777827501297
step: 4700, Loss: 3.6956355571746826
step: 4800, Loss: 0.17703965306282043
step: 4900, Loss: 0.13308382034301758
step: 5000, Loss: 0.1369398534297943
step: 5100, Loss: 0.13375148177146912
step: 5200, Loss: 0.13483978807926178
step: 5300, Loss: 0.13094963133335114
step: 5400, Loss: 0.12405478209257126
step: 5500, Loss: 0.12941974401474
step: 5600, Loss: 0.12450745701789856
step: 5700, Loss: 0.12422023713588715
step: 5800, Loss: 0.12666988372802734
step: 5900, Loss: 0.11976215243339539
step: 6000, Loss: 0.1181592345237732
step: 6100, Loss: 0.11950197070837021
step: 6200, Loss: 0.11791455745697021
step: 6300, Loss: 0.12461134791374207
step: 6400, Loss: 0.11875408887863159
step: 6500, Loss: 0.12125180661678314
step: 6600, Loss: 0.11842618882656097
step: 6700, Loss: 0.11535413563251495
step: 6800, Loss: 0.12016121298074722
step: 6900, Loss: 0.11925391852855682
step: 7000, Loss: 0.11709286272525787
step: 7100, Loss: 0.11509307473897934
step: 7200, Loss: 0.11659181118011475
step: 7300, Loss: 0.11643016338348389
step: 7400, Loss: 0.12106110155582428
step: 7500, Loss: 0.11451095342636108
step: 7600, Loss: 0.1164727658033371
step: 7700, Loss: 0.11488497257232666
step: 7800, Loss: 0.11506087332963943
step: 7900, Loss: 0.1163799837231636
step: 8000, Loss: 0.11563034355640411
step: 8100, Loss: 0.1146574467420578
step: 8200, Loss: 0.11351805925369263
step: 8300, Loss: 0.11518217623233795
step: 8400, Loss: 0.11497161537408829
step: 8500, Loss: 0.11442024260759354
step: 8600, Loss: 0.11397777497768402
step: 8700, Loss: 0.11427434533834457
step: 8800, Loss: 0.11388589441776276
step: 8900, Loss: 0.1140519380569458
step: 9000, Loss: 0.11396388709545135
step: 9100, Loss: 0.11325450241565704
step: 9200, Loss: 0.11473497748374939
step: 9300, Loss: 0.11447945237159729
step: 9400, Loss: 0.11575420945882797
step: 9500, Loss: 0.11433673650026321
step: 9600, Loss: 0.11414580047130585
step: 9700, Loss: 0.11283578723669052
step: 9800, Loss: 0.11424584686756134
step: 9900, Loss: 0.11322526633739471
training successfully ended.
validating...
validate data length:88
acc: 0.9659090909090909
precision: 0.9387755102040817
recall: 1.0
F_score: 0.968421052631579
******fold 5******

Training... train_data length:787
step: 0, Loss: 0.18598484992980957
step: 100, Loss: 0.13425999879837036
step: 200, Loss: 0.11742185056209564
step: 300, Loss: 0.1200380027294159
step: 400, Loss: 0.11471821367740631
step: 500, Loss: 0.11494298279285431
step: 600, Loss: 0.11551228910684586
step: 700, Loss: 0.11456168442964554
step: 800, Loss: 0.11541882902383804
step: 900, Loss: 0.11530828475952148
step: 1000, Loss: 0.114475779235363
step: 1100, Loss: 0.1146097257733345
step: 1200, Loss: 0.1148184984922409
step: 1300, Loss: 0.11453467607498169
step: 1400, Loss: 0.11552458256483078
step: 1500, Loss: 0.11682436615228653
step: 1600, Loss: 0.11409572511911392
step: 1700, Loss: 0.1148013174533844
step: 1800, Loss: 0.1143551841378212
step: 1900, Loss: 0.11370135098695755
step: 2000, Loss: 0.11358705908060074
step: 2100, Loss: 0.1133873388171196
step: 2200, Loss: 0.11346227675676346
step: 2300, Loss: 0.11496766656637192
step: 2400, Loss: 0.11357051134109497
step: 2500, Loss: 0.11339784413576126
step: 2600, Loss: 0.11367693543434143
step: 2700, Loss: 0.11549074947834015
step: 2800, Loss: 0.11593981832265854
step: 2900, Loss: 0.1151786595582962
step: 3000, Loss: 0.11858085542917252
step: 3100, Loss: 0.11358524858951569
step: 3200, Loss: 0.11448930203914642
step: 3300, Loss: 0.11511382460594177
step: 3400, Loss: 0.11593155562877655
step: 3500, Loss: 0.11389604210853577
step: 3600, Loss: 0.1140805333852768
step: 3700, Loss: 0.11536303162574768
step: 600, Loss: 0.12685152888298035
step: 700, Loss: 0.12329201400279999
step: 800, Loss: 0.12451360374689102
step: 900, Loss: 0.12014332413673401
step: 1000, Loss: 0.1216355711221695
step: 1100, Loss: 0.12494075298309326
step: 1200, Loss: 0.11662489920854568
step: 1300, Loss: 0.11803212761878967
step: 1400, Loss: 0.11691141128540039
step: 1500, Loss: 0.12037535011768341
step: 1600, Loss: 0.11469849944114685
step: 1700, Loss: 0.11476533114910126
step: 1800, Loss: 0.1190110445022583
step: 1900, Loss: 0.11677388846874237
step: 2000, Loss: 0.11923880875110626
step: 2100, Loss: 0.1190623864531517
step: 2200, Loss: 0.11593572050333023
step: 2300, Loss: 0.11549560725688934
step: 2400, Loss: 0.11642111837863922
step: 2500, Loss: 0.12038558721542358
step: 2600, Loss: 0.11760174483060837
step: 2700, Loss: 0.11589818447828293
step: 2800, Loss: 0.11508986353874207
step: 2900, Loss: 0.11757474392652512
step: 3000, Loss: 0.11818252503871918
step: 3100, Loss: 0.115735724568367
step: 3200, Loss: 0.11732304841279984
step: 3300, Loss: 0.11832664906978607
step: 3400, Loss: 9.734016418457031
step: 3500, Loss: 0.1546395868062973
step: 3600, Loss: 0.1570892482995987
step: 3700, Loss: 0.1325940489768982
step: 3800, Loss: 0.1286322921514511
step: 3900, Loss: 0.12244892865419388
step: 4000, Loss: 0.12807601690292358
step: 4100, Loss: 0.11869606375694275
step: 4200, Loss: 0.122695192694664
step: 4300, Loss: 0.12076328694820404
step: 4400, Loss: 0.1252482533454895
step: 4500, Loss: 0.11939702183008194
step: 4600, Loss: 0.11702404916286469
step: 4700, Loss: 0.11693070083856583
step: 4800, Loss: 0.11738227307796478
step: 4900, Loss: 0.11508923768997192
step: 5000, Loss: 0.12062431126832962
step: 5100, Loss: 0.11807146668434143
step: 5200, Loss: 0.11702166497707367
step: 5300, Loss: 0.11698094010353088
step: 5400, Loss: 0.11776468902826309
step: 5500, Loss: 0.11814670264720917
step: 5600, Loss: 0.11551440507173538
step: 5700, Loss: 0.11471927911043167
step: 5800, Loss: 0.11595170199871063
step: 5900, Loss: 0.11412191390991211
step: 6000, Loss: 0.11637820303440094
step: 6100, Loss: 0.11760175973176956
step: 6200, Loss: 0.1151464655995369
step: 6300, Loss: 0.11480648815631866
step: 6400, Loss: 0.11532050371170044
step: 6500, Loss: 0.11610528826713562
step: 6600, Loss: 0.11570267379283905
step: 6700, Loss: 0.1164451539516449
step: 6800, Loss: 0.11587859690189362
step: 6900, Loss: 0.1156812235713005
step: 7000, Loss: 0.11461719125509262
step: 7100, Loss: 0.11600833386182785
step: 7200, Loss: 0.1164177879691124
step: 7300, Loss: 0.11457674205303192
step: 7400, Loss: 0.11499994993209839
step: 7500, Loss: 0.11776132136583328
step: 7600, Loss: 0.11655566841363907
step: 7700, Loss: 0.1138727143406868
step: 7800, Loss: 0.11533889919519424
step: 7900, Loss: 0.11991776525974274
step: 8000, Loss: 0.11661165952682495
step: 8100, Loss: 0.1166955977678299
step: 8200, Loss: 0.11766932159662247
step: 8300, Loss: 0.11609486490488052
step: 8400, Loss: 0.11534713208675385
step: 8500, Loss: 0.11380758881568909
step: 8600, Loss: 0.11547000706195831
step: 8700, Loss: 0.11716775596141815
step: 8800, Loss: 0.11815215647220612
step: 8900, Loss: 0.11382352560758591
step: 9000, Loss: 0.1168624758720398
step: 9100, Loss: 0.11601512134075165
step: 9200, Loss: 0.11454876512289047
step: 9300, Loss: 0.11487861722707748
step: 9400, Loss: 0.114423006772995
step: 9500, Loss: 0.30309051275253296
step: 9600, Loss: 0.1621939241886139
step: 9700, Loss: 0.12882362306118011
step: 9800, Loss: 0.13191282749176025
step: 9900, Loss: 0.13209983706474304
training successfully ended.
validating...
validate data length:32
acc: 0.40625
precision: 0.3157894736842105
recall: 0.5
F_score: 0.3870967741935484
******fold 2******

Training... train_data length:280
step: 0, Loss: 1.1141102313995361
step: 100, Loss: 0.1537719964981079
step: 200, Loss: 0.13658124208450317
step: 300, Loss: 0.14912095665931702
step: 400, Loss: 0.1287972778081894
step: 500, Loss: 0.13622023165225983
step: 600, Loss: 0.1268962323665619
step: 700, Loss: 0.1269097775220871
step: 800, Loss: 0.11997103691101074
step: 900, Loss: 0.12453463673591614
step: 1000, Loss: 0.1192670613527298
step: 1100, Loss: 0.11988767981529236
step: 1200, Loss: 0.11746335029602051
step: 1300, Loss: 0.11577393114566803
step: 1400, Loss: 0.11782858520746231
step: 1500, Loss: 0.1157391145825386
step: 1600, Loss: 0.11977694183588028
step: 1700, Loss: 0.11618311703205109
step: 1800, Loss: 0.11601964384317398
step: 1900, Loss: 0.11764071881771088
step: 2000, Loss: 0.11954846233129501
step: 2100, Loss: 0.11472613364458084
step: 2200, Loss: 0.11688992381095886
step: 2300, Loss: 0.11596248298883438
step: 2400, Loss: 0.11586558073759079
step: 2500, Loss: 0.11508122086524963
step: 2600, Loss: 0.11421079933643341
step: 2700, Loss: 0.11555144190788269
step: 2800, Loss: 0.11631554365158081
step: 2900, Loss: 0.11491309851408005
step: 3000, Loss: 0.11545072495937347
step: 3100, Loss: 0.11513982713222504
step: 3200, Loss: 0.11579327285289764
step: 3300, Loss: 0.11457096040248871
step: 3400, Loss: 0.11336269974708557
step: 3500, Loss: 0.11568686366081238
step: 3600, Loss: 0.11752917617559433
step: 3700, Loss: 0.11764552444219589
step: 3800, Loss: 0.11350765079259872
step: 3900, Loss: 0.11494917422533035
step: 4000, Loss: 0.11406160891056061
step: 4100, Loss: 0.11576458811759949
step: 4200, Loss: 0.11560754477977753
step: 4300, Loss: 0.11535761505365372
step: 4400, Loss: 0.11592495441436768
step: 4500, Loss: 0.11490242183208466
step: 4600, Loss: 0.11506935954093933
step: 4700, Loss: 0.11812327802181244
step: 4800, Loss: 0.11511299014091492
step: 4900, Loss: 0.1143515557050705
step: 5000, Loss: 0.11363187432289124
step: 5100, Loss: 0.21885815262794495
step: 5200, Loss: 0.15199384093284607
step: 5300, Loss: 0.13978247344493866
step: 5400, Loss: 0.13768872618675232
step: 5500, Loss: 0.12812867760658264
step: 5600, Loss: 0.1329704225063324
step: 5700, Loss: 0.12076424062252045
step: 5800, Loss: 0.12087487429380417
step: 5900, Loss: 0.12410828471183777
step: 6000, Loss: 0.12028760462999344
step: 6100, Loss: 0.12047335505485535
step: 6200, Loss: 0.12176314741373062
step: 6300, Loss: 0.12322797626256943
step: 6400, Loss: 0.11744886636734009
step: 6500, Loss: 0.11571043729782104
step: 6600, Loss: 0.12465780228376389
step: 6700, Loss: 0.117947056889534
step: 6800, Loss: 0.11612030863761902
step: 6900, Loss: 0.11713847517967224
step: 7000, Loss: 0.11547841876745224
step: 7100, Loss: 0.12717768549919128
step: 7200, Loss: 0.11571221053600311
step: 7300, Loss: 0.11555862426757812
step: 7400, Loss: 0.11543421447277069
step: 7500, Loss: 0.11493265628814697
step: 7600, Loss: 0.11642423272132874
step: 7700, Loss: 0.11689132452011108
step: 7800, Loss: 0.1161164715886116
step: 7900, Loss: 0.11701927334070206
step: 8000, Loss: 0.11618731915950775
step: 8100, Loss: 0.1154211089015007
step: 8200, Loss: 0.11426681280136108
step: 8300, Loss: 0.11675382405519485
step: 8400, Loss: 0.11442968249320984
step: 8500, Loss: 0.11548789590597153
step: 8600, Loss: 0.1142839640378952
step: 8700, Loss: 0.11429385840892792
step: 8800, Loss: 0.1149170771241188
step: 8900, Loss: 0.11371409893035889
step: 9000, Loss: 0.11620211601257324
step: 9100, Loss: 0.11392086744308472
step: 9200, Loss: 0.11502127349376678
step: 9300, Loss: 0.11947061121463776
step: 9400, Loss: 0.1135568767786026
step: 9500, Loss: 0.11364735662937164
step: 9600, Loss: 0.11610033363103867
step: 9700, Loss: 0.12871213257312775
step: 9800, Loss: 0.1137256920337677
step: 9900, Loss: 0.11371015012264252
training successfully ended.
validating...
validate data length:32
acc: 0.875
precision: 0.8333333333333334
recall: 0.9375
F_score: 0.8823529411764706
******fold 3******

Training... train_data length:281
step: 0, Loss: 2.2465271949768066
step: 100, Loss: 0.12104405462741852
step: 200, Loss: 0.11767624318599701
step: 300, Loss: 0.11505195498466492
step: 400, Loss: 0.11591755598783493
step: 500, Loss: 0.11560529470443726
step: 600, Loss: 0.11568722128868103
step: 700, Loss: 0.11355829983949661
step: 800, Loss: 0.11419224739074707
step: 900, Loss: 0.1130838543176651
step: 1000, Loss: 0.11621987074613571
step: 1100, Loss: 0.11378031224012375
step: 3800, Loss: 0.11332441121339798
step: 3900, Loss: 0.11349912732839584
step: 4000, Loss: 0.11674422025680542
step: 4100, Loss: 4.937268257141113
step: 4200, Loss: 0.31734520196914673
step: 4300, Loss: 0.15407080948352814
step: 4400, Loss: 0.13993293046951294
step: 4500, Loss: 0.13880708813667297
step: 4600, Loss: 0.12781044840812683
step: 4700, Loss: 0.12837590277194977
step: 4800, Loss: 0.13292723894119263
step: 4900, Loss: 0.12560589611530304
step: 5000, Loss: 0.12907208502292633
step: 5100, Loss: 0.12330727279186249
step: 5200, Loss: 0.1232764720916748
step: 5300, Loss: 0.11971208453178406
step: 5400, Loss: 0.12920892238616943
step: 5500, Loss: 0.12046515196561813
step: 5600, Loss: 0.12290465831756592
step: 5700, Loss: 0.11618871986865997
step: 5800, Loss: 0.12231840193271637
step: 5900, Loss: 0.12074398249387741
step: 6000, Loss: 0.11593650281429291
step: 6100, Loss: 0.12081002444028854
step: 6200, Loss: 0.11651600897312164
step: 6300, Loss: 0.11832207441329956
step: 6400, Loss: 0.11502490192651749
step: 6500, Loss: 0.11950840055942535
step: 6600, Loss: 0.11584006994962692
step: 6700, Loss: 0.11621101200580597
step: 6800, Loss: 0.11478420346975327
step: 6900, Loss: 0.11502056568861008
step: 7000, Loss: 0.11681105941534042
step: 7100, Loss: 0.1149110198020935
step: 7200, Loss: 0.11490502953529358
step: 7300, Loss: 0.11595587432384491
step: 7400, Loss: 0.11452934890985489
step: 7500, Loss: 0.1140759289264679
step: 7600, Loss: 0.1153058409690857
step: 7700, Loss: 0.1142914891242981
step: 7800, Loss: 0.11558927595615387
step: 7900, Loss: 0.11389902979135513
step: 8000, Loss: 0.11382493376731873
step: 8100, Loss: 0.11441491544246674
step: 8200, Loss: 0.1136016696691513
step: 8300, Loss: 0.1137426421046257
step: 8400, Loss: 0.11341123282909393
step: 8500, Loss: 0.11595809459686279
step: 8600, Loss: 0.1139400377869606
step: 8700, Loss: 0.11557362228631973
step: 8800, Loss: 0.11438540369272232
step: 8900, Loss: 0.11598832160234451
step: 9000, Loss: 0.11445354670286179
step: 9100, Loss: 0.11425063759088516
step: 9200, Loss: 0.11363884806632996
step: 9300, Loss: 0.11281565576791763
step: 9400, Loss: 0.11409099400043488
step: 9500, Loss: 0.11374907195568085
step: 9600, Loss: 0.1133270189166069
step: 9700, Loss: 0.11402871459722519
step: 9800, Loss: 0.1140541359782219
step: 9900, Loss: 0.11400376260280609
training successfully ended.
validating...
validate data length:87
acc: 0.9875
precision: 0.9714285714285714
recall: 1.0
F_score: 0.9855072463768115
******fold 6******

Training... train_data length:787
step: 0, Loss: 0.127714604139328
step: 100, Loss: 0.11926376819610596
step: 200, Loss: 0.11736415326595306
step: 300, Loss: 0.11571270227432251
step: 400, Loss: 0.11418229341506958
step: 500, Loss: 0.11420267820358276
step: 600, Loss: 0.11470220983028412
step: 700, Loss: 0.11298683285713196
step: 800, Loss: 0.1145654171705246
step: 900, Loss: 0.11469210684299469
step: 1000, Loss: 0.1142454445362091
step: 1100, Loss: 0.11343573778867722
step: 1200, Loss: 0.11511147022247314
step: 1300, Loss: 0.11392772942781448
step: 1400, Loss: 0.11555004119873047
step: 1500, Loss: 0.11382990330457687
step: 1600, Loss: 0.1128702312707901
step: 1700, Loss: 0.11501374840736389
step: 1800, Loss: 0.11406512558460236
step: 1900, Loss: 0.1133498027920723
step: 2000, Loss: 0.11492979526519775
step: 2100, Loss: 0.11715760082006454
step: 2200, Loss: 0.11894761025905609
step: 2300, Loss: 0.11495188623666763
step: 2400, Loss: 0.11375972628593445
step: 2500, Loss: 0.11356062442064285
step: 2600, Loss: 0.11452171206474304
step: 2700, Loss: 0.1161356195807457
step: 2800, Loss: 1.7722452878952026
step: 2900, Loss: 0.4547141194343567
step: 3000, Loss: 0.15079382061958313
step: 3100, Loss: 0.14329904317855835
step: 3200, Loss: 0.1428147554397583
step: 3300, Loss: 0.12728387117385864
step: 3400, Loss: 0.1397559940814972
step: 3500, Loss: 0.1264616996049881
step: 3600, Loss: 0.1263309121131897
step: 3700, Loss: 0.1286819875240326
step: 3800, Loss: 0.12067660689353943
step: 3900, Loss: 0.12251436710357666
step: 4000, Loss: 0.12341386079788208
step: 4100, Loss: 0.11797904968261719
step: 4200, Loss: 0.11918872594833374
step: 4300, Loss: 0.12285362929105759
step: 4400, Loss: 0.12039056420326233
step: 4500, Loss: 0.12425551563501358
step: 4600, Loss: 0.11755618453025818
step: 4700, Loss: 0.11713857203722
step: 4800, Loss: 0.12367051839828491
step: 4900, Loss: 0.11616113781929016
step: 5000, Loss: 0.11950907856225967
step: 5100, Loss: 0.1189754456281662
step: 5200, Loss: 0.1165757030248642
step: 5300, Loss: 0.11692866683006287
step: 5400, Loss: 0.11918768286705017
step: 5500, Loss: 0.11656857281923294
step: 5600, Loss: 0.12038351595401764
step: 5700, Loss: 0.1144382655620575
step: 5800, Loss: 0.11610514670610428
step: 5900, Loss: 0.11544040590524673
step: 6000, Loss: 0.117025226354599
step: 6100, Loss: 0.11842755973339081
step: 6200, Loss: 0.11776624619960785
step: 6300, Loss: 0.11571212112903595
step: 6400, Loss: 0.11614135652780533
step: 6500, Loss: 0.11727043241262436
step: 6600, Loss: 0.11297360062599182
step: 6700, Loss: 0.11504755914211273
step: 6800, Loss: 0.1129240095615387
step: 6900, Loss: 0.11310335248708725
step: 7000, Loss: 0.11455020308494568
step: 7100, Loss: 0.11705160140991211
step: 7200, Loss: 0.1159871444106102
step: 7300, Loss: 0.1156199723482132
step: 7400, Loss: 0.11359919607639313
step: 7500, Loss: 0.1134948879480362
step: 7600, Loss: 0.11296460777521133
step: 7700, Loss: 0.11402493715286255
step: 7800, Loss: 0.11341365426778793
step: 7900, Loss: 0.11497096717357635
step: 8000, Loss: 0.1140270009636879
step: 8100, Loss: 0.11448259651660919
step: 8200, Loss: 0.11328201740980148
step: 8300, Loss: 0.11446334421634674
step: 8400, Loss: 0.11428344994783401
step: 8500, Loss: 0.1140347346663475
step: 8600, Loss: 0.11458400636911392
step: 8700, Loss: 0.11322695016860962
step: 8800, Loss: 0.11324886232614517
step: 8900, Loss: 0.11404573917388916
step: 9000, Loss: 0.11419710516929626
step: 9100, Loss: 0.11427082121372223
step: 9200, Loss: 0.11386129260063171
step: 9300, Loss: 0.11327248811721802
step: 9400, Loss: 0.11479218304157257
step: 9500, Loss: 0.1130993515253067
step: 9600, Loss: 0.11441949009895325
step: 9700, Loss: 0.11412341892719269
step: 9800, Loss: 0.11442473530769348
step: 9900, Loss: 0.11555258929729462
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:787
step: 0, Loss: 0.12503071129322052
step: 100, Loss: 0.11704540252685547
step: 200, Loss: 0.11684346199035645
step: 300, Loss: 0.11469735950231552
step: 400, Loss: 0.114761583507061
step: 500, Loss: 0.11397749185562134
step: 600, Loss: 0.11566652357578278
step: 700, Loss: 0.11439365148544312
step: 800, Loss: 0.11401059478521347
step: 900, Loss: 0.11331125348806381
step: 1000, Loss: 0.11437134444713593
step: 1100, Loss: 0.11357739567756653
step: 1200, Loss: 0.11445136368274689
step: 1300, Loss: 0.11528661102056503
step: 1400, Loss: 0.11563728004693985
step: 1500, Loss: 0.11437015235424042
step: 1600, Loss: 0.11515722423791885
step: 1700, Loss: 0.1149422749876976
step: 1800, Loss: 0.11565282195806503
step: 1900, Loss: 0.11393994092941284
step: 2000, Loss: 0.11405568569898605
step: 2100, Loss: 0.11338620632886887
step: 2200, Loss: 0.11427711695432663
step: 2300, Loss: 0.1154467836022377
step: 2400, Loss: 0.11418426036834717
step: 2500, Loss: 0.11435900628566742
step: 2600, Loss: 0.11451251059770584
step: 2700, Loss: 0.11657772213220596
step: 2800, Loss: 0.17704732716083527
step: 2900, Loss: 0.1421816051006317
step: 3000, Loss: 0.14694379270076752
step: 3100, Loss: 0.12446674704551697
step: 3200, Loss: 0.12204059958457947
step: 3300, Loss: 0.12155032902956009
step: 3400, Loss: 0.1283493936061859
step: 3500, Loss: 0.12458610534667969
step: 3600, Loss: 0.12723730504512787
step: 3700, Loss: 0.11989875137805939
step: 3800, Loss: 0.11790945380926132
step: 3900, Loss: 0.12046747654676437
step: 4000, Loss: 0.12138931453227997
step: 4100, Loss: 0.1218550056219101
step: 4200, Loss: 0.11863982677459717
step: 4300, Loss: 0.11442237347364426
step: 4400, Loss: 0.11788445711135864
step: 1200, Loss: 0.11488792300224304
step: 1300, Loss: 0.11413878202438354
step: 1400, Loss: 0.11561569571495056
step: 1500, Loss: 0.11540008336305618
step: 1600, Loss: 0.1160411536693573
step: 1700, Loss: 0.1135406345129013
step: 1800, Loss: 0.11551998555660248
step: 1900, Loss: 0.11399713158607483
step: 2000, Loss: 0.1144862249493599
step: 2100, Loss: 0.11523941159248352
step: 2200, Loss: 0.11571041494607925
step: 2300, Loss: 0.1161578893661499
step: 2400, Loss: 0.11542928218841553
step: 2500, Loss: 0.11357637494802475
step: 2600, Loss: 0.11541111022233963
step: 2700, Loss: 0.11616954952478409
step: 2800, Loss: 0.1141061931848526
step: 2900, Loss: 0.11442309617996216
step: 3000, Loss: 0.1140068918466568
step: 3100, Loss: 0.11418299376964569
step: 3200, Loss: 0.11380825191736221
step: 3300, Loss: 0.1144522950053215
step: 3400, Loss: 0.11612555384635925
step: 3500, Loss: 0.1163727194070816
step: 3600, Loss: 0.11477267742156982
step: 3700, Loss: 0.11482681334018707
step: 3800, Loss: 0.11375050246715546
step: 3900, Loss: 0.11539335548877716
step: 4000, Loss: 0.11495838314294815
step: 4100, Loss: 0.11420836299657822
step: 4200, Loss: 0.11525660753250122
step: 4300, Loss: 0.11388416588306427
step: 4400, Loss: 0.1135701835155487
step: 4500, Loss: 0.11625008285045624
step: 4600, Loss: 0.11443592607975006
step: 4700, Loss: 0.11554743349552155
step: 4800, Loss: 0.11611627787351608
step: 4900, Loss: 0.11550401151180267
step: 5000, Loss: 0.17527760565280914
step: 5100, Loss: 0.13693995773792267
step: 5200, Loss: 0.13102859258651733
step: 5300, Loss: 0.1244187206029892
step: 5400, Loss: 0.12225203216075897
step: 5500, Loss: 0.11922155320644379
step: 5600, Loss: 0.12249433994293213
step: 5700, Loss: 0.1181856244802475
step: 5800, Loss: 0.11788095533847809
step: 5900, Loss: 0.12196680158376694
step: 6000, Loss: 0.1166888102889061
step: 6100, Loss: 0.11599887907505035
step: 6200, Loss: 0.11496996879577637
step: 6300, Loss: 0.11614671349525452
step: 6400, Loss: 0.11579020321369171
step: 6500, Loss: 0.11578262597322464
step: 6600, Loss: 0.11607323586940765
step: 6700, Loss: 0.11467799544334412
step: 6800, Loss: 0.11567691713571548
step: 6900, Loss: 0.11670780181884766
step: 7000, Loss: 0.11670297384262085
step: 7100, Loss: 0.1153334528207779
step: 7200, Loss: 0.11730308085680008
step: 7300, Loss: 0.11314364522695541
step: 7400, Loss: 0.11520938575267792
step: 7500, Loss: 0.11523547768592834
step: 7600, Loss: 0.11561939120292664
step: 7700, Loss: 0.11509650945663452
step: 7800, Loss: 0.11746273934841156
step: 7900, Loss: 0.11518353223800659
step: 8000, Loss: 0.11556234210729599
step: 8100, Loss: 0.11801578849554062
step: 8200, Loss: 0.1159701719880104
step: 8300, Loss: 0.11665189266204834
step: 8400, Loss: 0.11438499391078949
step: 8500, Loss: 0.11493896692991257
step: 8600, Loss: 0.11417204886674881
step: 8700, Loss: 0.11584703624248505
step: 8800, Loss: 0.1148860901594162
step: 8900, Loss: 0.11285590380430222
step: 9000, Loss: 0.1145261898636818
step: 9100, Loss: 0.11459880322217941
step: 9200, Loss: 0.11457344889640808
step: 9300, Loss: 0.1137247160077095
step: 9400, Loss: 0.1133044958114624
step: 9500, Loss: 0.11765895783901215
step: 9600, Loss: 0.11509612947702408
step: 9700, Loss: 0.11553023755550385
step: 9800, Loss: 0.11457007378339767
step: 9900, Loss: 0.11389526724815369
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
******fold 4******

Training... train_data length:281
step: 0, Loss: 0.22264735400676727
step: 100, Loss: 0.11825596541166306
step: 200, Loss: 0.11678165197372437
step: 300, Loss: 0.1159132570028305
step: 400, Loss: 0.11717668175697327
step: 500, Loss: 0.1148730218410492
step: 600, Loss: 0.1158205047249794
step: 700, Loss: 0.11474083364009857
step: 800, Loss: 0.11733205616474152
step: 900, Loss: 0.11422444880008698
step: 1000, Loss: 0.11396379768848419
step: 1100, Loss: 0.11422549188137054
step: 1200, Loss: 0.11611557006835938
step: 1300, Loss: 0.114102803170681
step: 1400, Loss: 0.11489038914442062
step: 1500, Loss: 0.11494661867618561
step: 1600, Loss: 0.11300119757652283
step: 1700, Loss: 0.11365383863449097
step: 1800, Loss: 0.11346465349197388
step: 1900, Loss: 0.1147131398320198
step: 2000, Loss: 0.11463673412799835
step: 2100, Loss: 0.114279605448246
step: 2200, Loss: 0.11501671373844147
step: 2300, Loss: 0.11533357203006744
step: 2400, Loss: 0.11542309820652008
step: 2500, Loss: 0.1142754778265953
step: 2600, Loss: 0.12031060457229614
step: 2700, Loss: 0.1151970699429512
step: 2800, Loss: 0.11509595811367035
step: 2900, Loss: 0.11404523253440857
step: 3000, Loss: 0.1138724759221077
step: 3100, Loss: 0.11570324003696442
step: 3200, Loss: 0.11493459343910217
step: 3300, Loss: 0.11462967842817307
step: 3400, Loss: 0.11416476964950562
step: 3500, Loss: 0.11594811081886292
step: 3600, Loss: 0.11527132987976074
step: 3700, Loss: 0.11712628602981567
step: 3800, Loss: 0.11452507972717285
step: 3900, Loss: 0.11404778808355331
step: 4000, Loss: 0.1135181114077568
step: 4100, Loss: 0.11460140347480774
step: 4200, Loss: 0.1144196018576622
step: 4300, Loss: 0.11502040922641754
step: 4400, Loss: 0.11425513029098511
step: 4500, Loss: 0.11808301508426666
step: 4600, Loss: 0.11531849205493927
step: 4700, Loss: 0.11590418219566345
step: 4800, Loss: 0.11360998451709747
step: 4900, Loss: 0.11408308148384094
step: 5000, Loss: 0.11348217725753784
step: 5100, Loss: 0.11379588395357132
step: 5200, Loss: 0.11526767909526825
step: 5300, Loss: 0.11590135097503662
step: 5400, Loss: 0.11518581211566925
step: 5500, Loss: 0.11477348953485489
step: 5600, Loss: 0.1158212423324585
step: 5700, Loss: 0.11380958557128906
step: 5800, Loss: 0.11545291543006897
step: 5900, Loss: 0.11606870591640472
step: 6000, Loss: 0.11561659723520279
step: 6100, Loss: 0.11444342136383057
step: 6200, Loss: 0.11520623415708542
step: 6300, Loss: 0.11398809403181076
step: 6400, Loss: 0.11492504179477692
step: 6500, Loss: 0.11391156166791916
step: 6600, Loss: 2.946146011352539
step: 6700, Loss: 0.14493605494499207
step: 6800, Loss: 0.1362520158290863
step: 6900, Loss: 0.1360754817724228
step: 7000, Loss: 0.1284867525100708
step: 7100, Loss: 0.12467829883098602
step: 7200, Loss: 0.12177686393260956
step: 7300, Loss: 0.12706182897090912
step: 7400, Loss: 0.12281812727451324
step: 7500, Loss: 0.12151196599006653
step: 7600, Loss: 0.12298482656478882
step: 7700, Loss: 0.11615797877311707
step: 7800, Loss: 0.11849277466535568
step: 7900, Loss: 0.11722299456596375
step: 8000, Loss: 0.11682002991437912
step: 8100, Loss: 0.11738109588623047
step: 8200, Loss: 0.11462393403053284
step: 8300, Loss: 0.11672806739807129
step: 8400, Loss: 0.11647890508174896
step: 8500, Loss: 0.1155799850821495
step: 8600, Loss: 0.11708567291498184
step: 8700, Loss: 0.11536690592765808
step: 8800, Loss: 0.11597699671983719
step: 8900, Loss: 0.1142815351486206
step: 9000, Loss: 0.11588568985462189
step: 9100, Loss: 0.11432197690010071
step: 9200, Loss: 0.11566899716854095
step: 9300, Loss: 0.11440302431583405
step: 9400, Loss: 0.11388258635997772
step: 9500, Loss: 0.1137310341000557
step: 9600, Loss: 0.11459871381521225
step: 9700, Loss: 0.1144857332110405
step: 9800, Loss: 0.1140269935131073
step: 9900, Loss: 0.11496437340974808
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7368421052631579
recall: 0.9333333333333333
F_score: 0.8235294117647058
******fold 5******

Training... train_data length:281
step: 0, Loss: 0.3126896321773529
step: 100, Loss: 0.11746031045913696
step: 200, Loss: 0.11553451418876648
step: 300, Loss: 0.11818927526473999
step: 400, Loss: 0.11545845866203308
step: 500, Loss: 0.11756493896245956
step: 600, Loss: 0.1169339269399643
step: 700, Loss: 0.11642000079154968
step: 800, Loss: 0.1154906153678894
step: 900, Loss: 0.11417068541049957
step: 1000, Loss: 0.11369608342647552
step: 1100, Loss: 0.11382247507572174
step: 1200, Loss: 0.11394770443439484
step: 1300, Loss: 0.11329882591962814
step: 1400, Loss: 0.11472655087709427
step: 1500, Loss: 0.11498812586069107
step: 1600, Loss: 0.11503608524799347
step: 1700, Loss: 0.11298123002052307
step: 4500, Loss: 0.1204630434513092
step: 4600, Loss: 0.11686389148235321
step: 4700, Loss: 0.11821698397397995
step: 4800, Loss: 0.1156596913933754
step: 4900, Loss: 0.11505614966154099
step: 5000, Loss: 0.11768420040607452
step: 5100, Loss: 0.11683491617441177
step: 5200, Loss: 0.11845757812261581
step: 5300, Loss: 0.11561336368322372
step: 5400, Loss: 0.11334622651338577
step: 5500, Loss: 0.11345210671424866
step: 5600, Loss: 0.11461848020553589
step: 5700, Loss: 0.11516471952199936
step: 5800, Loss: 0.11566518247127533
step: 5900, Loss: 0.11546751111745834
step: 6000, Loss: 0.11426525563001633
step: 6100, Loss: 0.11629290133714676
step: 6200, Loss: 0.11452434211969376
step: 6300, Loss: 0.1163550391793251
step: 6400, Loss: 0.11430845409631729
step: 6500, Loss: 0.1133391484618187
step: 6600, Loss: 0.1139821782708168
step: 6700, Loss: 0.11479109525680542
step: 6800, Loss: 0.11458335816860199
step: 6900, Loss: 0.113645538687706
step: 7000, Loss: 0.11416689306497574
step: 7100, Loss: 0.11418251693248749
step: 7200, Loss: 0.11523603647947311
step: 7300, Loss: 0.11394233256578445
step: 7400, Loss: 0.11458975821733475
step: 7500, Loss: 0.11331011354923248
step: 7600, Loss: 0.11402253806591034
step: 7700, Loss: 0.11352100968360901
step: 7800, Loss: 0.11361637711524963
step: 7900, Loss: 0.11285494267940521
step: 8000, Loss: 0.11350322514772415
step: 8100, Loss: 0.11461950093507767
step: 8200, Loss: 0.11384734511375427
step: 8300, Loss: 0.11529651284217834
step: 8400, Loss: 0.11422339081764221
step: 8500, Loss: 0.11341798305511475
step: 8600, Loss: 0.1127154678106308
step: 8700, Loss: 0.11466533690690994
step: 8800, Loss: 0.115339495241642
step: 8900, Loss: 0.11446615308523178
step: 9000, Loss: 0.11384419351816177
step: 9100, Loss: 0.11282632499933243
step: 9200, Loss: 0.11505606770515442
step: 9300, Loss: 0.11471990495920181
step: 9400, Loss: 0.11381617188453674
step: 9500, Loss: 0.1126265823841095
step: 9600, Loss: 0.11526600271463394
step: 9700, Loss: 0.11446362733840942
step: 9800, Loss: 0.11450422555208206
step: 9900, Loss: 0.11452843993902206
training successfully ended.
validating...
validate data length:87
acc: 0.9375
precision: 0.8947368421052632
recall: 0.9714285714285714
F_score: 0.9315068493150684
******fold 8******

Training... train_data length:787
step: 0, Loss: 0.13028743863105774
step: 100, Loss: 0.11712464690208435
step: 200, Loss: 0.1145041286945343
step: 300, Loss: 0.11668717116117477
step: 400, Loss: 0.11520620435476303
step: 500, Loss: 0.11322605609893799
step: 600, Loss: 0.11474054306745529
step: 700, Loss: 0.1145842969417572
step: 800, Loss: 0.11396828293800354
step: 900, Loss: 0.11490330100059509
step: 1000, Loss: 0.11530272662639618
step: 1100, Loss: 0.11517518758773804
step: 1200, Loss: 0.11447125673294067
step: 1300, Loss: 0.11416539549827576
step: 1400, Loss: 0.11314623802900314
step: 1500, Loss: 0.11443527042865753
step: 1600, Loss: 0.11365847289562225
step: 1700, Loss: 0.11479846388101578
step: 1800, Loss: 0.11353450268507004
step: 1900, Loss: 0.11535826325416565
step: 2000, Loss: 0.1135716512799263
step: 2100, Loss: 0.11489500105381012
step: 2200, Loss: 0.11458342522382736
step: 2300, Loss: 0.11346649378538132
step: 2400, Loss: 0.11400414258241653
step: 2500, Loss: 0.11388052254915237
step: 2600, Loss: 0.1164831593632698
step: 2700, Loss: 0.11464805901050568
step: 2800, Loss: 0.11634866893291473
step: 2900, Loss: 0.11373309046030045
step: 3000, Loss: 0.11803089827299118
step: 3100, Loss: 0.11378470808267593
step: 3200, Loss: 0.11485555022954941
step: 3300, Loss: 0.1139516532421112
step: 3400, Loss: 9.424209594726562
step: 3500, Loss: 0.1616191416978836
step: 3600, Loss: 0.14211712777614594
step: 3700, Loss: 0.13336247205734253
step: 3800, Loss: 0.12944069504737854
step: 3900, Loss: 0.1252504289150238
step: 4000, Loss: 0.12525291740894318
step: 4100, Loss: 0.1245306134223938
step: 4200, Loss: 0.120713971555233
step: 4300, Loss: 0.1184505820274353
step: 4400, Loss: 0.12028703093528748
step: 4500, Loss: 0.1210416704416275
step: 4600, Loss: 0.11713801324367523
step: 4700, Loss: 0.11996836960315704
step: 4800, Loss: 0.12150747328996658
step: 4900, Loss: 0.11972135305404663
step: 5000, Loss: 0.11767073720693588
step: 5100, Loss: 0.11776944249868393
step: 5200, Loss: 0.11817754060029984
step: 5300, Loss: 0.11803747713565826
step: 5400, Loss: 0.115391306579113
step: 5500, Loss: 0.11529916524887085
step: 5600, Loss: 0.11668188869953156
step: 5700, Loss: 0.11489161849021912
step: 5800, Loss: 0.11803604662418365
step: 5900, Loss: 0.1183784008026123
step: 6000, Loss: 0.11706142127513885
step: 6100, Loss: 0.11490338295698166
step: 6200, Loss: 0.11623705923557281
step: 6300, Loss: 0.11601109057664871
step: 6400, Loss: 0.11659517884254456
step: 6500, Loss: 0.11374887079000473
step: 6600, Loss: 0.11581005156040192
step: 6700, Loss: 0.11395028978586197
step: 6800, Loss: 0.11426883935928345
step: 6900, Loss: 0.11475108563899994
step: 7000, Loss: 0.11474911123514175
step: 7100, Loss: 0.11342956125736237
step: 7200, Loss: 0.11407054960727692
step: 7300, Loss: 0.11618022620677948
step: 7400, Loss: 0.11594506353139877
step: 7500, Loss: 0.11589779704809189
step: 7600, Loss: 0.11355036497116089
step: 7700, Loss: 0.11347823590040207
step: 7800, Loss: 0.11336270719766617
step: 7900, Loss: 0.11410170048475266
step: 8000, Loss: 0.1136758103966713
step: 8100, Loss: 0.1144385039806366
step: 8200, Loss: 0.11273742467164993
step: 8300, Loss: 0.11365164071321487
step: 8400, Loss: 0.11367183923721313
step: 8500, Loss: 0.11387543380260468
step: 8600, Loss: 0.11369682103395462
step: 8700, Loss: 0.11291950941085815
step: 8800, Loss: 0.11421576142311096
step: 8900, Loss: 0.11489503085613251
step: 9000, Loss: 0.1129005178809166
step: 9100, Loss: 0.11352483183145523
step: 9200, Loss: 0.11420230567455292
step: 9300, Loss: 0.1133585199713707
step: 9400, Loss: 0.11565347015857697
step: 9500, Loss: 0.1136973425745964
step: 9600, Loss: 0.11500823497772217
step: 9700, Loss: 0.11394791305065155
step: 9800, Loss: 0.11430385708808899
step: 9900, Loss: 0.11317232996225357
training successfully ended.
validating...
validate data length:87
acc: 0.975
precision: 1.0
recall: 0.9523809523809523
F_score: 0.975609756097561
******fold 9******

Training... train_data length:787
step: 0, Loss: 0.1337454915046692
step: 100, Loss: 0.11793231964111328
step: 200, Loss: 0.11495668441057205
step: 300, Loss: 0.11539548635482788
step: 400, Loss: 0.11360792070627213
step: 500, Loss: 0.11321505904197693
step: 600, Loss: 0.11400309950113297
step: 700, Loss: 0.11316198855638504
step: 800, Loss: 0.1134597659111023
step: 900, Loss: 0.11488890647888184
step: 1000, Loss: 0.11398673057556152
step: 1100, Loss: 0.11483490467071533
step: 1200, Loss: 0.11502830684185028
step: 1300, Loss: 0.11321268230676651
step: 1400, Loss: 0.11516109108924866
step: 1500, Loss: 0.11405624449253082
step: 1600, Loss: 0.11366566270589828
step: 1700, Loss: 0.1134953647851944
step: 1800, Loss: 0.11370223015546799
step: 1900, Loss: 0.11471002548933029
step: 2000, Loss: 0.12040683627128601
step: 2100, Loss: 0.11406034976243973
step: 2200, Loss: 0.11447741091251373
step: 2300, Loss: 0.11378205567598343
step: 2400, Loss: 0.11445352435112
step: 2500, Loss: 0.1141125038266182
step: 2600, Loss: 0.12684132158756256
step: 2700, Loss: 0.9958354830741882
step: 2800, Loss: 0.1375197172164917
step: 2900, Loss: 0.13253526389598846
step: 3000, Loss: 0.13494566082954407
step: 3100, Loss: 0.13150712847709656
step: 3200, Loss: 0.12862075865268707
step: 3300, Loss: 0.12454589456319809
step: 3400, Loss: 0.12307499349117279
step: 3500, Loss: 0.12031745165586472
step: 3600, Loss: 0.12021055072546005
step: 3700, Loss: 0.11981046199798584
step: 3800, Loss: 0.11925478279590607
step: 3900, Loss: 0.12348008155822754
step: 4000, Loss: 0.1188351958990097
step: 4100, Loss: 0.12170799821615219
step: 4200, Loss: 0.11623048037290573
step: 4300, Loss: 0.11885643005371094
step: 4400, Loss: 0.11765003204345703
step: 4500, Loss: 0.1159873753786087
step: 4600, Loss: 0.11584237217903137
step: 4700, Loss: 0.11631493270397186
step: 4800, Loss: 0.115755595266819
step: 4900, Loss: 0.11670876294374466
step: 5000, Loss: 0.11560157686471939
step: 1800, Loss: 0.11454671621322632
step: 1900, Loss: 0.11488684266805649
step: 2000, Loss: 0.11469969153404236
step: 2100, Loss: 0.11415993422269821
step: 2200, Loss: 0.11432278901338577
step: 2300, Loss: 0.11871546506881714
step: 2400, Loss: 0.11573110520839691
step: 2500, Loss: 0.11446702480316162
step: 2600, Loss: 0.11788688600063324
step: 2700, Loss: 0.11441361904144287
step: 2800, Loss: 0.11305978894233704
step: 2900, Loss: 0.11544471979141235
step: 3000, Loss: 0.11880432069301605
step: 3100, Loss: 0.11652275919914246
step: 3200, Loss: 0.11454132199287415
step: 3300, Loss: 0.11407574266195297
step: 3400, Loss: 0.11440027505159378
step: 3500, Loss: 0.11466795951128006
step: 3600, Loss: 0.11408747732639313
step: 3700, Loss: 0.11577355116605759
step: 3800, Loss: 0.11405421048402786
step: 3900, Loss: 0.11441577970981598
step: 4000, Loss: 0.11444593966007233
step: 4100, Loss: 0.11486530303955078
step: 4200, Loss: 0.1163676455616951
step: 4300, Loss: 0.11683331429958344
step: 4400, Loss: 0.11593198776245117
step: 4500, Loss: 0.11596716940402985
step: 4600, Loss: 0.11459214240312576
step: 4700, Loss: 0.11427468806505203
step: 4800, Loss: 0.11598849296569824
step: 4900, Loss: 0.11390674859285355
step: 5000, Loss: 0.11506767570972443
step: 5100, Loss: 0.1178123950958252
step: 5200, Loss: 0.11404599249362946
step: 5300, Loss: 0.11543934047222137
step: 5400, Loss: 0.11409686505794525
step: 5500, Loss: 0.11373768001794815
step: 5600, Loss: 0.27622637152671814
step: 5700, Loss: 0.14296582341194153
step: 5800, Loss: 0.12892615795135498
step: 5900, Loss: 0.12887126207351685
step: 6000, Loss: 0.1254197359085083
step: 6100, Loss: 0.12163284420967102
step: 6200, Loss: 0.12107391655445099
step: 6300, Loss: 0.11937028169631958
step: 6400, Loss: 0.11797367036342621
step: 6500, Loss: 0.11674940586090088
step: 6600, Loss: 0.11675943434238434
step: 6700, Loss: 0.11874105036258698
step: 6800, Loss: 0.11578834056854248
step: 6900, Loss: 0.11711668968200684
step: 7000, Loss: 0.11704966425895691
step: 7100, Loss: 0.11526096612215042
step: 7200, Loss: 0.11587284505367279
step: 7300, Loss: 0.11932821571826935
step: 7400, Loss: 0.11466988176107407
step: 7500, Loss: 0.11694789677858353
step: 7600, Loss: 0.11905691027641296
step: 7700, Loss: 0.11775221675634384
step: 7800, Loss: 0.11671135574579239
step: 7900, Loss: 0.11571092903614044
step: 8000, Loss: 0.11702455580234528
step: 8100, Loss: 0.11565738916397095
step: 8200, Loss: 0.11509905755519867
step: 8300, Loss: 0.11582738906145096
step: 8400, Loss: 0.11410112679004669
step: 8500, Loss: 0.11491486430168152
step: 8600, Loss: 0.1157279759645462
step: 8700, Loss: 0.11355134844779968
step: 8800, Loss: 0.11694508790969849
step: 8900, Loss: 0.11415637284517288
step: 9000, Loss: 0.11682897061109543
step: 9100, Loss: 0.11458314210176468
step: 9200, Loss: 0.11647610366344452
step: 9300, Loss: 0.11434884369373322
step: 9400, Loss: 0.11532118916511536
step: 9500, Loss: 0.11580425500869751
step: 9600, Loss: 0.11511491984128952
step: 9700, Loss: 0.1139255091547966
step: 9800, Loss: 0.11536888778209686
step: 9900, Loss: 0.11789301037788391
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8095238095238095
recall: 0.9444444444444444
F_score: 0.8717948717948718
******fold 6******

Training... train_data length:281
step: 0, Loss: 0.315868079662323
step: 100, Loss: 0.12018510699272156
step: 200, Loss: 0.11743006110191345
step: 300, Loss: 0.11536543816328049
step: 400, Loss: 0.11515945196151733
step: 500, Loss: 0.11447001993656158
step: 600, Loss: 0.1173521876335144
step: 700, Loss: 0.11497832834720612
step: 800, Loss: 0.11575879156589508
step: 900, Loss: 0.11474268138408661
step: 1000, Loss: 0.11422523856163025
step: 1100, Loss: 0.11429473757743835
step: 1200, Loss: 0.11382679641246796
step: 1300, Loss: 0.11453143507242203
step: 1400, Loss: 0.11331894248723984
step: 1500, Loss: 0.11444555968046188
step: 1600, Loss: 0.11295897513628006
step: 1700, Loss: 0.11453495174646378
step: 1800, Loss: 0.11439043283462524
step: 1900, Loss: 0.11361231654882431
step: 2000, Loss: 0.11377072334289551
step: 2100, Loss: 0.11435393989086151
step: 2200, Loss: 0.11432857066392899
step: 2300, Loss: 0.1156228706240654
step: 2400, Loss: 0.1134563460946083
step: 2500, Loss: 0.11639495939016342
step: 2600, Loss: 0.1148080825805664
step: 2700, Loss: 0.11473244428634644
step: 2800, Loss: 0.11350031942129135
step: 2900, Loss: 0.11387692391872406
step: 3000, Loss: 0.11374708265066147
step: 3100, Loss: 0.11478455364704132
step: 3200, Loss: 0.11365704238414764
step: 3300, Loss: 0.11737822741270065
step: 3400, Loss: 0.11388260871171951
step: 3500, Loss: 0.11309003084897995
step: 3600, Loss: 0.11494044959545135
step: 3700, Loss: 0.11508283019065857
step: 3800, Loss: 0.1150020956993103
step: 3900, Loss: 0.11546563357114792
step: 4000, Loss: 0.11521133780479431
step: 4100, Loss: 0.11421486735343933
step: 4200, Loss: 0.11319637298583984
step: 4300, Loss: 0.11379168927669525
step: 4400, Loss: 0.11562852561473846
step: 4500, Loss: 0.1146925836801529
step: 4600, Loss: 0.11378983408212662
step: 4700, Loss: 0.11525487899780273
step: 4800, Loss: 0.11402948200702667
step: 4900, Loss: 0.11520206928253174
step: 5000, Loss: 0.11578555405139923
step: 5100, Loss: 0.1145310252904892
step: 5200, Loss: 0.11521164327859879
step: 5300, Loss: 0.11443518102169037
step: 5400, Loss: 0.11546009033918381
step: 5500, Loss: 0.11373819410800934
step: 5600, Loss: 0.11357817053794861
step: 5700, Loss: 0.11552093923091888
step: 5800, Loss: 0.11368896812200546
step: 5900, Loss: 0.41679900884628296
step: 6000, Loss: 0.1344693899154663
step: 6100, Loss: 0.12450368702411652
step: 6200, Loss: 0.1252264529466629
step: 6300, Loss: 0.11946794390678406
step: 6400, Loss: 0.1224328950047493
step: 6500, Loss: 0.12074922770261765
step: 6600, Loss: 0.11727810651063919
step: 6700, Loss: 0.12034063786268234
step: 6800, Loss: 0.11559955030679703
step: 6900, Loss: 0.11646141111850739
step: 7000, Loss: 0.11750449240207672
step: 7100, Loss: 0.11653618514537811
step: 7200, Loss: 0.11469284445047379
step: 7300, Loss: 0.11694344878196716
step: 7400, Loss: 0.12442358583211899
step: 7500, Loss: 0.12381063401699066
step: 7600, Loss: 0.11373637616634369
step: 7700, Loss: 0.11517375707626343
step: 7800, Loss: 0.11471684277057648
step: 7900, Loss: 0.1146315187215805
step: 8000, Loss: 0.11412999033927917
step: 8100, Loss: 0.11406467109918594
step: 8200, Loss: 0.11384347081184387
step: 8300, Loss: 0.11408305168151855
step: 8400, Loss: 0.11568871140480042
step: 8500, Loss: 0.11535640060901642
step: 8600, Loss: 0.11518584936857224
step: 8700, Loss: 0.11442077159881592
step: 8800, Loss: 0.11419439315795898
step: 8900, Loss: 0.114013671875
step: 9000, Loss: 0.11510583758354187
step: 9100, Loss: 0.1157335489988327
step: 9200, Loss: 0.1164512038230896
step: 9300, Loss: 0.1148027703166008
step: 9400, Loss: 0.11372755467891693
step: 9500, Loss: 0.11672596633434296
step: 9600, Loss: 0.11573243141174316
step: 9700, Loss: 0.11406990885734558
step: 9800, Loss: 0.11415213346481323
step: 9900, Loss: 0.11308363825082779
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.8333333333333334
recall: 0.8333333333333334
F_score: 0.8333333333333334
******fold 7******

Training... train_data length:281
step: 0, Loss: 0.3339160680770874
step: 100, Loss: 0.11871849000453949
step: 200, Loss: 0.11536233127117157
step: 300, Loss: 0.11405543982982635
step: 400, Loss: 0.11473838239908218
step: 500, Loss: 0.11421973258256912
step: 600, Loss: 0.11430540680885315
step: 700, Loss: 0.11696392297744751
step: 800, Loss: 0.11401114612817764
step: 900, Loss: 0.11353465914726257
step: 1000, Loss: 0.11303768306970596
step: 1100, Loss: 0.11382128298282623
step: 1200, Loss: 0.1150788888335228
step: 1300, Loss: 0.1143357902765274
step: 1400, Loss: 0.11310548335313797
step: 1500, Loss: 0.11402826756238937
step: 1600, Loss: 0.11380220949649811
step: 1700, Loss: 0.11286744475364685
step: 1800, Loss: 0.1141369417309761
step: 1900, Loss: 0.11412058770656586
step: 2000, Loss: 0.11634807288646698
step: 2100, Loss: 0.11329718679189682
step: 2200, Loss: 0.11496579647064209
step: 5100, Loss: 0.11482031643390656
step: 5200, Loss: 0.11594411730766296
step: 5300, Loss: 0.11431902647018433
step: 5400, Loss: 0.11715291440486908
step: 5500, Loss: 0.1164056807756424
step: 5600, Loss: 0.11900810152292252
step: 5700, Loss: 0.11441319435834885
step: 5800, Loss: 0.11482232064008713
step: 5900, Loss: 0.1140025407075882
step: 6000, Loss: 0.11485917866230011
step: 6100, Loss: 0.11567606776952744
step: 6200, Loss: 0.11502564698457718
step: 6300, Loss: 0.11543141305446625
step: 6400, Loss: 0.11456060409545898
step: 6500, Loss: 0.1145385354757309
step: 6600, Loss: 0.11387865245342255
step: 6700, Loss: 0.1145976334810257
step: 6800, Loss: 0.11492768675088882
step: 6900, Loss: 0.11361231654882431
step: 7000, Loss: 0.11350348591804504
step: 7100, Loss: 0.11343483626842499
step: 7200, Loss: 0.11333262920379639
step: 7300, Loss: 0.1149400845170021
step: 7400, Loss: 0.11445878446102142
step: 7500, Loss: 0.11400125920772552
step: 7600, Loss: 0.11606363952159882
step: 7700, Loss: 0.11305782943964005
step: 7800, Loss: 0.1140688806772232
step: 7900, Loss: 0.11361141502857208
step: 8000, Loss: 0.11413317918777466
step: 8100, Loss: 0.11442922800779343
step: 8200, Loss: 0.11397694051265717
step: 8300, Loss: 0.11476367712020874
step: 8400, Loss: 0.11435076594352722
step: 8500, Loss: 0.11318698525428772
step: 8600, Loss: 0.1140550747513771
step: 8700, Loss: 0.11397784948348999
step: 8800, Loss: 0.11364984512329102
step: 8900, Loss: 0.11305428296327591
step: 9000, Loss: 0.11486645042896271
step: 9100, Loss: 0.11460841447114944
step: 9200, Loss: 0.11306308209896088
step: 9300, Loss: 0.11498700827360153
step: 9400, Loss: 0.1134788990020752
step: 9500, Loss: 0.11435903608798981
step: 9600, Loss: 0.1178235411643982
step: 9700, Loss: 0.11389563232660294
step: 9800, Loss: 0.11483834683895111
step: 9900, Loss: 0.11483211070299149
training successfully ended.
validating...
validate data length:87
acc: 0.8625
precision: 0.9024390243902439
recall: 0.8409090909090909
F_score: 0.8705882352941177
******fold 10******

Training... train_data length:787
step: 0, Loss: 0.1256638914346695
step: 100, Loss: 0.11869874596595764
step: 200, Loss: 0.11586249619722366
step: 300, Loss: 0.1136084496974945
step: 400, Loss: 0.11556484550237656
step: 500, Loss: 0.11270825564861298
step: 600, Loss: 0.11400917917490005
step: 700, Loss: 0.11414244771003723
step: 800, Loss: 0.11450394988059998
step: 900, Loss: 0.11329987645149231
step: 1000, Loss: 0.11328475177288055
step: 1100, Loss: 0.1140044704079628
step: 1200, Loss: 0.11265219748020172
step: 1300, Loss: 0.11304755508899689
step: 1400, Loss: 0.11402768641710281
step: 1500, Loss: 0.11535825580358505
step: 1600, Loss: 0.11528800427913666
step: 1700, Loss: 0.11475111544132233
step: 1800, Loss: 0.11551906913518906
step: 1900, Loss: 0.112993985414505
step: 2000, Loss: 0.11519885808229446
step: 2100, Loss: 0.1150568425655365
step: 2200, Loss: 0.11438868939876556
step: 2300, Loss: 0.11409103870391846
step: 2400, Loss: 0.11314475536346436
step: 2500, Loss: 0.11497865617275238
step: 2600, Loss: 0.29882124066352844
step: 2700, Loss: 0.14073660969734192
step: 2800, Loss: 0.1309918612241745
step: 2900, Loss: 0.1417989432811737
step: 3000, Loss: 0.1231585219502449
step: 3100, Loss: 0.12253446877002716
step: 3200, Loss: 0.125440776348114
step: 3300, Loss: 0.12477224320173264
step: 3400, Loss: 0.12019229680299759
step: 3500, Loss: 0.12370280921459198
step: 3600, Loss: 0.12266278266906738
step: 3700, Loss: 0.12012873589992523
step: 3800, Loss: 0.11773955076932907
step: 3900, Loss: 0.12057236582040787
step: 4000, Loss: 0.12097230553627014
step: 4100, Loss: 0.11891874670982361
step: 4200, Loss: 0.11626581847667694
step: 4300, Loss: 0.11777220666408539
step: 4400, Loss: 0.11718246340751648
step: 4500, Loss: 0.1174512729048729
step: 4600, Loss: 0.11690174043178558
step: 4700, Loss: 0.12079934775829315
step: 4800, Loss: 0.12022827565670013
step: 4900, Loss: 0.11452029645442963
step: 5000, Loss: 0.11428997665643692
step: 5100, Loss: 0.11665759235620499
step: 5200, Loss: 0.11595460772514343
step: 5300, Loss: 0.11469084769487381
step: 5400, Loss: 0.11545484513044357
step: 5500, Loss: 0.11446680128574371
step: 5600, Loss: 0.1150689646601677
step: 5700, Loss: 0.11402760446071625
step: 5800, Loss: 0.1163344532251358
step: 5900, Loss: 0.11509730666875839
step: 6000, Loss: 0.11482872068881989
step: 6100, Loss: 0.11524059623479843
step: 6200, Loss: 0.11383131146430969
step: 6300, Loss: 0.1138087660074234
step: 6400, Loss: 0.11395709216594696
step: 6500, Loss: 0.11316590011119843
step: 6600, Loss: 0.1132887452840805
step: 6700, Loss: 0.11524729430675507
step: 6800, Loss: 0.11566931009292603
step: 6900, Loss: 0.11365096271038055
step: 7000, Loss: 0.11362006515264511
step: 7100, Loss: 0.11403323709964752
step: 7200, Loss: 0.11616584658622742
step: 7300, Loss: 0.11414314061403275
step: 7400, Loss: 0.11448343843221664
step: 7500, Loss: 0.11470511555671692
step: 7600, Loss: 0.11415959894657135
step: 7700, Loss: 0.1145169585943222
step: 7800, Loss: 0.11371393501758575
step: 7900, Loss: 0.11431701481342316
step: 8000, Loss: 0.1136823520064354
step: 8100, Loss: 0.11322811990976334
step: 8200, Loss: 0.11382202804088593
step: 8300, Loss: 0.11380422860383987
step: 8400, Loss: 0.11400298774242401
step: 8500, Loss: 0.11376938223838806
step: 8600, Loss: 0.11395854502916336
step: 8700, Loss: 0.11474096775054932
step: 8800, Loss: 0.11397188156843185
step: 8900, Loss: 0.11386843025684357
step: 9000, Loss: 0.11265639215707779
step: 9100, Loss: 0.1153055876493454
step: 9200, Loss: 0.11489184200763702
step: 9300, Loss: 0.1146383062005043
step: 9400, Loss: 0.11423380672931671
step: 9500, Loss: 0.11525657773017883
step: 9600, Loss: 0.11407484114170074
step: 9700, Loss: 0.11565318703651428
step: 9800, Loss: 0.11415340006351471
step: 9900, Loss: 0.11343514919281006
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 19 Avgacc: 0.9512500000000002 Avgfscore: 0.952595602558149 
 Max acc:1.0, Max f score:1.0
******** mix subject_20 ********

[361, 399]
******fold 1******

Training... train_data length:684
step: 0, Loss: 50.51557540893555
step: 100, Loss: 9.171950340270996
step: 200, Loss: 2.4953596591949463
step: 300, Loss: 1.6018779277801514
step: 400, Loss: 1.5441776514053345
step: 500, Loss: 0.8469058275222778
step: 600, Loss: 0.24310776591300964
step: 700, Loss: 0.17508336901664734
step: 800, Loss: 0.15683117508888245
step: 900, Loss: 0.14151835441589355
step: 1000, Loss: 0.1540375053882599
step: 1100, Loss: 0.16766071319580078
step: 1200, Loss: 0.15084728598594666
step: 1300, Loss: 0.162339985370636
step: 1400, Loss: 0.13130907714366913
step: 1500, Loss: 0.23356083035469055
step: 1600, Loss: 0.14217963814735413
step: 1700, Loss: 0.12814253568649292
step: 1800, Loss: 0.14458820223808289
step: 1900, Loss: 0.13633032143115997
step: 2000, Loss: 0.1234966367483139
step: 2100, Loss: 0.1334136724472046
step: 2200, Loss: 0.13457688689231873
step: 2300, Loss: 0.12986204028129578
step: 2400, Loss: 0.12973397970199585
step: 2500, Loss: 0.12326882034540176
step: 2600, Loss: 0.12784922122955322
step: 2700, Loss: 0.12098044157028198
step: 2800, Loss: 0.12364122271537781
step: 2900, Loss: 0.1239069476723671
step: 3000, Loss: 0.12720882892608643
step: 3100, Loss: 0.12297821789979935
step: 3200, Loss: 0.11692264676094055
step: 3300, Loss: 0.11766481399536133
step: 3400, Loss: 0.2041618525981903
step: 3500, Loss: 0.12001602351665497
step: 3600, Loss: 0.11940677464008331
step: 3700, Loss: 0.11918378621339798
step: 3800, Loss: 0.12164083123207092
step: 3900, Loss: 0.11537644267082214
step: 4000, Loss: 0.11701156944036484
step: 4100, Loss: 0.1157328262925148
step: 4200, Loss: 0.11680401861667633
step: 4300, Loss: 0.1159180998802185
step: 4400, Loss: 0.11665640771389008
step: 4500, Loss: 0.11530362069606781
step: 4600, Loss: 0.11721659451723099
step: 4700, Loss: 0.11543136835098267
step: 4800, Loss: 0.11840946972370148
step: 4900, Loss: 0.11853859573602676
step: 5000, Loss: 0.11640958487987518
step: 5100, Loss: 4.822300434112549
step: 5200, Loss: 3.3180959224700928
step: 5300, Loss: 0.2597050666809082
step: 2300, Loss: 0.11428538709878922
step: 2400, Loss: 0.11361654102802277
step: 2500, Loss: 0.11527301371097565
step: 2600, Loss: 0.11468835175037384
step: 2700, Loss: 0.11714957654476166
step: 2800, Loss: 0.11420358717441559
step: 2900, Loss: 0.11584040522575378
step: 3000, Loss: 0.11381161212921143
step: 3100, Loss: 0.11357031762599945
step: 3200, Loss: 0.11455043405294418
step: 3300, Loss: 0.11530645191669464
step: 3400, Loss: 0.11470796167850494
step: 3500, Loss: 0.11352717131376266
step: 3600, Loss: 0.11488508433103561
step: 3700, Loss: 0.11385181546211243
step: 3800, Loss: 0.11440393328666687
step: 3900, Loss: 0.11616392433643341
step: 4000, Loss: 0.11794812977313995
step: 4100, Loss: 0.11391659080982208
step: 4200, Loss: 0.11571267247200012
step: 4300, Loss: 0.11933021247386932
step: 4400, Loss: 0.11542125791311264
step: 4500, Loss: 0.11478131264448166
step: 4600, Loss: 0.11558233201503754
step: 4700, Loss: 0.1137491911649704
step: 4800, Loss: 0.11320465803146362
step: 4900, Loss: 0.11473037302494049
step: 5000, Loss: 0.11566562950611115
step: 5100, Loss: 0.11569085717201233
step: 5200, Loss: 0.11398618668317795
step: 5300, Loss: 0.11525855958461761
step: 5400, Loss: 0.11466288566589355
step: 5500, Loss: 0.21027067303657532
step: 5600, Loss: 0.48285359144210815
step: 5700, Loss: 0.14271503686904907
step: 5800, Loss: 0.13576611876487732
step: 5900, Loss: 0.12814582884311676
step: 6000, Loss: 0.12630048394203186
step: 6100, Loss: 0.12625998258590698
step: 6200, Loss: 0.12506482005119324
step: 6300, Loss: 0.12240822613239288
step: 6400, Loss: 0.12413045018911362
step: 6500, Loss: 0.11886461824178696
step: 6600, Loss: 0.12139928340911865
step: 6700, Loss: 0.11849883198738098
step: 6800, Loss: 0.1210298091173172
step: 6900, Loss: 0.1172754168510437
step: 7000, Loss: 0.1183670237660408
step: 7100, Loss: 0.11749613285064697
step: 7200, Loss: 0.11815173923969269
step: 7300, Loss: 0.11737757921218872
step: 7400, Loss: 0.11652712523937225
step: 7500, Loss: 0.1156257838010788
step: 7600, Loss: 0.11814456433057785
step: 7700, Loss: 0.11588544398546219
step: 7800, Loss: 0.11702197045087814
step: 7900, Loss: 0.11497443914413452
step: 8000, Loss: 0.1175292581319809
step: 8100, Loss: 0.11516635119915009
step: 8200, Loss: 0.11706926673650742
step: 8300, Loss: 0.11472343653440475
step: 8400, Loss: 0.116334468126297
step: 8500, Loss: 0.11567988246679306
step: 8600, Loss: 0.116535484790802
step: 8700, Loss: 0.11404217779636383
step: 8800, Loss: 0.11609482020139694
step: 8900, Loss: 0.11656919121742249
step: 9000, Loss: 0.11434100568294525
step: 9100, Loss: 0.11344613134860992
step: 9200, Loss: 0.11623813211917877
step: 9300, Loss: 0.11672256141901016
step: 9400, Loss: 0.11472159624099731
step: 9500, Loss: 0.11517423391342163
step: 9600, Loss: 0.11409550160169601
step: 9700, Loss: 0.115704245865345
step: 9800, Loss: 0.11397082358598709
step: 9900, Loss: 0.11527206003665924
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8
recall: 0.9411764705882353
F_score: 0.8648648648648648
******fold 8******

Training... train_data length:281
step: 0, Loss: 0.33448904752731323
step: 100, Loss: 0.1158503070473671
step: 200, Loss: 0.11714645475149155
step: 300, Loss: 0.11509066820144653
step: 400, Loss: 0.11466123163700104
step: 500, Loss: 0.11582614481449127
step: 600, Loss: 0.11291776597499847
step: 700, Loss: 0.11383423954248428
step: 800, Loss: 0.11513995379209518
step: 900, Loss: 0.11393321305513382
step: 1000, Loss: 0.1155557781457901
step: 1100, Loss: 0.11579062044620514
step: 1200, Loss: 0.11322824656963348
step: 1300, Loss: 0.11733220517635345
step: 1400, Loss: 0.11370985209941864
step: 1500, Loss: 0.11417345702648163
step: 1600, Loss: 0.11628509312868118
step: 1700, Loss: 0.1175568550825119
step: 1800, Loss: 0.1160261258482933
step: 1900, Loss: 0.11424586176872253
step: 2000, Loss: 0.11480140686035156
step: 2100, Loss: 0.11622174829244614
step: 2200, Loss: 0.11450428515672684
step: 2300, Loss: 0.11964762210845947
step: 2400, Loss: 0.11506738513708115
step: 2500, Loss: 0.11498773097991943
step: 2600, Loss: 0.11476171761751175
step: 2700, Loss: 0.11583410203456879
step: 2800, Loss: 0.11531171202659607
step: 2900, Loss: 0.11570963263511658
step: 3000, Loss: 0.11391496658325195
step: 3100, Loss: 0.11569476127624512
step: 3200, Loss: 0.11423319578170776
step: 3300, Loss: 0.11396655440330505
step: 3400, Loss: 0.11406019330024719
step: 3500, Loss: 0.11437106877565384
step: 3600, Loss: 0.11348845809698105
step: 3700, Loss: 0.11527673900127411
step: 3800, Loss: 0.114498570561409
step: 3900, Loss: 0.11805116385221481
step: 4000, Loss: 0.11505024135112762
step: 4100, Loss: 0.11554411798715591
step: 4200, Loss: 0.11332768201828003
step: 4300, Loss: 0.11559361219406128
step: 4400, Loss: 0.11475224047899246
step: 4500, Loss: 0.11440403759479523
step: 4600, Loss: 0.11463908851146698
step: 4700, Loss: 0.1143568903207779
step: 4800, Loss: 0.11326530575752258
step: 4900, Loss: 0.11869688332080841
step: 5000, Loss: 0.11461175978183746
step: 5100, Loss: 0.11730242520570755
step: 5200, Loss: 0.11479350924491882
step: 5300, Loss: 0.11406392604112625
step: 5400, Loss: 0.11350472271442413
step: 5500, Loss: 0.11533477157354355
step: 5600, Loss: 0.11284969747066498
step: 5700, Loss: 0.1152118593454361
step: 5800, Loss: 0.11429384350776672
step: 5900, Loss: 0.11493989825248718
step: 6000, Loss: 0.23880478739738464
step: 6100, Loss: 0.15211144089698792
step: 6200, Loss: 0.13149422407150269
step: 6300, Loss: 0.12601207196712494
step: 6400, Loss: 0.12207835167646408
step: 6500, Loss: 0.12200715392827988
step: 6600, Loss: 0.11982743442058563
step: 6700, Loss: 0.11853209882974625
step: 6800, Loss: 0.116834357380867
step: 6900, Loss: 0.12043259292840958
step: 7000, Loss: 0.1154148057103157
step: 7100, Loss: 0.11748712509870529
step: 7200, Loss: 0.11784069240093231
step: 7300, Loss: 0.11957567185163498
step: 7400, Loss: 0.1192416325211525
step: 7500, Loss: 0.11643704771995544
step: 7600, Loss: 0.11415857821702957
step: 7700, Loss: 0.11925149708986282
step: 7800, Loss: 0.11473147571086884
step: 7900, Loss: 0.11769187450408936
step: 8000, Loss: 0.11620014905929565
step: 8100, Loss: 0.12165024131536484
step: 8200, Loss: 0.11499127000570297
step: 8300, Loss: 0.11631226539611816
step: 8400, Loss: 0.11527059972286224
step: 8500, Loss: 0.11502788960933685
step: 8600, Loss: 0.11370030790567398
step: 8700, Loss: 0.11696130782365799
step: 8800, Loss: 0.114200159907341
step: 8900, Loss: 0.11406257003545761
step: 9000, Loss: 0.11495263874530792
step: 9100, Loss: 0.11525379866361618
step: 9200, Loss: 0.11560624092817307
step: 9300, Loss: 0.1145087480545044
step: 9400, Loss: 0.11359585076570511
step: 9500, Loss: 0.11583195626735687
step: 9600, Loss: 0.11320354044437408
step: 9700, Loss: 0.11420252919197083
step: 9800, Loss: 0.11539844423532486
step: 9900, Loss: 0.11439265310764313
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8125
recall: 0.8666666666666667
F_score: 0.8387096774193549
******fold 9******

Training... train_data length:281
step: 0, Loss: 0.3055509030818939
step: 100, Loss: 0.11936940252780914
step: 200, Loss: 0.11557438224554062
step: 300, Loss: 0.11577016115188599
step: 400, Loss: 0.11899013817310333
step: 500, Loss: 0.11473150551319122
step: 600, Loss: 0.11513662338256836
step: 700, Loss: 0.1151367723941803
step: 800, Loss: 0.11489509791135788
step: 900, Loss: 0.11531016230583191
step: 1000, Loss: 0.11534570157527924
step: 1100, Loss: 0.11618757247924805
step: 1200, Loss: 0.11432185024023056
step: 1300, Loss: 0.1160682886838913
step: 1400, Loss: 0.1167578399181366
step: 1500, Loss: 0.11443853378295898
step: 1600, Loss: 0.11543361097574234
step: 1700, Loss: 0.11433050781488419
step: 1800, Loss: 0.114122673869133
step: 1900, Loss: 0.11459249258041382
step: 2000, Loss: 0.11549025028944016
step: 2100, Loss: 0.11816354095935822
step: 2200, Loss: 0.11408121138811111
step: 2300, Loss: 0.11450724303722382
step: 2400, Loss: 0.11399208009243011
step: 2500, Loss: 0.120480477809906
step: 2600, Loss: 0.114630326628685
step: 2700, Loss: 0.1146952360868454
step: 5400, Loss: 0.14919942617416382
step: 5500, Loss: 0.15617874264717102
step: 5600, Loss: 0.15665140748023987
step: 5700, Loss: 0.14230062067508698
step: 5800, Loss: 0.12541215121746063
step: 5900, Loss: 0.12803439795970917
step: 6000, Loss: 0.14136087894439697
step: 6100, Loss: 0.13069917261600494
step: 6200, Loss: 0.13272064924240112
step: 6300, Loss: 0.12352141737937927
step: 6400, Loss: 0.12598691880702972
step: 6500, Loss: 0.12590543925762177
step: 6600, Loss: 0.12367574870586395
step: 6700, Loss: 0.12321038544178009
step: 6800, Loss: 0.12704192101955414
step: 6900, Loss: 0.11884383112192154
step: 7000, Loss: 0.11712570488452911
step: 7100, Loss: 0.12276516854763031
step: 7200, Loss: 0.20763885974884033
step: 7300, Loss: 0.11918189376592636
step: 7400, Loss: 0.11895731091499329
step: 7500, Loss: 0.12826529145240784
step: 7600, Loss: 0.12262427061796188
step: 7700, Loss: 0.11491943150758743
step: 7800, Loss: 0.11807502806186676
step: 7900, Loss: 0.1195501834154129
step: 8000, Loss: 0.11911439150571823
step: 8100, Loss: 0.11922599375247955
step: 8200, Loss: 0.11542534083127975
step: 8300, Loss: 0.11477307230234146
step: 8400, Loss: 0.11684241890907288
step: 8500, Loss: 0.11856037378311157
step: 8600, Loss: 0.11682555824518204
step: 8700, Loss: 0.11561862379312515
step: 8800, Loss: 0.11629420518875122
step: 8900, Loss: 0.11571431159973145
step: 9000, Loss: 0.1140204444527626
step: 9100, Loss: 0.19529259204864502
step: 9200, Loss: 0.11464755982160568
step: 9300, Loss: 0.11618298292160034
step: 9400, Loss: 0.11634629964828491
step: 9500, Loss: 0.11447067558765411
step: 9600, Loss: 0.11478874087333679
step: 9700, Loss: 0.1155012920498848
step: 9800, Loss: 0.11380743980407715
step: 9900, Loss: 0.11412228643894196
training successfully ended.
validating...
validate data length:76
acc: 0.6805555555555556
precision: 0.5777777777777777
recall: 0.8666666666666667
F_score: 0.6933333333333332
******fold 2******

Training... train_data length:684
step: 0, Loss: 4.509044170379639
step: 100, Loss: 0.16178584098815918
step: 200, Loss: 0.14217612147331238
step: 300, Loss: 0.13335540890693665
step: 400, Loss: 0.12819348275661469
step: 500, Loss: 0.12746986746788025
step: 600, Loss: 0.12462002038955688
step: 700, Loss: 0.11805903166532516
step: 800, Loss: 0.11707580834627151
step: 900, Loss: 0.12065176665782928
step: 1000, Loss: 0.1165594607591629
step: 1100, Loss: 0.11971959471702576
step: 1200, Loss: 0.1153302788734436
step: 1300, Loss: 0.11695984750986099
step: 1400, Loss: 0.12063319981098175
step: 1500, Loss: 0.19573044776916504
step: 1600, Loss: 0.11595840752124786
step: 1700, Loss: 0.11752036213874817
step: 1800, Loss: 0.11485659331083298
step: 1900, Loss: 0.11702685058116913
step: 2000, Loss: 0.1169714629650116
step: 2100, Loss: 0.11554932594299316
step: 2200, Loss: 0.11616333574056625
step: 2300, Loss: 0.11479648947715759
step: 2400, Loss: 0.11363141983747482
step: 2500, Loss: 0.11673292517662048
step: 2600, Loss: 0.11517371982336044
step: 2700, Loss: 0.11364368349313736
step: 2800, Loss: 0.11494913697242737
step: 2900, Loss: 0.11778050661087036
step: 3000, Loss: 0.11943282932043076
step: 3100, Loss: 7.277800559997559
step: 3200, Loss: 0.26331138610839844
step: 3300, Loss: 1.0358072519302368
step: 3400, Loss: 0.2592369318008423
step: 3500, Loss: 0.15215231478214264
step: 3600, Loss: 0.1402415633201599
step: 3700, Loss: 0.12985245883464813
step: 3800, Loss: 0.1285376250743866
step: 3900, Loss: 0.12401841580867767
step: 4000, Loss: 0.13565897941589355
step: 4100, Loss: 0.13412614166736603
step: 4200, Loss: 0.12416209280490875
step: 4300, Loss: 0.1265868842601776
step: 4400, Loss: 0.12344729900360107
step: 4500, Loss: 0.12113434076309204
step: 4600, Loss: 0.12119539082050323
step: 4700, Loss: 0.1245066449046135
step: 4800, Loss: 0.12579505145549774
step: 4900, Loss: 0.11867295950651169
step: 5000, Loss: 0.11542071402072906
step: 5100, Loss: 0.11902916431427002
step: 5200, Loss: 0.11787610501050949
step: 5300, Loss: 0.20262593030929565
step: 5400, Loss: 0.12169867753982544
step: 5500, Loss: 0.11703389137983322
step: 5600, Loss: 0.11568532139062881
step: 5700, Loss: 0.11647360026836395
step: 5800, Loss: 0.11587218195199966
step: 5900, Loss: 0.12015703320503235
step: 6000, Loss: 0.11822506785392761
step: 6100, Loss: 0.11759248375892639
step: 6200, Loss: 0.11664961278438568
step: 6300, Loss: 0.11598923802375793
step: 6400, Loss: 0.11496232450008392
step: 6500, Loss: 0.11570513993501663
step: 6600, Loss: 0.11696607619524002
step: 6700, Loss: 0.11541315913200378
step: 6800, Loss: 0.11419027298688889
step: 6900, Loss: 0.11316891759634018
step: 7000, Loss: 0.11374372243881226
step: 7100, Loss: 0.11445958912372589
step: 7200, Loss: 0.19563645124435425
step: 7300, Loss: 0.11480018496513367
step: 7400, Loss: 0.11670217663049698
step: 7500, Loss: 0.11321230232715607
step: 7600, Loss: 0.11385485529899597
step: 7700, Loss: 0.11546248197555542
step: 7800, Loss: 0.11514613032341003
step: 7900, Loss: 0.11428333818912506
step: 8000, Loss: 0.11404751241207123
step: 8100, Loss: 0.11514203250408173
step: 8200, Loss: 0.11402633041143417
step: 8300, Loss: 0.11410510540008545
step: 8400, Loss: 0.11506227403879166
step: 8500, Loss: 0.11355606466531754
step: 8600, Loss: 0.11712739616632462
step: 8700, Loss: 0.11715150624513626
step: 8800, Loss: 0.11334787309169769
step: 8900, Loss: 0.12156516313552856
step: 9000, Loss: 0.1172521710395813
step: 9100, Loss: 0.20087936520576477
step: 9200, Loss: 0.11464668065309525
step: 9300, Loss: 0.11588874459266663
step: 9400, Loss: 0.1146479994058609
step: 9500, Loss: 0.12140300124883652
step: 9600, Loss: 0.12106584757566452
step: 9700, Loss: 0.11564884334802628
step: 9800, Loss: 0.1194167286157608
step: 9900, Loss: 0.11824214458465576
training successfully ended.
validating...
validate data length:76
acc: 0.8472222222222222
precision: 0.7857142857142857
recall: 0.9428571428571428
F_score: 0.8571428571428571
******fold 3******

Training... train_data length:684
step: 0, Loss: 1.0548794269561768
step: 100, Loss: 0.1292656660079956
step: 200, Loss: 0.1215474009513855
step: 300, Loss: 0.12278196215629578
step: 400, Loss: 0.12226653099060059
step: 500, Loss: 0.1176224946975708
step: 600, Loss: 0.11749125272035599
step: 700, Loss: 0.1171286553144455
step: 800, Loss: 0.11507830023765564
step: 900, Loss: 0.1160227507352829
step: 1000, Loss: 0.11643628031015396
step: 1100, Loss: 0.1152314618229866
step: 1200, Loss: 0.11409235000610352
step: 1300, Loss: 0.11716130375862122
step: 1400, Loss: 0.1148088201880455
step: 1500, Loss: 0.19086934626102448
step: 1600, Loss: 0.11424913257360458
step: 1700, Loss: 0.11614351719617844
step: 1800, Loss: 0.11403270065784454
step: 1900, Loss: 0.11430762708187103
step: 2000, Loss: 0.11583580076694489
step: 2100, Loss: 0.1121162474155426
step: 2200, Loss: 0.11484110355377197
step: 2300, Loss: 0.11332182586193085
step: 2400, Loss: 0.1143636479973793
step: 2500, Loss: 0.11308722198009491
step: 2600, Loss: 0.11457939445972443
step: 2700, Loss: 0.11350914090871811
step: 2800, Loss: 0.11352509260177612
step: 2900, Loss: 0.11462002247571945
step: 3000, Loss: 0.11399836838245392
step: 3100, Loss: 0.11344385892152786
step: 3200, Loss: 0.11509287357330322
step: 3300, Loss: 0.1150708794593811
step: 3400, Loss: 0.19491015374660492
step: 3500, Loss: 0.11902996897697449
step: 3600, Loss: 0.11692380905151367
step: 3700, Loss: 0.11387118697166443
step: 3800, Loss: 0.11683302372694016
step: 3900, Loss: 6.9290056228637695
step: 4000, Loss: 0.35097941756248474
step: 4100, Loss: 0.14902882277965546
step: 4200, Loss: 0.1437452882528305
step: 4300, Loss: 0.13180691003799438
step: 4400, Loss: 0.14203211665153503
step: 4500, Loss: 0.13698570430278778
step: 4600, Loss: 0.12386228889226913
step: 4700, Loss: 0.1251639574766159
step: 4800, Loss: 0.13087353110313416
step: 4900, Loss: 0.1196548193693161
step: 5000, Loss: 0.12257009744644165
step: 5100, Loss: 0.11833911389112473
step: 5200, Loss: 0.12428990006446838
step: 5300, Loss: 0.20305661857128143
step: 5400, Loss: 0.11659100651741028
step: 5500, Loss: 0.11746697127819061
step: 5600, Loss: 0.11781861633062363
step: 5700, Loss: 0.12259456515312195
step: 5800, Loss: 0.11948023736476898
step: 2800, Loss: 0.11511947959661484
step: 2900, Loss: 0.11614307016134262
step: 3000, Loss: 0.11409320682287216
step: 3100, Loss: 0.11613599956035614
step: 3200, Loss: 0.11363297700881958
step: 3300, Loss: 0.11502815037965775
step: 3400, Loss: 0.11685682833194733
step: 3500, Loss: 0.11451565474271774
step: 3600, Loss: 0.11605724692344666
step: 3700, Loss: 0.11421220004558563
step: 3800, Loss: 0.11440345644950867
step: 3900, Loss: 0.11567763984203339
step: 4000, Loss: 0.11441721022129059
step: 4100, Loss: 0.11574633419513702
step: 4200, Loss: 0.11397118866443634
step: 4300, Loss: 0.11884887516498566
step: 4400, Loss: 0.11545015871524811
step: 4500, Loss: 0.12243876606225967
step: 4600, Loss: 0.11553383618593216
step: 4700, Loss: 0.11491149663925171
step: 4800, Loss: 0.11389724165201187
step: 4900, Loss: 0.11543738096952438
step: 5000, Loss: 0.11442657560110092
step: 5100, Loss: 0.11572638899087906
step: 5200, Loss: 0.11542102694511414
step: 5300, Loss: 0.11742325127124786
step: 5400, Loss: 0.1139683946967125
step: 5500, Loss: 0.1281771957874298
step: 5600, Loss: 0.11408963054418564
step: 5700, Loss: 0.11542752385139465
step: 5800, Loss: 0.11394517123699188
step: 5900, Loss: 0.12057942897081375
step: 6000, Loss: 0.15680783987045288
step: 6100, Loss: 0.13241489231586456
step: 6200, Loss: 0.1281834989786148
step: 6300, Loss: 0.11931532621383667
step: 6400, Loss: 0.11958179622888565
step: 6500, Loss: 0.11928587406873703
step: 6600, Loss: 0.11903589218854904
step: 6700, Loss: 0.12125521153211594
step: 6800, Loss: 0.11801203340291977
step: 6900, Loss: 0.11767363548278809
step: 7000, Loss: 0.11744032055139542
step: 7100, Loss: 0.11618254333734512
step: 7200, Loss: 0.11465588957071304
step: 7300, Loss: 0.11777032166719437
step: 7400, Loss: 0.11552552878856659
step: 7500, Loss: 0.11972495168447495
step: 7600, Loss: 0.1184321716427803
step: 7700, Loss: 0.12078306078910828
step: 7800, Loss: 0.11510926485061646
step: 7900, Loss: 0.11589974164962769
step: 8000, Loss: 0.11692618578672409
step: 8100, Loss: 0.1220434382557869
step: 8200, Loss: 0.11859370768070221
step: 8300, Loss: 0.11451950669288635
step: 8400, Loss: 0.11745966225862503
step: 8500, Loss: 0.1162988543510437
step: 8600, Loss: 0.11535745114088058
step: 8700, Loss: 0.11735561490058899
step: 8800, Loss: 0.1134801134467125
step: 8900, Loss: 0.11481914669275284
step: 9000, Loss: 0.1173943430185318
step: 9100, Loss: 0.11533985286951065
step: 9200, Loss: 0.11501816660165787
step: 9300, Loss: 0.11683827638626099
step: 9400, Loss: 0.11596448719501495
step: 9500, Loss: 0.1158738061785698
step: 9600, Loss: 0.11365202069282532
step: 9700, Loss: 0.11631154268980026
step: 9800, Loss: 0.11435230076313019
step: 9900, Loss: 0.11556147783994675
training successfully ended.
validating...
validate data length:31
acc: 0.8333333333333334
precision: 0.8333333333333334
recall: 0.8823529411764706
F_score: 0.8571428571428571
******fold 10******

Training... train_data length:281
step: 0, Loss: 0.2847205698490143
step: 100, Loss: 0.11828377842903137
step: 200, Loss: 0.11509394645690918
step: 300, Loss: 0.11714335530996323
step: 400, Loss: 0.11552076041698456
step: 500, Loss: 0.11557358503341675
step: 600, Loss: 0.11589883267879486
step: 700, Loss: 0.11562027782201767
step: 800, Loss: 0.11513258516788483
step: 900, Loss: 0.11455512791872025
step: 1000, Loss: 0.11458496749401093
step: 1100, Loss: 0.11400977522134781
step: 1200, Loss: 0.11630302667617798
step: 1300, Loss: 0.11600328236818314
step: 1400, Loss: 0.1183696836233139
step: 1500, Loss: 0.1146555170416832
step: 1600, Loss: 0.11441610008478165
step: 1700, Loss: 0.11454697698354721
step: 1800, Loss: 0.11472261697053909
step: 1900, Loss: 0.11477101594209671
step: 2000, Loss: 0.11483768373727798
step: 2100, Loss: 0.11533757299184799
step: 2200, Loss: 0.11414208263158798
step: 2300, Loss: 0.11427084356546402
step: 2400, Loss: 0.1142134964466095
step: 2500, Loss: 0.11450731754302979
step: 2600, Loss: 0.11500738561153412
step: 2700, Loss: 0.114371158182621
step: 2800, Loss: 0.11506582796573639
step: 2900, Loss: 0.1148795410990715
step: 3000, Loss: 0.11466633528470993
step: 3100, Loss: 0.11439062654972076
step: 3200, Loss: 0.11483664065599442
step: 3300, Loss: 0.11539749056100845
step: 3400, Loss: 0.11510717868804932
step: 3500, Loss: 0.11558089405298233
step: 3600, Loss: 0.1138172447681427
step: 3700, Loss: 0.11521293967962265
step: 3800, Loss: 0.11335624754428864
step: 3900, Loss: 0.11630937457084656
step: 4000, Loss: 0.11465522646903992
step: 4100, Loss: 0.1156051754951477
step: 4200, Loss: 0.11402431130409241
step: 4300, Loss: 0.11621420830488205
step: 4400, Loss: 0.11417846381664276
step: 4500, Loss: 0.11623916029930115
step: 4600, Loss: 0.11565423011779785
step: 4700, Loss: 0.11454614996910095
step: 4800, Loss: 0.11346909403800964
step: 4900, Loss: 0.11483459919691086
step: 5000, Loss: 0.11406893283128738
step: 5100, Loss: 0.11507585644721985
step: 5200, Loss: 0.11598347127437592
step: 5300, Loss: 0.11414116621017456
step: 5400, Loss: 0.11517876386642456
step: 5500, Loss: 0.11555555462837219
step: 5600, Loss: 0.11668094992637634
step: 5700, Loss: 0.11412972211837769
step: 5800, Loss: 0.11460967361927032
step: 5900, Loss: 1.0232114791870117
step: 6000, Loss: 0.1382121443748474
step: 6100, Loss: 0.13627766072750092
step: 6200, Loss: 0.12952183187007904
step: 6300, Loss: 0.12287811934947968
step: 6400, Loss: 0.12383341789245605
step: 6500, Loss: 0.1182076558470726
step: 6600, Loss: 0.12071121484041214
step: 6700, Loss: 0.11593997478485107
step: 6800, Loss: 0.11555667966604233
step: 6900, Loss: 0.11865952610969543
step: 7000, Loss: 0.11563032865524292
step: 7100, Loss: 0.1200554296374321
step: 7200, Loss: 0.12128990888595581
step: 7300, Loss: 0.11728337407112122
step: 7400, Loss: 0.11594913899898529
step: 7500, Loss: 0.11507444083690643
step: 7600, Loss: 0.12005674839019775
step: 7700, Loss: 0.1159200593829155
step: 7800, Loss: 0.11879466474056244
step: 7900, Loss: 0.11722087115049362
step: 8000, Loss: 0.11557316035032272
step: 8100, Loss: 0.11679056286811829
step: 8200, Loss: 0.11512481421232224
step: 8300, Loss: 0.11896976828575134
step: 8400, Loss: 0.1141345426440239
step: 8500, Loss: 0.11393675208091736
step: 8600, Loss: 0.11434590816497803
step: 8700, Loss: 0.11571301519870758
step: 8800, Loss: 0.1205330565571785
step: 8900, Loss: 0.11362135410308838
step: 9000, Loss: 0.11543243378400803
step: 9100, Loss: 0.11493565887212753
step: 9200, Loss: 0.11489330232143402
step: 9300, Loss: 0.11558368802070618
step: 9400, Loss: 0.11478643119335175
step: 9500, Loss: 0.11529037356376648
step: 9600, Loss: 0.11405333876609802
step: 9700, Loss: 0.11400092393159866
step: 9800, Loss: 0.11376193165779114
step: 9900, Loss: 0.11300675570964813
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9230769230769231
recall: 1.0
F_score: 0.9600000000000001
subject 19 Avgacc: 0.8147916666666666 Avgfscore: 0.8286566667173878 
 Max acc:0.9666666666666667, Max f score:0.967741935483871
******** mix subject_20 ********

[156, 156]
******fold 1******

Training... train_data length:280
step: 0, Loss: 39.446258544921875
step: 100, Loss: 0.4814791679382324
step: 200, Loss: 0.1424235701560974
step: 300, Loss: 0.14242079854011536
step: 400, Loss: 0.1268051713705063
step: 500, Loss: 0.12242038547992706
step: 600, Loss: 0.12068628519773483
step: 700, Loss: 0.1252215951681137
step: 800, Loss: 0.12310324609279633
step: 900, Loss: 0.12004201114177704
step: 1000, Loss: 0.12073032557964325
step: 1100, Loss: 0.11950451135635376
step: 1200, Loss: 0.11857261508703232
step: 1300, Loss: 0.120631642639637
step: 1400, Loss: 0.12618176639080048
step: 1500, Loss: 0.1173241138458252
step: 1600, Loss: 0.11866475641727448
step: 1700, Loss: 0.11628581583499908
step: 1800, Loss: 0.11359141021966934
step: 1900, Loss: 0.11808151006698608
step: 2000, Loss: 0.11487995088100433
step: 2100, Loss: 0.11457397043704987
step: 2200, Loss: 0.11698665469884872
step: 2300, Loss: 0.11691434681415558
step: 2400, Loss: 0.11529361456632614
step: 2500, Loss: 0.11471264809370041
step: 2600, Loss: 0.11627356708049774
step: 2700, Loss: 0.11527864634990692
step: 5900, Loss: 0.11680762469768524
step: 6000, Loss: 0.11920854449272156
step: 6100, Loss: 0.11449408531188965
step: 6200, Loss: 0.12088239192962646
step: 6300, Loss: 0.1178910881280899
step: 6400, Loss: 0.11855362355709076
step: 6500, Loss: 0.11655424535274506
step: 6600, Loss: 0.11816544830799103
step: 6700, Loss: 0.11602004617452621
step: 6800, Loss: 0.11475762724876404
step: 6900, Loss: 0.11536048352718353
step: 7000, Loss: 0.1130727156996727
step: 7100, Loss: 0.11435162276029587
step: 7200, Loss: 0.1972341388463974
step: 7300, Loss: 0.11479096114635468
step: 7400, Loss: 0.1153910681605339
step: 7500, Loss: 0.11474717408418655
step: 7600, Loss: 0.11522185802459717
step: 7700, Loss: 0.11416193842887878
step: 7800, Loss: 0.11380089074373245
step: 7900, Loss: 0.11376466602087021
step: 8000, Loss: 0.11448894441127777
step: 8100, Loss: 0.11417290568351746
step: 8200, Loss: 0.11543851345777512
step: 8300, Loss: 0.11713003367185593
step: 8400, Loss: 0.11429126560688019
step: 8500, Loss: 0.1145419329404831
step: 8600, Loss: 0.11481199413537979
step: 8700, Loss: 0.11368623375892639
step: 8800, Loss: 0.1137370765209198
step: 8900, Loss: 0.11406676471233368
step: 9000, Loss: 0.11413992941379547
step: 9100, Loss: 0.19510892033576965
step: 9200, Loss: 0.11432217061519623
step: 9300, Loss: 0.11366835236549377
step: 9400, Loss: 0.11435918509960175
step: 9500, Loss: 0.11601868271827698
step: 9600, Loss: 0.11785366386175156
step: 9700, Loss: 0.12125711888074875
step: 9800, Loss: 0.11450857669115067
step: 9900, Loss: 0.1136462390422821
training successfully ended.
validating...
validate data length:76
acc: 0.9027777777777778
precision: 0.8604651162790697
recall: 0.9736842105263158
F_score: 0.9135802469135803
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.1837318241596222
step: 100, Loss: 0.13287192583084106
step: 200, Loss: 0.11878589540719986
step: 300, Loss: 0.12420603632926941
step: 400, Loss: 0.11583935469388962
step: 500, Loss: 0.11817747354507446
step: 600, Loss: 0.11885815858840942
step: 700, Loss: 0.11527380347251892
step: 800, Loss: 0.11571715027093887
step: 900, Loss: 0.11465376615524292
step: 1000, Loss: 0.11576972901821136
step: 1100, Loss: 0.11686541885137558
step: 1200, Loss: 0.1144014522433281
step: 1300, Loss: 0.11351358890533447
step: 1400, Loss: 0.11576385051012039
step: 1500, Loss: 0.1935066282749176
step: 1600, Loss: 0.11424124240875244
step: 1700, Loss: 0.11385689675807953
step: 1800, Loss: 0.11489470303058624
step: 1900, Loss: 0.11507504433393478
step: 2000, Loss: 0.11451208591461182
step: 2100, Loss: 0.11328303068876266
step: 2200, Loss: 0.1133795902132988
step: 2300, Loss: 0.11433221399784088
step: 2400, Loss: 0.11356066912412643
step: 2500, Loss: 0.1138908639550209
step: 2600, Loss: 0.11326637864112854
step: 2700, Loss: 0.11278147995471954
step: 2800, Loss: 0.11430799216032028
step: 2900, Loss: 0.11405952274799347
step: 3000, Loss: 0.11621934175491333
step: 3100, Loss: 0.11363184452056885
step: 3200, Loss: 0.11396709829568863
step: 3300, Loss: 0.11469163000583649
step: 3400, Loss: 0.19176465272903442
step: 3500, Loss: 0.1149701178073883
step: 3600, Loss: 0.11739730834960938
step: 3700, Loss: 0.12381413578987122
step: 3800, Loss: 0.1161990836262703
step: 3900, Loss: 0.11643114686012268
step: 4000, Loss: 0.1146102100610733
step: 4100, Loss: 0.11570294201374054
step: 4200, Loss: 0.11414191126823425
step: 4300, Loss: 0.11351627111434937
step: 4400, Loss: 0.115953229367733
step: 4500, Loss: 0.11623454093933105
step: 4600, Loss: 0.12201472371816635
step: 4700, Loss: 0.11685134470462799
step: 4800, Loss: 0.11406443268060684
step: 4900, Loss: 0.11592158675193787
step: 5000, Loss: 0.1177268847823143
step: 5100, Loss: 4.6478047370910645
step: 5200, Loss: 1.8627772331237793
step: 5300, Loss: 0.24114830791950226
step: 5400, Loss: 0.13058464229106903
step: 5500, Loss: 0.1329018473625183
step: 5600, Loss: 0.1371675431728363
step: 5700, Loss: 0.1285649538040161
step: 5800, Loss: 0.12229897081851959
step: 5900, Loss: 0.12226401269435883
step: 6000, Loss: 0.13409987092018127
step: 6100, Loss: 0.12485016882419586
step: 6200, Loss: 0.12167879194021225
step: 6300, Loss: 0.11944080889225006
step: 6400, Loss: 0.11952076852321625
step: 6500, Loss: 0.116937555372715
step: 6600, Loss: 0.1183234453201294
step: 6700, Loss: 0.12297666072845459
step: 6800, Loss: 0.11569996178150177
step: 6900, Loss: 0.11519321799278259
step: 7000, Loss: 0.11653535068035126
step: 7100, Loss: 0.11594507843255997
step: 7200, Loss: 0.19950222969055176
step: 7300, Loss: 0.11658570170402527
step: 7400, Loss: 0.11746161431074142
step: 7500, Loss: 0.11810122430324554
step: 7600, Loss: 0.11546675860881805
step: 7700, Loss: 0.11723655462265015
step: 7800, Loss: 0.1158541887998581
step: 7900, Loss: 0.11978138983249664
step: 8000, Loss: 0.11694762110710144
step: 8100, Loss: 0.11557343602180481
step: 8200, Loss: 0.1151585504412651
step: 8300, Loss: 0.11477073282003403
step: 8400, Loss: 0.1147129014134407
step: 8500, Loss: 0.11297495663166046
step: 8600, Loss: 0.11706986278295517
step: 8700, Loss: 0.11456625163555145
step: 8800, Loss: 0.11554771661758423
step: 8900, Loss: 0.1145128458738327
step: 9000, Loss: 0.11331436783075333
step: 9100, Loss: 0.1940704584121704
step: 9200, Loss: 0.11433638632297516
step: 9300, Loss: 0.11420375108718872
step: 9400, Loss: 0.11351247131824493
step: 9500, Loss: 0.11443302780389786
step: 9600, Loss: 0.11419659852981567
step: 9700, Loss: 0.11373736709356308
step: 9800, Loss: 0.11610282957553864
step: 9900, Loss: 0.114320769906044
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.875
recall: 1.0
F_score: 0.9333333333333333
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.13355806469917297
step: 100, Loss: 0.12887825071811676
step: 200, Loss: 0.12410800904035568
step: 300, Loss: 0.12093620002269745
step: 400, Loss: 0.11877286434173584
step: 500, Loss: 0.11647318303585052
step: 600, Loss: 0.11652560532093048
step: 700, Loss: 0.11399425566196442
step: 800, Loss: 0.11408156901597977
step: 900, Loss: 0.11359855532646179
step: 1000, Loss: 0.11406318843364716
step: 1100, Loss: 0.11424606293439865
step: 1200, Loss: 0.11491605639457703
step: 1300, Loss: 0.11329634487628937
step: 1400, Loss: 0.11408773809671402
step: 1500, Loss: 0.19107887148857117
step: 1600, Loss: 0.11491341888904572
step: 1700, Loss: 0.114422507584095
step: 1800, Loss: 0.11399823427200317
step: 1900, Loss: 0.11324800550937653
step: 2000, Loss: 0.11439540982246399
step: 2100, Loss: 0.11406074464321136
step: 2200, Loss: 0.11414525657892227
step: 2300, Loss: 0.1145256906747818
step: 2400, Loss: 0.11476446688175201
step: 2500, Loss: 0.11305542290210724
step: 2600, Loss: 0.11430278420448303
step: 2700, Loss: 0.11514485627412796
step: 2800, Loss: 0.11490903049707413
step: 2900, Loss: 0.11722104251384735
step: 3000, Loss: 0.11518266797065735
step: 3100, Loss: 0.11467000097036362
step: 3200, Loss: 0.11482232809066772
step: 3300, Loss: 0.11451634764671326
step: 3400, Loss: 0.19601717591285706
step: 3500, Loss: 0.11765028536319733
step: 3600, Loss: 0.13800323009490967
step: 3700, Loss: 0.1796034276485443
step: 3800, Loss: 0.15064112842082977
step: 3900, Loss: 0.1359613984823227
step: 4000, Loss: 0.12866084277629852
step: 4100, Loss: 0.1292082667350769
step: 4200, Loss: 0.12737508118152618
step: 4300, Loss: 0.1235099732875824
step: 4400, Loss: 0.12523679435253143
step: 4500, Loss: 0.11862124502658844
step: 4600, Loss: 0.12078570574522018
step: 4700, Loss: 0.12397228926420212
step: 4800, Loss: 0.11601731181144714
step: 4900, Loss: 0.11902792006731033
step: 5000, Loss: 0.11876945197582245
step: 5100, Loss: 0.11887997388839722
step: 5200, Loss: 0.11657822877168655
step: 5300, Loss: 0.20074060559272766
step: 5400, Loss: 0.11723291128873825
step: 5500, Loss: 0.1155829131603241
step: 5600, Loss: 0.11720645427703857
step: 5700, Loss: 0.11927806586027145
step: 5800, Loss: 0.11533167213201523
step: 5900, Loss: 0.11634433269500732
step: 6000, Loss: 0.11642701923847198
step: 6100, Loss: 0.11549434810876846
step: 6200, Loss: 0.11640990525484085
step: 6300, Loss: 0.11561810225248337
step: 2800, Loss: 0.11469380557537079
step: 2900, Loss: 0.11588015407323837
step: 3000, Loss: 0.11697162687778473
step: 3100, Loss: 0.11602184921503067
step: 3200, Loss: 0.11465838551521301
step: 3300, Loss: 0.1140366941690445
step: 3400, Loss: 0.11515051126480103
step: 3500, Loss: 0.11650627851486206
step: 3600, Loss: 0.11564213782548904
step: 3700, Loss: 0.11967135965824127
step: 3800, Loss: 0.11414679884910583
step: 3900, Loss: 0.11485882103443146
step: 4000, Loss: 0.11671630293130875
step: 4100, Loss: 0.11667731404304504
step: 4200, Loss: 0.11477766931056976
step: 4300, Loss: 0.1150629073381424
step: 4400, Loss: 0.11420710384845734
step: 4500, Loss: 0.11408977955579758
step: 4600, Loss: 0.11508390307426453
step: 4700, Loss: 0.11424724757671356
step: 4800, Loss: 0.1171015202999115
step: 4900, Loss: 0.11343968659639359
step: 5000, Loss: 0.11478174477815628
step: 5100, Loss: 0.11487007141113281
step: 5200, Loss: 0.11372701823711395
step: 5300, Loss: 0.11474260687828064
step: 5400, Loss: 0.11472015082836151
step: 5500, Loss: 0.11366889625787735
step: 5600, Loss: 0.11617910116910934
step: 5700, Loss: 0.11450236290693283
step: 5800, Loss: 0.11498592793941498
step: 5900, Loss: 0.1144169270992279
step: 6000, Loss: 0.11403103917837143
step: 6100, Loss: 0.11417161673307419
step: 6200, Loss: 0.11423253268003464
step: 6300, Loss: 0.11470098793506622
step: 6400, Loss: 0.11474163830280304
step: 6500, Loss: 0.11520299315452576
step: 6600, Loss: 0.1159476637840271
step: 6700, Loss: 0.11496424674987793
step: 6800, Loss: 0.11571236699819565
step: 6900, Loss: 0.11499467492103577
step: 7000, Loss: 0.1137877032160759
step: 7100, Loss: 0.11396802961826324
step: 7200, Loss: 0.11467106640338898
step: 7300, Loss: 0.11631501466035843
step: 7400, Loss: 0.11675338447093964
step: 7500, Loss: 0.11487731337547302
step: 7600, Loss: 0.1159338504076004
step: 7700, Loss: 0.11499306559562683
step: 7800, Loss: 0.1132320761680603
step: 7900, Loss: 0.11521342396736145
step: 8000, Loss: 0.11374465376138687
step: 8100, Loss: 0.11444184184074402
step: 8200, Loss: 0.11494743078947067
step: 8300, Loss: 0.6491708755493164
step: 8400, Loss: 0.13095247745513916
step: 8500, Loss: 0.13241028785705566
step: 8600, Loss: 0.12329819053411484
step: 8700, Loss: 0.12168831378221512
step: 8800, Loss: 0.12163963168859482
step: 8900, Loss: 0.11891607195138931
step: 9000, Loss: 0.11858274042606354
step: 9100, Loss: 0.11827941983938217
step: 9200, Loss: 0.11876650899648666
step: 9300, Loss: 0.1170094758272171
step: 9400, Loss: 0.11444224417209625
step: 9500, Loss: 0.1161171942949295
step: 9600, Loss: 0.11605606973171234
step: 9700, Loss: 0.11513180285692215
step: 9800, Loss: 0.1146993488073349
step: 9900, Loss: 0.12076157331466675
training successfully ended.
validating...
validate data length:32
acc: 0.5
precision: 0.3888888888888889
recall: 0.5833333333333334
F_score: 0.4666666666666666
******fold 2******

Training... train_data length:280
step: 0, Loss: 2.0106170177459717
step: 100, Loss: 0.13471323251724243
step: 200, Loss: 0.12256021052598953
step: 300, Loss: 0.12372617423534393
step: 400, Loss: 0.1194116473197937
step: 500, Loss: 0.1158871203660965
step: 600, Loss: 0.1185670718550682
step: 700, Loss: 0.11540353298187256
step: 800, Loss: 0.11678709089756012
step: 900, Loss: 0.12008818984031677
step: 1000, Loss: 0.1160905510187149
step: 1100, Loss: 0.1160319373011589
step: 1200, Loss: 0.11451675742864609
step: 1300, Loss: 0.1142718642950058
step: 1400, Loss: 0.11481238901615143
step: 1500, Loss: 0.11680575460195541
step: 1600, Loss: 0.11470582336187363
step: 1700, Loss: 0.11641320586204529
step: 1800, Loss: 0.11456175893545151
step: 1900, Loss: 0.11740177869796753
step: 2000, Loss: 0.11515137553215027
step: 2100, Loss: 0.11469462513923645
step: 2200, Loss: 0.11552038788795471
step: 2300, Loss: 0.11913485080003738
step: 2400, Loss: 0.11454138159751892
step: 2500, Loss: 0.11588477343320847
step: 2600, Loss: 0.11441944539546967
step: 2700, Loss: 0.1169377863407135
step: 2800, Loss: 0.11297144740819931
step: 2900, Loss: 0.11468479782342911
step: 3000, Loss: 0.11591289937496185
step: 3100, Loss: 0.11672623455524445
step: 3200, Loss: 0.11550367623567581
step: 3300, Loss: 0.1147555559873581
step: 3400, Loss: 0.11520382016897202
step: 3500, Loss: 0.11588059365749359
step: 3600, Loss: 0.1172550767660141
step: 3700, Loss: 0.11563634872436523
step: 3800, Loss: 0.11494874209165573
step: 3900, Loss: 0.11615737527608871
step: 4000, Loss: 0.11391398310661316
step: 4100, Loss: 0.11402624100446701
step: 4200, Loss: 0.11436599493026733
step: 4300, Loss: 0.11696814745664597
step: 4400, Loss: 0.11632976680994034
step: 4500, Loss: 0.11581580340862274
step: 4600, Loss: 0.11359462887048721
step: 4700, Loss: 0.11535543203353882
step: 4800, Loss: 0.1168917641043663
step: 4900, Loss: 0.11321103572845459
step: 5000, Loss: 0.11712513864040375
step: 5100, Loss: 0.1195438951253891
step: 5200, Loss: 0.11347486078739166
step: 5300, Loss: 0.1135822981595993
step: 5400, Loss: 0.11439469456672668
step: 5500, Loss: 0.11399465799331665
step: 5600, Loss: 0.11529450863599777
step: 5700, Loss: 0.11337629705667496
step: 5800, Loss: 0.11399677395820618
step: 5900, Loss: 0.11289883404970169
step: 6000, Loss: 0.11331383883953094
step: 6100, Loss: 0.11431803554296494
step: 6200, Loss: 0.11355782300233841
step: 6300, Loss: 0.11344365775585175
step: 6400, Loss: 0.11406517773866653
step: 6500, Loss: 0.11354604363441467
step: 6600, Loss: 0.11512552201747894
step: 6700, Loss: 0.11324037611484528
step: 6800, Loss: 0.11568011343479156
step: 6900, Loss: 0.11440865695476532
step: 7000, Loss: 0.11372389644384384
step: 7100, Loss: 0.11395524442195892
step: 7200, Loss: 0.11312709003686905
step: 7300, Loss: 0.1135500892996788
step: 7400, Loss: 0.11419038474559784
step: 7500, Loss: 0.11420361697673798
step: 7600, Loss: 0.11274942755699158
step: 7700, Loss: 0.11375300586223602
step: 7800, Loss: 0.11299315094947815
step: 7900, Loss: 0.11393623054027557
step: 8000, Loss: 0.11388837546110153
step: 8100, Loss: 0.13117170333862305
step: 8200, Loss: 0.12029080092906952
step: 8300, Loss: 0.11592410504817963
step: 8400, Loss: 0.11734439432621002
step: 8500, Loss: 0.11528660356998444
step: 8600, Loss: 0.11785392463207245
step: 8700, Loss: 0.11525875329971313
step: 8800, Loss: 0.11407019197940826
step: 8900, Loss: 0.11548889428377151
step: 9000, Loss: 0.11611609160900116
step: 9100, Loss: 0.11479739099740982
step: 9200, Loss: 0.11479544639587402
step: 9300, Loss: 0.1175636574625969
step: 9400, Loss: 0.11418222635984421
step: 9500, Loss: 0.1136237382888794
step: 9600, Loss: 0.11508147418498993
step: 9700, Loss: 0.1140376627445221
step: 9800, Loss: 0.11357852816581726
step: 9900, Loss: 0.11326272785663605
training successfully ended.
validating...
validate data length:32
acc: 0.8125
precision: 0.8125
recall: 0.8125
F_score: 0.8125
******fold 3******

Training... train_data length:281
step: 0, Loss: 1.110142469406128
step: 100, Loss: 0.15074267983436584
step: 200, Loss: 0.11807525157928467
step: 300, Loss: 0.12043221294879913
step: 400, Loss: 0.11506188660860062
step: 500, Loss: 0.1148119568824768
step: 600, Loss: 0.1131150871515274
step: 700, Loss: 0.11610634624958038
step: 800, Loss: 0.11311440914869308
step: 900, Loss: 0.11516355723142624
step: 1000, Loss: 0.11434226483106613
step: 1100, Loss: 0.11413177847862244
step: 1200, Loss: 0.1143534928560257
step: 1300, Loss: 0.11385832726955414
step: 1400, Loss: 0.11301527172327042
step: 1500, Loss: 0.11353127658367157
step: 1600, Loss: 0.11471685022115707
step: 1700, Loss: 0.113791324198246
step: 1800, Loss: 0.11402299255132675
step: 1900, Loss: 0.11352752894163132
step: 2000, Loss: 0.12021557986736298
step: 2100, Loss: 0.11386500298976898
step: 2200, Loss: 0.11398874968290329
step: 2300, Loss: 0.11354199796915054
step: 2400, Loss: 0.1133829802274704
step: 2500, Loss: 0.11531684547662735
step: 2600, Loss: 0.11284516751766205
step: 2700, Loss: 0.11278889328241348
step: 2800, Loss: 0.1150210052728653
step: 2900, Loss: 0.11325721442699432
step: 3000, Loss: 0.1130889356136322
step: 3100, Loss: 0.11362456530332565
step: 3200, Loss: 0.1138550341129303
step: 3300, Loss: 0.11460700631141663
step: 6400, Loss: 0.11491385847330093
step: 6500, Loss: 0.11491413414478302
step: 6600, Loss: 0.11481700837612152
step: 6700, Loss: 0.11630730330944061
step: 6800, Loss: 0.1158948615193367
step: 6900, Loss: 0.11595283448696136
step: 7000, Loss: 0.11788357049226761
step: 7100, Loss: 0.11491620540618896
step: 7200, Loss: 0.19461122155189514
step: 7300, Loss: 0.11470510065555573
step: 7400, Loss: 0.11428653448820114
step: 7500, Loss: 0.1152748316526413
step: 7600, Loss: 0.11393474042415619
step: 7700, Loss: 0.1132391095161438
step: 7800, Loss: 0.11425157636404037
step: 7900, Loss: 0.11379702389240265
step: 8000, Loss: 0.11384640634059906
step: 8100, Loss: 0.1148054450750351
step: 8200, Loss: 0.11394498497247696
step: 8300, Loss: 0.11375350505113602
step: 8400, Loss: 0.11447549611330032
step: 8500, Loss: 0.11388693004846573
step: 8600, Loss: 0.11372541636228561
step: 8700, Loss: 0.11376716196537018
step: 8800, Loss: 0.11404714733362198
step: 8900, Loss: 0.11408442258834839
step: 9000, Loss: 0.11300485581159592
step: 9100, Loss: 0.1930190622806549
step: 9200, Loss: 0.11512927711009979
step: 9300, Loss: 0.11340122669935226
step: 9400, Loss: 0.11439438909292221
step: 9500, Loss: 0.11538522690534592
step: 9600, Loss: 0.11630050837993622
step: 9700, Loss: 0.11402537673711777
step: 9800, Loss: 0.1179676353931427
step: 9900, Loss: 0.1159282773733139
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9772727272727273
recall: 1.0
F_score: 0.9885057471264368
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.4459504187107086
step: 100, Loss: 0.12229298055171967
step: 200, Loss: 0.11686810851097107
step: 300, Loss: 0.11514922976493835
step: 400, Loss: 0.11584305763244629
step: 500, Loss: 0.11662264168262482
step: 600, Loss: 0.11412384361028671
step: 700, Loss: 0.11443832516670227
step: 800, Loss: 0.11436112225055695
step: 900, Loss: 0.11393935233354568
step: 1000, Loss: 0.11519268900156021
step: 1100, Loss: 0.1126723662018776
step: 1200, Loss: 0.11449378728866577
step: 1300, Loss: 0.11460430920124054
step: 1400, Loss: 0.11386451870203018
step: 1500, Loss: 0.19101010262966156
step: 1600, Loss: 0.11329405009746552
step: 1700, Loss: 0.11317984014749527
step: 1800, Loss: 0.11447850614786148
step: 1900, Loss: 0.11381956189870834
step: 2000, Loss: 0.11263148486614227
step: 2100, Loss: 0.11472538113594055
step: 2200, Loss: 0.1143144965171814
step: 2300, Loss: 0.11367907375097275
step: 2400, Loss: 0.11946459859609604
step: 2500, Loss: 0.11802051961421967
step: 2600, Loss: 0.11312723159790039
step: 2700, Loss: 0.11458853632211685
step: 2800, Loss: 0.11795089393854141
step: 2900, Loss: 0.11654318124055862
step: 3000, Loss: 0.11454662680625916
step: 3100, Loss: 0.11758241802453995
step: 3200, Loss: 0.11397203803062439
step: 3300, Loss: 0.11756861209869385
step: 3400, Loss: 0.19360172748565674
step: 3500, Loss: 0.11624705791473389
step: 3600, Loss: 0.1151711493730545
step: 3700, Loss: 5.211358547210693
step: 3800, Loss: 0.15406233072280884
step: 3900, Loss: 0.14722290635108948
step: 4000, Loss: 0.1361299306154251
step: 4100, Loss: 0.12849511206150055
step: 4200, Loss: 0.12438690662384033
step: 4300, Loss: 0.12411428987979889
step: 4400, Loss: 0.12372313439846039
step: 4500, Loss: 0.11725057661533356
step: 4600, Loss: 0.12153376638889313
step: 4700, Loss: 0.1198720932006836
step: 4800, Loss: 0.11751502752304077
step: 4900, Loss: 0.11880189180374146
step: 5000, Loss: 0.1185222640633583
step: 5100, Loss: 0.1192375123500824
step: 5200, Loss: 0.11951888352632523
step: 5300, Loss: 0.20048320293426514
step: 5400, Loss: 0.11781422793865204
step: 5500, Loss: 0.11407778412103653
step: 5600, Loss: 0.11828954517841339
step: 5700, Loss: 0.11685476452112198
step: 5800, Loss: 0.11953052878379822
step: 5900, Loss: 0.11653062701225281
step: 6000, Loss: 0.11851704120635986
step: 6100, Loss: 0.11601688712835312
step: 6200, Loss: 0.11440691351890564
step: 6300, Loss: 0.1172558069229126
step: 6400, Loss: 0.11399324983358383
step: 6500, Loss: 0.11433906108140945
step: 6600, Loss: 0.11434423178434372
step: 6700, Loss: 0.11397391557693481
step: 6800, Loss: 0.1148248016834259
step: 6900, Loss: 0.11466821283102036
step: 7000, Loss: 0.11398030817508698
step: 7100, Loss: 0.11460773646831512
step: 7200, Loss: 0.19552427530288696
step: 7300, Loss: 0.11479087173938751
step: 7400, Loss: 0.11409023404121399
step: 7500, Loss: 0.11424224823713303
step: 7600, Loss: 0.11462350934743881
step: 7700, Loss: 0.11429157853126526
step: 7800, Loss: 0.11463844776153564
step: 7900, Loss: 0.11489767581224442
step: 8000, Loss: 0.11436344683170319
step: 8100, Loss: 0.1146966964006424
step: 8200, Loss: 0.11304153501987457
step: 8300, Loss: 0.11495088040828705
step: 8400, Loss: 0.11371294409036636
step: 8500, Loss: 0.11400926113128662
step: 8600, Loss: 0.11437873542308807
step: 8700, Loss: 0.11384259164333344
step: 8800, Loss: 0.11381946504116058
step: 8900, Loss: 0.11369700729846954
step: 9000, Loss: 0.11387237906455994
step: 9100, Loss: 0.1915031224489212
step: 9200, Loss: 0.11390355229377747
step: 9300, Loss: 0.11435873061418533
step: 9400, Loss: 0.11375400424003601
step: 9500, Loss: 0.1131657138466835
step: 9600, Loss: 0.11327257752418518
step: 9700, Loss: 0.11450988799333572
step: 9800, Loss: 0.114423468708992
step: 9900, Loss: 0.11351858079433441
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.9354838709677419
recall: 0.90625
F_score: 0.9206349206349206
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.36657339334487915
step: 100, Loss: 0.12332630157470703
step: 200, Loss: 0.11866393685340881
step: 300, Loss: 0.11534057557582855
step: 400, Loss: 0.11503877490758896
step: 500, Loss: 0.11592341214418411
step: 600, Loss: 0.11438784003257751
step: 700, Loss: 0.11396399885416031
step: 800, Loss: 0.1138620674610138
step: 900, Loss: 0.11374477297067642
step: 1000, Loss: 0.114322230219841
step: 1100, Loss: 0.11259409785270691
step: 1200, Loss: 0.11374867707490921
step: 1300, Loss: 0.1145457923412323
step: 1400, Loss: 0.11376939713954926
step: 1500, Loss: 0.19232764840126038
step: 1600, Loss: 0.11425289511680603
step: 1700, Loss: 0.11512219905853271
step: 1800, Loss: 0.11329343914985657
step: 1900, Loss: 0.1144137755036354
step: 2000, Loss: 0.11617916822433472
step: 2100, Loss: 0.11632180958986282
step: 2200, Loss: 0.11692743748426437
step: 2300, Loss: 0.11341730505228043
step: 2400, Loss: 0.11589393764734268
step: 2500, Loss: 0.11703802645206451
step: 2600, Loss: 0.11797994375228882
step: 2700, Loss: 0.1300540268421173
step: 2800, Loss: 0.8013806939125061
step: 2900, Loss: 0.18931974470615387
step: 3000, Loss: 0.1411292850971222
step: 3100, Loss: 0.12461487203836441
step: 3200, Loss: 0.12819960713386536
step: 3300, Loss: 0.123505599796772
step: 3400, Loss: 0.212617427110672
step: 3500, Loss: 0.12103155255317688
step: 3600, Loss: 0.12341604381799698
step: 3700, Loss: 0.11679723858833313
step: 3800, Loss: 0.11887024343013763
step: 3900, Loss: 0.11904208362102509
step: 4000, Loss: 0.11996228992938995
step: 4100, Loss: 0.11800840497016907
step: 4200, Loss: 0.12214551866054535
step: 4300, Loss: 0.12050613760948181
step: 4400, Loss: 0.11809799075126648
step: 4500, Loss: 0.11788961291313171
step: 4600, Loss: 0.1173894852399826
step: 4700, Loss: 0.11655855178833008
step: 4800, Loss: 0.1177748367190361
step: 4900, Loss: 0.123406782746315
step: 5000, Loss: 0.11639326810836792
step: 5100, Loss: 0.11537043750286102
step: 5200, Loss: 0.11458936333656311
step: 5300, Loss: 0.19769228994846344
step: 5400, Loss: 0.11392026394605637
step: 5500, Loss: 0.11395847052335739
step: 5600, Loss: 0.11373194307088852
step: 5700, Loss: 0.11487068235874176
step: 5800, Loss: 0.11535117775201797
step: 5900, Loss: 0.11463817954063416
step: 6000, Loss: 0.11756269633769989
step: 6100, Loss: 0.11431406438350677
step: 6200, Loss: 0.11454811692237854
step: 6300, Loss: 0.11830233782529831
step: 6400, Loss: 0.1154894158244133
step: 6500, Loss: 0.11368300020694733
step: 6600, Loss: 0.11477132141590118
step: 6700, Loss: 0.1152605414390564
step: 6800, Loss: 0.11447979509830475
step: 3400, Loss: 0.1137382760643959
step: 3500, Loss: 0.11254872381687164
step: 3600, Loss: 0.11319263279438019
step: 3700, Loss: 0.1129218265414238
step: 3800, Loss: 0.11429250985383987
step: 3900, Loss: 0.1138969361782074
step: 4000, Loss: 0.11469173431396484
step: 4100, Loss: 0.11370070278644562
step: 4200, Loss: 0.11315726488828659
step: 4300, Loss: 0.1133919358253479
step: 4400, Loss: 0.1133575513958931
step: 4500, Loss: 0.11445313692092896
step: 4600, Loss: 0.11378161609172821
step: 4700, Loss: 0.11512577533721924
step: 4800, Loss: 0.11363663524389267
step: 4900, Loss: 0.11277022212743759
step: 5000, Loss: 0.11295136064291
step: 5100, Loss: 0.11397984623908997
step: 5200, Loss: 0.11466678977012634
step: 5300, Loss: 0.11270035058259964
step: 5400, Loss: 0.11333610117435455
step: 5500, Loss: 0.11346590518951416
step: 5600, Loss: 0.11426131427288055
step: 5700, Loss: 0.11531590670347214
step: 5800, Loss: 0.11345872282981873
step: 5900, Loss: 0.977723240852356
step: 6000, Loss: 0.1327713429927826
step: 6100, Loss: 0.12618957459926605
step: 6200, Loss: 0.1232830360531807
step: 6300, Loss: 0.12189256399869919
step: 6400, Loss: 0.12090861797332764
step: 6500, Loss: 0.12162918597459793
step: 6600, Loss: 0.1183554008603096
step: 6700, Loss: 0.11842907965183258
step: 6800, Loss: 0.11453959345817566
step: 6900, Loss: 0.11446487903594971
step: 7000, Loss: 0.11633497476577759
step: 7100, Loss: 0.11488468945026398
step: 7200, Loss: 0.11378112435340881
step: 7300, Loss: 0.11515995860099792
step: 7400, Loss: 0.11622987687587738
step: 7500, Loss: 0.1153167113661766
step: 7600, Loss: 0.1148039922118187
step: 7700, Loss: 0.11472614109516144
step: 7800, Loss: 0.11497809737920761
step: 7900, Loss: 0.11386437714099884
step: 8000, Loss: 0.11387479305267334
step: 8100, Loss: 0.11296294629573822
step: 8200, Loss: 0.1135033369064331
step: 8300, Loss: 0.11395423114299774
step: 8400, Loss: 0.11389575153589249
step: 8500, Loss: 0.11321907490491867
step: 8600, Loss: 0.11352799832820892
step: 8700, Loss: 0.11567375063896179
step: 8800, Loss: 0.1135595291852951
step: 8900, Loss: 0.11313097178936005
step: 9000, Loss: 0.11258339136838913
step: 9100, Loss: 0.11343146115541458
step: 9200, Loss: 0.11507623642683029
step: 9300, Loss: 0.1142977774143219
step: 9400, Loss: 0.11463665962219238
step: 9500, Loss: 0.11399783194065094
step: 9600, Loss: 0.1133665069937706
step: 9700, Loss: 0.11538529396057129
step: 9800, Loss: 0.11351774632930756
step: 9900, Loss: 0.11396172642707825
training successfully ended.
validating...
validate data length:31
acc: 0.8666666666666667
precision: 0.8666666666666667
recall: 0.8666666666666667
F_score: 0.8666666666666667
******fold 4******

Training... train_data length:281
step: 0, Loss: 2.5906050205230713
step: 100, Loss: 0.11606290936470032
step: 200, Loss: 0.11671444773674011
step: 300, Loss: 0.11481396108865738
step: 400, Loss: 0.11440819501876831
step: 500, Loss: 0.11467129737138748
step: 600, Loss: 0.11386030167341232
step: 700, Loss: 0.11548807471990585
step: 800, Loss: 0.11418401449918747
step: 900, Loss: 0.11555463820695877
step: 1000, Loss: 0.11467627435922623
step: 1100, Loss: 0.11358810216188431
step: 1200, Loss: 0.11313755810260773
step: 1300, Loss: 0.11357155442237854
step: 1400, Loss: 0.1142667755484581
step: 1500, Loss: 0.11284169554710388
step: 1600, Loss: 0.11346656084060669
step: 1700, Loss: 0.11425894498825073
step: 1800, Loss: 0.11350249499082565
step: 1900, Loss: 0.1159699335694313
step: 2000, Loss: 0.11307339370250702
step: 2100, Loss: 0.11509087681770325
step: 2200, Loss: 0.11369924247264862
step: 2300, Loss: 0.1148219034075737
step: 2400, Loss: 0.11287494003772736
step: 2500, Loss: 0.11478361487388611
step: 2600, Loss: 0.11379597336053848
step: 2700, Loss: 0.11338786780834198
step: 2800, Loss: 0.11333069205284119
step: 2900, Loss: 0.11375879496335983
step: 3000, Loss: 0.11325710266828537
step: 3100, Loss: 0.11417552828788757
step: 3200, Loss: 0.11567363888025284
step: 3300, Loss: 0.11329055577516556
step: 3400, Loss: 0.11290974169969559
step: 3500, Loss: 0.11421013623476028
step: 3600, Loss: 0.11279374361038208
step: 3700, Loss: 0.11363351345062256
step: 3800, Loss: 0.1128692775964737
step: 3900, Loss: 0.11333911120891571
step: 4000, Loss: 0.11396151781082153
step: 4100, Loss: 0.11273793876171112
step: 4200, Loss: 0.11428949981927872
step: 4300, Loss: 0.11375871300697327
step: 4400, Loss: 0.11391586810350418
step: 4500, Loss: 0.1144619956612587
step: 4600, Loss: 0.11225314438343048
step: 4700, Loss: 0.11332826316356659
step: 4800, Loss: 0.11293628811836243
step: 4900, Loss: 0.11425809562206268
step: 5000, Loss: 0.1134125143289566
step: 5100, Loss: 0.11336995661258698
step: 5200, Loss: 0.11476877331733704
step: 5300, Loss: 0.11475373804569244
step: 5400, Loss: 0.11565323919057846
step: 5500, Loss: 0.11298538744449615
step: 5600, Loss: 0.11363440006971359
step: 5700, Loss: 0.11329102516174316
step: 5800, Loss: 0.11245523393154144
step: 5900, Loss: 0.11394837498664856
step: 6000, Loss: 0.11314088851213455
step: 6100, Loss: 0.11518125236034393
step: 6200, Loss: 0.1141258105635643
step: 6300, Loss: 0.11357954889535904
step: 6400, Loss: 0.11251990497112274
step: 6500, Loss: 0.11387144029140472
step: 6600, Loss: 0.11352398991584778
step: 6700, Loss: 0.11439348012208939
step: 6800, Loss: 2.04997181892395
step: 6900, Loss: 0.14581190049648285
step: 7000, Loss: 0.12001335620880127
step: 7100, Loss: 0.12093701958656311
step: 7200, Loss: 0.11855696141719818
step: 7300, Loss: 0.11593911051750183
step: 7400, Loss: 0.11687447130680084
step: 7500, Loss: 0.11826372891664505
step: 7600, Loss: 0.11734148859977722
step: 7700, Loss: 0.11445960402488708
step: 7800, Loss: 0.11725428700447083
step: 7900, Loss: 0.11791746318340302
step: 8000, Loss: 0.11484812945127487
step: 8100, Loss: 0.11466524004936218
step: 8200, Loss: 0.11493945121765137
step: 8300, Loss: 0.12001130729913712
step: 8400, Loss: 0.11536534875631332
step: 8500, Loss: 0.11447244882583618
step: 8600, Loss: 0.11426083743572235
step: 8700, Loss: 0.11823273450136185
step: 8800, Loss: 0.11317703127861023
step: 8900, Loss: 0.11497357487678528
step: 9000, Loss: 0.11384584754705429
step: 9100, Loss: 0.11575142294168472
step: 9200, Loss: 0.11393716186285019
step: 9300, Loss: 0.11496812105178833
step: 9400, Loss: 0.11502362787723541
step: 9500, Loss: 0.11351920664310455
step: 9600, Loss: 0.11501000076532364
step: 9700, Loss: 0.11430823057889938
step: 9800, Loss: 0.11292841285467148
step: 9900, Loss: 0.11900240182876587
training successfully ended.
validating...
validate data length:31
acc: 0.8
precision: 0.7647058823529411
recall: 0.8666666666666667
F_score: 0.8125
******fold 5******

Training... train_data length:281
step: 0, Loss: 2.890378713607788
step: 100, Loss: 0.11791485548019409
step: 200, Loss: 0.11512845009565353
step: 300, Loss: 0.11580851674079895
step: 400, Loss: 0.11390715092420578
step: 500, Loss: 0.11360055208206177
step: 600, Loss: 0.11493855714797974
step: 700, Loss: 0.1136653795838356
step: 800, Loss: 0.11328783631324768
step: 900, Loss: 0.11309527605772018
step: 1000, Loss: 0.11441438645124435
step: 1100, Loss: 0.11483688652515411
step: 1200, Loss: 0.11362890899181366
step: 1300, Loss: 0.1129913479089737
step: 1400, Loss: 0.11357343941926956
step: 1500, Loss: 0.11365345120429993
step: 1600, Loss: 0.11328565329313278
step: 1700, Loss: 0.1136927530169487
step: 1800, Loss: 0.1150069385766983
step: 1900, Loss: 0.11301098763942719
step: 2000, Loss: 0.11388614773750305
step: 2100, Loss: 0.11255525797605515
step: 2200, Loss: 0.11389602720737457
step: 2300, Loss: 0.11347299814224243
step: 2400, Loss: 0.11304791271686554
step: 2500, Loss: 0.11311717331409454
step: 2600, Loss: 0.11486522853374481
step: 2700, Loss: 0.11419454216957092
step: 2800, Loss: 0.11328665167093277
step: 2900, Loss: 0.11417225003242493
step: 3000, Loss: 0.11319656670093536
step: 3100, Loss: 0.11277040094137192
step: 3200, Loss: 0.11341521143913269
step: 3300, Loss: 0.11394268274307251
step: 3400, Loss: 0.11333523690700531
step: 3500, Loss: 0.1126425489783287
step: 3600, Loss: 0.11564655601978302
step: 3700, Loss: 0.11313673108816147
step: 3800, Loss: 0.11450871080160141
step: 6900, Loss: 0.11420471966266632
step: 7000, Loss: 0.11356324702501297
step: 7100, Loss: 0.11432504653930664
step: 7200, Loss: 0.19192999601364136
step: 7300, Loss: 0.11493758112192154
step: 7400, Loss: 0.11420659720897675
step: 7500, Loss: 0.1128796935081482
step: 7600, Loss: 0.11346925795078278
step: 7700, Loss: 0.1128649190068245
step: 7800, Loss: 0.11534153670072556
step: 7900, Loss: 0.11344459652900696
step: 8000, Loss: 0.11409004032611847
step: 8100, Loss: 0.1136646419763565
step: 8200, Loss: 0.11459670960903168
step: 8300, Loss: 0.11424428224563599
step: 8400, Loss: 0.11341866850852966
step: 8500, Loss: 0.11431970447301865
step: 8600, Loss: 0.11403778940439224
step: 8700, Loss: 0.114267997443676
step: 8800, Loss: 0.11419667303562164
step: 8900, Loss: 0.11625871807336807
step: 9000, Loss: 0.11422401666641235
step: 9100, Loss: 0.1939399689435959
step: 9200, Loss: 0.11640095710754395
step: 9300, Loss: 0.11689818650484085
step: 9400, Loss: 0.11353651434183121
step: 9500, Loss: 0.11402424424886703
step: 9600, Loss: 0.11692634224891663
step: 9700, Loss: 0.11421194672584534
step: 9800, Loss: 0.11544167250394821
step: 9900, Loss: 0.11665723472833633
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.9393939393939394
recall: 0.9393939393939394
F_score: 0.9393939393939394
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.2627250552177429
step: 100, Loss: 0.12050449848175049
step: 200, Loss: 0.12013344466686249
step: 300, Loss: 0.11483247578144073
step: 400, Loss: 0.11448992043733597
step: 500, Loss: 0.11703327298164368
step: 600, Loss: 0.11364858597517014
step: 700, Loss: 0.11538691073656082
step: 800, Loss: 0.11406067758798599
step: 900, Loss: 0.11397119611501694
step: 1000, Loss: 0.11420631408691406
step: 1100, Loss: 0.11358333379030228
step: 1200, Loss: 0.11333112418651581
step: 1300, Loss: 0.11399461328983307
step: 1400, Loss: 0.11322273313999176
step: 1500, Loss: 0.19832225143909454
step: 1600, Loss: 0.11464979499578476
step: 1700, Loss: 0.11653478443622589
step: 1800, Loss: 0.11716467142105103
step: 1900, Loss: 0.1178465485572815
step: 2000, Loss: 0.11433681845664978
step: 2100, Loss: 0.11462714523077011
step: 2200, Loss: 0.11528443545103073
step: 2300, Loss: 0.11563856154680252
step: 2400, Loss: 0.11536914855241776
step: 2500, Loss: 0.1158391609787941
step: 2600, Loss: 3.7103960514068604
step: 2700, Loss: 0.4046732783317566
step: 2800, Loss: 0.13477444648742676
step: 2900, Loss: 0.13418230414390564
step: 3000, Loss: 0.1262422502040863
step: 3100, Loss: 0.1255795657634735
step: 3200, Loss: 0.12386021018028259
step: 3300, Loss: 0.11811888217926025
step: 3400, Loss: 0.19926822185516357
step: 3500, Loss: 0.1247817650437355
step: 3600, Loss: 0.12515056133270264
step: 3700, Loss: 0.12068436294794083
step: 3800, Loss: 0.12314775586128235
step: 3900, Loss: 0.11599112302064896
step: 4000, Loss: 0.12006138265132904
step: 4100, Loss: 0.11798125505447388
step: 4200, Loss: 0.11733691394329071
step: 4300, Loss: 0.11590065062046051
step: 4400, Loss: 0.11680098623037338
step: 4500, Loss: 0.11572879552841187
step: 4600, Loss: 0.11648950725793839
step: 4700, Loss: 0.1163901686668396
step: 4800, Loss: 0.11799588799476624
step: 4900, Loss: 0.11645650863647461
step: 5000, Loss: 0.11520088464021683
step: 5100, Loss: 0.11543028056621552
step: 5200, Loss: 0.11492317169904709
step: 5300, Loss: 0.19106096029281616
step: 5400, Loss: 0.1155032366514206
step: 5500, Loss: 0.11596523225307465
step: 5600, Loss: 0.11547629535198212
step: 5700, Loss: 0.11438660323619843
step: 5800, Loss: 0.1156250387430191
step: 5900, Loss: 0.11510829627513885
step: 6000, Loss: 0.11609112471342087
step: 6100, Loss: 0.11436428129673004
step: 6200, Loss: 0.11393541842699051
step: 6300, Loss: 0.11537250876426697
step: 6400, Loss: 0.11392537504434586
step: 6500, Loss: 0.11359171569347382
step: 6600, Loss: 0.11422982811927795
step: 6700, Loss: 0.11456061899662018
step: 6800, Loss: 0.11398781836032867
step: 6900, Loss: 0.11318988353013992
step: 7000, Loss: 0.11399251222610474
step: 7100, Loss: 0.11471725255250931
step: 7200, Loss: 0.19144460558891296
step: 7300, Loss: 0.11339573562145233
step: 7400, Loss: 0.11381687968969345
step: 7500, Loss: 0.11449605971574783
step: 7600, Loss: 0.1147180050611496
step: 7700, Loss: 0.1154514029622078
step: 7800, Loss: 0.11359001696109772
step: 7900, Loss: 0.11515650153160095
step: 8000, Loss: 0.11343961209058762
step: 8100, Loss: 0.11318674683570862
step: 8200, Loss: 0.1131935641169548
step: 8300, Loss: 0.113307885825634
step: 8400, Loss: 0.11411761492490768
step: 8500, Loss: 0.11369176208972931
step: 8600, Loss: 0.11369071900844574
step: 8700, Loss: 0.1145053505897522
step: 8800, Loss: 0.1136685237288475
step: 8900, Loss: 0.11465263366699219
step: 9000, Loss: 0.11299137026071548
step: 9100, Loss: 0.1932186484336853
step: 9200, Loss: 0.11404779553413391
step: 9300, Loss: 0.11381363123655319
step: 9400, Loss: 0.11473830044269562
step: 9500, Loss: 0.11519283801317215
step: 9600, Loss: 0.11660696566104889
step: 9700, Loss: 0.11877170205116272
step: 9800, Loss: 0.11573797464370728
step: 9900, Loss: 0.11454454064369202
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.8888888888888888
recall: 1.0
F_score: 0.9411764705882353
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.44926851987838745
step: 100, Loss: 0.12342459708452225
step: 200, Loss: 0.11933019012212753
step: 300, Loss: 0.1163349598646164
step: 400, Loss: 0.11591081321239471
step: 500, Loss: 0.116724893450737
step: 600, Loss: 0.11551517248153687
step: 700, Loss: 0.11420983076095581
step: 800, Loss: 0.11569130420684814
step: 900, Loss: 0.11423663794994354
step: 1000, Loss: 0.11410707235336304
step: 1100, Loss: 0.11588610708713531
step: 1200, Loss: 0.11449438333511353
step: 1300, Loss: 0.11452273279428482
step: 1400, Loss: 0.11380688846111298
step: 1500, Loss: 0.19080203771591187
step: 1600, Loss: 0.11330965906381607
step: 1700, Loss: 0.1134130135178566
step: 1800, Loss: 0.11457259953022003
step: 1900, Loss: 0.1142963394522667
step: 2000, Loss: 0.11505615711212158
step: 2100, Loss: 0.11333362758159637
step: 2200, Loss: 0.11691662669181824
step: 2300, Loss: 0.1166980117559433
step: 2400, Loss: 0.11504654586315155
step: 2500, Loss: 0.11398455500602722
step: 2600, Loss: 0.11507707834243774
step: 2700, Loss: 0.11302068084478378
step: 2800, Loss: 0.11440829932689667
step: 2900, Loss: 0.11646221578121185
step: 3000, Loss: 0.11296103894710541
step: 3100, Loss: 0.11332838237285614
step: 3200, Loss: 0.11559363454580307
step: 3300, Loss: 0.11649748682975769
step: 3400, Loss: 0.19802451133728027
step: 3500, Loss: 0.11481443047523499
step: 3600, Loss: 0.11499419808387756
step: 3700, Loss: 0.11966733634471893
step: 3800, Loss: 1.6615841388702393
step: 3900, Loss: 0.5078384280204773
step: 4000, Loss: 0.13291583955287933
step: 4100, Loss: 0.13043677806854248
step: 4200, Loss: 0.1328783929347992
step: 4300, Loss: 0.12643930315971375
step: 4400, Loss: 0.1273684948682785
step: 4500, Loss: 0.11820820719003677
step: 4600, Loss: 0.1210041269659996
step: 4700, Loss: 0.12051893025636673
step: 4800, Loss: 0.12078530341386795
step: 4900, Loss: 0.12158691138029099
step: 5000, Loss: 0.11712313443422318
step: 5100, Loss: 0.11711469292640686
step: 5200, Loss: 0.11577942967414856
step: 5300, Loss: 0.19841152429580688
step: 5400, Loss: 0.11879371851682663
step: 5500, Loss: 0.1237446740269661
step: 5600, Loss: 0.11951637268066406
step: 5700, Loss: 0.11825236678123474
step: 5800, Loss: 0.1142408549785614
step: 5900, Loss: 0.1166875883936882
step: 6000, Loss: 0.11650129407644272
step: 6100, Loss: 0.11628636717796326
step: 6200, Loss: 0.11635507643222809
step: 6300, Loss: 0.11785304546356201
step: 6400, Loss: 0.11539064347743988
step: 6500, Loss: 0.11406277865171432
step: 6600, Loss: 0.11478541791439056
step: 6700, Loss: 0.11652778089046478
step: 6800, Loss: 0.11434071511030197
step: 6900, Loss: 0.11398681998252869
step: 7000, Loss: 0.11657550930976868
step: 7100, Loss: 0.11615129560232162
step: 7200, Loss: 0.19639234244823456
step: 7300, Loss: 0.1141795814037323
step: 3900, Loss: 0.1154046356678009
step: 4000, Loss: 0.11306845396757126
step: 4100, Loss: 0.11299820244312286
step: 4200, Loss: 0.11461572349071503
step: 4300, Loss: 0.11341696232557297
step: 4400, Loss: 0.1135442703962326
step: 4500, Loss: 0.11312618106603622
step: 4600, Loss: 0.11480467021465302
step: 4700, Loss: 0.11303751170635223
step: 4800, Loss: 0.11303868889808655
step: 4900, Loss: 0.11394087970256805
step: 5000, Loss: 0.11471724510192871
step: 5100, Loss: 0.11426857113838196
step: 5200, Loss: 0.11450975388288498
step: 5300, Loss: 0.11484009772539139
step: 5400, Loss: 0.11298505961894989
step: 5500, Loss: 0.11379323899745941
step: 5600, Loss: 0.11317755281925201
step: 5700, Loss: 0.11413337290287018
step: 5800, Loss: 0.11351947486400604
step: 5900, Loss: 0.1131708174943924
step: 6000, Loss: 0.11425215005874634
step: 6100, Loss: 0.11551803350448608
step: 6200, Loss: 0.113795205950737
step: 6300, Loss: 0.11370206624269485
step: 6400, Loss: 0.11391007155179977
step: 6500, Loss: 0.1793651282787323
step: 6600, Loss: 0.13825350999832153
step: 6700, Loss: 0.12772183120250702
step: 6800, Loss: 0.1293768286705017
step: 6900, Loss: 0.1271531730890274
step: 7000, Loss: 0.11897720396518707
step: 7100, Loss: 0.11711173504590988
step: 7200, Loss: 0.11906982958316803
step: 7300, Loss: 0.1158713847398758
step: 7400, Loss: 0.12007370591163635
step: 7500, Loss: 0.11557823419570923
step: 7600, Loss: 0.1193428561091423
step: 7700, Loss: 0.11372016370296478
step: 7800, Loss: 0.1162484735250473
step: 7900, Loss: 0.11653009057044983
step: 8000, Loss: 0.11427974700927734
step: 8100, Loss: 0.11688896268606186
step: 8200, Loss: 0.11385222524404526
step: 8300, Loss: 0.11645302921533585
step: 8400, Loss: 0.11430176347494125
step: 8500, Loss: 0.11660823225975037
step: 8600, Loss: 0.11486819386482239
step: 8700, Loss: 0.11604849994182587
step: 8800, Loss: 0.1151503324508667
step: 8900, Loss: 0.11412326991558075
step: 9000, Loss: 0.1139807477593422
step: 9100, Loss: 0.11487528681755066
step: 9200, Loss: 0.11534228920936584
step: 9300, Loss: 0.11399718374013901
step: 9400, Loss: 0.11393707245588303
step: 9500, Loss: 0.11308008432388306
step: 9600, Loss: 0.11438743770122528
step: 9700, Loss: 0.11435658484697342
step: 9800, Loss: 0.11401329934597015
step: 9900, Loss: 0.114289291203022
training successfully ended.
validating...
validate data length:31
acc: 0.7666666666666667
precision: 0.7894736842105263
recall: 0.8333333333333334
F_score: 0.8108108108108109
******fold 6******

Training... train_data length:281
step: 0, Loss: 2.656798839569092
step: 100, Loss: 0.11724965274333954
step: 200, Loss: 0.11618820577859879
step: 300, Loss: 0.11473491787910461
step: 400, Loss: 0.11472968012094498
step: 500, Loss: 0.11468979716300964
step: 600, Loss: 0.11397293210029602
step: 700, Loss: 0.11344289779663086
step: 800, Loss: 0.11494357138872147
step: 900, Loss: 0.11416598409414291
step: 1000, Loss: 0.11351270228624344
step: 1100, Loss: 0.11400425434112549
step: 1200, Loss: 0.11361005902290344
step: 1300, Loss: 0.11286309361457825
step: 1400, Loss: 0.11297624558210373
step: 1500, Loss: 0.1127263605594635
step: 1600, Loss: 0.11491403728723526
step: 1700, Loss: 0.1134028285741806
step: 1800, Loss: 0.11401628702878952
step: 1900, Loss: 0.11374565213918686
step: 2000, Loss: 0.11378359794616699
step: 2100, Loss: 0.11293180286884308
step: 2200, Loss: 0.11269038170576096
step: 2300, Loss: 0.11330186575651169
step: 2400, Loss: 0.11429253965616226
step: 2500, Loss: 0.11347359418869019
step: 2600, Loss: 0.11270955204963684
step: 2700, Loss: 0.11361522972583771
step: 2800, Loss: 0.11273585259914398
step: 2900, Loss: 0.11296061426401138
step: 3000, Loss: 0.11431093513965607
step: 3100, Loss: 0.11331868916749954
step: 3200, Loss: 0.11347257345914841
step: 3300, Loss: 0.11286388337612152
step: 3400, Loss: 0.11331911385059357
step: 3500, Loss: 0.1133934035897255
step: 3600, Loss: 0.11430150270462036
step: 3700, Loss: 0.11330142617225647
step: 3800, Loss: 0.11273746192455292
step: 3900, Loss: 0.11442841589450836
step: 4000, Loss: 0.11338061839342117
step: 4100, Loss: 0.11334635317325592
step: 4200, Loss: 0.11401723325252533
step: 4300, Loss: 0.11392528563737869
step: 4400, Loss: 0.11455520242452621
step: 4500, Loss: 0.11487951129674911
step: 4600, Loss: 0.11323703080415726
step: 4700, Loss: 0.11607728153467178
step: 4800, Loss: 0.11372430622577667
step: 4900, Loss: 0.11358566582202911
step: 5000, Loss: 0.1145201325416565
step: 5100, Loss: 0.11347343027591705
step: 5200, Loss: 0.11275675892829895
step: 5300, Loss: 0.11298944056034088
step: 5400, Loss: 0.11598560959100723
step: 5500, Loss: 0.11267579346895218
step: 5600, Loss: 0.11318017542362213
step: 5700, Loss: 0.11321911960840225
step: 5800, Loss: 0.11349379271268845
step: 5900, Loss: 0.11323592066764832
step: 6000, Loss: 0.11357948184013367
step: 6100, Loss: 0.11313807964324951
step: 6200, Loss: 0.11365243792533875
step: 6300, Loss: 0.11433690786361694
step: 6400, Loss: 0.11251567304134369
step: 6500, Loss: 0.11292410641908646
step: 6600, Loss: 0.11392103135585785
step: 6700, Loss: 0.5105924010276794
step: 6800, Loss: 0.16426587104797363
step: 6900, Loss: 0.12589164078235626
step: 7000, Loss: 0.12156470119953156
step: 7100, Loss: 0.12463771551847458
step: 7200, Loss: 0.11745546758174896
step: 7300, Loss: 0.11926323175430298
step: 7400, Loss: 0.11647862195968628
step: 7500, Loss: 0.117349773645401
step: 7600, Loss: 0.11850602924823761
step: 7700, Loss: 0.11955225467681885
step: 7800, Loss: 0.11830485612154007
step: 7900, Loss: 0.11647465080022812
step: 8000, Loss: 0.11724230647087097
step: 8100, Loss: 0.11583308130502701
step: 8200, Loss: 0.11699303239583969
step: 8300, Loss: 0.11640791594982147
step: 8400, Loss: 0.11453723907470703
step: 8500, Loss: 0.1162169799208641
step: 8600, Loss: 0.1160564050078392
step: 8700, Loss: 0.11626550555229187
step: 8800, Loss: 0.11552261561155319
step: 8900, Loss: 0.11282313615083694
step: 9000, Loss: 0.1348007321357727
step: 9100, Loss: 0.1139654815196991
step: 9200, Loss: 0.11450456082820892
step: 9300, Loss: 0.11763377487659454
step: 9400, Loss: 0.11506949365139008
step: 9500, Loss: 0.11542730033397675
step: 9600, Loss: 0.11441318690776825
step: 9700, Loss: 0.11583399772644043
step: 9800, Loss: 0.11564415693283081
step: 9900, Loss: 0.1149926409125328
training successfully ended.
validating...
validate data length:31
acc: 0.7
precision: 0.8461538461538461
recall: 0.6111111111111112
F_score: 0.7096774193548387
******fold 7******

Traceback (most recent call last):
  File "/home/sjf/eegall/main.py", line 487, in <module>
    harm_x = torch.load('/home/sjf/eegall/data/FACED/all_nwreharmon_de_features.pt')
  File "/home/sjf/eegall/main.py", line 298, in cross_validation
    # train_idx = train_idx.astype(int)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/serialization.py", line 997, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped()
RuntimeError: PytorchStreamReader failed reading file data/227: file read failed
step: 7400, Loss: 0.11619162559509277
step: 7500, Loss: 0.11364233493804932
step: 7600, Loss: 0.11508198082447052
step: 7700, Loss: 0.11387434601783752
step: 7800, Loss: 0.11324384063482285
step: 7900, Loss: 0.11458638310432434
step: 8000, Loss: 0.11443622410297394
step: 8100, Loss: 0.11397664248943329
step: 8200, Loss: 0.11335445940494537
step: 8300, Loss: 0.11431468278169632
step: 8400, Loss: 0.1140294000506401
step: 8500, Loss: 0.11357415467500687
step: 8600, Loss: 0.11413499712944031
step: 8700, Loss: 0.11491439491510391
step: 8800, Loss: 0.11362265795469284
step: 8900, Loss: 0.11327152699232101
step: 9000, Loss: 0.11360450088977814
step: 9100, Loss: 0.19304408133029938
step: 9200, Loss: 0.11453405767679214
step: 9300, Loss: 0.11352325975894928
step: 9400, Loss: 0.11342386156320572
step: 9500, Loss: 0.11262820661067963
step: 9600, Loss: 0.1136227548122406
step: 9700, Loss: 0.11341562122106552
step: 9800, Loss: 0.11499160528182983
step: 9900, Loss: 0.11395600438117981
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.972972972972973
recall: 0.9473684210526315
F_score: 0.9599999999999999
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.42237526178359985
step: 100, Loss: 0.11883045732975006
step: 200, Loss: 0.11519035696983337
step: 300, Loss: 0.11677302420139313
step: 400, Loss: 0.11478225141763687
step: 500, Loss: 0.11661305278539658
step: 600, Loss: 0.1147361621260643
step: 700, Loss: 0.11498302966356277
step: 800, Loss: 0.1143660694360733
step: 900, Loss: 0.11325078457593918
step: 1000, Loss: 0.11314357817173004
step: 1100, Loss: 0.11350154876708984
step: 1200, Loss: 0.11471392214298248
step: 1300, Loss: 0.11424629390239716
step: 1400, Loss: 0.11269354075193405
step: 1500, Loss: 0.19176790118217468
step: 1600, Loss: 0.11435390263795853
step: 1700, Loss: 0.1159760132431984
step: 1800, Loss: 0.1151396632194519
step: 1900, Loss: 0.11427833884954453
step: 2000, Loss: 0.11475562304258347
step: 2100, Loss: 0.11349158734083176
step: 2200, Loss: 0.11479880660772324
step: 2300, Loss: 0.11502014845609665
step: 2400, Loss: 0.11660314351320267
step: 2500, Loss: 0.11648816615343094
step: 2600, Loss: 0.11742384731769562
step: 2700, Loss: 0.11626242101192474
step: 2800, Loss: 0.1138245016336441
step: 2900, Loss: 0.11471731215715408
step: 3000, Loss: 0.11706449836492538
step: 3100, Loss: 0.11585822701454163
step: 3200, Loss: 0.11521957069635391
step: 3300, Loss: 0.11560295522212982
step: 3400, Loss: 0.3382469415664673
step: 3500, Loss: 0.1476430594921112
step: 3600, Loss: 0.1306142956018448
step: 3700, Loss: 0.12536852061748505
step: 3800, Loss: 0.12571127712726593
step: 3900, Loss: 0.12433885037899017
step: 4000, Loss: 0.12763410806655884
step: 4100, Loss: 0.12348318099975586
step: 4200, Loss: 0.1236945390701294
step: 4300, Loss: 0.11831445246934891
step: 4400, Loss: 0.11909317970275879
step: 4500, Loss: 0.11892060935497284
step: 4600, Loss: 0.1197117269039154
step: 4700, Loss: 0.1177753359079361
step: 4800, Loss: 0.11731268465518951
step: 4900, Loss: 0.11935929954051971
step: 5000, Loss: 0.11704742163419724
step: 5100, Loss: 0.11625395715236664
step: 5200, Loss: 0.11748488247394562
step: 5300, Loss: 0.20020721852779388
step: 5400, Loss: 0.11504051089286804
step: 5500, Loss: 0.11592403799295425
step: 5600, Loss: 0.11600633710622787
step: 5700, Loss: 0.11597131937742233
step: 5800, Loss: 0.11423323303461075
step: 5900, Loss: 0.11668392270803452
step: 6000, Loss: 0.11468083411455154
step: 6100, Loss: 0.1137903556227684
step: 6200, Loss: 0.11463531106710434
step: 6300, Loss: 0.11335929483175278
step: 6400, Loss: 0.11370885372161865
step: 6500, Loss: 0.11425116658210754
step: 6600, Loss: 0.1143307089805603
step: 6700, Loss: 0.11343153566122055
step: 6800, Loss: 0.11404166370630264
step: 6900, Loss: 0.11511033773422241
step: 7000, Loss: 0.11609871685504913
step: 7100, Loss: 0.11343888938426971
step: 7200, Loss: 0.1950307935476303
step: 7300, Loss: 0.11395300179719925
step: 7400, Loss: 0.11369680613279343
step: 7500, Loss: 0.11252407729625702
step: 7600, Loss: 0.11313490569591522
step: 7700, Loss: 0.1135815754532814
step: 7800, Loss: 0.11358552426099777
step: 7900, Loss: 0.11315922439098358
step: 8000, Loss: 0.11389114707708359
step: 8100, Loss: 0.11390545219182968
step: 8200, Loss: 0.11327432096004486
step: 8300, Loss: 0.11386771500110626
step: 8400, Loss: 0.11506791412830353
step: 8500, Loss: 0.1141611859202385
step: 8600, Loss: 0.11344774067401886
step: 8700, Loss: 0.11374172568321228
step: 8800, Loss: 0.11433447897434235
step: 8900, Loss: 0.11378197371959686
step: 9000, Loss: 0.11400899291038513
step: 9100, Loss: 0.19779373705387115
step: 9200, Loss: 0.11354735493659973
step: 9300, Loss: 0.11463107913732529
step: 9400, Loss: 0.11605703085660934
step: 9500, Loss: 0.1134452074766159
step: 9600, Loss: 0.11444839835166931
step: 9700, Loss: 0.11480677127838135
step: 9800, Loss: 0.11332287639379501
step: 9900, Loss: 0.11647754907608032
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9333333333333333
recall: 0.9655172413793104
F_score: 0.9491525423728815
subject 20 Avgacc: 0.9097222222222223 Avgfscore: 0.9096253390839518 
 Max acc:0.9861111111111112, Max f score:0.9885057471264368
******** mix subject_21 ********

[342, 418]
******fold 1******

Training... train_data length:684
step: 0, Loss: 49.07435607910156
step: 100, Loss: 2.29347825050354
step: 200, Loss: 0.34698358178138733
step: 300, Loss: 2.3933043479919434
step: 400, Loss: 0.1693558245897293
step: 500, Loss: 0.15747010707855225
step: 600, Loss: 0.1575019657611847
step: 700, Loss: 0.16347075998783112
step: 800, Loss: 0.14496393501758575
step: 900, Loss: 0.14174026250839233
step: 1000, Loss: 0.138864204287529
step: 1100, Loss: 0.14934691786766052
step: 1200, Loss: 0.13244777917861938
step: 1300, Loss: 0.13979452848434448
step: 1400, Loss: 0.13270723819732666
step: 1500, Loss: 0.20680448412895203
step: 1600, Loss: 0.1381930708885193
step: 1700, Loss: 0.13830795884132385
step: 1800, Loss: 0.1254124492406845
step: 1900, Loss: 0.1258576363325119
step: 2000, Loss: 0.12590444087982178
step: 2100, Loss: 0.12579795718193054
step: 2200, Loss: 0.12493747472763062
step: 2300, Loss: 0.12073180079460144
step: 2400, Loss: 0.12552762031555176
step: 2500, Loss: 0.12245313823223114
step: 2600, Loss: 0.12236253917217255
step: 2700, Loss: 0.11932709068059921
step: 2800, Loss: 0.12123283743858337
step: 2900, Loss: 0.11801706254482269
step: 3000, Loss: 0.11873971670866013
step: 3100, Loss: 0.11631157249212265
step: 3200, Loss: 0.11883047223091125
step: 3300, Loss: 0.11712867021560669
step: 3400, Loss: 0.19597551226615906
step: 3500, Loss: 0.1191096156835556
step: 3600, Loss: 0.11701888591051102
step: 3700, Loss: 0.11456191539764404
step: 3800, Loss: 0.11812037974596024
step: 3900, Loss: 0.11614718288183212
step: 4000, Loss: 0.1161378026008606
step: 4100, Loss: 0.11499370634555817
step: 4200, Loss: 0.11453849822282791
step: 4300, Loss: 0.11629541218280792
step: 4400, Loss: 0.11663573235273361
step: 4500, Loss: 0.11436136066913605
step: 4600, Loss: 0.11391238123178482
step: 4700, Loss: 0.11601559817790985
step: 4800, Loss: 0.11461629718542099
step: 4900, Loss: 0.11697640269994736
step: 5000, Loss: 0.11547762155532837
step: 5100, Loss: 0.11848413944244385
step: 5200, Loss: 0.11312663555145264
step: 5300, Loss: 0.1938939392566681
step: 5400, Loss: 0.11433830857276917
step: 5500, Loss: 0.11595775187015533
step: 5600, Loss: 0.11461493372917175
step: 5700, Loss: 0.11510949581861496
step: 5800, Loss: 0.11877083033323288
step: 5900, Loss: 0.12018261104822159
step: 6000, Loss: 0.3745480179786682
step: 6100, Loss: 2.482471466064453
step: 6200, Loss: 0.14943164587020874
step: 6300, Loss: 0.13199254870414734
step: 6400, Loss: 0.13318613171577454
step: 6500, Loss: 0.1397038847208023
step: 6600, Loss: 0.12684620916843414
step: 6700, Loss: 0.12861448526382446
step: 6800, Loss: 0.12758292257785797
step: 6900, Loss: 0.12235266715288162
step: 7000, Loss: 0.14304542541503906
step: 7100, Loss: 0.12806110084056854
step: 7200, Loss: 0.20283035933971405
step: 7300, Loss: 0.12732471525669098
step: 7400, Loss: 0.12433387339115143
step: 7500, Loss: 0.12667545676231384
step: 7600, Loss: 0.11653447151184082
step: 7700, Loss: 0.1221669465303421
step: 7800, Loss: 0.1181769073009491
step: 7900, Loss: 0.11820612102746964
step: 8000, Loss: 0.12041809409856796
step: 8100, Loss: 0.1182158887386322
step: 8200, Loss: 0.11689537018537521
step: 8300, Loss: 0.11909078061580658
step: 8400, Loss: 0.1158689633011818
step: 8500, Loss: 0.1157146468758583
step: 8600, Loss: 0.11631264537572861
step: 8700, Loss: 0.11723670363426208
step: 8800, Loss: 0.11652383953332901
step: 8900, Loss: 0.11986614763736725
step: 9000, Loss: 0.11618994921445847
step: 9100, Loss: 0.19684797525405884
step: 9200, Loss: 0.11729100346565247
step: 9300, Loss: 0.11713796854019165
step: 9400, Loss: 0.11681490391492844
step: 9500, Loss: 0.11323915421962738
step: 9600, Loss: 0.11640878021717072
step: 9700, Loss: 0.11529436707496643
step: 9800, Loss: 0.11603517085313797
step: 9900, Loss: 0.11748156696557999
training successfully ended.
validating...
validate data length:76
acc: 0.8333333333333334
precision: 0.75
recall: 0.9
F_score: 0.8181818181818182
******fold 2******

Training... train_data length:684
step: 0, Loss: 0.9290107488632202
step: 100, Loss: 0.14506055414676666
step: 200, Loss: 0.1330810934305191
step: 300, Loss: 0.12533889710903168
step: 400, Loss: 0.12200553715229034
step: 500, Loss: 0.11881907284259796
step: 600, Loss: 0.1210961639881134
step: 700, Loss: 0.11952490359544754
step: 800, Loss: 0.11367238312959671
step: 900, Loss: 0.11591476202011108
step: 1000, Loss: 0.11713753640651703
step: 1100, Loss: 0.12095780670642853
step: 1200, Loss: 0.11408039182424545
step: 1300, Loss: 0.11779788136482239
step: 1400, Loss: 0.11489155888557434
step: 1500, Loss: 0.19925501942634583
step: 1600, Loss: 0.1157810389995575
step: 1700, Loss: 0.11536146700382233
step: 1800, Loss: 0.11546825617551804
step: 1900, Loss: 0.11428102850914001
step: 2000, Loss: 0.11436253786087036
step: 2100, Loss: 0.11546087265014648
step: 2200, Loss: 0.11389832198619843
step: 2300, Loss: 0.1137886568903923
step: 2400, Loss: 0.11431778967380524
step: 2500, Loss: 0.11517620086669922
step: 2600, Loss: 0.11422146111726761
step: 2700, Loss: 0.1143910214304924
step: 2800, Loss: 0.11418954282999039
step: 2900, Loss: 0.11354097723960876
step: 3000, Loss: 0.11365778744220734
step: 3100, Loss: 0.11337985843420029
step: 3200, Loss: 0.1163618415594101
step: 3300, Loss: 0.11409451067447662
step: 3400, Loss: 0.19154369831085205
step: 3500, Loss: 0.11630528420209885
step: 3600, Loss: 0.11470021307468414
step: 3700, Loss: 0.11614352464675903
step: 3800, Loss: 0.11936917901039124
step: 3900, Loss: 0.11652730405330658
step: 4000, Loss: 0.11724105477333069
step: 4100, Loss: 0.11492003500461578
step: 4200, Loss: 0.11277744919061661
step: 4300, Loss: 0.11408449709415436
step: 4400, Loss: 0.12001648545265198
step: 4500, Loss: 0.2839134931564331
step: 4600, Loss: 0.16161015629768372
step: 4700, Loss: 0.12775401771068573
step: 4800, Loss: 0.12987345457077026
step: 4900, Loss: 0.13086678087711334
step: 5000, Loss: 0.1262333244085312
step: 5100, Loss: 0.12755373120307922
step: 5200, Loss: 0.12770310044288635
step: 5300, Loss: 0.20554883778095245
step: 5400, Loss: 0.12364272773265839
step: 5500, Loss: 0.12092889845371246
step: 5600, Loss: 0.11988217383623123
step: 5700, Loss: 0.11974089592695236
step: 5800, Loss: 0.12020653486251831
step: 5900, Loss: 0.1211494505405426
step: 6000, Loss: 0.11989547312259674
step: 6100, Loss: 0.12002400308847427
step: 6200, Loss: 0.12354960292577744
step: 6300, Loss: 0.11760013550519943
step: 6400, Loss: 0.11719225347042084
step: 6500, Loss: 0.11564981192350388
step: 6600, Loss: 0.11512555927038193
step: 6700, Loss: 0.1168099120259285
step: 6800, Loss: 0.11642403900623322
step: 6900, Loss: 0.11632281541824341
step: 7000, Loss: 0.11476832628250122
step: 7100, Loss: 0.11390232294797897
step: 7200, Loss: 0.19694523513317108
step: 7300, Loss: 0.11683663725852966
step: 7400, Loss: 0.11581648141145706
step: 7500, Loss: 0.11468762159347534
step: 7600, Loss: 0.11571729183197021
step: 7700, Loss: 0.11534244567155838
step: 7800, Loss: 0.11541429907083511
step: 7900, Loss: 0.11489694565534592
step: 8000, Loss: 0.11522206664085388
step: 8100, Loss: 0.11585264652967453
step: 8200, Loss: 0.11345820128917694
step: 8300, Loss: 0.11535194516181946
step: 8400, Loss: 0.11713707447052002
step: 8500, Loss: 0.11616140604019165
step: 8600, Loss: 0.11345984786748886
step: 8700, Loss: 0.11401621997356415
step: 8800, Loss: 0.11362394690513611
step: 8900, Loss: 0.1136607974767685
step: 9000, Loss: 0.11512421071529388
step: 9100, Loss: 0.19063778221607208
step: 9200, Loss: 0.11704651266336441
step: 9300, Loss: 0.11489227414131165
step: 9400, Loss: 0.11421454697847366
step: 9500, Loss: 0.11459315568208694
step: 9600, Loss: 0.11254855245351791
step: 9700, Loss: 0.11362163722515106
step: 9800, Loss: 0.11403819918632507
step: 9900, Loss: 0.11446364969015121
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.975
recall: 1.0
F_score: 0.9873417721518987
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.2181776911020279
step: 100, Loss: 0.12117510288953781
step: 200, Loss: 0.1186714768409729
step: 300, Loss: 0.11534949392080307
step: 400, Loss: 0.11664164066314697
step: 500, Loss: 0.11575020104646683
step: 600, Loss: 0.11435523629188538
step: 700, Loss: 0.11536475270986557
step: 800, Loss: 0.11382804811000824
step: 900, Loss: 0.11455807089805603
step: 1000, Loss: 0.1150275319814682
step: 1100, Loss: 0.11450418829917908
step: 1200, Loss: 0.11446496099233627
step: 1300, Loss: 0.11302728950977325
step: 1400, Loss: 0.11430253833532333
step: 1500, Loss: 0.19302548468112946
step: 1600, Loss: 0.11339619010686874
step: 1700, Loss: 0.11407357454299927
step: 1800, Loss: 0.11346250772476196
step: 1900, Loss: 0.11411938816308975
step: 2000, Loss: 0.11493512988090515
step: 2100, Loss: 0.11316709965467453
step: 2200, Loss: 0.11334691941738129
step: 2300, Loss: 0.11459329724311829
step: 2400, Loss: 0.1123523935675621
step: 2500, Loss: 0.11566241830587387
step: 2600, Loss: 0.11418598890304565
step: 2700, Loss: 0.1127680316567421
step: 2800, Loss: 0.11353424936532974
step: 2900, Loss: 0.1173725575208664
step: 3000, Loss: 0.11544892936944962
step: 3100, Loss: 0.1182846650481224
step: 3200, Loss: 0.11848551034927368
step: 3300, Loss: 0.11546920239925385
step: 3400, Loss: 0.1960039883852005
step: 3500, Loss: 0.7328136563301086
step: 3600, Loss: 0.2128586620092392
step: 3700, Loss: 0.12926891446113586
step: 3800, Loss: 0.13025040924549103
step: 3900, Loss: 0.12948408722877502
step: 4000, Loss: 0.12547403573989868
step: 4100, Loss: 0.12081939727067947
step: 4200, Loss: 0.1250663846731186
step: 4300, Loss: 0.1234215795993805
step: 4400, Loss: 0.12122280895709991
step: 4500, Loss: 0.12231453508138657
step: 4600, Loss: 0.12077516317367554
step: 4700, Loss: 0.11915748566389084
step: 4800, Loss: 0.12068098038434982
step: 4900, Loss: 0.11599499732255936
step: 5000, Loss: 0.11768929660320282
step: 5100, Loss: 0.11689801514148712
step: 5200, Loss: 0.12159012258052826
step: 5300, Loss: 0.19722230732440948
step: 5400, Loss: 0.11882740259170532
step: 5500, Loss: 0.11570416390895844
step: 5600, Loss: 0.11538075655698776
step: 5700, Loss: 0.11611180752515793
step: 5800, Loss: 0.1188865527510643
step: 5900, Loss: 0.11644899845123291
step: 6000, Loss: 0.11521318554878235
step: 6100, Loss: 0.11380734294652939
step: 6200, Loss: 0.1151394471526146
step: 6300, Loss: 0.1163957342505455
step: 6400, Loss: 0.11593833565711975
step: 6500, Loss: 0.11496096849441528
step: 6600, Loss: 0.11594881117343903
step: 6700, Loss: 0.11402487009763718
step: 6800, Loss: 0.11478656530380249
step: 6900, Loss: 0.11408493667840958
step: 7000, Loss: 0.1150573343038559
step: 7100, Loss: 0.11665713787078857
step: 7200, Loss: 0.1958616077899933
step: 7300, Loss: 0.11448656022548676
step: 7400, Loss: 0.11419182270765305
step: 7500, Loss: 0.11385723948478699
step: 7600, Loss: 0.11403785645961761
step: 7700, Loss: 0.11428186297416687
step: 7800, Loss: 0.11569789052009583
step: 7900, Loss: 0.11440684646368027
step: 8000, Loss: 0.11362044513225555
step: 8100, Loss: 0.11323652416467667
step: 8200, Loss: 0.11536148190498352
step: 8300, Loss: 0.11435986310243607
step: 8400, Loss: 0.11322006583213806
step: 8500, Loss: 0.1139446571469307
step: 8600, Loss: 0.1126633733510971
step: 8700, Loss: 0.11372074484825134
step: 8800, Loss: 0.11449774354696274
step: 8900, Loss: 0.11387014389038086
step: 9000, Loss: 0.1137452945113182
step: 9100, Loss: 0.19294501841068268
step: 9200, Loss: 0.11354509741067886
step: 9300, Loss: 0.11335618793964386
step: 9400, Loss: 0.11362802237272263
step: 9500, Loss: 0.11545433104038239
step: 9600, Loss: 0.1131880059838295
step: 9700, Loss: 0.11420442909002304
step: 9800, Loss: 0.11442632228136063
step: 9900, Loss: 0.11376176029443741
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9142857142857143
recall: 1.0
F_score: 0.955223880597015
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.22068563103675842
step: 100, Loss: 0.1194160208106041
step: 200, Loss: 0.11891184747219086
step: 300, Loss: 0.11418524384498596
step: 400, Loss: 0.11619015038013458
step: 500, Loss: 0.11367998272180557
step: 600, Loss: 0.11588732898235321
step: 700, Loss: 0.11597086489200592
step: 800, Loss: 0.11258494853973389
step: 900, Loss: 0.1133514866232872
step: 1000, Loss: 0.11568384617567062
step: 1100, Loss: 0.11336161941289902
step: 1200, Loss: 0.11374141275882721
step: 1300, Loss: 0.11387895792722702
step: 1400, Loss: 0.11301130801439285
step: 1500, Loss: 0.19592265784740448
step: 1600, Loss: 0.11414006352424622
step: 1700, Loss: 0.11416327953338623
step: 1800, Loss: 0.116550512611866
step: 1900, Loss: 0.11343759298324585
step: 2000, Loss: 0.11394159495830536
step: 2100, Loss: 0.11514490842819214
step: 2200, Loss: 0.11326324194669724
step: 2300, Loss: 0.11363137513399124
step: 2400, Loss: 0.11537431180477142
step: 2500, Loss: 0.11587558686733246
step: 2600, Loss: 0.1175379753112793
step: 2700, Loss: 0.11424047499895096
step: 2800, Loss: 0.11412764340639114
step: 2900, Loss: 0.11675858497619629
step: 3000, Loss: 0.8464838266372681
step: 3100, Loss: 0.21475479006767273
step: 3200, Loss: 0.1393587291240692
step: 3300, Loss: 0.12954244017601013
step: 3400, Loss: 0.2091895341873169
step: 3500, Loss: 0.11894766986370087
step: 3600, Loss: 0.12233781814575195
step: 3700, Loss: 0.11918114125728607
step: 3800, Loss: 0.12241356819868088
step: 3900, Loss: 0.11967168003320694
step: 4000, Loss: 0.12131471931934357
step: 4100, Loss: 0.11869299411773682
step: 4200, Loss: 0.12044673413038254
step: 4300, Loss: 0.11921672523021698
step: 4400, Loss: 0.12052252143621445
step: 4500, Loss: 0.11760525405406952
step: 4600, Loss: 0.11827322840690613
step: 4700, Loss: 0.11663885414600372
step: 4800, Loss: 0.1203627958893776
step: 4900, Loss: 0.1172078475356102
step: 5000, Loss: 0.11516612768173218
step: 5100, Loss: 0.11571897566318512
step: 5200, Loss: 0.11562418192625046
step: 5300, Loss: 0.19568324089050293
step: 5400, Loss: 0.1145881935954094
step: 5500, Loss: 0.11562348902225494
step: 5600, Loss: 0.11464641988277435
step: 5700, Loss: 0.11743635684251785
step: 5800, Loss: 0.11571277678012848
step: 5900, Loss: 0.11524044722318649
step: 6000, Loss: 0.11559833586215973
step: 6100, Loss: 0.11265787482261658
step: 6200, Loss: 0.11404316872358322
step: 6300, Loss: 0.11405906826257706
step: 6400, Loss: 0.11556120216846466
step: 6500, Loss: 0.11383175104856491
step: 6600, Loss: 0.11375495791435242
step: 6700, Loss: 0.11577928066253662
step: 6800, Loss: 0.11535901576280594
step: 6900, Loss: 0.11401290446519852
step: 7000, Loss: 0.11461163312196732
step: 7100, Loss: 0.11499680578708649
step: 7200, Loss: 0.1909153163433075
step: 7300, Loss: 0.11318868398666382
step: 7400, Loss: 0.11359604448080063
step: 7500, Loss: 0.113370381295681
step: 7600, Loss: 0.1137758418917656
step: 7700, Loss: 0.11338134855031967
step: 7800, Loss: 0.11369391530752182
step: 7900, Loss: 0.11443264782428741
step: 8000, Loss: 0.11393251270055771
step: 8100, Loss: 0.11419782787561417
step: 8200, Loss: 0.11441658437252045
step: 8300, Loss: 0.11402936279773712
step: 8400, Loss: 0.11394181847572327
step: 8500, Loss: 0.11345675587654114
step: 8600, Loss: 0.11362674832344055
step: 8700, Loss: 0.11334098875522614
step: 8800, Loss: 0.11353321373462677
step: 8900, Loss: 0.11387918889522552
step: 9000, Loss: 0.11363409459590912
step: 9100, Loss: 0.1926499456167221
step: 9200, Loss: 0.11333110928535461
step: 9300, Loss: 0.11463743448257446
step: 9400, Loss: 0.11270803958177567
step: 9500, Loss: 0.11490435153245926
step: 9600, Loss: 0.11377780884504318
step: 9700, Loss: 0.11618542671203613
step: 9800, Loss: 0.11495610326528549
step: 9900, Loss: 0.1129140555858612
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9722222222222222
recall: 1.0
F_score: 0.9859154929577464
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.1849764585494995
step: 100, Loss: 0.11809080839157104
step: 200, Loss: 0.11639754474163055
step: 300, Loss: 0.11683434247970581
step: 400, Loss: 0.11706818640232086
step: 500, Loss: 0.11457450687885284
step: 600, Loss: 0.11403904110193253
step: 700, Loss: 0.1143573671579361
step: 800, Loss: 0.11512015759944916
step: 900, Loss: 0.11378849297761917
step: 1000, Loss: 0.11343507468700409
step: 1100, Loss: 0.1145663782954216
step: 1200, Loss: 0.11447826027870178
step: 1300, Loss: 0.1145373284816742
step: 1400, Loss: 0.11293995380401611
step: 1500, Loss: 0.19340601563453674
step: 1600, Loss: 0.11421401053667068
step: 1700, Loss: 0.11300890892744064
step: 1800, Loss: 0.11350462585687637
step: 1900, Loss: 0.1142035573720932
step: 2000, Loss: 0.11438314616680145
step: 2100, Loss: 0.11411220580339432
step: 2200, Loss: 0.11454837769269943
step: 2300, Loss: 0.11413322389125824
step: 2400, Loss: 0.116544708609581
step: 2500, Loss: 0.11273842304944992
step: 2600, Loss: 0.11318180710077286
step: 2700, Loss: 0.11438116431236267
step: 2800, Loss: 0.11507066339254379
step: 2900, Loss: 0.11573372036218643
step: 3000, Loss: 0.11487971991300583
step: 3100, Loss: 0.1134297102689743
step: 3200, Loss: 0.11757141351699829
step: 3300, Loss: 0.11528250575065613
step: 3400, Loss: 0.19377383589744568
step: 3500, Loss: 0.11445102095603943
step: 3600, Loss: 0.1172366812825203
step: 3700, Loss: 0.11359277367591858
step: 3800, Loss: 0.1141340360045433
step: 3900, Loss: 0.11459771543741226
step: 4000, Loss: 0.11910274624824524
step: 4100, Loss: 0.17638587951660156
step: 4200, Loss: 0.13768921792507172
step: 4300, Loss: 0.12898190319538116
step: 4400, Loss: 0.1307416409254074
step: 4500, Loss: 0.1296902596950531
step: 4600, Loss: 0.1254810094833374
step: 4700, Loss: 0.12422057241201401
step: 4800, Loss: 0.1246357262134552
step: 4900, Loss: 0.11802613735198975
step: 5000, Loss: 0.1203974112868309
step: 5100, Loss: 0.11811776459217072
step: 5200, Loss: 0.11679209768772125
step: 5300, Loss: 0.19786715507507324
step: 5400, Loss: 0.11890042573213577
step: 5500, Loss: 0.11998514831066132
step: 5600, Loss: 0.1166662722826004
step: 5700, Loss: 0.11914477497339249
step: 5800, Loss: 0.11810283362865448
step: 5900, Loss: 0.11816216260194778
step: 6000, Loss: 0.11687672883272171
step: 6100, Loss: 0.1158798560500145
step: 6200, Loss: 0.11606130003929138
step: 6300, Loss: 0.11625191569328308
step: 6400, Loss: 0.11621830612421036
step: 6500, Loss: 0.11888198554515839
step: 6600, Loss: 0.11705926060676575
step: 6700, Loss: 0.11652681231498718
step: 6800, Loss: 0.11525270342826843
step: 6900, Loss: 0.11675484478473663
step: 7000, Loss: 0.11420262604951859
step: 7100, Loss: 0.11290676891803741
step: 7200, Loss: 0.19367443025112152
step: 7300, Loss: 0.11391527205705643
step: 7400, Loss: 0.11489427089691162
step: 7500, Loss: 0.11516067385673523
step: 7600, Loss: 0.11298460513353348
step: 7700, Loss: 0.1163109764456749
step: 7800, Loss: 0.11405553668737411
step: 7900, Loss: 0.1144646555185318
step: 8000, Loss: 0.11387577652931213
step: 8100, Loss: 0.11293531209230423
step: 8200, Loss: 0.11377212405204773
step: 8300, Loss: 0.11299975216388702
step: 8400, Loss: 0.11431343108415604
step: 8500, Loss: 0.11298896372318268
step: 8600, Loss: 0.1143893450498581
step: 8700, Loss: 0.11380166560411453
step: 8800, Loss: 0.11382056772708893
step: 8900, Loss: 0.1138320118188858
step: 9000, Loss: 0.11397816985845566
step: 9100, Loss: 0.19032131135463715
step: 9200, Loss: 0.11347562074661255
step: 9300, Loss: 0.113844133913517
step: 9400, Loss: 0.11319383978843689
step: 9500, Loss: 0.11302835494279861
step: 9600, Loss: 0.11350322514772415
step: 9700, Loss: 0.1145932674407959
step: 9800, Loss: 0.11425584554672241
step: 9900, Loss: 0.11247038096189499
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9705882352941176
recall: 1.0
F_score: 0.9850746268656716
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.16951410472393036
step: 100, Loss: 0.12272130697965622
step: 200, Loss: 0.11665334552526474
step: 300, Loss: 0.11439371109008789
step: 400, Loss: 0.11769816279411316
step: 500, Loss: 0.11641772091388702
step: 600, Loss: 0.11542631685733795
step: 700, Loss: 0.11360035836696625
step: 800, Loss: 0.11518064141273499
step: 900, Loss: 0.1141231581568718
step: 1000, Loss: 0.11448436975479126
step: 1100, Loss: 0.11362648010253906
step: 1200, Loss: 0.11534574627876282
step: 1300, Loss: 0.11639633774757385
step: 1400, Loss: 0.11347974836826324
step: 1500, Loss: 0.1912916749715805
step: 1600, Loss: 0.11316817998886108
step: 1700, Loss: 0.11396416276693344
step: 1800, Loss: 0.11499078571796417
step: 1900, Loss: 0.11402643471956253
step: 2000, Loss: 0.11473357677459717
step: 2100, Loss: 0.11345532536506653
step: 2200, Loss: 0.1139623373746872
step: 2300, Loss: 0.11427288502454758
step: 2400, Loss: 0.1142767071723938
step: 2500, Loss: 0.11359511315822601
step: 2600, Loss: 0.1170060783624649
step: 2700, Loss: 0.11462705582380295
step: 2800, Loss: 0.11384023725986481
step: 2900, Loss: 0.11648552119731903
step: 3000, Loss: 0.11612272262573242
step: 3100, Loss: 0.11612127721309662
step: 3200, Loss: 0.11470343917608261
step: 3300, Loss: 0.11467406153678894
step: 3400, Loss: 0.1935541033744812
step: 3500, Loss: 0.11425617337226868
step: 3600, Loss: 0.11697743088006973
step: 3700, Loss: 0.11430925875902176
step: 3800, Loss: 0.11446519196033478
step: 3900, Loss: 0.11396646499633789
step: 4000, Loss: 0.12020894885063171
step: 4100, Loss: 0.12141109257936478
step: 4200, Loss: 0.16654780507087708
step: 4300, Loss: 0.12929758429527283
step: 4400, Loss: 0.12749770283699036
step: 4500, Loss: 0.12302028387784958
step: 4600, Loss: 0.12588614225387573
step: 4700, Loss: 0.12223871052265167
step: 4800, Loss: 0.12639805674552917
step: 4900, Loss: 0.11939038336277008
step: 5000, Loss: 0.11982688307762146
step: 5100, Loss: 0.12177872657775879
step: 5200, Loss: 0.11787354201078415
step: 5300, Loss: 0.1996995508670807
step: 5400, Loss: 0.12047577649354935
step: 5500, Loss: 0.12052156776189804
step: 5600, Loss: 0.11628368496894836
step: 5700, Loss: 0.11614733934402466
step: 5800, Loss: 0.11932331323623657
step: 5900, Loss: 0.11596490442752838
step: 6000, Loss: 0.11600225418806076
step: 6100, Loss: 0.11675006151199341
step: 6200, Loss: 0.11514393240213394
step: 6300, Loss: 0.1170874759554863
step: 6400, Loss: 0.11463094502687454
step: 6500, Loss: 0.114832803606987
step: 6600, Loss: 0.1143939271569252
step: 6700, Loss: 0.11540261656045914
step: 6800, Loss: 0.11327609419822693
step: 6900, Loss: 0.11370594799518585
step: 7000, Loss: 0.11529751867055893
step: 7100, Loss: 0.1140933409333229
step: 7200, Loss: 0.1942211389541626
step: 7300, Loss: 0.11626211553812027
step: 7400, Loss: 0.11345791816711426
step: 7500, Loss: 0.11482246220111847
step: 7600, Loss: 0.11434011906385422
step: 7700, Loss: 0.11565495282411575
step: 7800, Loss: 0.11365315318107605
step: 7900, Loss: 0.11511383950710297
step: 8000, Loss: 0.11397805064916611
step: 8100, Loss: 0.11303763091564178
step: 8200, Loss: 0.11243216693401337
step: 8300, Loss: 0.11387892812490463
step: 8400, Loss: 0.11394038796424866
step: 8500, Loss: 0.11428532749414444
step: 8600, Loss: 0.11362267285585403
step: 8700, Loss: 0.1138278990983963
step: 8800, Loss: 0.11382137984037399
step: 8900, Loss: 0.11319532990455627
step: 9000, Loss: 0.11328790336847305
step: 9100, Loss: 0.19421429932117462
step: 9200, Loss: 0.1142498105764389
step: 9300, Loss: 0.11346907913684845
step: 9400, Loss: 0.11351855099201202
step: 9500, Loss: 0.1132594645023346
step: 9600, Loss: 0.1134885624051094
step: 9700, Loss: 0.11374995857477188
step: 9800, Loss: 0.1155283972620964
step: 9900, Loss: 0.11395962536334991
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.14103347063064575
step: 100, Loss: 0.11843385547399521
step: 200, Loss: 0.11532603949308395
step: 300, Loss: 0.117428258061409
step: 400, Loss: 0.11457638442516327
step: 500, Loss: 0.11423183232545853
step: 600, Loss: 0.11435423791408539
step: 700, Loss: 0.1145419254899025
step: 800, Loss: 0.11316526681184769
step: 900, Loss: 0.11342927813529968
step: 1000, Loss: 0.11427930742502213
step: 1100, Loss: 0.11440671980381012
step: 1200, Loss: 0.11522435396909714
step: 1300, Loss: 0.11364340037107468
step: 1400, Loss: 0.11361909657716751
step: 1500, Loss: 0.19483810663223267
step: 1600, Loss: 0.11367598921060562
step: 1700, Loss: 0.11450248211622238
step: 1800, Loss: 0.11459024995565414
step: 1900, Loss: 0.11467157304286957
step: 2000, Loss: 0.11515261232852936
step: 2100, Loss: 0.11441400647163391
step: 2200, Loss: 0.1145433783531189
step: 2300, Loss: 0.11365956813097
step: 2400, Loss: 0.11551317572593689
step: 2500, Loss: 0.11419246345758438
step: 2600, Loss: 0.11473047733306885
step: 2700, Loss: 1.8422632217407227
step: 2800, Loss: 0.16586530208587646
step: 2900, Loss: 0.1444856971502304
step: 3000, Loss: 0.125987708568573
step: 3100, Loss: 0.12101231515407562
step: 3200, Loss: 0.13070432841777802
step: 3300, Loss: 0.12800434231758118
step: 3400, Loss: 0.20100539922714233
step: 3500, Loss: 0.12216372787952423
step: 3600, Loss: 0.12047994136810303
step: 3700, Loss: 0.12272600829601288
step: 3800, Loss: 0.12034033238887787
step: 3900, Loss: 0.12038873881101608
step: 4000, Loss: 0.1159190684556961
step: 4100, Loss: 0.1168774962425232
step: 4200, Loss: 0.11539970338344574
step: 4300, Loss: 0.11770294606685638
step: 4400, Loss: 0.11792267113924026
step: 4500, Loss: 0.11693578958511353
step: 4600, Loss: 0.11851073801517487
step: 4700, Loss: 0.11357653141021729
step: 4800, Loss: 0.11681680381298065
step: 4900, Loss: 0.11673037707805634
step: 5000, Loss: 0.11651937663555145
step: 5100, Loss: 0.11716756224632263
step: 5200, Loss: 0.11589528620243073
step: 5300, Loss: 0.19108597934246063
step: 5400, Loss: 0.11589301377534866
step: 5500, Loss: 0.11343315988779068
step: 5600, Loss: 0.1131444126367569
step: 5700, Loss: 0.11556218564510345
step: 5800, Loss: 0.11474643647670746
step: 5900, Loss: 0.11626428365707397
step: 6000, Loss: 0.11471747606992722
step: 6100, Loss: 0.11485523730516434
step: 6200, Loss: 0.1150524914264679
step: 6300, Loss: 0.11419843137264252
step: 6400, Loss: 0.11453010141849518
step: 6500, Loss: 0.11418911069631577
step: 6600, Loss: 0.11395013332366943
step: 6700, Loss: 0.11527144908905029
step: 6800, Loss: 0.11379960924386978
step: 6900, Loss: 0.11383894085884094
step: 7000, Loss: 0.11401738226413727
step: 7100, Loss: 0.1150425374507904
step: 7200, Loss: 0.19202210009098053
step: 7300, Loss: 0.11382271349430084
step: 7400, Loss: 0.11328762769699097
step: 7500, Loss: 0.11352772265672684
step: 7600, Loss: 0.11342354118824005
step: 7700, Loss: 0.1147313043475151
step: 7800, Loss: 0.11302172392606735
step: 7900, Loss: 0.11284814774990082
step: 8000, Loss: 0.11408038437366486
step: 8100, Loss: 0.11365550756454468
step: 8200, Loss: 0.11377427726984024
step: 8300, Loss: 0.11375025659799576
step: 8400, Loss: 0.11326021701097488
step: 8500, Loss: 0.11299095302820206
step: 8600, Loss: 0.11313749849796295
step: 8700, Loss: 0.11256824433803558
step: 8800, Loss: 0.11420968174934387
step: 8900, Loss: 0.11328386515378952
step: 9000, Loss: 0.11344743520021439
step: 9100, Loss: 0.19413292407989502
step: 9200, Loss: 0.11453144252300262
step: 9300, Loss: 0.11396074295043945
step: 9400, Loss: 0.11434347182512283
step: 9500, Loss: 0.11360321938991547
step: 9600, Loss: 0.11550714820623398
step: 9700, Loss: 0.11282258480787277
step: 9800, Loss: 0.11502385139465332
step: 9900, Loss: 0.11438677459955215
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.15475834906101227
step: 100, Loss: 0.11855271458625793
step: 200, Loss: 0.11623790860176086
step: 300, Loss: 0.11714892089366913
step: 400, Loss: 0.11525199562311172
step: 500, Loss: 0.11530670523643494
step: 600, Loss: 0.11662071943283081
step: 700, Loss: 0.1142357885837555
step: 800, Loss: 0.11358806490898132
step: 900, Loss: 0.11480718106031418
step: 1000, Loss: 0.11385631561279297
step: 1100, Loss: 0.11408074200153351
step: 1200, Loss: 0.1129273846745491
step: 1300, Loss: 0.11421597003936768
step: 1400, Loss: 0.11318136006593704
step: 1500, Loss: 0.19081708788871765
step: 1600, Loss: 0.11378943920135498
step: 1700, Loss: 0.11386778950691223
step: 1800, Loss: 0.11301145702600479
step: 1900, Loss: 0.11304215341806412
step: 2000, Loss: 0.11407364159822464
step: 2100, Loss: 0.11441560089588165
step: 2200, Loss: 0.11407414078712463
step: 2300, Loss: 0.11452946066856384
step: 2400, Loss: 0.11331067979335785
step: 2500, Loss: 0.1137944757938385
step: 2600, Loss: 0.11389262974262238
step: 2700, Loss: 0.1128108948469162
step: 2800, Loss: 0.1148938536643982
step: 2900, Loss: 0.11361606419086456
step: 3000, Loss: 0.11439014971256256
step: 3100, Loss: 0.11442096531391144
step: 3200, Loss: 0.11455351859331131
step: 3300, Loss: 0.11351433396339417
step: 3400, Loss: 0.19255542755126953
step: 3500, Loss: 0.11461682617664337
step: 3600, Loss: 0.11525966972112656
step: 3700, Loss: 0.11538255214691162
step: 3800, Loss: 0.11545562744140625
step: 3900, Loss: 0.11559085547924042
step: 4000, Loss: 0.11330317705869675
step: 4100, Loss: 0.11323955655097961
step: 4200, Loss: 0.11367299407720566
step: 4300, Loss: 0.11381664127111435
step: 4400, Loss: 0.11520789563655853
step: 4500, Loss: 0.11420327425003052
step: 4600, Loss: 0.11544422805309296
step: 4700, Loss: 0.11409300565719604
step: 4800, Loss: 0.11351980268955231
step: 4900, Loss: 0.11488261818885803
step: 5000, Loss: 0.11501839011907578
step: 5100, Loss: 0.3476056158542633
step: 5200, Loss: 0.14960871636867523
step: 5300, Loss: 0.23979011178016663
step: 5400, Loss: 0.1214727908372879
step: 5500, Loss: 0.12614165246486664
step: 5600, Loss: 0.12609592080116272
step: 5700, Loss: 0.12193787097930908
step: 5800, Loss: 0.11664284765720367
step: 5900, Loss: 0.12424683570861816
step: 6000, Loss: 0.1196756362915039
step: 6100, Loss: 0.11801296472549438
step: 6200, Loss: 0.11887507140636444
step: 6300, Loss: 0.12184219807386398
step: 6400, Loss: 0.12149222195148468
step: 6500, Loss: 0.11720307171344757
step: 6600, Loss: 0.11698829382658005
step: 6700, Loss: 0.12009355425834656
step: 6800, Loss: 0.11833379417657852
step: 6900, Loss: 0.11718997359275818
step: 7000, Loss: 0.11784003674983978
step: 7100, Loss: 0.11753511428833008
step: 7200, Loss: 0.19499829411506653
step: 7300, Loss: 0.11515572667121887
step: 7400, Loss: 0.11717277765274048
step: 7500, Loss: 0.117756687104702
step: 7600, Loss: 0.11480849236249924
step: 7700, Loss: 0.11565519869327545
step: 7800, Loss: 0.11519180238246918
step: 7900, Loss: 0.1160140186548233
step: 8000, Loss: 0.1153065413236618
step: 8100, Loss: 0.11574935913085938
step: 8200, Loss: 0.11566594243049622
step: 8300, Loss: 0.11417938768863678
step: 8400, Loss: 0.115422822535038
step: 8500, Loss: 0.11534789204597473
step: 8600, Loss: 0.11422941833734512
step: 8700, Loss: 0.11619335412979126
step: 8800, Loss: 0.11521510034799576
step: 8900, Loss: 0.11400943249464035
step: 9000, Loss: 0.11447777599096298
step: 9100, Loss: 0.19203005731105804
step: 9200, Loss: 0.11438441276550293
step: 9300, Loss: 0.1144912987947464
step: 9400, Loss: 0.11430563032627106
step: 9500, Loss: 0.11429531127214432
step: 9600, Loss: 0.11372824758291245
step: 9700, Loss: 0.11420105397701263
step: 9800, Loss: 0.11385355144739151
step: 9900, Loss: 0.1130729392170906
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9285714285714286
recall: 1.0
F_score: 0.962962962962963
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.15864863991737366
step: 100, Loss: 0.14019599556922913
step: 200, Loss: 0.11804867535829544
step: 300, Loss: 0.11791656911373138
step: 400, Loss: 0.11602714657783508
step: 500, Loss: 0.11480142921209335
step: 600, Loss: 0.11682825535535812
step: 700, Loss: 0.11323773860931396
step: 800, Loss: 0.11480927467346191
step: 900, Loss: 0.11498314142227173
step: 1000, Loss: 0.11549346894025803
step: 1100, Loss: 0.11601676791906357
step: 1200, Loss: 0.11353340744972229
step: 1300, Loss: 0.1153167113661766
step: 1400, Loss: 0.11299099773168564
step: 1500, Loss: 0.1915654093027115
step: 1600, Loss: 0.11405522376298904
step: 1700, Loss: 0.1136474758386612
step: 1800, Loss: 0.11349188536405563
step: 1900, Loss: 0.11302883923053741
step: 2000, Loss: 0.11321806162595749
step: 2100, Loss: 0.11490637063980103
step: 2200, Loss: 0.11581560224294662
step: 2300, Loss: 0.11343520879745483
step: 2400, Loss: 0.11427249014377594
step: 2500, Loss: 0.11340107023715973
step: 2600, Loss: 0.11315715312957764
step: 2700, Loss: 0.11355274170637131
step: 2800, Loss: 0.11397768557071686
step: 2900, Loss: 0.11364713311195374
step: 3000, Loss: 0.11368568986654282
step: 3100, Loss: 0.11324089765548706
step: 3200, Loss: 0.11375508457422256
step: 3300, Loss: 0.1142582893371582
step: 3400, Loss: 0.19216813147068024
step: 3500, Loss: 0.11298929154872894
step: 3600, Loss: 0.11452415585517883
step: 3700, Loss: 0.11567973345518112
step: 3800, Loss: 0.11425311863422394
step: 3900, Loss: 0.11419212073087692
step: 4000, Loss: 0.11495514214038849
step: 4100, Loss: 0.11421451717615128
step: 4200, Loss: 0.11421902477741241
step: 4300, Loss: 0.11676468700170517
step: 4400, Loss: 0.11549018323421478
step: 4500, Loss: 0.11494158953428268
step: 4600, Loss: 0.11309652775526047
step: 4700, Loss: 0.1168234646320343
step: 4800, Loss: 1.3725746870040894
step: 4900, Loss: 0.1474781632423401
step: 5000, Loss: 0.13524501025676727
step: 5100, Loss: 0.1359829157590866
step: 5200, Loss: 0.12495869398117065
step: 5300, Loss: 0.20744194090366364
step: 5400, Loss: 0.12056486308574677
step: 5500, Loss: 0.12168697267770767
step: 5600, Loss: 0.12686781585216522
step: 5700, Loss: 0.12045188248157501
step: 5800, Loss: 0.11766362935304642
step: 5900, Loss: 0.11673323065042496
step: 6000, Loss: 0.11784966289997101
step: 6100, Loss: 0.11691935360431671
step: 6200, Loss: 0.11784406006336212
step: 6300, Loss: 0.11481384932994843
step: 6400, Loss: 0.11617711931467056
step: 6500, Loss: 0.1195947602391243
step: 6600, Loss: 0.11627940833568573
step: 6700, Loss: 0.11577574163675308
step: 6800, Loss: 0.1143956333398819
step: 6900, Loss: 0.11702726781368256
step: 7000, Loss: 0.11698956787586212
step: 7100, Loss: 0.11478762328624725
step: 7200, Loss: 0.19575881958007812
step: 7300, Loss: 0.11579909175634384
step: 7400, Loss: 0.1168900802731514
step: 7500, Loss: 0.11845279484987259
step: 7600, Loss: 0.11462840437889099
step: 7700, Loss: 0.11594512313604355
step: 7800, Loss: 0.11430247873067856
step: 7900, Loss: 0.11414745450019836
step: 8000, Loss: 0.11415395140647888
step: 8100, Loss: 0.11302222311496735
step: 8200, Loss: 0.11452865600585938
step: 8300, Loss: 0.1138390377163887
step: 8400, Loss: 0.1145223006606102
step: 8500, Loss: 0.11368328332901001
step: 8600, Loss: 0.11422320455312729
step: 8700, Loss: 0.11384586244821548
step: 8800, Loss: 0.11358633637428284
step: 8900, Loss: 0.11483200639486313
step: 9000, Loss: 0.11388343572616577
step: 9100, Loss: 0.19205716252326965
step: 9200, Loss: 0.11439764499664307
step: 9300, Loss: 0.11443294584751129
step: 9400, Loss: 0.11515771597623825
step: 9500, Loss: 0.114126056432724
step: 9600, Loss: 0.11491569876670837
step: 9700, Loss: 0.11462269723415375
step: 9800, Loss: 0.11318299174308777
step: 9900, Loss: 0.1142457127571106
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.17921550571918488
step: 100, Loss: 0.12351204454898834
step: 200, Loss: 0.1164013221859932
step: 300, Loss: 0.11901870369911194
step: 400, Loss: 0.1142774149775505
step: 500, Loss: 0.11509891599416733
step: 600, Loss: 0.11552727967500687
step: 700, Loss: 0.11524974554777145
step: 800, Loss: 0.11400140821933746
step: 900, Loss: 0.11377187073230743
step: 1000, Loss: 0.11382057517766953
step: 1100, Loss: 0.11498957872390747
step: 1200, Loss: 0.11324848234653473
step: 1300, Loss: 0.11548832058906555
step: 1400, Loss: 0.11299577355384827
step: 1500, Loss: 0.1911378651857376
step: 1600, Loss: 0.11337500810623169
step: 1700, Loss: 0.11312737315893173
step: 1800, Loss: 0.1150507926940918
step: 1900, Loss: 0.11302177608013153
step: 2000, Loss: 0.11458712816238403
step: 2100, Loss: 0.11391636729240417
step: 2200, Loss: 0.11404276639223099
step: 2300, Loss: 0.11554016172885895
step: 2400, Loss: 0.11349497735500336
step: 2500, Loss: 0.11434387415647507
step: 2600, Loss: 0.11298725008964539
step: 2700, Loss: 0.11399607360363007
step: 2800, Loss: 0.1138429045677185
step: 2900, Loss: 0.11435550451278687
step: 3000, Loss: 0.11453729122877121
step: 3100, Loss: 0.11340421438217163
step: 3200, Loss: 0.11580924689769745
step: 3300, Loss: 0.11348074674606323
step: 3400, Loss: 0.19205345213413239
step: 3500, Loss: 0.11523322761058807
step: 3600, Loss: 0.11448690295219421
step: 3700, Loss: 0.11436322331428528
step: 3800, Loss: 0.11404037475585938
step: 3900, Loss: 0.12032877653837204
step: 4000, Loss: 0.2499641627073288
step: 4100, Loss: 0.14769649505615234
step: 4200, Loss: 0.1295377016067505
step: 4300, Loss: 0.1236288845539093
step: 4400, Loss: 0.12907692790031433
step: 4500, Loss: 0.12177840620279312
step: 4600, Loss: 0.11711113154888153
step: 4700, Loss: 0.11996274441480637
step: 4800, Loss: 0.12332199513912201
step: 4900, Loss: 0.11841685324907303
step: 5000, Loss: 0.11997932195663452
step: 5100, Loss: 0.11707055568695068
step: 5200, Loss: 0.11922241747379303
step: 5300, Loss: 0.19845068454742432
step: 5400, Loss: 0.11684241890907288
step: 5500, Loss: 0.12084239721298218
step: 5600, Loss: 0.11540214717388153
step: 5700, Loss: 0.1160707175731659
step: 5800, Loss: 0.11666847765445709
step: 5900, Loss: 0.11607277393341064
step: 6000, Loss: 0.116688072681427
step: 6100, Loss: 0.11690845340490341
step: 6200, Loss: 0.1139623150229454
step: 6300, Loss: 0.11406243592500687
step: 6400, Loss: 0.1157129555940628
step: 6500, Loss: 0.11450308561325073
step: 6600, Loss: 0.11492971330881119
step: 6700, Loss: 0.11724705994129181
step: 6800, Loss: 0.11393570899963379
step: 6900, Loss: 0.11541390419006348
step: 7000, Loss: 0.11540137231349945
step: 7100, Loss: 0.11394408345222473
step: 7200, Loss: 0.19737660884857178
step: 7300, Loss: 0.11523114144802094
step: 7400, Loss: 0.11628206819295883
step: 7500, Loss: 0.11530394852161407
step: 7600, Loss: 0.11437182128429413
step: 7700, Loss: 0.1145075112581253
step: 7800, Loss: 0.11483029276132584
step: 7900, Loss: 0.11414662003517151
step: 8000, Loss: 0.11466078460216522
step: 8100, Loss: 0.11484608054161072
step: 8200, Loss: 0.11347602307796478
step: 8300, Loss: 0.11364801973104477
step: 8400, Loss: 0.11296730488538742
step: 8500, Loss: 0.11386141180992126
step: 8600, Loss: 0.11621937155723572
step: 8700, Loss: 0.11282702535390854
step: 8800, Loss: 0.11405449360609055
step: 8900, Loss: 0.11371709406375885
step: 9000, Loss: 0.11343204230070114
step: 9100, Loss: 0.19251501560211182
step: 9200, Loss: 0.11357865482568741
step: 9300, Loss: 0.11406329274177551
step: 9400, Loss: 0.11485166847705841
step: 9500, Loss: 0.11409325152635574
step: 9600, Loss: 0.11321745812892914
step: 9700, Loss: 0.11275407671928406
step: 9800, Loss: 0.11376768350601196
step: 9900, Loss: 0.11455164849758148
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.96875
recall: 1.0
F_score: 0.9841269841269841
subject 21 Avgacc: 0.9708333333333334 Avgfscore: 0.9678827537844097 
 Max acc:1.0, Max f score:1.0
******** mix subject_22 ********

[285, 475]
******fold 1******

Training... train_data length:855
step: 0, Loss: 31.389606475830078
step: 100, Loss: 5.913870811462402
step: 200, Loss: 3.010314702987671
step: 300, Loss: 1.5560492277145386
step: 400, Loss: 1.4142168760299683
step: 500, Loss: 0.650509238243103
step: 600, Loss: 2.546074151992798
step: 700, Loss: 1.1535061597824097
step: 800, Loss: 0.5125166177749634
step: 900, Loss: 0.3032112717628479
step: 1000, Loss: 0.9184094667434692
step: 1100, Loss: 0.6713142395019531
step: 1200, Loss: 0.15255951881408691
step: 1300, Loss: 0.1554962396621704
step: 1400, Loss: 0.15499532222747803
step: 1500, Loss: 0.1664840131998062
step: 1600, Loss: 0.15065351128578186
step: 1700, Loss: 0.14531421661376953
step: 1800, Loss: 0.14130492508411407
step: 1900, Loss: 0.14072397351264954
step: 2000, Loss: 0.14368367195129395
step: 2100, Loss: 0.14441177248954773
step: 2200, Loss: 0.1425657421350479
step: 2300, Loss: 0.1359526813030243
step: 2400, Loss: 0.1336939036846161
step: 2500, Loss: 0.1300826370716095
step: 2600, Loss: 0.12392791360616684
step: 2700, Loss: 0.14015138149261475
step: 2800, Loss: 0.1361279934644699
step: 2900, Loss: 0.12839406728744507
step: 3000, Loss: 0.12600819766521454
step: 3100, Loss: 0.12559294700622559
step: 3200, Loss: 0.12760025262832642
step: 3300, Loss: 0.1262015998363495
step: 3400, Loss: 0.13113044202327728
step: 3500, Loss: 0.12896469235420227
step: 3600, Loss: 0.12639401853084564
step: 3700, Loss: 0.12625452876091003
step: 3800, Loss: 0.1265978366136551
step: 3900, Loss: 0.13544507324695587
step: 4000, Loss: 0.1273586004972458
step: 4100, Loss: 0.1288975328207016
step: 4200, Loss: 0.12278294563293457
step: 4300, Loss: 7.426443099975586
step: 4400, Loss: 0.6820103526115417
step: 4500, Loss: 0.8379819393157959
step: 4600, Loss: 0.2674115300178528
step: 4700, Loss: 0.14237917959690094
step: 4800, Loss: 0.14103637635707855
step: 4900, Loss: 0.15132376551628113
step: 5000, Loss: 0.14164584875106812
step: 5100, Loss: 0.13158875703811646
step: 5200, Loss: 0.1433097869157791
step: 5300, Loss: 0.1300172209739685
step: 5400, Loss: 0.1297069638967514
step: 5500, Loss: 0.13061973452568054
step: 5600, Loss: 0.13160362839698792
step: 5700, Loss: 0.12348590791225433
step: 5800, Loss: 0.13633859157562256
step: 5900, Loss: 0.12393185496330261
step: 6000, Loss: 0.12657442688941956
step: 6100, Loss: 0.12794581055641174
step: 6200, Loss: 0.12649913132190704
step: 6300, Loss: 0.12238078564405441
step: 6400, Loss: 0.132437065243721
step: 6500, Loss: 0.12191706150770187
step: 6600, Loss: 0.12568387389183044
step: 6700, Loss: 0.1245148628950119
step: 6800, Loss: 0.12308363616466522
step: 6900, Loss: 0.11957179009914398
step: 7000, Loss: 0.12781502306461334
step: 7100, Loss: 0.11779563128948212
step: 7200, Loss: 0.12084938585758209
step: 7300, Loss: 0.12103959172964096
step: 7400, Loss: 0.1254126876592636
step: 7500, Loss: 0.12237247824668884
step: 7600, Loss: 0.12451580166816711
step: 7700, Loss: 0.11542569845914841
step: 7800, Loss: 0.11851518601179123
step: 7900, Loss: 0.11833325773477554
step: 8000, Loss: 0.12812530994415283
step: 8100, Loss: 0.13735856115818024
step: 8200, Loss: 1.3228988647460938
step: 8300, Loss: 2.1163437366485596
step: 8400, Loss: 0.21520064771175385
step: 8500, Loss: 0.13934685289859772
step: 8600, Loss: 0.15135610103607178
step: 8700, Loss: 0.14450962841510773
step: 8800, Loss: 0.13383445143699646
step: 8900, Loss: 0.13058669865131378
step: 9000, Loss: 0.12907180190086365
step: 9100, Loss: 0.12384733557701111
step: 9200, Loss: 0.12981925904750824
step: 9300, Loss: 0.12948784232139587
step: 9400, Loss: 0.12685900926589966
step: 9500, Loss: 0.1273898184299469
step: 9600, Loss: 0.12311496585607529
step: 9700, Loss: 0.12945237755775452
step: 9800, Loss: 0.12847834825515747
step: 9900, Loss: 0.12321306765079498
training successfully ended.
validating...
validate data length:95
acc: 0.9204545454545454
precision: 0.9183673469387755
recall: 0.9375
F_score: 0.9278350515463918
******fold 2******

Training... train_data length:855
step: 0, Loss: 0.1441572606563568
step: 100, Loss: 0.21900904178619385
step: 200, Loss: 0.13693207502365112
step: 300, Loss: 0.13806921243667603
step: 400, Loss: 0.1385354995727539
step: 500, Loss: 0.1287735402584076
step: 600, Loss: 0.12279718369245529
step: 700, Loss: 0.12467961758375168
step: 800, Loss: 0.1279584914445877
step: 900, Loss: 0.12630370259284973
step: 1000, Loss: 0.12626910209655762
step: 1100, Loss: 0.12631316483020782
step: 1200, Loss: 0.11687320470809937
step: 1300, Loss: 0.11797565221786499
step: 1400, Loss: 0.12735581398010254
step: 1500, Loss: 0.12150534987449646
step: 1600, Loss: 0.12019370496273041
step: 1700, Loss: 0.12466910481452942
step: 1800, Loss: 0.11747627705335617
step: 1900, Loss: 0.1191401407122612
step: 2000, Loss: 0.12804800271987915
step: 2100, Loss: 0.14589440822601318
step: 2200, Loss: 4.336813449859619
step: 2300, Loss: 0.9662566184997559
step: 2400, Loss: 0.15348295867443085
step: 2500, Loss: 0.14900891482830048
step: 2600, Loss: 0.159816175699234
step: 2700, Loss: 0.1419447809457779
step: 2800, Loss: 0.1433834731578827
step: 2900, Loss: 0.1466643512248993
step: 3000, Loss: 0.1302420198917389
step: 3100, Loss: 0.13132986426353455
step: 3200, Loss: 0.13477042317390442
step: 3300, Loss: 0.1230541542172432
step: 3400, Loss: 0.13064831495285034
step: 3500, Loss: 0.13870999217033386
step: 3600, Loss: 0.12138927727937698
step: 3700, Loss: 0.12569133937358856
step: 3800, Loss: 0.1287471503019333
step: 3900, Loss: 0.12135935574769974
step: 4000, Loss: 0.12570635974407196
step: 4100, Loss: 0.13231313228607178
step: 4200, Loss: 0.12013736367225647
step: 4300, Loss: 0.11891576647758484
step: 4400, Loss: 0.12297633290290833
step: 4500, Loss: 0.1195567175745964
step: 4600, Loss: 0.12255968153476715
step: 4700, Loss: 0.12498237937688828
step: 4800, Loss: 0.1164751797914505
step: 4900, Loss: 0.11589115858078003
step: 5000, Loss: 0.11806182563304901
step: 5100, Loss: 0.11744587123394012
step: 5200, Loss: 0.12468957901000977
step: 5300, Loss: 0.12163525819778442
step: 5400, Loss: 0.11575772613286972
step: 5500, Loss: 0.11738099157810211
step: 5600, Loss: 0.11586633324623108
step: 5700, Loss: 0.11541137099266052
step: 5800, Loss: 0.11929946392774582
step: 5900, Loss: 0.1201419085264206
step: 6000, Loss: 0.11676084995269775
step: 6100, Loss: 0.11749419569969177
step: 6200, Loss: 0.11604713648557663
step: 6300, Loss: 0.11615191400051117
step: 6400, Loss: 0.11966073513031006
step: 6500, Loss: 0.12013950943946838
step: 6600, Loss: 0.11393700540065765
step: 6700, Loss: 0.11789244413375854
step: 6800, Loss: 0.11697813868522644
step: 6900, Loss: 0.11665741354227066
step: 7000, Loss: 0.11625448614358902
step: 7100, Loss: 0.12631364166736603
step: 7200, Loss: 1.4149339199066162
step: 7300, Loss: 0.18313127756118774
step: 7400, Loss: 0.2196689248085022
step: 7500, Loss: 0.12971946597099304
step: 7600, Loss: 0.1356482207775116
step: 7700, Loss: 0.12930411100387573
step: 7800, Loss: 0.1262848824262619
step: 7900, Loss: 0.1307656317949295
step: 8000, Loss: 0.12297500669956207
step: 8100, Loss: 0.12094441801309586
step: 8200, Loss: 0.12865892052650452
step: 8300, Loss: 0.12218986451625824
step: 8400, Loss: 0.12359382212162018
step: 8500, Loss: 0.12719270586967468
step: 8600, Loss: 0.11849766969680786
step: 8700, Loss: 0.11953181028366089
step: 8800, Loss: 0.12184546142816544
step: 8900, Loss: 0.12131263315677643
step: 9000, Loss: 0.11888554692268372
step: 9100, Loss: 0.11866240203380585
step: 9200, Loss: 0.1187690868973732
step: 9300, Loss: 0.11806631088256836
step: 9400, Loss: 0.11798180639743805
step: 9500, Loss: 0.11848428100347519
step: 9600, Loss: 0.12139015644788742
step: 9700, Loss: 0.11724788695573807
step: 9800, Loss: 0.115943044424057
step: 9900, Loss: 0.11472156643867493
training successfully ended.
validating...
validate data length:95
acc: 0.9545454545454546
precision: 0.9047619047619048
recall: 1.0
F_score: 0.9500000000000001
******fold 3******

Training... train_data length:855
step: 0, Loss: 0.33114728331565857
step: 100, Loss: 0.13893933594226837
step: 200, Loss: 0.12248221784830093
step: 300, Loss: 0.122634157538414
step: 400, Loss: 0.1288650929927826
step: 500, Loss: 0.11681585013866425
step: 600, Loss: 0.11960873752832413
step: 700, Loss: 0.11608024686574936
step: 800, Loss: 0.11906212568283081
step: 900, Loss: 0.12609712779521942
step: 1000, Loss: 0.1226501315832138
step: 1100, Loss: 0.1193825900554657
step: 1200, Loss: 0.11807672679424286
step: 1300, Loss: 0.11579031497240067
step: 1400, Loss: 0.12550325691699982
step: 1500, Loss: 0.11620867252349854
step: 1600, Loss: 0.12854015827178955
step: 1700, Loss: 0.12113820016384125
step: 1800, Loss: 11.834681510925293
step: 1900, Loss: 0.3715680241584778
step: 2000, Loss: 0.18758699297904968
step: 2100, Loss: 0.15379846096038818
step: 2200, Loss: 0.13857078552246094
step: 2300, Loss: 0.12656170129776
step: 2400, Loss: 0.1332612931728363
step: 2500, Loss: 0.13254903256893158
step: 2600, Loss: 0.12448038160800934
step: 2700, Loss: 0.12813988327980042
step: 2800, Loss: 0.12610626220703125
step: 2900, Loss: 0.11928938329219818
step: 3000, Loss: 0.12606477737426758
step: 3100, Loss: 0.12464214116334915
step: 3200, Loss: 0.11856363713741302
step: 3300, Loss: 0.12329020351171494
step: 3400, Loss: 0.12039318680763245
step: 3500, Loss: 0.11730044335126877
step: 3600, Loss: 0.1209663525223732
step: 3700, Loss: 0.11709922552108765
step: 3800, Loss: 0.11679519712924957
step: 3900, Loss: 0.11706997454166412
step: 4000, Loss: 0.11918500065803528
step: 4100, Loss: 0.11672982573509216
step: 4200, Loss: 0.11933332681655884
step: 4300, Loss: 0.12000849097967148
step: 4400, Loss: 0.11570555716753006
step: 4500, Loss: 0.11854048073291779
step: 4600, Loss: 0.12042704224586487
step: 4700, Loss: 0.1150890439748764
step: 4800, Loss: 0.11790189892053604
step: 4900, Loss: 0.11670204997062683
step: 5000, Loss: 0.11469534039497375
step: 5100, Loss: 0.11810652911663055
step: 5200, Loss: 0.11594156920909882
step: 5300, Loss: 0.11582499742507935
step: 5400, Loss: 0.11750239878892899
step: 5500, Loss: 0.11674600839614868
step: 5600, Loss: 0.11834974586963654
step: 5700, Loss: 0.11694872379302979
step: 5800, Loss: 0.11442817747592926
step: 5900, Loss: 0.11784560978412628
step: 6000, Loss: 0.11586505174636841
step: 6100, Loss: 0.11552593857049942
step: 6200, Loss: 0.1158490851521492
step: 6300, Loss: 0.1154680997133255
step: 6400, Loss: 0.11763632297515869
step: 6500, Loss: 0.11657366901636124
step: 6600, Loss: 0.12035320699214935
step: 6700, Loss: 0.11402390152215958
step: 6800, Loss: 0.11728307604789734
step: 6900, Loss: 1.767494559288025
step: 7000, Loss: 2.1880300045013428
step: 7100, Loss: 0.14114001393318176
step: 7200, Loss: 0.1469031125307083
step: 7300, Loss: 0.13150450587272644
step: 7400, Loss: 0.13203883171081543
step: 7500, Loss: 0.14138677716255188
step: 7600, Loss: 0.13465473055839539
step: 7700, Loss: 0.12297394871711731
step: 7800, Loss: 0.12802650034427643
step: 7900, Loss: 0.12380532920360565
step: 8000, Loss: 0.12161454558372498
step: 8100, Loss: 0.13251851499080658
step: 8200, Loss: 0.1294475495815277
step: 8300, Loss: 0.12054168432950974
step: 8400, Loss: 0.12122568488121033
step: 8500, Loss: 0.11835332214832306
step: 8600, Loss: 0.11926498264074326
step: 8700, Loss: 0.12345963716506958
step: 8800, Loss: 0.12437830865383148
step: 8900, Loss: 0.11774993687868118
step: 9000, Loss: 0.11786133050918579
step: 9100, Loss: 0.11657416075468063
step: 9200, Loss: 0.11824363470077515
step: 9300, Loss: 0.11948864161968231
step: 9400, Loss: 0.11985302716493607
step: 9500, Loss: 0.11849334090948105
step: 9600, Loss: 0.11697586625814438
step: 9700, Loss: 0.1152419000864029
step: 9800, Loss: 0.1173592209815979
step: 9900, Loss: 0.12164206802845001
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 0.9333333333333333
recall: 1.0
F_score: 0.9655172413793104
******fold 4******

Training... train_data length:855
step: 0, Loss: 0.11867380142211914
step: 100, Loss: 0.12687169015407562
step: 200, Loss: 0.11750852316617966
step: 300, Loss: 0.12249443680047989
step: 400, Loss: 0.12017049640417099
step: 500, Loss: 0.115162193775177
step: 600, Loss: 0.11797056347131729
step: 700, Loss: 0.11494699120521545
step: 800, Loss: 0.11687485873699188
step: 900, Loss: 0.11775719374418259
step: 1000, Loss: 0.11544356495141983
step: 1100, Loss: 0.11427349597215652
step: 1200, Loss: 0.1145666241645813
step: 1300, Loss: 0.11577373743057251
step: 1400, Loss: 0.11571846902370453
step: 1500, Loss: 0.11828404664993286
step: 1600, Loss: 0.11926433444023132
step: 1700, Loss: 0.11592058092355728
step: 1800, Loss: 0.11437822878360748
step: 1900, Loss: 0.11489468812942505
step: 2000, Loss: 0.11497385054826736
step: 2100, Loss: 0.12244091928005219
step: 2200, Loss: 0.11582598835229874
step: 2300, Loss: 0.11465926468372345
step: 2400, Loss: 0.11540146917104721
step: 2500, Loss: 0.11885891854763031
step: 2600, Loss: 0.23692858219146729
step: 2700, Loss: 1.5684521198272705
step: 2800, Loss: 0.2962489128112793
step: 2900, Loss: 0.13790565729141235
step: 3000, Loss: 0.13161951303482056
step: 3100, Loss: 0.13757361471652985
step: 3200, Loss: 0.14106465876102448
step: 3300, Loss: 0.134720578789711
step: 3400, Loss: 0.13346458971500397
step: 3500, Loss: 0.12323067337274551
step: 3600, Loss: 0.12560532987117767
step: 3700, Loss: 0.125043123960495
step: 3800, Loss: 0.12825706601142883
step: 3900, Loss: 0.12367933243513107
step: 4000, Loss: 0.12335778027772903
step: 4100, Loss: 0.11861647665500641
step: 4200, Loss: 0.12009044736623764
step: 4300, Loss: 0.1243724673986435
step: 4400, Loss: 0.12565839290618896
step: 4500, Loss: 0.12006162106990814
step: 4600, Loss: 0.11974781006574631
step: 4700, Loss: 0.11832970380783081
step: 4800, Loss: 0.11882255226373672
step: 4900, Loss: 0.12081794440746307
step: 5000, Loss: 0.12153784930706024
step: 5100, Loss: 0.1202709823846817
step: 5200, Loss: 0.11605574190616608
step: 5300, Loss: 0.11562839150428772
step: 5400, Loss: 0.11723984777927399
step: 5500, Loss: 0.11698737740516663
step: 5600, Loss: 0.1232868880033493
step: 5700, Loss: 0.11628001183271408
step: 5800, Loss: 0.11410195380449295
step: 5900, Loss: 0.11401133984327316
step: 6000, Loss: 0.11696258932352066
step: 6100, Loss: 0.11686594039201736
step: 6200, Loss: 0.11734607070684433
step: 6300, Loss: 0.11644721776247025
step: 6400, Loss: 0.11582186818122864
step: 6500, Loss: 0.11335960775613785
step: 6600, Loss: 0.1152111366391182
step: 6700, Loss: 0.11650380492210388
step: 6800, Loss: 0.11462448537349701
step: 6900, Loss: 0.12278200685977936
step: 7000, Loss: 0.11688042432069778
step: 7100, Loss: 0.11748621612787247
step: 7200, Loss: 0.1145545244216919
step: 7300, Loss: 0.11521317064762115
step: 7400, Loss: 0.11572472751140594
step: 7500, Loss: 0.1146385669708252
step: 7600, Loss: 0.11447116732597351
step: 7700, Loss: 0.11385196447372437
step: 7800, Loss: 0.11456236988306046
step: 7900, Loss: 0.11544288694858551
step: 8000, Loss: 0.11568933725357056
step: 8100, Loss: 0.11407337337732315
step: 8200, Loss: 0.11558304727077484
step: 8300, Loss: 0.11534351110458374
step: 8400, Loss: 0.11581282317638397
step: 8500, Loss: 0.11583555489778519
step: 8600, Loss: 0.11602839827537537
step: 8700, Loss: 0.11751557886600494
step: 8800, Loss: 0.11381776630878448
step: 8900, Loss: 0.11393748968839645
step: 9000, Loss: 0.11652486026287079
step: 9100, Loss: 0.11461514234542847
step: 9200, Loss: 0.11719302088022232
step: 9300, Loss: 0.11722709238529205
step: 9400, Loss: 0.11755675822496414
step: 9500, Loss: 3.0771987438201904
step: 9600, Loss: 0.16418492794036865
step: 9700, Loss: 0.12605047225952148
step: 9800, Loss: 0.13373948633670807
step: 9900, Loss: 0.14203530550003052
training successfully ended.
validating...
validate data length:95
acc: 0.9886363636363636
precision: 1.0
recall: 0.9761904761904762
F_score: 0.9879518072289156
******fold 5******

Training... train_data length:855
step: 0, Loss: 0.12347665429115295
step: 100, Loss: 0.13276273012161255
step: 200, Loss: 0.12429875880479813
step: 300, Loss: 0.12659747898578644
step: 400, Loss: 0.12316848337650299
step: 500, Loss: 0.11893071979284286
step: 600, Loss: 0.11807925254106522
step: 700, Loss: 0.12013204395771027
step: 800, Loss: 0.11696317046880722
step: 900, Loss: 0.11932267993688583
step: 1000, Loss: 0.1175936609506607
step: 1100, Loss: 0.11513642221689224
step: 1200, Loss: 0.4983929395675659
step: 1300, Loss: 0.14426858723163605
step: 1400, Loss: 0.13535961508750916
step: 1500, Loss: 0.14657998085021973
step: 1600, Loss: 0.12940630316734314
step: 1700, Loss: 0.12633149325847626
step: 1800, Loss: 0.12170129269361496
step: 1900, Loss: 0.12081117182970047
step: 2000, Loss: 0.1203484982252121
step: 2100, Loss: 0.12498313188552856
step: 2200, Loss: 0.12285911291837692
step: 2300, Loss: 0.11735893040895462
step: 2400, Loss: 0.12164615839719772
step: 2500, Loss: 0.11713886260986328
step: 2600, Loss: 0.11794787645339966
step: 2700, Loss: 0.1205851286649704
step: 2800, Loss: 0.1173744797706604
step: 2900, Loss: 0.1165037527680397
step: 3000, Loss: 0.11781420558691025
step: 3100, Loss: 0.11577728390693665
step: 3200, Loss: 0.11434538662433624
step: 3300, Loss: 0.11864126473665237
step: 3400, Loss: 0.11614678800106049
step: 3500, Loss: 0.1145540177822113
step: 3600, Loss: 0.11693531274795532
step: 3700, Loss: 0.11645349860191345
step: 3800, Loss: 0.11449030786752701
step: 3900, Loss: 0.11872698366641998
step: 4000, Loss: 0.115602508187294
step: 4100, Loss: 0.11382099241018295
step: 4200, Loss: 0.11500334739685059
step: 4300, Loss: 0.11634790897369385
step: 4400, Loss: 0.1133369654417038
step: 4500, Loss: 0.11782574653625488
step: 4600, Loss: 0.11401486396789551
step: 4700, Loss: 0.11420048773288727
step: 4800, Loss: 0.11430078744888306
step: 4900, Loss: 0.11584337800741196
step: 5000, Loss: 0.11493444442749023
step: 5100, Loss: 0.11530537903308868
step: 5200, Loss: 0.11466780304908752
step: 5300, Loss: 0.1145457774400711
step: 5400, Loss: 0.11465207487344742
step: 5500, Loss: 0.1138605922460556
step: 5600, Loss: 0.11362537741661072
step: 5700, Loss: 0.11763607710599899
step: 5800, Loss: 0.1149817630648613
step: 5900, Loss: 0.11571422219276428
step: 6000, Loss: 0.11496753990650177
step: 6100, Loss: 0.11491167545318604
step: 6200, Loss: 0.1157095730304718
step: 6300, Loss: 0.1154618114233017
step: 6400, Loss: 0.11604556441307068
step: 6500, Loss: 0.1143369972705841
step: 6600, Loss: 0.11452475935220718
step: 6700, Loss: 0.11671903729438782
step: 6800, Loss: 0.11520818620920181
step: 6900, Loss: 2.2019448280334473
step: 7000, Loss: 0.22966638207435608
step: 7100, Loss: 0.1398424357175827
step: 7200, Loss: 0.1312818080186844
step: 7300, Loss: 0.13126879930496216
step: 7400, Loss: 0.1268913894891739
step: 7500, Loss: 0.1298883557319641
step: 7600, Loss: 0.12383026629686356
step: 7700, Loss: 0.11961813271045685
step: 7800, Loss: 0.12329494953155518
step: 7900, Loss: 0.12083874642848969
step: 8000, Loss: 0.12202351540327072
step: 8100, Loss: 0.12054258584976196
step: 8200, Loss: 0.12150578945875168
step: 8300, Loss: 0.11694592237472534
step: 8400, Loss: 0.11822890490293503
step: 8500, Loss: 0.11956052482128143
step: 8600, Loss: 0.11982294917106628
step: 8700, Loss: 0.1169949322938919
step: 8800, Loss: 0.11909227073192596
step: 8900, Loss: 0.11556419730186462
step: 9000, Loss: 0.11915234476327896
step: 9100, Loss: 0.11608772724866867
step: 9200, Loss: 0.11573470383882523
step: 9300, Loss: 0.1171775832772255
step: 9400, Loss: 0.1158723309636116
step: 9500, Loss: 0.114430733025074
step: 9600, Loss: 0.11631505191326141
step: 9700, Loss: 0.11410804837942123
step: 9800, Loss: 0.11526589095592499
step: 9900, Loss: 0.11596360802650452
training successfully ended.
validating...
validate data length:95
acc: 0.9886363636363636
precision: 0.9811320754716981
recall: 1.0
F_score: 0.9904761904761905
******fold 6******

Training... train_data length:855
step: 0, Loss: 0.12414757907390594
step: 100, Loss: 0.12631385028362274
step: 200, Loss: 0.14017677307128906
step: 300, Loss: 0.12813805043697357
step: 400, Loss: 0.12264281511306763
step: 500, Loss: 0.12044811993837357
step: 600, Loss: 0.12102945894002914
step: 700, Loss: 0.1187155470252037
step: 800, Loss: 0.11971498280763626
step: 900, Loss: 0.12208853662014008
step: 1000, Loss: 0.11744065582752228
step: 1100, Loss: 0.5766312479972839
step: 1200, Loss: 0.17687204480171204
step: 1300, Loss: 0.1507030427455902
step: 1400, Loss: 0.14658920466899872
step: 1500, Loss: 0.1327054649591446
step: 1600, Loss: 0.12230873107910156
step: 1700, Loss: 0.12353543937206268
step: 1800, Loss: 0.1271568089723587
step: 1900, Loss: 0.1245884820818901
step: 2000, Loss: 0.14004310965538025
step: 2100, Loss: 0.12342765927314758
step: 2200, Loss: 0.12158419191837311
step: 2300, Loss: 0.11902879923582077
step: 2400, Loss: 0.12098328024148941
step: 2500, Loss: 0.12104883044958115
step: 2600, Loss: 0.1279737949371338
step: 2700, Loss: 0.1171942800283432
step: 2800, Loss: 0.11690746992826462
step: 2900, Loss: 0.11768031120300293
step: 3000, Loss: 0.12075448781251907
step: 3100, Loss: 0.1183982565999031
step: 3200, Loss: 0.12009725719690323
step: 3300, Loss: 0.11916494369506836
step: 3400, Loss: 0.11557695269584656
step: 3500, Loss: 0.11649857461452484
step: 3600, Loss: 0.11529038846492767
step: 3700, Loss: 0.11651689559221268
step: 3800, Loss: 0.11728817224502563
step: 3900, Loss: 0.11666791141033173
step: 4000, Loss: 0.11505294591188431
step: 4100, Loss: 0.11634363234043121
step: 4200, Loss: 0.11493648588657379
step: 4300, Loss: 0.11515330523252487
step: 4400, Loss: 0.1199241653084755
step: 4500, Loss: 0.11617657542228699
step: 4600, Loss: 0.11805248260498047
step: 4700, Loss: 0.11476552486419678
step: 4800, Loss: 0.11354202032089233
step: 4900, Loss: 0.1164841502904892
step: 5000, Loss: 0.11731692403554916
step: 5100, Loss: 0.11745801568031311
step: 5200, Loss: 0.11418332904577255
step: 5300, Loss: 0.11654742062091827
step: 5400, Loss: 0.11301616579294205
step: 5500, Loss: 0.11430296301841736
step: 5600, Loss: 0.116750068962574
step: 5700, Loss: 0.11604271829128265
step: 5800, Loss: 0.11485223472118378
step: 5900, Loss: 0.11392633616924286
step: 6000, Loss: 0.1139516532421112
step: 6100, Loss: 0.11736524105072021
step: 6200, Loss: 0.9680056571960449
step: 6300, Loss: 0.210410013794899
step: 6400, Loss: 0.13873901963233948
step: 6500, Loss: 0.12912926077842712
step: 6600, Loss: 0.136585533618927
step: 6700, Loss: 0.1345190852880478
step: 6800, Loss: 0.13745887577533722
step: 6900, Loss: 0.128932923078537
step: 7000, Loss: 0.12703390419483185
step: 7100, Loss: 0.12432393431663513
step: 7200, Loss: 0.12412196397781372
step: 7300, Loss: 0.1181773841381073
step: 7400, Loss: 0.12831175327301025
step: 7500, Loss: 0.12585201859474182
step: 7600, Loss: 0.11818623542785645
step: 7700, Loss: 0.12069154530763626
step: 7800, Loss: 0.11985622346401215
step: 7900, Loss: 0.11492039263248444
step: 8000, Loss: 0.12148469686508179
step: 8100, Loss: 0.1218739002943039
step: 8200, Loss: 0.11592964828014374
step: 8300, Loss: 0.11591464281082153
step: 8400, Loss: 0.11806931346654892
step: 8500, Loss: 0.11535592377185822
step: 8600, Loss: 0.11899329721927643
step: 8700, Loss: 0.11693774163722992
step: 8800, Loss: 0.11443990468978882
step: 8900, Loss: 0.11583449691534042
step: 9000, Loss: 0.1162126213312149
step: 9100, Loss: 0.11333658546209335
step: 9200, Loss: 0.11691412329673767
step: 9300, Loss: 0.11845694482326508
step: 9400, Loss: 0.11467527598142624
step: 9500, Loss: 0.11465960741043091
step: 9600, Loss: 0.11535102874040604
step: 9700, Loss: 0.11370806396007538
step: 9800, Loss: 0.12102924287319183
step: 9900, Loss: 0.11841902136802673
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 0.9302325581395349
recall: 1.0
F_score: 0.963855421686747
******fold 7******

Training... train_data length:855
step: 0, Loss: 0.12754131853580475
step: 100, Loss: 0.12837621569633484
step: 200, Loss: 0.13027535378932953
step: 300, Loss: 0.13090966641902924
step: 400, Loss: 0.11894206702709198
step: 500, Loss: 0.12043401598930359
step: 600, Loss: 0.12047990411520004
step: 700, Loss: 0.11804871261119843
step: 800, Loss: 0.12077373266220093
step: 900, Loss: 0.11918509006500244
step: 1000, Loss: 0.11657197773456573
step: 1100, Loss: 0.11705098301172256
step: 1200, Loss: 0.18602272868156433
step: 1300, Loss: 0.14574891328811646
step: 1400, Loss: 0.14288164675235748
step: 1500, Loss: 0.136857807636261
step: 1600, Loss: 0.12581504881381989
step: 1700, Loss: 0.12643539905548096
step: 1800, Loss: 0.12374809384346008
step: 1900, Loss: 0.12164828926324844
step: 2000, Loss: 0.12694185972213745
step: 2100, Loss: 0.12306591123342514
step: 2200, Loss: 0.1201014593243599
step: 2300, Loss: 0.1228579729795456
step: 2400, Loss: 0.11929850280284882
step: 2500, Loss: 0.11960046738386154
step: 2600, Loss: 0.12157178670167923
step: 2700, Loss: 0.1168157085776329
step: 2800, Loss: 0.11663312464952469
step: 2900, Loss: 0.1171814575791359
step: 3000, Loss: 0.1167297288775444
step: 3100, Loss: 0.11579550057649612
step: 3200, Loss: 0.11803571879863739
step: 3300, Loss: 0.11571095883846283
step: 3400, Loss: 0.11463897675275803
step: 3500, Loss: 0.11510787904262543
step: 3600, Loss: 0.11552631855010986
step: 3700, Loss: 0.11511727422475815
step: 3800, Loss: 0.11588136106729507
step: 3900, Loss: 0.11708855628967285
step: 4000, Loss: 0.11447618901729584
step: 4100, Loss: 0.11630284786224365
step: 4200, Loss: 0.11681564152240753
step: 4300, Loss: 0.11725389957427979
step: 4400, Loss: 0.11566367745399475
step: 4500, Loss: 0.11736224591732025
step: 4600, Loss: 0.11335582286119461
step: 4700, Loss: 0.1152271032333374
step: 4800, Loss: 0.11424656957387924
step: 4900, Loss: 0.11427665501832962
step: 5000, Loss: 0.11448162794113159
step: 5100, Loss: 0.1145804226398468
step: 5200, Loss: 0.11372456699609756
step: 5300, Loss: 0.12386377155780792
step: 5400, Loss: 0.11586326360702515
step: 5500, Loss: 0.11687764525413513
step: 5600, Loss: 0.11445624381303787
step: 5700, Loss: 0.11800111085176468
step: 5800, Loss: 0.11411505192518234
step: 5900, Loss: 0.11643186956644058
step: 6000, Loss: 0.11391740292310715
step: 6100, Loss: 0.11792927980422974
step: 6200, Loss: 0.11353275179862976
step: 6300, Loss: 0.11486877501010895
step: 6400, Loss: 0.113579660654068
step: 6500, Loss: 0.11489151418209076
step: 6600, Loss: 0.11409230530261993
step: 6700, Loss: 0.1157679483294487
step: 6800, Loss: 0.1147167831659317
step: 6900, Loss: 0.1172434538602829
step: 7000, Loss: 0.11405648291110992
step: 7100, Loss: 0.11551574617624283
step: 7200, Loss: 0.1171501949429512
step: 7300, Loss: 0.1153528019785881
step: 7400, Loss: 0.11539730429649353
step: 7500, Loss: 0.1462176889181137
step: 7600, Loss: 0.26250892877578735
step: 7700, Loss: 0.1718325912952423
step: 7800, Loss: 0.1306033432483673
step: 7900, Loss: 0.13027843832969666
step: 8000, Loss: 0.13565650582313538
step: 8100, Loss: 0.1311577558517456
step: 8200, Loss: 0.12683521211147308
step: 8300, Loss: 0.12181828916072845
step: 8400, Loss: 0.11894965916872025
step: 8500, Loss: 0.12032335996627808
step: 8600, Loss: 0.12211830914020538
step: 8700, Loss: 0.1277821958065033
step: 8800, Loss: 0.11972268670797348
step: 8900, Loss: 0.11843323707580566
step: 9000, Loss: 0.11562145501375198
step: 9100, Loss: 0.11993376165628433
step: 9200, Loss: 0.12132186442613602
step: 9300, Loss: 0.12108626961708069
step: 9400, Loss: 0.11687213182449341
step: 9500, Loss: 0.11755430698394775
step: 9600, Loss: 0.11569049954414368
step: 9700, Loss: 0.1172831803560257
step: 9800, Loss: 0.11828916519880295
step: 9900, Loss: 0.11826635897159576
training successfully ended.
validating...
validate data length:95
acc: 0.9318181818181818
precision: 0.8846153846153846
recall: 1.0
F_score: 0.9387755102040816
******fold 8******

Training... train_data length:855
step: 0, Loss: 0.12656047940254211
step: 100, Loss: 0.13618594408035278
step: 200, Loss: 0.13596133887767792
step: 300, Loss: 0.12904970347881317
step: 400, Loss: 0.12446774542331696
step: 500, Loss: 0.12121160328388214
step: 600, Loss: 0.1179317981004715
step: 700, Loss: 0.11940530687570572
step: 800, Loss: 0.12172316759824753
step: 900, Loss: 0.12389668077230453
step: 1000, Loss: 0.12071318924427032
step: 1100, Loss: 0.11849422752857208
step: 1200, Loss: 0.11434190720319748
step: 1300, Loss: 0.11699618399143219
step: 1400, Loss: 0.12233918160200119
step: 1500, Loss: 0.11882315576076508
step: 1600, Loss: 0.1172487735748291
step: 1700, Loss: 0.11486233025789261
step: 1800, Loss: 0.11455271393060684
step: 1900, Loss: 0.1157308965921402
step: 2000, Loss: 0.11778093874454498
step: 2100, Loss: 0.11952069401741028
step: 2200, Loss: 0.11467860639095306
step: 2300, Loss: 0.11603131890296936
step: 2400, Loss: 0.1142474040389061
step: 2500, Loss: 0.11526031792163849
step: 2600, Loss: 0.11402088403701782
step: 2700, Loss: 0.11658888310194016
step: 2800, Loss: 0.1135316789150238
step: 2900, Loss: 0.11464503407478333
step: 3000, Loss: 0.11551664769649506
step: 3100, Loss: 0.11456184834241867
step: 3200, Loss: 2.4320876598358154
step: 3300, Loss: 0.9241451621055603
step: 3400, Loss: 0.14590944349765778
step: 3500, Loss: 0.13865366578102112
step: 3600, Loss: 0.12985734641551971
step: 3700, Loss: 0.1284429281949997
step: 3800, Loss: 0.12774550914764404
step: 3900, Loss: 0.13147571682929993
step: 4000, Loss: 0.12166047096252441
step: 4100, Loss: 0.12551581859588623
step: 4200, Loss: 0.12196044623851776
step: 4300, Loss: 0.1188405305147171
step: 4400, Loss: 0.12415008991956711
step: 4500, Loss: 0.12358290702104568
step: 4600, Loss: 0.11700520664453506
step: 4700, Loss: 0.12128546088933945
step: 4800, Loss: 0.11915107071399689
step: 4900, Loss: 0.11774808168411255
step: 5000, Loss: 0.12045624852180481
step: 5100, Loss: 0.11838191747665405
step: 5200, Loss: 0.11537079513072968
step: 5300, Loss: 0.11683797836303711
step: 5400, Loss: 0.11776410043239594
step: 5500, Loss: 0.114576555788517
step: 5600, Loss: 0.1195654571056366
step: 5700, Loss: 0.11774325370788574
step: 5800, Loss: 0.11649098992347717
step: 5900, Loss: 0.11543525755405426
step: 6000, Loss: 0.11768577992916107
step: 6100, Loss: 0.11659549176692963
step: 6200, Loss: 0.11735621094703674
step: 6300, Loss: 0.11476482450962067
step: 6400, Loss: 0.1161242201924324
step: 6500, Loss: 0.11472751945257187
step: 6600, Loss: 0.11393745243549347
step: 6700, Loss: 0.1132092997431755
step: 6800, Loss: 0.11584165692329407
step: 6900, Loss: 0.11596062779426575
step: 7000, Loss: 0.11427799612283707
step: 7100, Loss: 0.11567699164152145
step: 7200, Loss: 0.11370441317558289
step: 7300, Loss: 0.11386969685554504
step: 7400, Loss: 0.11634893715381622
step: 7500, Loss: 0.11442872881889343
step: 7600, Loss: 0.1135125681757927
step: 7700, Loss: 0.11566907167434692
step: 7800, Loss: 0.11393088102340698
step: 7900, Loss: 0.1139344796538353
step: 8000, Loss: 0.11673013865947723
step: 8100, Loss: 0.11616800725460052
step: 8200, Loss: 0.11408001184463501
step: 8300, Loss: 0.11653129756450653
step: 8400, Loss: 0.11461683362722397
step: 8500, Loss: 0.1148274764418602
step: 8600, Loss: 0.11608599126338959
step: 8700, Loss: 0.11616075038909912
step: 8800, Loss: 0.11406626552343369
step: 8900, Loss: 0.11527398228645325
step: 9000, Loss: 0.11382237076759338
step: 9100, Loss: 0.11702204495668411
step: 9200, Loss: 0.11422205716371536
step: 9300, Loss: 3.0000717639923096
step: 9400, Loss: 0.17558707296848297
step: 9500, Loss: 0.134837806224823
step: 9600, Loss: 0.12850850820541382
step: 9700, Loss: 0.12531042098999023
step: 9800, Loss: 0.13306789100170135
step: 9900, Loss: 0.1300942301750183
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 0.9375
recall: 1.0
F_score: 0.967741935483871
******fold 9******

Training... train_data length:855
step: 0, Loss: 0.1221233531832695
step: 100, Loss: 0.12624675035476685
step: 200, Loss: 0.12853436172008514
step: 300, Loss: 0.12476067245006561
step: 400, Loss: 0.12141679227352142
step: 500, Loss: 0.12278790771961212
step: 600, Loss: 0.12361098825931549
step: 700, Loss: 0.11703675985336304
step: 800, Loss: 0.1195768266916275
step: 900, Loss: 0.11713346838951111
step: 1000, Loss: 0.11718037724494934
step: 1100, Loss: 0.11521849781274796
step: 1200, Loss: 0.11816462874412537
step: 1300, Loss: 0.11466990411281586
step: 1400, Loss: 0.1170056089758873
step: 1500, Loss: 0.11515271663665771
step: 1600, Loss: 0.11576566845178604
step: 1700, Loss: 0.11613278090953827
step: 1800, Loss: 0.1175888329744339
step: 1900, Loss: 0.11361054331064224
step: 2000, Loss: 0.11591066420078278
step: 2100, Loss: 0.11468952894210815
step: 2200, Loss: 0.12001103162765503
step: 2300, Loss: 0.6299760937690735
step: 2400, Loss: 0.15664729475975037
step: 2500, Loss: 0.1298118680715561
step: 2600, Loss: 0.1268448382616043
step: 2700, Loss: 0.1353737860918045
step: 2800, Loss: 0.12381276488304138
step: 2900, Loss: 0.11999104917049408
step: 3000, Loss: 0.1255447417497635
step: 3100, Loss: 0.12190723419189453
step: 3200, Loss: 0.12171168625354767
step: 3300, Loss: 0.12108729779720306
step: 3400, Loss: 0.12036651372909546
step: 3500, Loss: 0.11838403344154358
step: 3600, Loss: 0.11736705154180527
step: 3700, Loss: 0.12172091007232666
step: 3800, Loss: 0.11693765968084335
step: 3900, Loss: 0.1173437312245369
step: 4000, Loss: 0.1159033477306366
step: 4100, Loss: 0.11679623275995255
step: 4200, Loss: 0.11911413073539734
step: 4300, Loss: 0.11724826693534851
step: 4400, Loss: 0.11433367431163788
step: 4500, Loss: 0.11721451580524445
step: 4600, Loss: 0.11516325920820236
step: 4700, Loss: 0.11434650421142578
step: 4800, Loss: 0.11499480158090591
step: 4900, Loss: 0.11528810113668442
step: 5000, Loss: 0.11407655477523804
step: 5100, Loss: 0.11609416455030441
step: 5200, Loss: 0.11481771618127823
step: 5300, Loss: 0.11299652606248856
step: 5400, Loss: 0.11531419306993484
step: 5500, Loss: 0.11429912596940994
step: 5600, Loss: 0.11428329348564148
step: 5700, Loss: 0.11639202386140823
step: 5800, Loss: 0.11365143954753876
step: 5900, Loss: 0.11390978842973709
step: 6000, Loss: 0.11406660825014114
step: 6100, Loss: 0.11532685905694962
step: 6200, Loss: 0.11429156363010406
step: 6300, Loss: 0.11424220353364944
step: 6400, Loss: 0.11376229673624039
step: 6500, Loss: 0.11382444947957993
step: 6600, Loss: 0.11386790871620178
step: 6700, Loss: 0.11329486966133118
step: 6800, Loss: 0.11448200792074203
step: 6900, Loss: 0.11440461874008179
step: 7000, Loss: 0.11308825016021729
step: 7100, Loss: 0.11475229263305664
step: 7200, Loss: 0.11326795816421509
step: 7300, Loss: 0.11711408197879791
step: 7400, Loss: 0.11608166247606277
step: 7500, Loss: 0.11828812211751938
step: 7600, Loss: 0.11908525228500366
step: 7700, Loss: 0.12066960334777832
step: 7800, Loss: 0.11963462829589844
step: 7900, Loss: 1.9904987812042236
step: 8000, Loss: 0.2098088413476944
step: 8100, Loss: 0.14163312315940857
step: 8200, Loss: 0.14814704656600952
step: 8300, Loss: 0.13434259593486786
step: 8400, Loss: 0.13852791488170624
step: 8500, Loss: 0.12756875157356262
step: 8600, Loss: 0.12968119978904724
step: 8700, Loss: 0.1257111132144928
step: 8800, Loss: 0.1279064267873764
step: 8900, Loss: 0.12593436241149902
step: 9000, Loss: 0.12688730657100677
step: 9100, Loss: 0.11893802881240845
step: 9200, Loss: 0.12114128470420837
step: 9300, Loss: 0.11946666240692139
step: 9400, Loss: 0.12341723591089249
step: 9500, Loss: 0.11800147593021393
step: 9600, Loss: 0.12435256689786911
step: 9700, Loss: 0.11791806668043137
step: 9800, Loss: 0.11645320057868958
step: 9900, Loss: 0.11827994883060455
training successfully ended.
validating...
validate data length:95
acc: 0.9545454545454546
precision: 0.9230769230769231
recall: 1.0
F_score: 0.9600000000000001
******fold 10******

Training... train_data length:855
step: 0, Loss: 0.12655271589756012
step: 100, Loss: 0.13080888986587524
step: 200, Loss: 0.13077038526535034
step: 300, Loss: 0.12458343803882599
step: 400, Loss: 0.12464306503534317
step: 500, Loss: 0.11716246604919434
step: 600, Loss: 0.1187015101313591
step: 700, Loss: 0.11647789925336838
step: 800, Loss: 0.11932535469532013
step: 900, Loss: 0.11665277183055878
step: 1000, Loss: 0.114448681473732
step: 1100, Loss: 0.1161654144525528
step: 1200, Loss: 0.11627119034528732
step: 1300, Loss: 0.11524926126003265
step: 1400, Loss: 0.11893908679485321
step: 1500, Loss: 0.11865639686584473
step: 1600, Loss: 0.11363089829683304
step: 1700, Loss: 0.11838928610086441
step: 1800, Loss: 0.11556357890367508
step: 1900, Loss: 0.1153784692287445
step: 2000, Loss: 3.3499414920806885
step: 2100, Loss: 0.3338109850883484
step: 2200, Loss: 0.14301134645938873
step: 2300, Loss: 0.12638799846172333
step: 2400, Loss: 0.12875553965568542
step: 2500, Loss: 0.12198366969823837
step: 2600, Loss: 0.1233997717499733
step: 2700, Loss: 0.12196075916290283
step: 2800, Loss: 0.12448939681053162
step: 2900, Loss: 0.11797342449426651
step: 3000, Loss: 0.12067236751317978
step: 3100, Loss: 0.1188640147447586
step: 3200, Loss: 0.11924168467521667
step: 3300, Loss: 0.11813580244779587
step: 3400, Loss: 0.12063886970281601
step: 3500, Loss: 0.11576507985591888
step: 3600, Loss: 0.11609037220478058
step: 3700, Loss: 0.11717776954174042
step: 3800, Loss: 0.11595294624567032
step: 3900, Loss: 0.11654814332723618
step: 4000, Loss: 0.1170404776930809
step: 4100, Loss: 0.11578705906867981
step: 4200, Loss: 0.11500486731529236
step: 4300, Loss: 0.11456258594989777
step: 4400, Loss: 0.1151731088757515
step: 4500, Loss: 0.11509643495082855
step: 4600, Loss: 0.11567311733961105
step: 4700, Loss: 0.11480877548456192
step: 4800, Loss: 0.11527876555919647
step: 4900, Loss: 0.11556932330131531
step: 5000, Loss: 0.11411605775356293
step: 5100, Loss: 0.1148984432220459
step: 5200, Loss: 0.11645811796188354
step: 5300, Loss: 0.11375696212053299
step: 5400, Loss: 0.1140461340546608
step: 5500, Loss: 0.11501732468605042
step: 5600, Loss: 0.11460728198289871
step: 5700, Loss: 0.11505138128995895
step: 5800, Loss: 0.11463242769241333
step: 5900, Loss: 0.11413008719682693
step: 6000, Loss: 0.1144229844212532
step: 6100, Loss: 0.11522518843412399
step: 6200, Loss: 0.11413854360580444
step: 6300, Loss: 0.1136067658662796
step: 6400, Loss: 0.11406084150075912
step: 6500, Loss: 0.11429452896118164
step: 6600, Loss: 0.11388126015663147
step: 6700, Loss: 0.11555066704750061
step: 6800, Loss: 0.11888444423675537
step: 6900, Loss: 1.132617473602295
step: 7000, Loss: 0.1854715198278427
step: 7100, Loss: 0.13630251586437225
step: 7200, Loss: 0.13005998730659485
step: 7300, Loss: 0.12508703768253326
step: 7400, Loss: 0.1342776119709015
step: 7500, Loss: 0.13705728948116302
step: 7600, Loss: 0.12760809063911438
step: 7700, Loss: 0.12087962031364441
step: 7800, Loss: 0.12069021910429001
step: 7900, Loss: 0.12133952975273132
step: 8000, Loss: 0.12345007061958313
step: 8100, Loss: 0.12597985565662384
step: 8200, Loss: 0.12013527005910873
step: 8300, Loss: 0.11557134985923767
step: 8400, Loss: 0.11927100270986557
step: 8500, Loss: 0.11721201241016388
step: 8600, Loss: 0.12013383954763412
step: 8700, Loss: 0.12118929624557495
step: 8800, Loss: 0.11777465045452118
step: 8900, Loss: 0.11699941754341125
step: 9000, Loss: 0.11837509274482727
step: 9100, Loss: 0.11666087806224823
step: 9200, Loss: 0.11708729714155197
step: 9300, Loss: 0.11882107704877853
step: 9400, Loss: 0.1162039190530777
step: 9500, Loss: 0.11411947757005692
step: 9600, Loss: 0.11563213169574738
step: 9700, Loss: 0.11485287547111511
step: 9800, Loss: 0.1159999817609787
step: 9900, Loss: 0.11978140473365784
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 22 Avgacc: 0.9636363636363636 Avgfscore: 0.965215315800551 
 Max acc:1.0, Max f score:1.0
******** mix subject_23 ********

[228, 532]
******fold 1******

Training... train_data length:957
step: 0, Loss: 29.28752326965332
step: 100, Loss: 1.202467918395996
step: 200, Loss: 0.21801979839801788
step: 300, Loss: 0.187707781791687
step: 400, Loss: 0.18515758216381073
step: 500, Loss: 0.16257166862487793
step: 600, Loss: 0.16357582807540894
step: 700, Loss: 0.14986278116703033
step: 800, Loss: 0.14009204506874084
step: 900, Loss: 0.16581177711486816
step: 1000, Loss: 0.1284399777650833
step: 1100, Loss: 0.127102330327034
step: 1200, Loss: 0.13016945123672485
step: 1300, Loss: 0.12238344550132751
step: 1400, Loss: 0.1279342919588089
step: 1500, Loss: 0.13751143217086792
step: 1600, Loss: 0.12334156036376953
step: 1700, Loss: 0.12643225491046906
step: 1800, Loss: 0.117549367249012
step: 1900, Loss: 0.12003821134567261
step: 2000, Loss: 0.11997289955615997
step: 2100, Loss: 0.11812754720449448
step: 2200, Loss: 0.13345271348953247
step: 2300, Loss: 0.12241463363170624
step: 2400, Loss: 0.11720854789018631
step: 2500, Loss: 0.12097491323947906
step: 2600, Loss: 0.11718566715717316
step: 2700, Loss: 0.11533989012241364
step: 2800, Loss: 0.12752768397331238
step: 2900, Loss: 0.11626384407281876
step: 3000, Loss: 0.11731694638729095
step: 3100, Loss: 0.11687273532152176
step: 3200, Loss: 0.1139766126871109
step: 3300, Loss: 0.11733938753604889
step: 3400, Loss: 0.1176353171467781
step: 3500, Loss: 0.12679463624954224
step: 3600, Loss: 0.11785870790481567
step: 3700, Loss: 0.11471930146217346
step: 3800, Loss: 0.11624176800251007
step: 3900, Loss: 0.11585557460784912
step: 4000, Loss: 0.11632341146469116
step: 4100, Loss: 0.15939602255821228
step: 4200, Loss: 0.7468184232711792
step: 4300, Loss: 0.204337477684021
step: 4400, Loss: 0.1344461590051651
step: 4500, Loss: 0.1619335412979126
step: 4600, Loss: 0.1397908627986908
step: 4700, Loss: 0.14069005846977234
step: 4800, Loss: 0.1416904479265213
step: 4900, Loss: 0.1247972697019577
step: 5000, Loss: 0.13531844317913055
step: 5100, Loss: 0.1269572228193283
step: 5200, Loss: 0.12242511659860611
step: 5300, Loss: 0.1258043497800827
step: 5400, Loss: 0.12989328801631927
step: 5500, Loss: 0.11757729202508926
step: 5600, Loss: 0.12467382848262787
step: 5700, Loss: 0.11870124936103821
step: 5800, Loss: 0.1250065416097641
step: 5900, Loss: 0.12055773288011551
step: 6000, Loss: 0.12105105072259903
step: 6100, Loss: 0.12824562191963196
step: 6200, Loss: 0.11577384918928146
step: 6300, Loss: 0.1233738362789154
step: 6400, Loss: 0.11905907839536667
step: 6500, Loss: 0.11663661897182465
step: 6600, Loss: 0.1154555231332779
step: 6700, Loss: 0.12076925486326218
step: 6800, Loss: 0.11821862310171127
step: 6900, Loss: 0.11684679985046387
step: 7000, Loss: 0.11688866466283798
step: 7100, Loss: 0.11753272265195847
step: 7200, Loss: 0.1172640472650528
step: 7300, Loss: 0.11640423536300659
step: 7400, Loss: 0.11863111704587936
step: 7500, Loss: 0.11400674283504486
step: 7600, Loss: 0.11529205739498138
step: 7700, Loss: 0.11450351774692535
step: 7800, Loss: 0.11429262161254883
step: 7900, Loss: 0.11475928872823715
step: 8000, Loss: 0.11445934325456619
step: 8100, Loss: 0.11358951032161713
step: 8200, Loss: 0.11569663137197495
step: 8300, Loss: 0.11400303989648819
step: 8400, Loss: 0.1150854229927063
step: 8500, Loss: 0.11350306123495102
step: 8600, Loss: 0.11351971328258514
step: 8700, Loss: 0.11399346590042114
step: 8800, Loss: 0.11478998512029648
step: 8900, Loss: 0.1146310344338417
step: 9000, Loss: 0.11324669420719147
step: 9100, Loss: 0.11317504942417145
step: 9200, Loss: 0.1140042096376419
step: 9300, Loss: 0.11436890810728073
step: 9400, Loss: 0.1136179268360138
step: 9500, Loss: 0.11477521061897278
step: 9600, Loss: 0.11350645124912262
step: 9700, Loss: 0.11414653807878494
step: 9800, Loss: 0.11389854550361633
step: 9900, Loss: 0.11388850212097168
training successfully ended.
validating...
validate data length:107
acc: 0.9423076923076923
precision: 0.90625
recall: 1.0
F_score: 0.9508196721311475
******fold 2******

Training... train_data length:957
step: 0, Loss: 0.27060186862945557
step: 100, Loss: 0.1280774176120758
step: 200, Loss: 0.1184651106595993
step: 300, Loss: 0.11727102845907211
step: 400, Loss: 0.11845570802688599
step: 500, Loss: 0.11750853806734085
step: 600, Loss: 0.1170247420668602
step: 700, Loss: 0.11536221206188202
step: 800, Loss: 0.11342530697584152
step: 900, Loss: 0.11351242661476135
step: 1000, Loss: 0.11418300122022629
step: 1100, Loss: 0.11526620388031006
step: 1200, Loss: 0.1145920380949974
step: 1300, Loss: 0.11338091641664505
step: 1400, Loss: 0.11385691165924072
step: 1500, Loss: 0.11349517107009888
step: 1600, Loss: 0.11395104229450226
step: 1700, Loss: 0.11311832070350647
step: 1800, Loss: 0.11289399862289429
step: 1900, Loss: 0.11431869864463806
step: 2000, Loss: 0.11388912796974182
step: 2100, Loss: 0.11330374330282211
step: 2200, Loss: 0.11339081078767776
step: 2300, Loss: 0.11353852599859238
step: 2400, Loss: 0.11498026549816132
step: 2500, Loss: 0.11377626657485962
step: 2600, Loss: 0.1164737194776535
step: 2700, Loss: 0.11556264758110046
step: 2800, Loss: 0.11421914398670197
step: 2900, Loss: 0.11435022950172424
step: 3000, Loss: 0.11528526991605759
step: 3100, Loss: 0.11485303938388824
step: 3200, Loss: 0.1145705059170723
step: 3300, Loss: 0.11506583541631699
step: 3400, Loss: 0.11929121613502502
step: 3500, Loss: 0.11639326810836792
step: 3600, Loss: 0.11469145864248276
step: 3700, Loss: 0.11499762535095215
step: 3800, Loss: 0.11582548916339874
step: 3900, Loss: 0.11550289392471313
step: 4000, Loss: 0.1151869148015976
step: 4100, Loss: 0.11480816453695297
step: 4200, Loss: 0.11300556361675262
step: 4300, Loss: 0.11711861193180084
step: 4400, Loss: 0.11592739075422287
step: 4500, Loss: 0.11566908657550812
step: 4600, Loss: 0.11338358372449875
step: 4700, Loss: 0.11501724272966385
step: 4800, Loss: 0.11281542479991913
step: 4900, Loss: 0.11393938213586807
step: 5000, Loss: 0.11352542787790298
step: 5100, Loss: 0.11316664516925812
step: 5200, Loss: 0.11444106698036194
step: 5300, Loss: 0.11730633676052094
step: 5400, Loss: 0.11629202961921692
step: 5500, Loss: 0.11551597714424133
step: 5600, Loss: 0.11424312740564346
step: 5700, Loss: 0.24681603908538818
step: 5800, Loss: 0.16068035364151
step: 5900, Loss: 0.12659208476543427
step: 6000, Loss: 0.13538943231105804
step: 6100, Loss: 0.12972930073738098
step: 6200, Loss: 0.12334462255239487
step: 6300, Loss: 0.12112171947956085
step: 6400, Loss: 0.11900464445352554
step: 6500, Loss: 0.12077642232179642
step: 6600, Loss: 0.12181288003921509
step: 6700, Loss: 0.117243692278862
step: 6800, Loss: 0.11865450441837311
step: 6900, Loss: 0.11848542839288712
step: 7000, Loss: 0.11959882080554962
step: 7100, Loss: 0.11948397755622864
step: 7200, Loss: 0.11603618413209915
step: 7300, Loss: 0.11898916959762573
step: 7400, Loss: 0.11810015141963959
step: 7500, Loss: 0.11836470663547516
step: 7600, Loss: 0.11614594608545303
step: 7700, Loss: 0.116590216755867
step: 7800, Loss: 0.11474449932575226
step: 7900, Loss: 0.1166086345911026
step: 8000, Loss: 0.11568182706832886
step: 8100, Loss: 0.11482074856758118
step: 8200, Loss: 0.11459335684776306
step: 8300, Loss: 0.1144348680973053
step: 8400, Loss: 0.11383099853992462
step: 8500, Loss: 0.11383484303951263
step: 8600, Loss: 0.11415513604879379
step: 8700, Loss: 0.1145908534526825
step: 8800, Loss: 0.11569194495677948
step: 8900, Loss: 0.11493697762489319
step: 9000, Loss: 0.11351232975721359
step: 9100, Loss: 0.1131829172372818
step: 9200, Loss: 0.1153852716088295
step: 9300, Loss: 0.11447614431381226
step: 9400, Loss: 0.11370313912630081
step: 9500, Loss: 0.11450862884521484
step: 9600, Loss: 0.11469460278749466
step: 9700, Loss: 0.11335603892803192
step: 9800, Loss: 0.1128096878528595
step: 9900, Loss: 0.11374062299728394
training successfully ended.
validating...
validate data length:107
acc: 0.9903846153846154
precision: 0.9761904761904762
recall: 1.0
F_score: 0.9879518072289156
******fold 3******

Training... train_data length:957
step: 0, Loss: 0.12353011220693588
step: 100, Loss: 0.11791884899139404
step: 200, Loss: 0.11482227593660355
step: 300, Loss: 0.114534392952919
step: 400, Loss: 0.11375424265861511
step: 500, Loss: 0.11514482647180557
step: 600, Loss: 0.11410795152187347
step: 700, Loss: 0.11559300124645233
step: 800, Loss: 0.11442772299051285
step: 900, Loss: 0.11277985572814941
step: 1000, Loss: 0.11388850212097168
step: 1100, Loss: 0.11500585824251175
step: 1200, Loss: 0.11544395983219147
step: 1300, Loss: 0.11245651543140411
step: 1400, Loss: 0.11277550458908081
step: 1500, Loss: 0.11296715587377548
step: 1600, Loss: 0.11349182575941086
step: 1700, Loss: 0.11368436366319656
step: 1800, Loss: 0.11444710195064545
step: 1900, Loss: 0.1134328842163086
step: 2000, Loss: 0.1130526065826416
step: 2100, Loss: 0.11388678848743439
step: 2200, Loss: 0.11423734575510025
step: 2300, Loss: 0.1134922131896019
step: 2400, Loss: 0.11356467753648758
step: 2500, Loss: 0.11426886916160583
step: 2600, Loss: 0.11574480682611465
step: 2700, Loss: 0.11343729496002197
step: 2800, Loss: 0.11332660913467407
step: 2900, Loss: 0.11396272480487823
step: 3000, Loss: 0.11396581679582596
step: 3100, Loss: 0.11370469629764557
step: 3200, Loss: 0.11462564766407013
step: 3300, Loss: 0.11313364654779434
step: 3400, Loss: 0.11426274478435516
step: 3500, Loss: 0.11612646281719208
step: 3600, Loss: 0.11364341527223587
step: 3700, Loss: 0.11412964761257172
step: 3800, Loss: 0.11315487325191498
step: 3900, Loss: 0.11473923921585083
step: 4000, Loss: 0.1143016442656517
step: 4100, Loss: 0.11315067112445831
step: 4200, Loss: 0.11372056603431702
step: 4300, Loss: 0.11468099057674408
step: 4400, Loss: 0.1129780113697052
step: 4500, Loss: 0.11393090337514877
step: 4600, Loss: 0.11492188274860382
step: 4700, Loss: 0.11446763575077057
step: 4800, Loss: 0.1630271077156067
step: 4900, Loss: 0.14371246099472046
step: 5000, Loss: 0.12637744843959808
step: 5100, Loss: 0.12245403230190277
step: 5200, Loss: 0.12402728199958801
step: 5300, Loss: 0.12124644964933395
step: 5400, Loss: 0.11935511976480484
step: 5500, Loss: 0.12173141539096832
step: 5600, Loss: 0.12438563257455826
step: 5700, Loss: 0.11646947264671326
step: 5800, Loss: 0.11839782446622849
step: 5900, Loss: 0.11720035970211029
step: 6000, Loss: 0.11727990210056305
step: 6100, Loss: 0.11815603822469711
step: 6200, Loss: 0.11681446433067322
step: 6300, Loss: 0.1167449951171875
step: 6400, Loss: 0.11688617616891861
step: 6500, Loss: 0.11471175402402878
step: 6600, Loss: 0.11517544835805893
step: 6700, Loss: 0.11752612888813019
step: 6800, Loss: 0.1158565878868103
step: 6900, Loss: 0.1143278107047081
step: 7000, Loss: 0.11395356059074402
step: 7100, Loss: 0.1159093976020813
step: 7200, Loss: 0.11477681249380112
step: 7300, Loss: 0.11581186950206757
step: 7400, Loss: 0.11486451327800751
step: 7500, Loss: 0.1137906163930893
step: 7600, Loss: 0.11468713730573654
step: 7700, Loss: 0.11287194490432739
step: 7800, Loss: 0.11496376246213913
step: 7900, Loss: 0.11310611665248871
step: 8000, Loss: 0.1145094633102417
step: 8100, Loss: 0.11339394748210907
step: 8200, Loss: 0.11438082158565521
step: 8300, Loss: 0.11450812220573425
step: 8400, Loss: 0.11360228061676025
step: 8500, Loss: 0.11416539549827576
step: 8600, Loss: 0.11417722702026367
step: 8700, Loss: 0.11383549869060516
step: 8800, Loss: 0.11388447135686874
step: 8900, Loss: 0.11415515840053558
step: 9000, Loss: 0.11346323043107986
step: 9100, Loss: 0.11329371482133865
step: 9200, Loss: 0.11269906908273697
step: 9300, Loss: 0.11390179395675659
step: 9400, Loss: 0.11441802978515625
step: 9500, Loss: 0.1140754371881485
step: 9600, Loss: 0.11379941552877426
step: 9700, Loss: 0.11344816535711288
step: 9800, Loss: 0.11441273242235184
step: 9900, Loss: 0.11467666923999786
training successfully ended.
validating...
validate data length:107
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

Training... train_data length:957
step: 0, Loss: 0.1131906732916832
step: 100, Loss: 0.11602622270584106
step: 200, Loss: 0.11515983939170837
step: 300, Loss: 0.11383894830942154
step: 400, Loss: 0.11475411057472229
step: 500, Loss: 0.11306645721197128
step: 600, Loss: 0.11355115473270416
step: 700, Loss: 0.11429666727781296
step: 800, Loss: 0.11496187001466751
step: 900, Loss: 0.11367513984441757
step: 1000, Loss: 0.11453917622566223
step: 1100, Loss: 0.11322653293609619
step: 1200, Loss: 0.11337891966104507
step: 1300, Loss: 0.11273332685232162
step: 1400, Loss: 0.11366813629865646
step: 1500, Loss: 0.11299991607666016
step: 1600, Loss: 0.11367841064929962
step: 1700, Loss: 0.1124974861741066
step: 1800, Loss: 0.11353667080402374
step: 1900, Loss: 0.11312679946422577
step: 2000, Loss: 0.11346866190433502
step: 2100, Loss: 0.11350426077842712
step: 2200, Loss: 0.11452577263116837
step: 2300, Loss: 0.11299668997526169
step: 2400, Loss: 0.11378005146980286
step: 2500, Loss: 0.11270593106746674
step: 2600, Loss: 0.11344341933727264
step: 2700, Loss: 0.11464916169643402
step: 2800, Loss: 0.11344008147716522
step: 2900, Loss: 0.11380200833082199
step: 3000, Loss: 0.11373069882392883
step: 3100, Loss: 0.11381179839372635
step: 3200, Loss: 0.2248617708683014
step: 3300, Loss: 0.1229015588760376
step: 3400, Loss: 0.12702569365501404
step: 3500, Loss: 0.1211966797709465
step: 3600, Loss: 0.11869384348392487
step: 3700, Loss: 0.12431678920984268
step: 3800, Loss: 0.11730069667100906
step: 3900, Loss: 0.11938931792974472
step: 4000, Loss: 0.11479754000902176
step: 4100, Loss: 0.11715751886367798
step: 4200, Loss: 0.11812399327754974
step: 4300, Loss: 0.11477459222078323
step: 4400, Loss: 0.11669684201478958
step: 4500, Loss: 0.11700482666492462
step: 4600, Loss: 0.11479748785495758
step: 4700, Loss: 0.1142396405339241
step: 4800, Loss: 0.11476451903581619
step: 4900, Loss: 0.11723433434963226
step: 5000, Loss: 0.11585816740989685
step: 5100, Loss: 0.11436821520328522
step: 5200, Loss: 0.1157655119895935
step: 5300, Loss: 0.1131063848733902
step: 5400, Loss: 0.11430460959672928
step: 5500, Loss: 0.11500174552202225
step: 5600, Loss: 0.11428984254598618
step: 5700, Loss: 0.11463301628828049
step: 5800, Loss: 0.11381835490465164
step: 5900, Loss: 0.113385409116745
step: 6000, Loss: 0.11406251043081284
step: 6100, Loss: 0.11435552686452866
step: 6200, Loss: 0.11342059075832367
step: 6300, Loss: 0.1135246753692627
step: 6400, Loss: 0.11364693194627762
step: 6500, Loss: 0.11400799453258514
step: 6600, Loss: 0.11396855860948563
step: 6700, Loss: 0.11507865786552429
step: 6800, Loss: 0.11370788514614105
step: 6900, Loss: 0.11359617114067078
step: 7000, Loss: 0.11325044929981232
step: 7100, Loss: 0.11402150988578796
step: 7200, Loss: 0.11384188383817673
step: 7300, Loss: 0.11373687535524368
step: 7400, Loss: 0.11387869715690613
step: 7500, Loss: 0.11290491372346878
step: 7600, Loss: 0.11436864733695984
step: 7700, Loss: 0.11386231333017349
step: 7800, Loss: 0.11586867272853851
step: 7900, Loss: 0.1143336296081543
step: 8000, Loss: 0.11454543471336365
step: 8100, Loss: 0.11541752517223358
step: 8200, Loss: 0.11360013484954834
step: 8300, Loss: 0.11401456594467163
step: 8400, Loss: 0.11370278894901276
step: 8500, Loss: 0.11322466284036636
step: 8600, Loss: 0.11318252235651016
step: 8700, Loss: 0.113663449883461
step: 8800, Loss: 0.1130455806851387
step: 8900, Loss: 0.11297838389873505
step: 9000, Loss: 0.11275541037321091
step: 9100, Loss: 0.1138065755367279
step: 9200, Loss: 0.11265265941619873
step: 9300, Loss: 0.11258753389120102
step: 9400, Loss: 0.11365735530853271
step: 9500, Loss: 0.11397279798984528
step: 9600, Loss: 0.11447151005268097
step: 9700, Loss: 0.11283513903617859
step: 9800, Loss: 0.11335133761167526
step: 9900, Loss: 0.11293400079011917
training successfully ended.
validating...
validate data length:107
acc: 0.9711538461538461
precision: 0.9827586206896551
recall: 0.9661016949152542
F_score: 0.9743589743589743
******fold 5******

Training... train_data length:958
step: 0, Loss: 0.11245881766080856
step: 100, Loss: 0.11529311537742615
step: 200, Loss: 0.11556630581617355
step: 300, Loss: 0.11311832070350647
step: 400, Loss: 0.11446459591388702
step: 500, Loss: 0.11470851302146912
step: 600, Loss: 0.11237359046936035
step: 700, Loss: 0.11368131637573242
step: 800, Loss: 0.11409405618906021
step: 900, Loss: 0.11426612734794617
step: 1000, Loss: 0.11272169649600983
step: 1100, Loss: 0.11411735415458679
step: 1200, Loss: 0.11275238543748856
step: 1300, Loss: 0.11415249854326248
step: 1400, Loss: 0.11320703476667404
step: 1500, Loss: 0.11270031332969666
step: 1600, Loss: 0.11418195068836212
step: 1700, Loss: 0.11407136917114258
step: 1800, Loss: 0.11245863884687424
step: 1900, Loss: 0.11313580721616745
step: 2000, Loss: 0.11338724195957184
step: 2100, Loss: 0.11361028254032135
step: 2200, Loss: 0.11373988538980484
step: 2300, Loss: 0.11586527526378632
step: 2400, Loss: 0.11428219079971313
step: 2500, Loss: 0.1130247563123703
step: 2600, Loss: 0.11376483738422394
step: 2700, Loss: 0.11390892416238785
step: 2800, Loss: 0.11301296949386597
step: 2900, Loss: 0.11393486708402634
step: 3000, Loss: 0.11371471732854843
step: 3100, Loss: 0.11384232342243195
step: 3200, Loss: 0.11604716628789902
step: 3300, Loss: 0.11355660855770111
step: 3400, Loss: 0.11409016698598862
step: 3500, Loss: 0.11731116473674774
step: 3600, Loss: 0.1138274073600769
step: 3700, Loss: 0.11429629474878311
step: 3800, Loss: 0.11489880084991455
step: 3900, Loss: 0.11285407096147537
step: 4000, Loss: 1.5445029735565186
step: 4100, Loss: 0.1529262810945511
step: 4200, Loss: 0.1264851689338684
step: 4300, Loss: 0.12279294431209564
step: 4400, Loss: 0.12430819869041443
step: 4500, Loss: 0.12334249168634415
step: 4600, Loss: 0.12082807719707489
step: 4700, Loss: 0.11887163668870926
step: 4800, Loss: 0.12009987235069275
step: 4900, Loss: 0.11649799346923828
step: 5000, Loss: 0.11714289337396622
step: 5100, Loss: 0.12098332494497299
step: 5200, Loss: 0.11788277328014374
step: 5300, Loss: 0.11725708842277527
step: 5400, Loss: 0.11398957669734955
step: 5500, Loss: 0.11646609008312225
step: 5600, Loss: 0.11630310863256454
step: 5700, Loss: 0.11576762050390244
step: 5800, Loss: 0.11512363702058792
step: 5900, Loss: 0.11458256840705872
step: 6000, Loss: 0.11461219191551208
step: 6100, Loss: 0.11756753921508789
step: 6200, Loss: 0.1143348217010498
step: 6300, Loss: 0.11833569407463074
step: 6400, Loss: 0.11423066258430481
step: 6500, Loss: 0.11588147282600403
step: 6600, Loss: 0.11425469070672989
step: 6700, Loss: 0.11443278193473816
step: 6800, Loss: 0.11564333736896515
step: 6900, Loss: 0.11527944356203079
step: 7000, Loss: 0.11725131422281265
step: 7100, Loss: 0.11355341970920563
step: 7200, Loss: 0.11393458396196365
step: 7300, Loss: 0.11363448202610016
step: 7400, Loss: 0.11601752042770386
step: 7500, Loss: 0.11376333236694336
step: 7600, Loss: 0.11455095559358597
step: 7700, Loss: 0.11359133571386337
step: 7800, Loss: 0.11295934021472931
step: 7900, Loss: 0.11367368698120117
step: 8000, Loss: 0.1140613853931427
step: 8100, Loss: 0.11390016973018646
step: 8200, Loss: 0.11350889503955841
step: 8300, Loss: 0.11494626104831696
step: 8400, Loss: 0.11436259746551514
step: 8500, Loss: 0.11398289352655411
step: 8600, Loss: 0.11335636675357819
step: 8700, Loss: 0.11311794072389603
step: 8800, Loss: 0.11358969658613205
step: 8900, Loss: 0.11439061909914017
step: 9000, Loss: 0.11349138617515564
step: 9100, Loss: 0.11364921927452087
step: 9200, Loss: 0.11384805291891098
step: 9300, Loss: 0.11238642781972885
step: 9400, Loss: 0.114084891974926
step: 9500, Loss: 0.11391577124595642
step: 9600, Loss: 0.11478158086538315
step: 9700, Loss: 0.11340541392564774
step: 9800, Loss: 0.1138949766755104
step: 9900, Loss: 0.11296355724334717
training successfully ended.
validating...
validate data length:106
acc: 0.9903846153846154
precision: 0.9833333333333333
recall: 1.0
F_score: 0.9915966386554621
******fold 6******

Training... train_data length:958
step: 0, Loss: 0.11373438686132431
step: 100, Loss: 0.1178472712635994
step: 200, Loss: 0.11496268957853317
step: 300, Loss: 0.11535287648439407
step: 400, Loss: 0.11482566595077515
step: 500, Loss: 0.11449925601482391
step: 600, Loss: 0.11345181614160538
step: 700, Loss: 0.11351385712623596
step: 800, Loss: 0.11329185217618942
step: 900, Loss: 0.11381449550390244
step: 1000, Loss: 0.11327724158763885
step: 1100, Loss: 0.11501505970954895
step: 1200, Loss: 0.11432167887687683
step: 1300, Loss: 0.11312199383974075
step: 1400, Loss: 0.11444791406393051
step: 1500, Loss: 0.11427552998065948
step: 1600, Loss: 0.11273875087499619
step: 1700, Loss: 0.11425035446882248
step: 1800, Loss: 0.11358919739723206
step: 1900, Loss: 0.11279187351465225
step: 2000, Loss: 0.11454110592603683
step: 2100, Loss: 0.1150522455573082
step: 2200, Loss: 0.11320873349905014
step: 2300, Loss: 0.1132490336894989
step: 2400, Loss: 0.11260007321834564
step: 2500, Loss: 0.11384879052639008
step: 2600, Loss: 0.11396610736846924
step: 2700, Loss: 0.11360662430524826
step: 2800, Loss: 0.11361578106880188
step: 2900, Loss: 0.11503177881240845
step: 3000, Loss: 1.9521865844726562
step: 3100, Loss: 0.13522696495056152
step: 3200, Loss: 0.1271001696586609
step: 3300, Loss: 0.11980177462100983
step: 3400, Loss: 0.12200269848108292
step: 3500, Loss: 0.12152125686407089
step: 3600, Loss: 0.12463416159152985
step: 3700, Loss: 0.11705948412418365
step: 3800, Loss: 0.1186196431517601
step: 3900, Loss: 0.11696983873844147
step: 4000, Loss: 0.1160108894109726
step: 4100, Loss: 0.11515030264854431
step: 4200, Loss: 0.11450503766536713
step: 4300, Loss: 0.11714929342269897
step: 4400, Loss: 0.11644033342599869
step: 4500, Loss: 0.11588206142187119
step: 4600, Loss: 0.1160951554775238
step: 4700, Loss: 0.11577965319156647
step: 4800, Loss: 0.11551295965909958
step: 4900, Loss: 0.11814447492361069
step: 5000, Loss: 0.11455501616001129
step: 5100, Loss: 0.11365392059087753
step: 5200, Loss: 0.11451699584722519
step: 5300, Loss: 0.11595863103866577
step: 5400, Loss: 0.11344370245933533
step: 5500, Loss: 0.11363417655229568
step: 5600, Loss: 0.1148240938782692
step: 5700, Loss: 0.11581055819988251
step: 5800, Loss: 0.11441175639629364
step: 5900, Loss: 0.11493600904941559
step: 6000, Loss: 0.1148051843047142
step: 6100, Loss: 0.11398454755544662
step: 6200, Loss: 0.11521582305431366
step: 6300, Loss: 0.11342901736497879
step: 6400, Loss: 0.11545673757791519
step: 6500, Loss: 0.11499597877264023
step: 6600, Loss: 0.11411788314580917
step: 6700, Loss: 0.11293775588274002
step: 6800, Loss: 0.11414622515439987
step: 6900, Loss: 0.11355715990066528
step: 7000, Loss: 0.1138424277305603
step: 7100, Loss: 0.11553880572319031
step: 7200, Loss: 0.11331909149885178
step: 7300, Loss: 0.11480874568223953
step: 7400, Loss: 0.11406996846199036
step: 7500, Loss: 0.11380604654550552
step: 7600, Loss: 0.11447358876466751
step: 7700, Loss: 0.11408918350934982
step: 7800, Loss: 0.11337269842624664
step: 7900, Loss: 0.11354094743728638
step: 8000, Loss: 0.11339826136827469
step: 8100, Loss: 0.11519213765859604
step: 8200, Loss: 0.11351124197244644
step: 8300, Loss: 0.11324676126241684
step: 8400, Loss: 0.11285354197025299
step: 8500, Loss: 0.11324658989906311
step: 8600, Loss: 0.11375563591718674
step: 8700, Loss: 0.11317945271730423
step: 8800, Loss: 0.11487450450658798
step: 8900, Loss: 0.11244150251150131
step: 9000, Loss: 0.11489439010620117
step: 9100, Loss: 0.11332208663225174
step: 9200, Loss: 0.11281293630599976
step: 9300, Loss: 0.11394545435905457
step: 9400, Loss: 0.11346686631441116
step: 9500, Loss: 0.11364415287971497
step: 9600, Loss: 0.11336436867713928
step: 9700, Loss: 0.11361993849277496
step: 9800, Loss: 0.11309655755758286
step: 9900, Loss: 0.11286415904760361
training successfully ended.
validating...
validate data length:106
acc: 0.9903846153846154
precision: 0.9807692307692307
recall: 1.0
F_score: 0.9902912621359222
******fold 7******

Training... train_data length:958
step: 0, Loss: 0.11364921182394028
step: 100, Loss: 0.1146850511431694
step: 200, Loss: 0.11422906816005707
step: 300, Loss: 0.11384154856204987
step: 400, Loss: 0.1138550192117691
step: 500, Loss: 0.11421374976634979
step: 600, Loss: 0.11512954533100128
step: 700, Loss: 0.11412907391786575
step: 800, Loss: 0.11349928379058838
step: 900, Loss: 0.11358336359262466
step: 1000, Loss: 0.11379458010196686
step: 1100, Loss: 0.11339419335126877
step: 1200, Loss: 0.11216534674167633
step: 1300, Loss: 0.11331165581941605
step: 1400, Loss: 0.11297512799501419
step: 1500, Loss: 0.11375480145215988
step: 1600, Loss: 0.11429702490568161
step: 1700, Loss: 0.11349611729383469
step: 1800, Loss: 0.11352088302373886
step: 1900, Loss: 0.11313650012016296
step: 2000, Loss: 0.11459416896104813
step: 2100, Loss: 0.1140219047665596
step: 2200, Loss: 0.11297988891601562
step: 2300, Loss: 0.11390215158462524
step: 2400, Loss: 0.11326476186513901
step: 2500, Loss: 0.11377985775470734
step: 2600, Loss: 0.11331062018871307
step: 2700, Loss: 0.11614657938480377
step: 2800, Loss: 0.1586650013923645
step: 2900, Loss: 0.12121579796075821
step: 3000, Loss: 0.11990249156951904
step: 3100, Loss: 0.13260990381240845
step: 3200, Loss: 0.12211962044239044
step: 3300, Loss: 0.1186753362417221
step: 3400, Loss: 0.12034250050783157
step: 3500, Loss: 0.11720550805330276
step: 3600, Loss: 0.11856444925069809
step: 3700, Loss: 0.11964957416057587
step: 3800, Loss: 0.1175328865647316
step: 3900, Loss: 0.11870001256465912
step: 4000, Loss: 0.11595387011766434
step: 4100, Loss: 0.1184920147061348
step: 4200, Loss: 0.11675597727298737
step: 4300, Loss: 0.1141151636838913
step: 4400, Loss: 0.11729170382022858
step: 4500, Loss: 0.11546514928340912
step: 4600, Loss: 0.11597102880477905
step: 4700, Loss: 0.11455550789833069
step: 4800, Loss: 0.11606640368700027
step: 4900, Loss: 0.11554477363824844
step: 5000, Loss: 0.11600157618522644
step: 5100, Loss: 0.1156565248966217
step: 5200, Loss: 0.11557500064373016
step: 5300, Loss: 0.11385469138622284
step: 5400, Loss: 0.11689579486846924
step: 5500, Loss: 0.11561630666255951
step: 5600, Loss: 0.1149345114827156
step: 5700, Loss: 0.11492948234081268
step: 5800, Loss: 0.11414310336112976
step: 5900, Loss: 0.11295676976442337
step: 6000, Loss: 0.11475353688001633
step: 6100, Loss: 0.1147805005311966
step: 6200, Loss: 0.11492636799812317
step: 6300, Loss: 0.11490002274513245
step: 6400, Loss: 0.11447837203741074
step: 6500, Loss: 0.11384227871894836
step: 6600, Loss: 0.11286015808582306
step: 6700, Loss: 0.1152142882347107
step: 6800, Loss: 0.11455093324184418
step: 6900, Loss: 0.1131376251578331
step: 7000, Loss: 0.11427368223667145
step: 7100, Loss: 0.1142878606915474
step: 7200, Loss: 0.11287262290716171
step: 7300, Loss: 0.11337122321128845
step: 7400, Loss: 0.11386455595493317
step: 7500, Loss: 0.11294858157634735
step: 7600, Loss: 0.11363857984542847
step: 7700, Loss: 0.11373059451580048
step: 7800, Loss: 0.11377815902233124
step: 7900, Loss: 0.11332276463508606
step: 8000, Loss: 0.11409258842468262
step: 8100, Loss: 0.11356403678655624
step: 8200, Loss: 0.11419067531824112
step: 8300, Loss: 0.11337818205356598
step: 8400, Loss: 0.11395177245140076
step: 8500, Loss: 0.11584779620170593
step: 8600, Loss: 0.11327406764030457
step: 8700, Loss: 0.11402124166488647
step: 8800, Loss: 0.11394347250461578
step: 8900, Loss: 0.11340115964412689
step: 9000, Loss: 0.11265285313129425
step: 9100, Loss: 0.11335895210504532
step: 9200, Loss: 0.11261548846960068
step: 9300, Loss: 0.11335252225399017
step: 9400, Loss: 0.11448567360639572
step: 9500, Loss: 0.11363985389471054
step: 9600, Loss: 0.11337064206600189
step: 9700, Loss: 0.11391976475715637
step: 9800, Loss: 0.11332616209983826
step: 9900, Loss: 0.11315380036830902
training successfully ended.
validating...
validate data length:106
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:958
step: 0, Loss: 0.11337698996067047
step: 100, Loss: 0.11415793001651764
step: 200, Loss: 0.1152435839176178
step: 300, Loss: 0.11416321992874146
step: 400, Loss: 0.11530252546072006
step: 500, Loss: 0.11373594403266907
step: 600, Loss: 0.11436246335506439
step: 700, Loss: 0.11352483183145523
step: 800, Loss: 0.1142428070306778
step: 900, Loss: 0.11369463801383972
step: 1000, Loss: 0.11388947069644928
step: 1100, Loss: 0.11569786071777344
step: 1200, Loss: 0.11342909932136536
step: 1300, Loss: 0.11298095434904099
step: 1400, Loss: 0.11277881264686584
step: 1500, Loss: 0.1128568947315216
step: 1600, Loss: 0.11266836524009705
step: 1700, Loss: 0.11317362636327744
step: 1800, Loss: 0.11371098458766937
step: 1900, Loss: 0.11327005922794342
step: 2000, Loss: 0.11555930227041245
step: 2100, Loss: 0.11543676257133484
step: 2200, Loss: 0.11308883875608444
step: 2300, Loss: 0.11439962685108185
step: 2400, Loss: 0.11542338132858276
step: 2500, Loss: 0.1128850057721138
step: 2600, Loss: 0.11328861117362976
step: 2700, Loss: 0.1139109879732132
step: 2800, Loss: 0.1139201745390892
step: 2900, Loss: 0.11357573419809341
step: 3000, Loss: 0.1145850345492363
step: 3100, Loss: 0.11362425237894058
step: 3200, Loss: 0.11436682939529419
step: 3300, Loss: 0.11485297977924347
step: 3400, Loss: 4.346766948699951
step: 3500, Loss: 0.14368696510791779
step: 3600, Loss: 0.12487579137086868
step: 3700, Loss: 0.12346380949020386
step: 3800, Loss: 0.12200901657342911
step: 3900, Loss: 0.12083560228347778
step: 4000, Loss: 0.11749923974275589
step: 4100, Loss: 0.12013277411460876
step: 4200, Loss: 0.12731313705444336
step: 4300, Loss: 0.11779438704252243
step: 4400, Loss: 0.12058354914188385
step: 4500, Loss: 0.12004555016756058
step: 4600, Loss: 0.11582222580909729
step: 4700, Loss: 0.1186414584517479
step: 4800, Loss: 0.11817315965890884
step: 4900, Loss: 0.11714932322502136
step: 5000, Loss: 0.11569473147392273
step: 5100, Loss: 0.11659777164459229
step: 5200, Loss: 0.11380546540021896
step: 5300, Loss: 0.11458352208137512
step: 5400, Loss: 0.11607587337493896
step: 5500, Loss: 0.1165260523557663
step: 5600, Loss: 0.11775389313697815
step: 5700, Loss: 0.11640552431344986
step: 5800, Loss: 0.11522834002971649
step: 5900, Loss: 0.11462746560573578
step: 6000, Loss: 0.11430477350950241
step: 6100, Loss: 0.11453843116760254
step: 6200, Loss: 0.11520804464817047
step: 6300, Loss: 0.11403132230043411
step: 6400, Loss: 0.11413935571908951
step: 6500, Loss: 0.11365143954753876
step: 6600, Loss: 0.1137266457080841
step: 6700, Loss: 0.11413887143135071
step: 6800, Loss: 0.1153782531619072
step: 6900, Loss: 0.11352483183145523
step: 7000, Loss: 0.11331233382225037
step: 7100, Loss: 0.11335968226194382
step: 7200, Loss: 0.11391090601682663
step: 7300, Loss: 0.11354462802410126
step: 7400, Loss: 0.11364395171403885
step: 7500, Loss: 0.11498409509658813
step: 7600, Loss: 0.11413665860891342
step: 7700, Loss: 0.11422345787286758
step: 7800, Loss: 0.11329726874828339
step: 7900, Loss: 0.11364055424928665
step: 8000, Loss: 0.11465489119291306
step: 8100, Loss: 0.11402978003025055
step: 8200, Loss: 0.11461273580789566
step: 8300, Loss: 0.11419065296649933
step: 8400, Loss: 0.11333418637514114
step: 8500, Loss: 0.11278532445430756
step: 8600, Loss: 0.11456348747015
step: 8700, Loss: 0.11363790929317474
step: 8800, Loss: 0.11286414414644241
step: 8900, Loss: 0.11428496241569519
step: 9000, Loss: 0.11447654664516449
step: 9100, Loss: 0.11312391608953476
step: 9200, Loss: 0.1131797581911087
step: 9300, Loss: 0.11382880806922913
step: 9400, Loss: 0.11256641894578934
step: 9500, Loss: 0.11250156909227371
step: 9600, Loss: 0.11360741406679153
step: 9700, Loss: 0.11349773406982422
step: 9800, Loss: 0.1139029860496521
step: 9900, Loss: 0.11444330215454102
training successfully ended.
validating...
validate data length:106
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:958
step: 0, Loss: 0.11427068710327148
step: 100, Loss: 0.11403337121009827
step: 200, Loss: 0.11459669470787048
step: 300, Loss: 0.11408515274524689
step: 400, Loss: 0.11591470241546631
step: 500, Loss: 0.11470114439725876
step: 600, Loss: 0.11426013708114624
step: 700, Loss: 0.11455144733190536
step: 800, Loss: 0.11311545222997665
step: 900, Loss: 0.113257996737957
step: 1000, Loss: 0.1135290339589119
step: 1100, Loss: 0.11276993155479431
step: 1200, Loss: 0.11277840286493301
step: 1300, Loss: 0.11345890164375305
step: 1400, Loss: 0.11272047460079193
step: 1500, Loss: 0.11417729407548904
step: 1600, Loss: 0.11288385093212128
step: 1700, Loss: 0.11376851797103882
step: 1800, Loss: 0.1131783127784729
step: 1900, Loss: 0.11506333947181702
step: 2000, Loss: 0.11430897563695908
step: 2100, Loss: 0.11316470801830292
step: 2200, Loss: 0.11323117464780807
step: 2300, Loss: 0.11287117749452591
step: 2400, Loss: 0.11470542848110199
step: 2500, Loss: 0.11345364153385162
step: 2600, Loss: 0.11341550201177597
step: 2700, Loss: 0.11407129466533661
step: 2800, Loss: 0.11419178545475006
step: 2900, Loss: 0.11371694505214691
step: 3000, Loss: 0.11280089616775513
step: 3100, Loss: 0.11444936692714691
step: 3200, Loss: 0.11351669579744339
step: 3300, Loss: 0.11442478746175766
step: 3400, Loss: 0.11363852024078369
step: 3500, Loss: 0.11574999988079071
step: 3600, Loss: 0.9221560955047607
step: 3700, Loss: 0.13052025437355042
step: 3800, Loss: 0.13215218484401703
step: 3900, Loss: 0.1224496066570282
step: 4000, Loss: 0.11981578916311264
step: 4100, Loss: 0.1222844272851944
step: 4200, Loss: 0.11944706737995148
step: 4300, Loss: 0.11928786337375641
step: 4400, Loss: 0.12426277250051498
step: 4500, Loss: 0.11690336465835571
step: 4600, Loss: 0.11637140810489655
step: 4700, Loss: 0.11557580530643463
step: 4800, Loss: 0.11811868846416473
step: 4900, Loss: 0.11484432965517044
step: 5000, Loss: 0.11518358439207077
step: 5100, Loss: 0.11558733135461807
step: 5200, Loss: 0.11510968953371048
step: 5300, Loss: 0.11545808613300323
step: 5400, Loss: 0.11702035367488861
step: 5500, Loss: 0.11516745388507843
step: 5600, Loss: 0.11496175825595856
step: 5700, Loss: 0.11552862077951431
step: 5800, Loss: 0.11503677070140839
step: 5900, Loss: 0.11489909142255783
step: 6000, Loss: 0.11669877171516418
step: 6100, Loss: 0.11403857916593552
step: 6200, Loss: 0.1145537793636322
step: 6300, Loss: 0.11491566896438599
step: 6400, Loss: 0.11495878547430038
step: 6500, Loss: 0.11426272988319397
step: 6600, Loss: 0.11379381269216537
step: 6700, Loss: 0.11508268862962723
step: 6800, Loss: 0.11360827833414078
step: 6900, Loss: 0.11427348852157593
step: 7000, Loss: 0.11452791839838028
step: 7100, Loss: 0.11547105014324188
step: 7200, Loss: 0.1137290671467781
step: 7300, Loss: 0.1136271059513092
step: 7400, Loss: 0.11327138543128967
step: 7500, Loss: 0.11343065649271011
step: 7600, Loss: 0.1140306144952774
step: 7700, Loss: 0.11355336755514145
step: 7800, Loss: 0.11488042771816254
step: 7900, Loss: 0.11329232901334763
step: 8000, Loss: 0.11433357000350952
step: 8100, Loss: 0.1131315678358078
step: 8200, Loss: 0.11365551501512527
step: 8300, Loss: 0.11397448182106018
step: 8400, Loss: 0.1130390390753746
step: 8500, Loss: 0.11303640156984329
step: 8600, Loss: 0.11342043429613113
step: 8700, Loss: 0.11306219547986984
step: 8800, Loss: 0.11342473328113556
step: 8900, Loss: 0.11536858230829239
step: 9000, Loss: 0.1141323447227478
step: 9100, Loss: 0.11390463262796402
step: 9200, Loss: 0.11245237290859222
step: 9300, Loss: 0.11442109942436218
step: 9400, Loss: 0.11273007839918137
step: 9500, Loss: 0.11332684755325317
step: 9600, Loss: 0.11278786510229111
step: 9700, Loss: 0.1128430962562561
step: 9800, Loss: 0.11325312405824661
step: 9900, Loss: 0.11450091749429703
training successfully ended.
validating...
validate data length:106
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:958
step: 0, Loss: 0.11300578713417053
step: 100, Loss: 0.11573997884988785
step: 200, Loss: 0.1153077483177185
step: 300, Loss: 0.11522413790225983
step: 400, Loss: 0.11532767117023468
step: 500, Loss: 0.11316095292568207
step: 600, Loss: 0.11394011974334717
step: 700, Loss: 0.11533626168966293
step: 800, Loss: 0.11416991800069809
step: 900, Loss: 0.11330760270357132
step: 1000, Loss: 0.11411362886428833
step: 1100, Loss: 0.11326802521944046
step: 1200, Loss: 0.11401113867759705
step: 1300, Loss: 0.11292729526758194
step: 1400, Loss: 0.11369083821773529
step: 1500, Loss: 0.1136956438422203
step: 1600, Loss: 0.11256422847509384
step: 1700, Loss: 0.11277361214160919
step: 1800, Loss: 0.11309237778186798
step: 1900, Loss: 0.11343906074762344
step: 2000, Loss: 0.1130288764834404
step: 2100, Loss: 0.11404533684253693
step: 2200, Loss: 0.11368227750062943
step: 2300, Loss: 0.11331558227539062
step: 2400, Loss: 0.11370045691728592
step: 2500, Loss: 0.1146489679813385
step: 2600, Loss: 0.11349412798881531
step: 2700, Loss: 0.11381494253873825
step: 2800, Loss: 0.1137765571475029
step: 2900, Loss: 0.11434660851955414
step: 3000, Loss: 0.11307185888290405
step: 3100, Loss: 0.11524226516485214
step: 3200, Loss: 0.11270985007286072
step: 3300, Loss: 0.11305276304483414
step: 3400, Loss: 0.4363848567008972
step: 3500, Loss: 0.12353073060512543
step: 3600, Loss: 0.1278073489665985
step: 3700, Loss: 0.12250208109617233
step: 3800, Loss: 0.11900032311677933
step: 3900, Loss: 0.1201925128698349
step: 4000, Loss: 0.11904177814722061
step: 4100, Loss: 0.11606651544570923
step: 4200, Loss: 0.11829151958227158
step: 4300, Loss: 0.1180180013179779
step: 4400, Loss: 0.11985792219638824
step: 4500, Loss: 0.11673550307750702
step: 4600, Loss: 0.11690816283226013
step: 4700, Loss: 0.11509359627962112
step: 4800, Loss: 0.11514133214950562
step: 4900, Loss: 0.11575264483690262
step: 5000, Loss: 0.11785184592008591
step: 5100, Loss: 0.11718560755252838
step: 5200, Loss: 0.11565869301557541
step: 5300, Loss: 0.11456957459449768
step: 5400, Loss: 0.11666586995124817
step: 5500, Loss: 0.11604179441928864
step: 5600, Loss: 0.11428936570882797
step: 5700, Loss: 0.11582435667514801
step: 5800, Loss: 0.11560741066932678
step: 5900, Loss: 0.11564332246780396
step: 6000, Loss: 0.11406400799751282
step: 6100, Loss: 0.11545193195343018
step: 6200, Loss: 0.11411696672439575
step: 6300, Loss: 0.11396587640047073
step: 6400, Loss: 0.11340834945440292
step: 6500, Loss: 0.11387623846530914
step: 6600, Loss: 0.11395378410816193
step: 6700, Loss: 0.11316949129104614
step: 6800, Loss: 0.11446671932935715
step: 6900, Loss: 0.11435878276824951
step: 7000, Loss: 0.11478621512651443
step: 7100, Loss: 0.11534113436937332
step: 7200, Loss: 0.11373741924762726
step: 7300, Loss: 0.11383511126041412
step: 7400, Loss: 0.11390403658151627
step: 7500, Loss: 0.11363166570663452
step: 7600, Loss: 0.11365767568349838
step: 7700, Loss: 0.11330890655517578
step: 7800, Loss: 0.11260908842086792
step: 7900, Loss: 0.11430440843105316
step: 8000, Loss: 0.11498568952083588
step: 8100, Loss: 0.11407259106636047
step: 8200, Loss: 0.11390221863985062
step: 8300, Loss: 0.11368101835250854
step: 8400, Loss: 0.11422127485275269
step: 8500, Loss: 0.11319537460803986
step: 8600, Loss: 0.1128784641623497
step: 8700, Loss: 0.11347975581884384
step: 8800, Loss: 0.11433978378772736
step: 8900, Loss: 0.11429736018180847
step: 9000, Loss: 0.11445172131061554
step: 9100, Loss: 0.11346235126256943
step: 9200, Loss: 0.11290409415960312
step: 9300, Loss: 0.112953782081604
step: 9400, Loss: 0.11282458156347275
step: 9500, Loss: 0.11263544857501984
step: 9600, Loss: 0.11619376391172409
step: 9700, Loss: 0.11307092010974884
step: 9800, Loss: 0.1141243427991867
step: 9900, Loss: 0.11291859298944473
training successfully ended.
validating...
validate data length:106
acc: 0.9807692307692307
precision: 0.96
recall: 1.0
F_score: 0.9795918367346939
subject 23 Avgacc: 0.9865384615384615 Avgfscore: 0.9874610191245115 
 Max acc:1.0, Max f score:1.0
******** mix subject_24 ********

[266, 494]
******fold 1******

Training... train_data length:889
step: 0, Loss: 35.9224739074707
step: 100, Loss: 4.5027756690979
step: 200, Loss: 0.8582062125205994
step: 300, Loss: 0.8319851160049438
step: 400, Loss: 0.1904917061328888
step: 500, Loss: 0.16558615863323212
step: 600, Loss: 0.14185860753059387
step: 700, Loss: 0.15413637459278107
step: 800, Loss: 0.1290329545736313
step: 900, Loss: 0.13425834476947784
step: 1000, Loss: 0.12935341894626617
step: 1100, Loss: 0.1339235156774521
step: 1200, Loss: 0.12231208384037018
step: 1300, Loss: 0.13165196776390076
step: 1400, Loss: 0.12411553412675858
step: 1500, Loss: 0.12203751504421234
step: 1600, Loss: 0.1243271678686142
step: 1700, Loss: 0.12399519979953766
step: 1800, Loss: 0.11769790947437286
step: 1900, Loss: 0.12370992451906204
step: 2000, Loss: 0.1187976747751236
step: 2100, Loss: 0.11986054480075836
step: 2200, Loss: 0.12264396250247955
step: 2300, Loss: 0.11918671429157257
step: 2400, Loss: 0.11847538501024246
step: 2500, Loss: 0.11941541731357574
step: 2600, Loss: 0.11635980010032654
step: 2700, Loss: 0.11945956945419312
step: 2800, Loss: 0.11624392867088318
step: 2900, Loss: 0.11838234961032867
step: 3000, Loss: 0.11496474593877792
step: 3100, Loss: 0.11842653900384903
step: 3200, Loss: 0.11914420127868652
step: 3300, Loss: 0.11701018363237381
step: 3400, Loss: 0.11832989752292633
step: 3500, Loss: 0.1176288053393364
step: 3600, Loss: 0.1146230548620224
step: 3700, Loss: 0.11715386062860489
step: 3800, Loss: 0.11442935466766357
step: 3900, Loss: 0.11530227214097977
step: 4000, Loss: 0.11522867530584335
step: 4100, Loss: 0.11605457961559296
step: 4200, Loss: 0.11712343990802765
step: 4300, Loss: 0.11534371227025986
step: 4400, Loss: 0.11353190243244171
step: 4500, Loss: 0.11551970988512039
step: 4600, Loss: 0.11528441309928894
step: 4700, Loss: 0.11409870535135269
step: 4800, Loss: 0.11495322734117508
step: 4900, Loss: 0.11383970081806183
step: 5000, Loss: 0.11391037702560425
step: 5100, Loss: 0.1140395998954773
step: 5200, Loss: 0.11456730961799622
step: 5300, Loss: 0.11510733515024185
step: 5400, Loss: 0.1143491193652153
step: 5500, Loss: 0.11295677721500397
step: 5600, Loss: 0.11387945711612701
step: 5700, Loss: 0.11346261948347092
step: 5800, Loss: 0.11326245218515396
step: 5900, Loss: 0.11347678303718567
step: 6000, Loss: 0.113706074655056
step: 6100, Loss: 0.11539988219738007
step: 6200, Loss: 0.5074431896209717
step: 6300, Loss: 0.34544000029563904
step: 6400, Loss: 0.131550595164299
step: 6500, Loss: 0.1329423487186432
step: 6600, Loss: 0.125055193901062
step: 6700, Loss: 0.12393468618392944
step: 6800, Loss: 0.12069649249315262
step: 6900, Loss: 0.12292571365833282
step: 7000, Loss: 0.12229331582784653
step: 7100, Loss: 0.11793169379234314
step: 7200, Loss: 0.12024353444576263
step: 7300, Loss: 0.12384118884801865
step: 7400, Loss: 0.1197330430150032
step: 7500, Loss: 0.11916424334049225
step: 7600, Loss: 0.11541537195444107
step: 7700, Loss: 0.11896885931491852
step: 7800, Loss: 0.11848029494285583
step: 7900, Loss: 0.12000815570354462
step: 8000, Loss: 0.11714274436235428
step: 8100, Loss: 0.119021475315094
step: 8200, Loss: 0.11756764352321625
step: 8300, Loss: 0.11675291508436203
step: 8400, Loss: 0.11766313016414642
step: 8500, Loss: 0.11770564317703247
step: 8600, Loss: 0.11548174172639847
step: 8700, Loss: 0.11891614645719528
step: 8800, Loss: 0.11687284708023071
step: 8900, Loss: 0.11568056046962738
step: 9000, Loss: 0.115846648812294
step: 9100, Loss: 0.11586058884859085
step: 9200, Loss: 0.11500655114650726
step: 9300, Loss: 0.11742282658815384
step: 9400, Loss: 0.11776398867368698
step: 9500, Loss: 0.11379491537809372
step: 9600, Loss: 0.11570912599563599
step: 9700, Loss: 0.11475065350532532
step: 9800, Loss: 0.1142210140824318
step: 9900, Loss: 0.11755982041358948
training successfully ended.
validating...
validate data length:99
acc: 0.9166666666666666
precision: 0.9032258064516129
recall: 0.9655172413793104
F_score: 0.9333333333333333
******fold 2******

Training... train_data length:889
step: 0, Loss: 0.1164156049489975
step: 100, Loss: 0.12657728791236877
step: 200, Loss: 0.11752427369356155
step: 300, Loss: 0.11940160393714905
step: 400, Loss: 0.11712928116321564
step: 500, Loss: 0.11867962032556534
step: 600, Loss: 0.11571211367845535
step: 700, Loss: 0.11436387896537781
step: 800, Loss: 0.11663305014371872
step: 900, Loss: 0.11689756065607071
step: 1000, Loss: 0.11473611742258072
step: 1100, Loss: 0.11467868089675903
step: 1200, Loss: 0.11698595434427261
step: 1300, Loss: 0.11435947567224503
step: 1400, Loss: 0.11454840004444122
step: 1500, Loss: 0.11493813246488571
step: 1600, Loss: 0.11440840363502502
step: 1700, Loss: 0.1139681488275528
step: 1800, Loss: 0.1141933724284172
step: 1900, Loss: 0.11381357908248901
step: 2000, Loss: 0.1142808198928833
step: 2100, Loss: 0.11362884938716888
step: 2200, Loss: 0.11336196959018707
step: 2300, Loss: 0.11296922713518143
step: 2400, Loss: 0.11482936888933182
step: 2500, Loss: 0.11378125846385956
step: 2600, Loss: 0.1139819324016571
step: 2700, Loss: 0.1136014387011528
step: 2800, Loss: 0.11498309671878815
step: 2900, Loss: 0.11341354995965958
step: 3000, Loss: 0.11337833851575851
step: 3100, Loss: 0.11301963776350021
step: 3200, Loss: 0.1144445464015007
step: 3300, Loss: 0.11411544680595398
step: 3400, Loss: 0.1159096509218216
step: 3500, Loss: 0.11452589929103851
step: 3600, Loss: 0.11458368599414825
step: 3700, Loss: 0.11409112811088562
step: 3800, Loss: 0.11401541531085968
step: 3900, Loss: 0.11423107236623764
step: 4000, Loss: 0.11374738812446594
step: 4100, Loss: 0.11523130536079407
step: 4200, Loss: 0.11430568248033524
step: 4300, Loss: 0.1152702197432518
step: 4400, Loss: 0.11411231011152267
step: 4500, Loss: 0.11247168481349945
step: 4600, Loss: 0.11359481513500214
step: 4700, Loss: 0.11373359709978104
step: 4800, Loss: 0.11383816599845886
step: 4900, Loss: 0.11344699561595917
step: 5000, Loss: 0.113357774913311
step: 5100, Loss: 0.11337211728096008
step: 5200, Loss: 0.11306995898485184
step: 5300, Loss: 0.11451675742864609
step: 5400, Loss: 0.1156960129737854
step: 5500, Loss: 0.1264495700597763
step: 5600, Loss: 0.25821906328201294
step: 5700, Loss: 0.1551041305065155
step: 5800, Loss: 0.1316329687833786
step: 5900, Loss: 0.1280200183391571
step: 6000, Loss: 0.12887436151504517
step: 6100, Loss: 0.12722979485988617
step: 6200, Loss: 0.11847183108329773
step: 6300, Loss: 0.1181933656334877
step: 6400, Loss: 0.11832777410745621
step: 6500, Loss: 0.11786852031946182
step: 6600, Loss: 0.11894005537033081
step: 6700, Loss: 0.11881192773580551
step: 6800, Loss: 0.11569861322641373
step: 6900, Loss: 0.11736834794282913
step: 7000, Loss: 0.11820460855960846
step: 7100, Loss: 0.11743461340665817
step: 7200, Loss: 0.11622284352779388
step: 7300, Loss: 0.11863957345485687
step: 7400, Loss: 0.11685487627983093
step: 7500, Loss: 0.11571161448955536
step: 7600, Loss: 0.1181228905916214
step: 7700, Loss: 0.11631476879119873
step: 7800, Loss: 0.11554516851902008
step: 7900, Loss: 0.11641106009483337
step: 8000, Loss: 0.11503902077674866
step: 8100, Loss: 0.11477511376142502
step: 8200, Loss: 0.1153971403837204
step: 8300, Loss: 0.11570480465888977
step: 8400, Loss: 0.11446122080087662
step: 8500, Loss: 0.11620952188968658
step: 8600, Loss: 0.11411360651254654
step: 8700, Loss: 0.11552391946315765
step: 8800, Loss: 0.11557915806770325
step: 8900, Loss: 0.11428879201412201
step: 9000, Loss: 0.1160622388124466
step: 9100, Loss: 0.11444762349128723
step: 9200, Loss: 0.11600581556558609
step: 9300, Loss: 0.11609213054180145
step: 9400, Loss: 0.11577470600605011
step: 9500, Loss: 0.11446445435285568
step: 9600, Loss: 0.11419246345758438
step: 9700, Loss: 0.11489309370517731
step: 9800, Loss: 0.11361263692378998
step: 9900, Loss: 0.11363044381141663
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.9565217391304348
recall: 1.0
F_score: 0.9777777777777777
******fold 3******

Training... train_data length:889
step: 0, Loss: 0.11511184275150299
step: 100, Loss: 0.12337157875299454
step: 200, Loss: 0.11496874690055847
step: 300, Loss: 0.11642307043075562
step: 400, Loss: 0.11544244736433029
step: 500, Loss: 0.11421392112970352
step: 600, Loss: 0.11423725634813309
step: 700, Loss: 0.11566543579101562
step: 800, Loss: 0.11554987728595734
step: 900, Loss: 0.11394891887903214
step: 1000, Loss: 0.11453819274902344
step: 1100, Loss: 0.11483680456876755
step: 1200, Loss: 0.11382589489221573
step: 1300, Loss: 0.1141868531703949
step: 1400, Loss: 0.1143621951341629
step: 1500, Loss: 0.11500325798988342
step: 1600, Loss: 0.11389806866645813
step: 1700, Loss: 0.11431463062763214
step: 1800, Loss: 0.11376526206731796
step: 1900, Loss: 0.11487473547458649
step: 2000, Loss: 0.1133110374212265
step: 2100, Loss: 0.11414021998643875
step: 2200, Loss: 0.11338849365711212
step: 2300, Loss: 0.1140177994966507
step: 2400, Loss: 0.11249715834856033
step: 2500, Loss: 0.11331896483898163
step: 2600, Loss: 0.11404610425233841
step: 2700, Loss: 0.11281885206699371
step: 2800, Loss: 0.11512760818004608
step: 2900, Loss: 0.11459396779537201
step: 3000, Loss: 0.11356963217258453
step: 3100, Loss: 0.11336591094732285
step: 3200, Loss: 0.11475572735071182
step: 3300, Loss: 0.11473822593688965
step: 3400, Loss: 0.11311405152082443
step: 3500, Loss: 0.11369027197360992
step: 3600, Loss: 0.11665226519107819
step: 3700, Loss: 0.11314661055803299
step: 3800, Loss: 0.11335586756467819
step: 3900, Loss: 0.11248984187841415
step: 4000, Loss: 0.1138390302658081
step: 4100, Loss: 0.11468490958213806
step: 4200, Loss: 0.11320813000202179
step: 4300, Loss: 0.11338597536087036
step: 4400, Loss: 0.1138579398393631
step: 4500, Loss: 0.11404070258140564
step: 4600, Loss: 0.11268603801727295
step: 4700, Loss: 0.11506206542253494
step: 4800, Loss: 0.11426956951618195
step: 4900, Loss: 0.11365118622779846
step: 5000, Loss: 0.1134210079908371
step: 5100, Loss: 0.1139734536409378
step: 5200, Loss: 0.11323235929012299
step: 5300, Loss: 0.1135963648557663
step: 5400, Loss: 0.11484009772539139
step: 5500, Loss: 0.11444547772407532
step: 5600, Loss: 1.7041674852371216
step: 5700, Loss: 0.1333383470773697
step: 5800, Loss: 0.1210079938173294
step: 5900, Loss: 0.12197516858577728
step: 6000, Loss: 0.1237100213766098
step: 6100, Loss: 0.1251906305551529
step: 6200, Loss: 0.11717265099287033
step: 6300, Loss: 0.12058647722005844
step: 6400, Loss: 0.11631228774785995
step: 6500, Loss: 0.11713165789842606
step: 6600, Loss: 0.11668498814105988
step: 6700, Loss: 0.11698204278945923
step: 6800, Loss: 0.11542019248008728
step: 6900, Loss: 0.11555778980255127
step: 7000, Loss: 0.11558179557323456
step: 7100, Loss: 0.11504187434911728
step: 7200, Loss: 0.11666245013475418
step: 7300, Loss: 0.11684834212064743
step: 7400, Loss: 0.11873279511928558
step: 7500, Loss: 0.11581113189458847
step: 7600, Loss: 0.11486349254846573
step: 7700, Loss: 0.11533214151859283
step: 7800, Loss: 0.11560352891683578
step: 7900, Loss: 0.11573875695466995
step: 8000, Loss: 0.11559687554836273
step: 8100, Loss: 0.11667217314243317
step: 8200, Loss: 0.114842489361763
step: 8300, Loss: 0.11423149704933167
step: 8400, Loss: 0.11510322242975235
step: 8500, Loss: 0.11401157826185226
step: 8600, Loss: 0.11339890211820602
step: 8700, Loss: 0.11351776123046875
step: 8800, Loss: 0.11453075706958771
step: 8900, Loss: 0.11290877312421799
step: 9000, Loss: 0.1134696751832962
step: 9100, Loss: 0.1145622655749321
step: 9200, Loss: 0.11318576335906982
step: 9300, Loss: 0.1142132505774498
step: 9400, Loss: 0.11593855917453766
step: 9500, Loss: 0.11395763605833054
step: 9600, Loss: 0.11292844265699387
step: 9700, Loss: 0.11429768800735474
step: 9800, Loss: 0.11477309465408325
step: 9900, Loss: 0.11491802334785461
training successfully ended.
validating...
validate data length:99
acc: 0.9895833333333334
precision: 0.9803921568627451
recall: 1.0
F_score: 0.99009900990099
******fold 4******

Training... train_data length:889
step: 0, Loss: 0.11434603482484818
step: 100, Loss: 0.116090327501297
step: 200, Loss: 0.11519699543714523
step: 300, Loss: 0.11550231277942657
step: 400, Loss: 0.11492754518985748
step: 500, Loss: 0.11509977281093597
step: 600, Loss: 0.11429071426391602
step: 700, Loss: 0.11417786031961441
step: 800, Loss: 0.11335369944572449
step: 900, Loss: 0.11456739157438278
step: 1000, Loss: 0.11308807879686356
step: 1100, Loss: 0.11446909606456757
step: 1200, Loss: 0.1140398383140564
step: 1300, Loss: 0.1143152266740799
step: 1400, Loss: 0.11394056677818298
step: 1500, Loss: 0.11511635780334473
step: 1600, Loss: 0.11451556533575058
step: 1700, Loss: 0.11386601626873016
step: 1800, Loss: 0.11312483251094818
step: 1900, Loss: 0.11492811888456345
step: 2000, Loss: 0.11411187052726746
step: 2100, Loss: 0.11298452317714691
step: 2200, Loss: 0.11288265138864517
step: 2300, Loss: 0.11521148681640625
step: 2400, Loss: 0.11440300941467285
step: 2500, Loss: 0.11358778923749924
step: 2600, Loss: 0.11401453614234924
step: 2700, Loss: 0.11362942308187485
step: 2800, Loss: 0.11471256613731384
step: 2900, Loss: 0.11319967359304428
step: 3000, Loss: 0.12345033138990402
step: 3100, Loss: 0.17406338453292847
step: 3200, Loss: 0.12542887032032013
step: 3300, Loss: 0.12476705759763718
step: 3400, Loss: 0.12525442242622375
step: 3500, Loss: 0.12012230604887009
step: 3600, Loss: 0.1187685877084732
step: 3700, Loss: 0.12083516269922256
step: 3800, Loss: 0.11797620356082916
step: 3900, Loss: 0.1162782832980156
step: 4000, Loss: 0.11694934219121933
step: 4100, Loss: 0.11659000813961029
step: 4200, Loss: 0.11619564890861511
step: 4300, Loss: 0.11766231060028076
step: 4400, Loss: 0.11675328016281128
step: 4500, Loss: 0.11476259678602219
step: 4600, Loss: 0.11710835248231888
step: 4700, Loss: 0.1159491017460823
step: 4800, Loss: 0.11523724347352982
step: 4900, Loss: 0.11710228770971298
step: 5000, Loss: 0.11563526093959808
step: 5100, Loss: 0.1157669872045517
step: 5200, Loss: 0.11533401906490326
step: 5300, Loss: 0.11467921733856201
step: 5400, Loss: 0.11469045281410217
step: 5500, Loss: 0.11428896337747574
step: 5600, Loss: 0.11526705324649811
step: 5700, Loss: 0.11418795585632324
step: 5800, Loss: 0.11393710970878601
step: 5900, Loss: 0.11560595780611038
step: 6000, Loss: 0.11618595570325851
step: 6100, Loss: 0.11554200202226639
step: 6200, Loss: 0.1142643466591835
step: 6300, Loss: 0.11438311636447906
step: 6400, Loss: 0.11523786187171936
step: 6500, Loss: 0.11416507512331009
step: 6600, Loss: 0.11382655799388885
step: 6700, Loss: 0.11501540243625641
step: 6800, Loss: 0.11461751908063889
step: 6900, Loss: 0.11306656897068024
step: 7000, Loss: 0.11292780935764313
step: 7100, Loss: 0.11365364491939545
step: 7200, Loss: 0.11331428587436676
step: 7300, Loss: 0.1141933873295784
step: 7400, Loss: 0.11324571818113327
step: 7500, Loss: 0.1136336624622345
step: 7600, Loss: 0.1138363629579544
step: 7700, Loss: 0.11345767229795456
step: 7800, Loss: 0.11387874186038971
step: 7900, Loss: 0.1141737699508667
step: 8000, Loss: 0.11360014975070953
step: 8100, Loss: 0.11288090795278549
step: 8200, Loss: 0.1128377914428711
step: 8300, Loss: 0.11287982016801834
step: 8400, Loss: 0.11403613537549973
step: 8500, Loss: 0.1132536381483078
step: 8600, Loss: 0.11291608959436417
step: 8700, Loss: 0.11329574882984161
step: 8800, Loss: 0.11493538320064545
step: 8900, Loss: 0.11364663392305374
step: 9000, Loss: 0.11351770907640457
step: 9100, Loss: 0.11300604045391083
step: 9200, Loss: 0.11343404650688171
step: 9300, Loss: 0.11331262439489365
step: 9400, Loss: 0.11386410892009735
step: 9500, Loss: 0.11365984380245209
step: 9600, Loss: 0.11403932422399521
step: 9700, Loss: 0.11287368834018707
step: 9800, Loss: 0.11314219981431961
step: 9900, Loss: 0.11222217977046967
training successfully ended.
validating...
validate data length:99
acc: 0.9791666666666666
precision: 0.9803921568627451
recall: 0.9803921568627451
F_score: 0.9803921568627451
******fold 5******

Training... train_data length:889
step: 0, Loss: 12.877151489257812
step: 100, Loss: 3.801464080810547
step: 200, Loss: 0.20995137095451355
step: 300, Loss: 0.15513388812541962
step: 400, Loss: 0.15602508187294006
step: 500, Loss: 0.15220898389816284
step: 600, Loss: 0.1567946970462799
step: 700, Loss: 0.13704970479011536
step: 800, Loss: 0.14574962854385376
step: 900, Loss: 0.12476678192615509
step: 1000, Loss: 0.13011805713176727
step: 1100, Loss: 0.12429941445589066
step: 1200, Loss: 0.13464388251304626
step: 1300, Loss: 0.12365330010652542
step: 1400, Loss: 0.12449926137924194
step: 1500, Loss: 0.12497463822364807
step: 1600, Loss: 0.12329747527837753
step: 1700, Loss: 0.1250995248556137
step: 1800, Loss: 0.12058883905410767
step: 1900, Loss: 0.12167450785636902
step: 2000, Loss: 0.12630149722099304
step: 2100, Loss: 0.1185980886220932
step: 2200, Loss: 0.11947514116764069
step: 2300, Loss: 0.1186770349740982
step: 2400, Loss: 0.11796946823596954
step: 2500, Loss: 0.11802744120359421
step: 2600, Loss: 0.12011643499135971
step: 2700, Loss: 0.11858347803354263
step: 2800, Loss: 0.11872692406177521
step: 2900, Loss: 0.11709253489971161
step: 3000, Loss: 0.11820726096630096
step: 3100, Loss: 0.11627824604511261
step: 3200, Loss: 0.11987505108118057
step: 3300, Loss: 0.11592014133930206
step: 3400, Loss: 0.11334584653377533
step: 3500, Loss: 0.11534759402275085
step: 3600, Loss: 0.11709975451231003
step: 3700, Loss: 0.116634801030159
step: 3800, Loss: 0.11937372386455536
step: 3900, Loss: 0.11499570310115814
step: 4000, Loss: 0.11669667810201645
step: 4100, Loss: 0.11737239360809326
step: 4200, Loss: 0.11965134739875793
step: 4300, Loss: 0.1191573515534401
step: 4400, Loss: 0.11588525772094727
step: 4500, Loss: 0.11541006714105606
step: 4600, Loss: 0.11788169294595718
step: 4700, Loss: 0.13437432050704956
step: 4800, Loss: 2.00870418548584
step: 4900, Loss: 0.2094229757785797
step: 5000, Loss: 0.16657832264900208
step: 5100, Loss: 0.1604139506816864
step: 5200, Loss: 0.1543099284172058
step: 5300, Loss: 0.13030515611171722
step: 5400, Loss: 0.1364879608154297
step: 5500, Loss: 0.13068844377994537
step: 5600, Loss: 0.14279213547706604
step: 5700, Loss: 0.128421813249588
step: 5800, Loss: 0.13167299330234528
step: 5900, Loss: 0.12239086627960205
step: 6000, Loss: 0.12520071864128113
step: 6100, Loss: 0.12201851606369019
step: 6200, Loss: 0.12327602505683899
step: 6300, Loss: 0.12059777975082397
step: 6400, Loss: 0.12414176762104034
step: 6500, Loss: 0.12054069340229034
step: 6600, Loss: 0.1186893954873085
step: 6700, Loss: 0.1175636351108551
step: 6800, Loss: 0.11725206673145294
step: 6900, Loss: 0.11564069241285324
step: 7000, Loss: 0.12363733351230621
step: 7100, Loss: 0.11825256794691086
step: 7200, Loss: 0.12065446376800537
step: 7300, Loss: 0.11808924376964569
step: 7400, Loss: 0.1196596771478653
step: 7500, Loss: 0.11722727864980698
step: 7600, Loss: 0.1177196279168129
step: 7700, Loss: 0.11740770936012268
step: 7800, Loss: 0.12009783089160919
step: 7900, Loss: 0.11717883497476578
step: 8000, Loss: 0.11894720047712326
step: 8100, Loss: 0.11521989107131958
step: 8200, Loss: 0.11932540684938431
step: 8300, Loss: 0.11639507114887238
step: 8400, Loss: 0.11776921898126602
step: 8500, Loss: 0.11592474579811096
step: 8600, Loss: 0.116766557097435
step: 8700, Loss: 0.11737527698278427
step: 8800, Loss: 0.11714678257703781
step: 8900, Loss: 0.11551257222890854
step: 9000, Loss: 0.1179027408361435
step: 9100, Loss: 0.11485566198825836
step: 9200, Loss: 0.11657895147800446
step: 9300, Loss: 0.11486910283565521
step: 9400, Loss: 0.11934806406497955
step: 9500, Loss: 0.11508651822805405
step: 9600, Loss: 0.11559173464775085
step: 9700, Loss: 0.11408258974552155
step: 9800, Loss: 0.11684302240610123
step: 9900, Loss: 0.11724536120891571
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.94
recall: 0.9791666666666666
F_score: 0.9591836734693877
******fold 6******

Training... train_data length:889
step: 0, Loss: 0.11375611275434494
step: 100, Loss: 0.114334337413311
step: 200, Loss: 0.11403834074735641
step: 300, Loss: 0.11511734873056412
step: 400, Loss: 0.11423631757497787
step: 500, Loss: 0.11329732090234756
step: 600, Loss: 0.11276748031377792
step: 700, Loss: 0.11419063806533813
step: 800, Loss: 0.11446446180343628
step: 900, Loss: 0.11406487226486206
step: 1000, Loss: 0.1129913181066513
step: 1100, Loss: 0.11406995356082916
step: 1200, Loss: 0.11357837170362473
step: 1300, Loss: 0.11358541995286942
step: 1400, Loss: 0.11336876451969147
step: 1500, Loss: 0.11349333822727203
step: 1600, Loss: 0.1134074404835701
step: 1700, Loss: 0.1141132116317749
step: 1800, Loss: 0.11340826749801636
step: 1900, Loss: 0.11301260441541672
step: 2000, Loss: 0.11266835033893585
step: 2100, Loss: 0.1141682118177414
step: 2200, Loss: 0.1149006336927414
step: 2300, Loss: 0.11379237473011017
step: 2400, Loss: 0.11313337087631226
step: 2500, Loss: 0.11404190212488174
step: 2600, Loss: 0.11354783177375793
step: 2700, Loss: 0.11346424371004105
step: 2800, Loss: 0.11336320638656616
step: 2900, Loss: 0.11356842517852783
step: 3000, Loss: 0.11279010772705078
step: 3100, Loss: 0.11349821835756302
step: 3200, Loss: 0.11375238746404648
step: 3300, Loss: 0.11376596242189407
step: 3400, Loss: 0.11257563531398773
step: 3500, Loss: 0.11325667798519135
step: 3600, Loss: 0.1135263442993164
step: 3700, Loss: 0.11354557424783707
step: 3800, Loss: 0.335431307554245
step: 3900, Loss: 0.3591020107269287
step: 4000, Loss: 0.1272045224905014
step: 4100, Loss: 0.1305249035358429
step: 4200, Loss: 0.12156175076961517
step: 4300, Loss: 0.12000244110822678
step: 4400, Loss: 0.11892664432525635
step: 4500, Loss: 0.11738350242376328
step: 4600, Loss: 0.11835847795009613
step: 4700, Loss: 0.12209624797105789
step: 4800, Loss: 0.11862830817699432
step: 4900, Loss: 0.11768124252557755
step: 5000, Loss: 0.11796526610851288
step: 5100, Loss: 0.11517151445150375
step: 5200, Loss: 0.11576514691114426
step: 5300, Loss: 0.11535636335611343
step: 5400, Loss: 0.11739353835582733
step: 5500, Loss: 0.11430618911981583
step: 5600, Loss: 0.116024449467659
step: 5700, Loss: 0.11482268571853638
step: 5800, Loss: 0.1166885569691658
step: 5900, Loss: 0.11546458303928375
step: 6000, Loss: 0.11693508177995682
step: 6100, Loss: 0.11476252228021622
step: 6200, Loss: 0.11603361368179321
step: 6300, Loss: 0.11615478992462158
step: 6400, Loss: 0.11692188680171967
step: 6500, Loss: 0.11637760698795319
step: 6600, Loss: 0.11503567546606064
step: 6700, Loss: 0.1153506338596344
step: 6800, Loss: 0.11387444287538528
step: 6900, Loss: 0.11539842188358307
step: 7000, Loss: 0.11530633270740509
step: 7100, Loss: 0.11433643847703934
step: 7200, Loss: 0.11576767265796661
step: 7300, Loss: 0.1130499392747879
step: 7400, Loss: 0.11545415222644806
step: 7500, Loss: 0.11479701101779938
step: 7600, Loss: 0.11377433687448502
step: 7700, Loss: 0.11392219364643097
step: 7800, Loss: 0.11384556442499161
step: 7900, Loss: 0.11438526958227158
step: 8000, Loss: 0.11496052145957947
step: 8100, Loss: 0.11494302749633789
step: 8200, Loss: 0.11290472745895386
step: 8300, Loss: 0.11335544288158417
step: 8400, Loss: 0.11292514204978943
step: 8500, Loss: 0.11496342718601227
step: 8600, Loss: 0.11454849690198898
step: 8700, Loss: 0.11497816443443298
step: 8800, Loss: 0.11490089446306229
step: 8900, Loss: 0.11308692395687103
step: 9000, Loss: 0.11436255276203156
step: 9100, Loss: 0.11390566825866699
step: 9200, Loss: 0.11388397216796875
step: 9300, Loss: 0.11324799060821533
step: 9400, Loss: 0.1136944442987442
step: 9500, Loss: 0.1136501133441925
step: 9600, Loss: 0.11271683126688004
step: 9700, Loss: 0.11268877238035202
step: 9800, Loss: 0.11265905201435089
step: 9900, Loss: 0.11392511427402496
training successfully ended.
validating...
validate data length:99
acc: 0.9895833333333334
precision: 0.9787234042553191
recall: 1.0
F_score: 0.989247311827957
******fold 7******

Training... train_data length:889
step: 0, Loss: 0.11401629447937012
step: 100, Loss: 0.11487831175327301
step: 200, Loss: 0.11486688256263733
step: 300, Loss: 0.11424541473388672
step: 400, Loss: 0.11348885297775269
step: 500, Loss: 0.11523088812828064
step: 600, Loss: 0.11259137094020844
step: 700, Loss: 0.11546426266431808
step: 800, Loss: 0.11485268175601959
step: 900, Loss: 0.11338692903518677
step: 1000, Loss: 0.1136285588145256
step: 1100, Loss: 0.11349430680274963
step: 1200, Loss: 0.1132984533905983
step: 1300, Loss: 0.11465341597795486
step: 1400, Loss: 0.11319942772388458
step: 1500, Loss: 0.1138308122754097
step: 1600, Loss: 0.11418154835700989
step: 1700, Loss: 0.11348499357700348
step: 1800, Loss: 0.1142406091094017
step: 1900, Loss: 0.11390873044729233
step: 2000, Loss: 0.11428602039813995
step: 2100, Loss: 0.11415567994117737
step: 2200, Loss: 0.1134951040148735
step: 2300, Loss: 0.1128145381808281
step: 2400, Loss: 0.11559049785137177
step: 2500, Loss: 0.11493781954050064
step: 2600, Loss: 0.11401012539863586
step: 2700, Loss: 0.11324045807123184
step: 2800, Loss: 0.11297641694545746
step: 2900, Loss: 0.11287939548492432
step: 3000, Loss: 0.11368421465158463
step: 3100, Loss: 0.11328113824129105
step: 3200, Loss: 0.11287081241607666
step: 3300, Loss: 0.11294599622488022
step: 3400, Loss: 0.11400514841079712
step: 3500, Loss: 0.1130891665816307
step: 3600, Loss: 0.113612100481987
step: 3700, Loss: 0.11396379768848419
step: 3800, Loss: 0.8913966417312622
step: 3900, Loss: 0.1822691261768341
step: 4000, Loss: 0.11714361608028412
step: 4100, Loss: 0.12172119319438934
step: 4200, Loss: 0.12250002473592758
step: 4300, Loss: 0.11790261417627335
step: 4400, Loss: 0.11775100976228714
step: 4500, Loss: 0.12005046010017395
step: 4600, Loss: 0.11694789677858353
step: 4700, Loss: 0.12014800310134888
step: 4800, Loss: 0.11710257828235626
step: 4900, Loss: 0.11719285696744919
step: 5000, Loss: 0.11471123993396759
step: 5100, Loss: 0.11944489181041718
step: 5200, Loss: 0.11445924639701843
step: 5300, Loss: 0.11390037089586258
step: 5400, Loss: 0.118411585688591
step: 5500, Loss: 0.11504090577363968
step: 5600, Loss: 0.11524652689695358
step: 5700, Loss: 0.11682167649269104
step: 5800, Loss: 0.11583605408668518
step: 5900, Loss: 0.11687065660953522
step: 6000, Loss: 0.1154354140162468
step: 6100, Loss: 0.11422834545373917
step: 6200, Loss: 0.11624860018491745
step: 6300, Loss: 0.11474648863077164
step: 6400, Loss: 0.11542487889528275
step: 6500, Loss: 0.11478693038225174
step: 6600, Loss: 0.11482884734869003
step: 6700, Loss: 0.11475925892591476
step: 6800, Loss: 0.11439649760723114
step: 6900, Loss: 0.11382431536912918
step: 7000, Loss: 0.11386337131261826
step: 7100, Loss: 0.11380431801080704
step: 7200, Loss: 0.11409901082515717
step: 7300, Loss: 0.11325520277023315
step: 7400, Loss: 0.1135474294424057
step: 7500, Loss: 0.1161573976278305
step: 7600, Loss: 0.11438798904418945
step: 7700, Loss: 0.11504755914211273
step: 7800, Loss: 0.11445523798465729
step: 7900, Loss: 0.11374130845069885
step: 8000, Loss: 0.11498266458511353
step: 8100, Loss: 0.11445578932762146
step: 8200, Loss: 0.11377987265586853
step: 8300, Loss: 0.11601775884628296
step: 8400, Loss: 0.11317707598209381
step: 8500, Loss: 0.11286625266075134
step: 8600, Loss: 0.1134117990732193
step: 8700, Loss: 0.11507962644100189
step: 8800, Loss: 0.11360835283994675
step: 8900, Loss: 0.11310184746980667
step: 9000, Loss: 0.11285822838544846
step: 9100, Loss: 0.11407697945833206
step: 9200, Loss: 0.11353927850723267
step: 9300, Loss: 0.11332457512617111
step: 9400, Loss: 0.11329317092895508
step: 9500, Loss: 0.11316791921854019
step: 9600, Loss: 0.11369200050830841
step: 9700, Loss: 0.11317650973796844
step: 9800, Loss: 0.11351330578327179
step: 9900, Loss: 0.11540129780769348
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:889
step: 0, Loss: 0.11773894727230072
step: 100, Loss: 0.11503972858190536
step: 200, Loss: 0.11393927782773972
step: 300, Loss: 0.11366251111030579
step: 400, Loss: 0.11437325924634933
step: 500, Loss: 0.11400912702083588
step: 600, Loss: 0.11349641531705856
step: 700, Loss: 0.11454789340496063
step: 800, Loss: 0.11419517546892166
step: 900, Loss: 0.11505480110645294
step: 1000, Loss: 0.1130824089050293
step: 1100, Loss: 0.11373280733823776
step: 1200, Loss: 0.11317913234233856
step: 1300, Loss: 0.11359865963459015
step: 1400, Loss: 0.11337774246931076
step: 1500, Loss: 0.1134139895439148
step: 1600, Loss: 0.11286690831184387
step: 1700, Loss: 0.11368328332901001
step: 1800, Loss: 0.11454857885837555
step: 1900, Loss: 0.11347603797912598
step: 2000, Loss: 0.11445122957229614
step: 2100, Loss: 0.1135658398270607
step: 2200, Loss: 0.11429274082183838
step: 2300, Loss: 0.11307503283023834
step: 2400, Loss: 0.11264213174581528
step: 2500, Loss: 0.11331818252801895
step: 2600, Loss: 0.11266689002513885
step: 2700, Loss: 0.11368897557258606
step: 2800, Loss: 0.11349277198314667
step: 2900, Loss: 0.11351217329502106
step: 3000, Loss: 0.11326996982097626
step: 3100, Loss: 0.1139862909913063
step: 3200, Loss: 0.11384662985801697
step: 3300, Loss: 0.11338453739881516
step: 3400, Loss: 0.11381594836711884
step: 3500, Loss: 0.11292935907840729
step: 3600, Loss: 0.1132265031337738
step: 3700, Loss: 6.070163726806641
step: 3800, Loss: 0.7611300945281982
step: 3900, Loss: 0.16544373333454132
step: 4000, Loss: 0.11697551608085632
step: 4100, Loss: 0.11811993271112442
step: 4200, Loss: 0.12160515785217285
step: 4300, Loss: 0.11686211824417114
step: 4400, Loss: 0.11780969798564911
step: 4500, Loss: 0.11655987054109573
step: 4600, Loss: 0.1154475063085556
step: 4700, Loss: 0.11455223709344864
step: 4800, Loss: 0.11931034177541733
step: 4900, Loss: 0.11493510752916336
step: 5000, Loss: 0.11421423405408859
step: 5100, Loss: 0.11765213310718536
step: 5200, Loss: 0.1155192032456398
step: 5300, Loss: 0.11645221710205078
step: 5400, Loss: 0.11630319803953171
step: 5500, Loss: 0.11639270186424255
step: 5600, Loss: 0.11516398191452026
step: 5700, Loss: 0.1156223714351654
step: 5800, Loss: 0.11518427729606628
step: 5900, Loss: 0.11449870467185974
step: 6000, Loss: 0.1147964596748352
step: 6100, Loss: 0.11527477204799652
step: 6200, Loss: 0.11584697663784027
step: 6300, Loss: 0.11467564851045609
step: 6400, Loss: 0.11427672952413559
step: 6500, Loss: 0.11382225900888443
step: 6600, Loss: 0.1162155345082283
step: 6700, Loss: 0.1151682659983635
step: 6800, Loss: 0.11465949565172195
step: 6900, Loss: 0.11389143764972687
step: 7000, Loss: 0.11563140898942947
step: 7100, Loss: 0.11396615952253342
step: 7200, Loss: 0.11609436571598053
step: 7300, Loss: 0.1150476485490799
step: 7400, Loss: 0.11381752789020538
step: 7500, Loss: 0.11312825977802277
step: 7600, Loss: 0.1150703877210617
step: 7700, Loss: 0.11405472457408905
step: 7800, Loss: 0.1146853044629097
step: 7900, Loss: 0.11331833899021149
step: 8000, Loss: 0.11374922841787338
step: 8100, Loss: 0.11333787441253662
step: 8200, Loss: 0.1128867119550705
step: 8300, Loss: 0.11333059519529343
step: 8400, Loss: 0.11320914328098297
step: 8500, Loss: 0.11366990953683853
step: 8600, Loss: 0.11293653398752213
step: 8700, Loss: 0.11368139833211899
step: 8800, Loss: 0.1133846715092659
step: 8900, Loss: 0.11268116533756256
step: 9000, Loss: 0.11335277557373047
step: 9100, Loss: 0.11318036168813705
step: 9200, Loss: 0.11416959017515182
step: 9300, Loss: 0.11272326856851578
step: 9400, Loss: 0.11307045817375183
step: 9500, Loss: 0.11284945905208588
step: 9600, Loss: 0.11313273012638092
step: 9700, Loss: 0.11277470737695694
step: 9800, Loss: 0.11321471631526947
step: 9900, Loss: 0.11405593156814575
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:890
step: 0, Loss: 0.11951800435781479
step: 100, Loss: 0.14680400490760803
step: 200, Loss: 0.11519847065210342
step: 300, Loss: 0.11466043442487717
step: 400, Loss: 0.11646737158298492
step: 500, Loss: 0.1160096749663353
step: 600, Loss: 0.11624018102884293
step: 700, Loss: 0.11416725814342499
step: 800, Loss: 0.11423949897289276
step: 900, Loss: 0.11552556604146957
step: 1000, Loss: 0.11435374617576599
step: 1100, Loss: 0.11505305767059326
step: 1200, Loss: 0.11338073015213013
step: 1300, Loss: 0.11391006410121918
step: 1400, Loss: 0.1134076789021492
step: 1500, Loss: 0.1148710623383522
step: 1600, Loss: 0.11446616798639297
step: 1700, Loss: 0.11321163177490234
step: 1800, Loss: 0.11328564584255219
step: 1900, Loss: 0.11297325789928436
step: 2000, Loss: 0.1140105128288269
step: 2100, Loss: 0.11341265588998795
step: 2200, Loss: 0.11327125132083893
step: 2300, Loss: 0.11414951831102371
step: 2400, Loss: 0.11406725645065308
step: 2500, Loss: 0.11394741386175156
step: 2600, Loss: 0.11423377692699432
step: 2700, Loss: 0.11367978900671005
step: 2800, Loss: 0.11340011656284332
step: 2900, Loss: 0.11259232461452484
step: 3000, Loss: 0.11379658430814743
step: 3100, Loss: 0.11470848321914673
step: 3200, Loss: 0.11385301500558853
step: 3300, Loss: 0.11397410184144974
step: 3400, Loss: 0.11607766151428223
step: 3500, Loss: 0.1132325828075409
step: 3600, Loss: 0.1131850928068161
step: 3700, Loss: 0.11320897936820984
step: 3800, Loss: 0.11448991298675537
step: 3900, Loss: 0.11289025098085403
step: 4000, Loss: 0.11377900093793869
step: 4100, Loss: 0.11374682188034058
step: 4200, Loss: 3.9212961196899414
step: 4300, Loss: 0.24369056522846222
step: 4400, Loss: 0.12484832108020782
step: 4500, Loss: 0.12103424221277237
step: 4600, Loss: 0.1190008893609047
step: 4700, Loss: 0.11870300769805908
step: 4800, Loss: 0.11972419917583466
step: 4900, Loss: 0.11828818172216415
step: 5000, Loss: 0.11803403496742249
step: 5100, Loss: 0.1189773827791214
step: 5200, Loss: 0.11515188217163086
step: 5300, Loss: 0.11717328429222107
step: 5400, Loss: 0.11860943585634232
step: 5500, Loss: 0.11608907580375671
step: 5600, Loss: 0.11558428406715393
step: 5700, Loss: 0.11758266389369965
step: 5800, Loss: 0.11602979898452759
step: 5900, Loss: 0.11589517444372177
step: 6000, Loss: 0.1146806925535202
step: 6100, Loss: 0.11380843073129654
step: 6200, Loss: 0.11731760948896408
step: 6300, Loss: 0.11603908240795135
step: 6400, Loss: 0.11517158895730972
step: 6500, Loss: 0.11431337147951126
step: 6600, Loss: 0.11424773931503296
step: 6700, Loss: 0.11570610105991364
step: 6800, Loss: 0.11512834578752518
step: 6900, Loss: 0.11821132898330688
step: 7000, Loss: 0.11775928735733032
step: 7100, Loss: 0.1145075187087059
step: 7200, Loss: 0.11547321081161499
step: 7300, Loss: 0.11487610638141632
step: 7400, Loss: 0.11380498111248016
step: 7500, Loss: 0.11607715487480164
step: 7600, Loss: 0.11446953564882278
step: 7700, Loss: 0.11663016676902771
step: 7800, Loss: 0.11338333040475845
step: 7900, Loss: 0.11510258913040161
step: 8000, Loss: 0.11407110840082169
step: 8100, Loss: 0.11462216824293137
step: 8200, Loss: 0.11427932977676392
step: 8300, Loss: 0.11374747008085251
step: 8400, Loss: 0.11460162699222565
step: 8500, Loss: 0.11340036988258362
step: 8600, Loss: 0.11391565203666687
step: 8700, Loss: 0.11378344893455505
step: 8800, Loss: 0.11491109430789948
step: 8900, Loss: 0.11305021494626999
step: 9000, Loss: 0.11365127563476562
step: 9100, Loss: 0.11449301987886429
step: 9200, Loss: 0.11398231238126755
step: 9300, Loss: 0.11432486772537231
step: 9400, Loss: 0.11458783596754074
step: 9500, Loss: 0.11338180303573608
step: 9600, Loss: 0.11426571756601334
step: 9700, Loss: 0.11391720920801163
step: 9800, Loss: 0.11418218910694122
step: 9900, Loss: 0.1132575049996376
training successfully ended.
validating...
validate data length:98
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:890
step: 0, Loss: 0.11559341847896576
step: 100, Loss: 0.11733416467905045
step: 200, Loss: 0.1163102462887764
step: 300, Loss: 0.11672627180814743
step: 400, Loss: 0.11454153805971146
step: 500, Loss: 0.11693460494279861
step: 600, Loss: 0.11646918207406998
step: 700, Loss: 0.1150956004858017
step: 800, Loss: 0.11515061557292938
step: 900, Loss: 0.11423324793577194
step: 1000, Loss: 0.1150677353143692
step: 1100, Loss: 0.11394688487052917
step: 1200, Loss: 0.11371123790740967
step: 1300, Loss: 0.11363508552312851
step: 1400, Loss: 0.11407206952571869
step: 1500, Loss: 0.11433948576450348
step: 1600, Loss: 0.11292605847120285
step: 1700, Loss: 0.11345027387142181
step: 1800, Loss: 0.11309517174959183
step: 1900, Loss: 0.11513276398181915
step: 2000, Loss: 0.11346232891082764
step: 2100, Loss: 0.11347652971744537
step: 2200, Loss: 0.1133653074502945
step: 2300, Loss: 0.11350570619106293
step: 2400, Loss: 0.11322703957557678
step: 2500, Loss: 0.11353994160890579
step: 2600, Loss: 0.11341679841279984
step: 2700, Loss: 0.11480012536048889
step: 2800, Loss: 0.11374791711568832
step: 2900, Loss: 0.1128401830792427
step: 3000, Loss: 0.11431925743818283
step: 3100, Loss: 0.11391115188598633
step: 3200, Loss: 0.11290783435106277
step: 3300, Loss: 0.11440426111221313
step: 3400, Loss: 0.11318975687026978
step: 3500, Loss: 0.11336929351091385
step: 3600, Loss: 0.11427952349185944
step: 3700, Loss: 0.11303404718637466
step: 3800, Loss: 0.11359870433807373
step: 3900, Loss: 0.11260543018579483
step: 4000, Loss: 0.11344699561595917
step: 4100, Loss: 0.11412419378757477
step: 4200, Loss: 0.11441873013973236
step: 4300, Loss: 0.1131076067686081
step: 4400, Loss: 0.11365577578544617
step: 4500, Loss: 0.11305677890777588
step: 4600, Loss: 0.11469453573226929
step: 4700, Loss: 0.5452709197998047
step: 4800, Loss: 1.0453790426254272
step: 4900, Loss: 0.13378432393074036
step: 5000, Loss: 0.12988132238388062
step: 5100, Loss: 0.12165899574756622
step: 5200, Loss: 0.12475030869245529
step: 5300, Loss: 0.1225757896900177
step: 5400, Loss: 0.12041915953159332
step: 5500, Loss: 0.11883135885000229
step: 5600, Loss: 0.12197113782167435
step: 5700, Loss: 0.11963602155447006
step: 5800, Loss: 0.11751972138881683
step: 5900, Loss: 0.11875846982002258
step: 6000, Loss: 0.11809372901916504
step: 6100, Loss: 0.11547622829675674
step: 6200, Loss: 0.11818031966686249
step: 6300, Loss: 0.11767946928739548
step: 6400, Loss: 0.11711731553077698
step: 6500, Loss: 0.11771190166473389
step: 6600, Loss: 0.11605100333690643
step: 6700, Loss: 0.11705804616212845
step: 6800, Loss: 0.11679229140281677
step: 6900, Loss: 0.11587163060903549
step: 7000, Loss: 0.11552860587835312
step: 7100, Loss: 0.11674223840236664
step: 7200, Loss: 0.11575900763273239
step: 7300, Loss: 0.11513856053352356
step: 7400, Loss: 0.11457224935293198
step: 7500, Loss: 0.11594026535749435
step: 7600, Loss: 0.11490253359079361
step: 7700, Loss: 0.11520397663116455
step: 7800, Loss: 0.11588943004608154
step: 7900, Loss: 0.11319375038146973
step: 8000, Loss: 0.1131351888179779
step: 8100, Loss: 0.11401131004095078
step: 8200, Loss: 0.1161273941397667
step: 8300, Loss: 0.11392763257026672
step: 8400, Loss: 0.1143563911318779
step: 8500, Loss: 0.11370789259672165
step: 8600, Loss: 0.11502209305763245
step: 8700, Loss: 0.11404711753129959
step: 8800, Loss: 0.11434382945299149
step: 8900, Loss: 0.11420068889856339
step: 9000, Loss: 0.11501611769199371
step: 9100, Loss: 0.1135006919503212
step: 9200, Loss: 0.11475841701030731
step: 9300, Loss: 0.11401303112506866
step: 9400, Loss: 0.11333302408456802
step: 9500, Loss: 0.11331988126039505
step: 9600, Loss: 0.11378016322851181
step: 9700, Loss: 0.11412276327610016
step: 9800, Loss: 0.11370255053043365
step: 9900, Loss: 0.11323271691799164
training successfully ended.
validating...
validate data length:98
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 24 Avgacc: 0.98125 Avgfscore: 0.983003326317219 
 Max acc:1.0, Max f score:1.0
******** mix subject_25 ********

[190, 570]
******fold 1******

Training... train_data length:1026
step: 0, Loss: 51.30058288574219
step: 100, Loss: 4.287647724151611
step: 200, Loss: 0.8095954656600952
step: 300, Loss: 0.1802304983139038
step: 400, Loss: 0.1755799800157547
step: 500, Loss: 0.14480790495872498
step: 600, Loss: 0.15855880081653595
step: 700, Loss: 0.14781123399734497
step: 800, Loss: 0.13642768561840057
step: 900, Loss: 0.12982723116874695
step: 1000, Loss: 0.1316826194524765
step: 1100, Loss: 0.13521024584770203
step: 1200, Loss: 0.13163073360919952
step: 1300, Loss: 0.13019448518753052
step: 1400, Loss: 0.13684716820716858
step: 1500, Loss: 0.12245072424411774
step: 1600, Loss: 0.1230979710817337
step: 1700, Loss: 0.12272018194198608
step: 1800, Loss: 0.1240045502781868
step: 1900, Loss: 0.11866141855716705
step: 2000, Loss: 0.11847580224275589
step: 2100, Loss: 0.12582984566688538
step: 2200, Loss: 0.11768291145563126
step: 2300, Loss: 0.12025924772024155
step: 2400, Loss: 0.1187763512134552
step: 2500, Loss: 0.11945682764053345
step: 2600, Loss: 0.11759053170681
step: 2700, Loss: 0.11718805879354477
step: 2800, Loss: 0.12083418667316437
step: 2900, Loss: 0.11665400117635727
step: 3000, Loss: 0.11551765352487564
step: 3100, Loss: 0.11732368171215057
step: 3200, Loss: 0.11634230613708496
step: 3300, Loss: 0.11609133332967758
step: 3400, Loss: 0.11724082380533218
step: 3500, Loss: 0.11817113310098648
step: 3600, Loss: 0.11541309952735901
step: 3700, Loss: 0.11591676622629166
step: 3800, Loss: 0.1148165613412857
step: 3900, Loss: 0.11408393830060959
step: 4000, Loss: 0.11492098867893219
step: 4100, Loss: 0.11705143749713898
step: 4200, Loss: 0.11425219476222992
step: 4300, Loss: 0.11364564299583435
step: 4400, Loss: 0.11353502422571182
step: 4500, Loss: 0.11576953530311584
step: 4600, Loss: 0.11482347548007965
step: 4700, Loss: 0.11499247699975967
step: 4800, Loss: 0.11418045312166214
step: 4900, Loss: 0.11515122652053833
step: 5000, Loss: 0.11423031985759735
step: 5100, Loss: 0.11383294314146042
step: 5200, Loss: 0.11443276703357697
step: 5300, Loss: 0.11447835713624954
step: 5400, Loss: 1.0794590711593628
step: 5500, Loss: 0.3220396637916565
step: 5600, Loss: 0.13700087368488312
step: 5700, Loss: 0.13056260347366333
step: 5800, Loss: 0.14248988032341003
step: 5900, Loss: 0.1231209933757782
step: 6000, Loss: 0.13410726189613342
step: 6100, Loss: 0.11909971386194229
step: 6200, Loss: 0.12160599231719971
step: 6300, Loss: 0.12007170170545578
step: 6400, Loss: 0.12348532676696777
step: 6500, Loss: 0.12238926440477371
step: 6600, Loss: 0.1216902956366539
step: 6700, Loss: 0.123956598341465
step: 6800, Loss: 0.11659621447324753
step: 6900, Loss: 0.11802826821804047
step: 7000, Loss: 0.1159454807639122
step: 7100, Loss: 0.11828349530696869
step: 7200, Loss: 0.11688241362571716
step: 7300, Loss: 0.11830699443817139
step: 7400, Loss: 0.11841462552547455
step: 7500, Loss: 0.11611489206552505
step: 7600, Loss: 0.11746974289417267
step: 7700, Loss: 0.11702688783407211
step: 7800, Loss: 0.11640099436044693
step: 7900, Loss: 0.11600863933563232
step: 8000, Loss: 0.11658129096031189
step: 8100, Loss: 0.11760829389095306
step: 8200, Loss: 0.11683611571788788
step: 8300, Loss: 0.11904653906822205
step: 8400, Loss: 0.11452439427375793
step: 8500, Loss: 0.11357281357049942
step: 8600, Loss: 0.11488329619169235
step: 8700, Loss: 0.1149553433060646
step: 8800, Loss: 0.11475880444049835
step: 8900, Loss: 0.11457919329404831
step: 9000, Loss: 0.1154397577047348
step: 9100, Loss: 0.11541949212551117
step: 9200, Loss: 0.11393443495035172
step: 9300, Loss: 0.11481686681509018
step: 9400, Loss: 0.11382582038640976
step: 9500, Loss: 0.11561351269483566
step: 9600, Loss: 0.11516234278678894
step: 9700, Loss: 0.11454250663518906
step: 9800, Loss: 0.1132754236459732
step: 9900, Loss: 0.11315875500440598
training successfully ended.
validating...
validate data length:114
acc: 0.9821428571428571
precision: 0.9666666666666667
recall: 1.0
F_score: 0.983050847457627
******fold 2******

Training... train_data length:1026
step: 0, Loss: 0.11472246795892715
step: 100, Loss: 0.11818568408489227
step: 200, Loss: 0.12078548967838287
step: 300, Loss: 0.11584118753671646
step: 400, Loss: 0.11601588129997253
step: 500, Loss: 0.11405080556869507
step: 600, Loss: 0.11414339393377304
step: 700, Loss: 0.11473242938518524
step: 800, Loss: 0.1148529052734375
step: 900, Loss: 0.11346973478794098
step: 1000, Loss: 0.11349393427371979
step: 1100, Loss: 0.11496153473854065
step: 1200, Loss: 0.11577074974775314
step: 1300, Loss: 0.11474143713712692
step: 1400, Loss: 0.1152377501130104
step: 1500, Loss: 0.11458548158407211
step: 1600, Loss: 0.1142304390668869
step: 1700, Loss: 0.11426113545894623
step: 1800, Loss: 0.11438588798046112
step: 1900, Loss: 0.11396332830190659
step: 2000, Loss: 0.11429200321435928
step: 2100, Loss: 0.11413886398077011
step: 2200, Loss: 0.11316044628620148
step: 2300, Loss: 0.11362253129482269
step: 2400, Loss: 0.11430919170379639
step: 2500, Loss: 0.11385451257228851
step: 2600, Loss: 0.11311864107847214
step: 2700, Loss: 0.11345960199832916
step: 2800, Loss: 0.11523237824440002
step: 2900, Loss: 0.11429738253355026
step: 3000, Loss: 0.11334977298974991
step: 3100, Loss: 0.11358454078435898
step: 3200, Loss: 0.11435286700725555
step: 3300, Loss: 0.11376982182264328
step: 3400, Loss: 0.11459125578403473
step: 3500, Loss: 0.113563172519207
step: 3600, Loss: 0.11362338811159134
step: 3700, Loss: 0.11453602463006973
step: 3800, Loss: 0.11488653719425201
step: 3900, Loss: 0.11368279159069061
step: 4000, Loss: 0.11446471512317657
step: 4100, Loss: 0.11458607017993927
step: 4200, Loss: 0.11475854367017746
step: 4300, Loss: 0.2237364947795868
step: 4400, Loss: 0.15657949447631836
step: 4500, Loss: 0.1276182383298874
step: 4600, Loss: 0.13288265466690063
step: 4700, Loss: 0.12051370739936829
step: 4800, Loss: 0.11899924278259277
step: 4900, Loss: 0.1175675019621849
step: 5000, Loss: 0.12087665498256683
step: 5100, Loss: 0.1186230480670929
step: 5200, Loss: 0.12026727944612503
step: 5300, Loss: 0.11514128744602203
step: 5400, Loss: 0.12085999548435211
step: 5500, Loss: 0.1164466068148613
step: 5600, Loss: 0.11464087665081024
step: 5700, Loss: 0.1149861067533493
step: 5800, Loss: 0.11635943502187729
step: 5900, Loss: 0.11980434507131577
step: 6000, Loss: 0.11556409299373627
step: 6100, Loss: 0.11474115401506424
step: 6200, Loss: 0.11498425155878067
step: 6300, Loss: 0.11451645195484161
step: 6400, Loss: 0.11668141931295395
step: 6500, Loss: 0.11561954021453857
step: 6600, Loss: 0.11660263687372208
step: 6700, Loss: 0.11541464179754257
step: 6800, Loss: 0.11415459215641022
step: 6900, Loss: 0.11382436752319336
step: 7000, Loss: 0.113949716091156
step: 7100, Loss: 0.11588118970394135
step: 7200, Loss: 0.11492449045181274
step: 7300, Loss: 0.1146194115281105
step: 7400, Loss: 0.11420351266860962
step: 7500, Loss: 0.11491695046424866
step: 7600, Loss: 0.11577121168375015
step: 7700, Loss: 0.11521559208631516
step: 7800, Loss: 0.11452345550060272
step: 7900, Loss: 0.11488986015319824
step: 8000, Loss: 0.11426227539777756
step: 8100, Loss: 0.11426359415054321
step: 8200, Loss: 0.11360739916563034
step: 8300, Loss: 0.11419804394245148
step: 8400, Loss: 0.11321081966161728
step: 8500, Loss: 0.1143583282828331
step: 8600, Loss: 0.11469557881355286
step: 8700, Loss: 0.11479555070400238
step: 8800, Loss: 0.11464360356330872
step: 8900, Loss: 0.11328477412462234
step: 9000, Loss: 0.11453013867139816
step: 9100, Loss: 0.11289922893047333
step: 9200, Loss: 0.11479315906763077
step: 9300, Loss: 0.11357001215219498
step: 9400, Loss: 0.11523652076721191
step: 9500, Loss: 0.11355510354042053
step: 9600, Loss: 0.11306144297122955
step: 9700, Loss: 0.11428368091583252
step: 9800, Loss: 0.11332245171070099
step: 9900, Loss: 0.11336831748485565
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******

Training... train_data length:1026
step: 0, Loss: 0.11635220050811768
step: 100, Loss: 0.1237921416759491
step: 200, Loss: 0.11501847952604294
step: 300, Loss: 0.1159476637840271
step: 400, Loss: 0.1137610524892807
step: 500, Loss: 0.11340344697237015
step: 600, Loss: 0.11443796753883362
step: 700, Loss: 0.11410035192966461
step: 800, Loss: 0.11477068811655045
step: 900, Loss: 0.1158381998538971
step: 1000, Loss: 0.11355138570070267
step: 1100, Loss: 0.11452112346887589
step: 1200, Loss: 0.11343706399202347
step: 1300, Loss: 0.1142059862613678
step: 1400, Loss: 0.11370082199573517
step: 1500, Loss: 0.11311622709035873
step: 1600, Loss: 0.1137404665350914
step: 1700, Loss: 0.1153450459241867
step: 1800, Loss: 0.11334262788295746
step: 1900, Loss: 0.11292325705289841
step: 2000, Loss: 0.11407792568206787
step: 2100, Loss: 0.11401276290416718
step: 2200, Loss: 0.11372411251068115
step: 2300, Loss: 0.11331582069396973
step: 2400, Loss: 0.11325886845588684
step: 2500, Loss: 0.11436808109283447
step: 2600, Loss: 0.11395392566919327
step: 2700, Loss: 0.1140298917889595
step: 2800, Loss: 0.1127127856016159
step: 2900, Loss: 0.11396363377571106
step: 3000, Loss: 0.11356142163276672
step: 3100, Loss: 0.11367575079202652
step: 3200, Loss: 0.11307482421398163
step: 3300, Loss: 0.11285429447889328
step: 3400, Loss: 0.11315108835697174
step: 3500, Loss: 0.11399264633655548
step: 3600, Loss: 0.11516066640615463
step: 3700, Loss: 0.113118976354599
step: 3800, Loss: 0.11321631819009781
step: 3900, Loss: 0.11421379446983337
step: 4000, Loss: 0.11385998129844666
step: 4100, Loss: 0.11496659368276596
step: 4200, Loss: 0.11353874951601028
step: 4300, Loss: 0.11288776993751526
step: 4400, Loss: 0.11329054832458496
step: 4500, Loss: 0.11282013356685638
step: 4600, Loss: 0.11447916179895401
step: 4700, Loss: 0.11472196877002716
step: 4800, Loss: 0.11368149518966675
step: 4900, Loss: 1.113633632659912
step: 5000, Loss: 0.13716843724250793
step: 5100, Loss: 0.12172006070613861
step: 5200, Loss: 0.12657827138900757
step: 5300, Loss: 0.1207679957151413
step: 5400, Loss: 0.12219477444887161
step: 5500, Loss: 0.11809325218200684
step: 5600, Loss: 0.11806367337703705
step: 5700, Loss: 0.11560117453336716
step: 5800, Loss: 0.11784707009792328
step: 5900, Loss: 0.11809968203306198
step: 6000, Loss: 0.11802254617214203
step: 6100, Loss: 0.12024694681167603
step: 6200, Loss: 0.11699075996875763
step: 6300, Loss: 0.11543833464384079
step: 6400, Loss: 0.11499848961830139
step: 6500, Loss: 0.11617345362901688
step: 6600, Loss: 0.11478239297866821
step: 6700, Loss: 0.11538670212030411
step: 6800, Loss: 0.11523973196744919
step: 6900, Loss: 0.11404017359018326
step: 7000, Loss: 0.11654751002788544
step: 7100, Loss: 0.11359990388154984
step: 7200, Loss: 0.11356708407402039
step: 7300, Loss: 0.11602376401424408
step: 7400, Loss: 0.11384967714548111
step: 7500, Loss: 0.11439910531044006
step: 7600, Loss: 0.11513006687164307
step: 7700, Loss: 0.11481288075447083
step: 7800, Loss: 0.11384717375040054
step: 7900, Loss: 0.11557641625404358
step: 8000, Loss: 0.11407861113548279
step: 8100, Loss: 0.11357727646827698
step: 8200, Loss: 0.11447136104106903
step: 8300, Loss: 0.11333311349153519
step: 8400, Loss: 0.11324426531791687
step: 8500, Loss: 0.11426927894353867
step: 8600, Loss: 0.11339053511619568
step: 8700, Loss: 0.11433927714824677
step: 8800, Loss: 0.11350864917039871
step: 8900, Loss: 0.11473722755908966
step: 9000, Loss: 0.11330240219831467
step: 9100, Loss: 0.11282774060964584
step: 9200, Loss: 0.11549492180347443
step: 9300, Loss: 0.11415696144104004
step: 9400, Loss: 0.1130627766251564
step: 9500, Loss: 0.11284142732620239
step: 9600, Loss: 0.11343207955360413
step: 9700, Loss: 0.11327361315488815
step: 9800, Loss: 0.11303674429655075
step: 9900, Loss: 0.11319196224212646
training successfully ended.
validating...
validate data length:114
acc: 0.9821428571428571
precision: 0.967741935483871
recall: 1.0
F_score: 0.9836065573770492
******fold 4******

Training... train_data length:1026
step: 0, Loss: 0.1179339736700058
step: 100, Loss: 0.11382456123828888
step: 200, Loss: 0.11309618502855301
step: 300, Loss: 0.11532959342002869
step: 400, Loss: 0.11567512899637222
step: 500, Loss: 0.11290659010410309
step: 600, Loss: 0.11294084042310715
step: 700, Loss: 0.11398788541555405
step: 800, Loss: 0.11445581912994385
step: 900, Loss: 0.11477897316217422
step: 1000, Loss: 0.11425374448299408
step: 1100, Loss: 0.11430235207080841
step: 1200, Loss: 0.11405324190855026
step: 1300, Loss: 4.922774791717529
step: 1400, Loss: 0.1559636890888214
step: 1500, Loss: 0.11802244186401367
step: 1600, Loss: 0.12126688659191132
step: 1700, Loss: 0.1166611835360527
step: 1800, Loss: 0.11563968658447266
step: 1900, Loss: 0.11854139715433121
step: 2000, Loss: 0.11778081953525543
step: 2100, Loss: 0.11743141710758209
step: 2200, Loss: 0.11773804575204849
step: 2300, Loss: 0.11590908467769623
step: 2400, Loss: 0.1144549772143364
step: 2500, Loss: 0.11502246558666229
step: 2600, Loss: 0.11512023955583572
step: 2700, Loss: 0.1149495542049408
step: 2800, Loss: 0.11421678960323334
step: 2900, Loss: 0.11522543430328369
step: 3000, Loss: 0.1158677488565445
step: 3100, Loss: 0.11545994877815247
step: 3200, Loss: 0.11367976665496826
step: 3300, Loss: 0.11415137350559235
step: 3400, Loss: 0.11417838931083679
step: 3500, Loss: 0.11552107334136963
step: 3600, Loss: 0.11638695746660233
step: 3700, Loss: 0.11454221606254578
step: 3800, Loss: 0.11425933986902237
step: 3900, Loss: 0.11474224179983139
step: 4000, Loss: 0.11347372084856033
step: 4100, Loss: 0.1152336597442627
step: 4200, Loss: 0.11351072788238525
step: 4300, Loss: 0.11392266303300858
step: 4400, Loss: 0.1139785498380661
step: 4500, Loss: 0.11434131115674973
step: 4600, Loss: 0.11494026333093643
step: 4700, Loss: 0.11511360108852386
step: 4800, Loss: 0.11354721337556839
step: 4900, Loss: 0.11359173059463501
step: 5000, Loss: 0.1131492406129837
step: 5100, Loss: 0.11328282207250595
step: 5200, Loss: 0.11388754844665527
step: 5300, Loss: 0.11336393654346466
step: 5400, Loss: 0.11458107084035873
step: 5500, Loss: 0.11440392583608627
step: 5600, Loss: 0.11505155265331268
step: 5700, Loss: 0.11404716968536377
step: 5800, Loss: 0.11289090663194656
step: 5900, Loss: 0.1139959990978241
step: 6000, Loss: 0.11328381299972534
step: 6100, Loss: 0.11341141909360886
step: 6200, Loss: 0.11362293362617493
step: 6300, Loss: 0.11330187320709229
step: 6400, Loss: 0.11460860818624496
step: 6500, Loss: 0.11404880881309509
step: 6600, Loss: 0.11381259560585022
step: 6700, Loss: 0.11312364041805267
step: 6800, Loss: 0.11371855437755585
step: 6900, Loss: 0.11318275332450867
step: 7000, Loss: 0.11368659883737564
step: 7100, Loss: 0.1133924275636673
step: 7200, Loss: 0.11287063360214233
step: 7300, Loss: 0.11241857707500458
step: 7400, Loss: 0.11323358863592148
step: 7500, Loss: 0.11373914033174515
step: 7600, Loss: 0.11441567540168762
step: 7700, Loss: 0.11410623788833618
step: 7800, Loss: 0.11317241191864014
step: 7900, Loss: 0.11233516037464142
step: 8000, Loss: 0.11384110152721405
step: 8100, Loss: 0.11282232403755188
step: 8200, Loss: 0.11332186311483383
step: 8300, Loss: 0.11307583004236221
step: 8400, Loss: 0.11303609609603882
step: 8500, Loss: 0.11324325948953629
step: 8600, Loss: 0.11412998288869858
step: 8700, Loss: 0.11398008465766907
step: 8800, Loss: 0.1804955005645752
step: 8900, Loss: 0.15175580978393555
step: 9000, Loss: 0.12222233414649963
step: 9100, Loss: 0.12068168818950653
step: 9200, Loss: 0.11791390180587769
step: 9300, Loss: 0.11827654391527176
step: 9400, Loss: 0.11613557487726212
step: 9500, Loss: 0.11687323451042175
step: 9600, Loss: 0.11910761892795563
step: 9700, Loss: 0.11735887825489044
step: 9800, Loss: 0.11581332981586456
step: 9900, Loss: 0.11591962724924088
training successfully ended.
validating...
validate data length:114
acc: 0.9821428571428571
precision: 0.9661016949152542
recall: 1.0
F_score: 0.9827586206896551
******fold 5******

Training... train_data length:1026
step: 0, Loss: 0.12212233245372772
step: 100, Loss: 0.11733070015907288
step: 200, Loss: 0.1154407262802124
step: 300, Loss: 0.11395518481731415
step: 400, Loss: 0.11433570832014084
step: 500, Loss: 0.11384819447994232
step: 600, Loss: 0.11359244585037231
step: 700, Loss: 0.1139172688126564
step: 800, Loss: 0.11470425128936768
step: 900, Loss: 0.11349493265151978
step: 1000, Loss: 0.11326953768730164
step: 1100, Loss: 0.11419866234064102
step: 1200, Loss: 0.11316817253828049
step: 1300, Loss: 0.11296810209751129
step: 1400, Loss: 0.11365672945976257
step: 1500, Loss: 0.11369766294956207
step: 1600, Loss: 0.11497172713279724
step: 1700, Loss: 0.11366518586874008
step: 1800, Loss: 0.11294753849506378
step: 1900, Loss: 0.11266417801380157
step: 2000, Loss: 0.11327804625034332
step: 2100, Loss: 0.11316704005002975
step: 2200, Loss: 0.1140931248664856
step: 2300, Loss: 0.11331465095281601
step: 2400, Loss: 0.11269799619913101
step: 2500, Loss: 0.11366385221481323
step: 2600, Loss: 0.11309623718261719
step: 2700, Loss: 0.11297370493412018
step: 2800, Loss: 0.11340240389108658
step: 2900, Loss: 0.114825040102005
step: 3000, Loss: 0.11338065564632416
step: 3100, Loss: 0.11324429512023926
step: 3200, Loss: 0.11369320750236511
step: 3300, Loss: 0.11326679587364197
step: 3400, Loss: 0.11295268684625626
step: 3500, Loss: 0.11256883293390274
step: 3600, Loss: 0.11323710530996323
step: 3700, Loss: 0.11362005770206451
step: 3800, Loss: 0.11671839654445648
step: 3900, Loss: 0.13689002394676208
step: 4000, Loss: 0.12152736634016037
step: 4100, Loss: 0.12331171333789825
step: 4200, Loss: 0.11836013197898865
step: 4300, Loss: 0.11863437294960022
step: 4400, Loss: 0.1165580302476883
step: 4500, Loss: 0.11780465394258499
step: 4600, Loss: 0.11679430305957794
step: 4700, Loss: 0.11718933284282684
step: 4800, Loss: 0.11726732552051544
step: 4900, Loss: 0.11544883251190186
step: 5000, Loss: 0.11429049074649811
step: 5100, Loss: 0.11843649297952652
step: 5200, Loss: 0.11591441929340363
step: 5300, Loss: 0.11620296537876129
step: 5400, Loss: 0.11745329201221466
step: 5500, Loss: 0.11439099907875061
step: 5600, Loss: 0.1142377182841301
step: 5700, Loss: 0.1144535169005394
step: 5800, Loss: 0.1139603778719902
step: 5900, Loss: 0.11423571407794952
step: 6000, Loss: 0.11488622426986694
step: 6100, Loss: 0.11333099752664566
step: 6200, Loss: 0.11359460651874542
step: 6300, Loss: 0.11475756764411926
step: 6400, Loss: 0.11455535888671875
step: 6500, Loss: 0.11372300982475281
step: 6600, Loss: 0.11431505531072617
step: 6700, Loss: 0.11462914198637009
step: 6800, Loss: 0.11416179686784744
step: 6900, Loss: 0.11381948739290237
step: 7000, Loss: 0.11378844827413559
step: 7100, Loss: 0.11484520137310028
step: 7200, Loss: 0.11397919803857803
step: 7300, Loss: 0.1130734458565712
step: 7400, Loss: 0.11402188241481781
step: 7500, Loss: 0.11342523247003555
step: 7600, Loss: 0.11394752562046051
step: 7700, Loss: 0.11296720802783966
step: 7800, Loss: 0.11409096419811249
step: 7900, Loss: 0.11438240110874176
step: 8000, Loss: 0.11335743218660355
step: 8100, Loss: 0.11343837529420853
step: 8200, Loss: 0.11403687298297882
step: 8300, Loss: 0.1138937920331955
step: 8400, Loss: 0.11310703307390213
step: 8500, Loss: 0.1144285500049591
step: 8600, Loss: 0.11350540071725845
step: 8700, Loss: 0.11300667375326157
step: 8800, Loss: 0.11249915510416031
step: 8900, Loss: 0.11298869550228119
step: 9000, Loss: 0.11244095861911774
step: 9100, Loss: 0.11324767023324966
step: 9200, Loss: 0.11552862823009491
step: 9300, Loss: 0.11328072845935822
step: 9400, Loss: 0.1130298599600792
step: 9500, Loss: 0.11373463273048401
step: 9600, Loss: 0.11241430044174194
step: 9700, Loss: 0.11415837705135345
step: 9800, Loss: 0.11282648146152496
step: 9900, Loss: 0.11417298018932343
training successfully ended.
validating...
validate data length:114
acc: 0.9910714285714286
precision: 0.9833333333333333
recall: 1.0
F_score: 0.9915966386554621
******fold 6******

Training... train_data length:1026
step: 0, Loss: 0.1154983639717102
step: 100, Loss: 0.11417165398597717
step: 200, Loss: 0.11361334472894669
step: 300, Loss: 0.1145402044057846
step: 400, Loss: 0.11452068388462067
step: 500, Loss: 0.11344992369413376
step: 600, Loss: 0.11403878778219223
step: 700, Loss: 0.11346881836652756
step: 800, Loss: 0.1131330206990242
step: 900, Loss: 0.11369039863348007
step: 1000, Loss: 0.1128336563706398
step: 1100, Loss: 0.11387038230895996
step: 1200, Loss: 0.11445128917694092
step: 1300, Loss: 0.11333043873310089
step: 1400, Loss: 0.11365294456481934
step: 1500, Loss: 0.11270403861999512
step: 1600, Loss: 1.3344147205352783
step: 1700, Loss: 0.13391783833503723
step: 1800, Loss: 0.12124346196651459
step: 1900, Loss: 0.12050162255764008
step: 2000, Loss: 0.11925418674945831
step: 2100, Loss: 0.11757626384496689
step: 2200, Loss: 0.11730962991714478
step: 2300, Loss: 0.11723251640796661
step: 2400, Loss: 0.11625269055366516
step: 2500, Loss: 0.11697982996702194
step: 2600, Loss: 0.1161961555480957
step: 2700, Loss: 0.11752311885356903
step: 2800, Loss: 0.11529810726642609
step: 2900, Loss: 0.1155749261379242
step: 3000, Loss: 0.11687802523374557
step: 3100, Loss: 0.1158362478017807
step: 3200, Loss: 0.11544071137905121
step: 3300, Loss: 0.11512601375579834
step: 3400, Loss: 0.11411718279123306
step: 3500, Loss: 0.1142774447798729
step: 3600, Loss: 0.11394578218460083
step: 3700, Loss: 0.11449388414621353
step: 3800, Loss: 0.11445916444063187
step: 3900, Loss: 0.11600194126367569
step: 4000, Loss: 0.11480086296796799
step: 4100, Loss: 0.11452159285545349
step: 4200, Loss: 0.11399060487747192
step: 4300, Loss: 0.11535763740539551
step: 4400, Loss: 0.11387251317501068
step: 4500, Loss: 0.11476035416126251
step: 4600, Loss: 0.11402854323387146
step: 4700, Loss: 0.1146872341632843
step: 4800, Loss: 0.11369641870260239
step: 4900, Loss: 0.11411848664283752
step: 5000, Loss: 0.11329786479473114
step: 5100, Loss: 0.11401782184839249
step: 5200, Loss: 0.11476688832044601
step: 5300, Loss: 0.11466287076473236
step: 5400, Loss: 0.11306864023208618
step: 5500, Loss: 0.11321645975112915
step: 5600, Loss: 0.11472512781620026
step: 5700, Loss: 0.11270502954721451
step: 5800, Loss: 0.11377829313278198
step: 5900, Loss: 0.11334122717380524
step: 6000, Loss: 0.11381055414676666
step: 6100, Loss: 0.11392302066087723
step: 6200, Loss: 0.11283265054225922
step: 6300, Loss: 0.11389010399580002
step: 6400, Loss: 0.1134343147277832
step: 6500, Loss: 0.11374959349632263
step: 6600, Loss: 0.11366003751754761
step: 6700, Loss: 0.11266680061817169
step: 6800, Loss: 0.11357338726520538
step: 6900, Loss: 0.11457621306180954
step: 7000, Loss: 0.11219292134046555
step: 7100, Loss: 0.1129242330789566
step: 7200, Loss: 0.11427947878837585
step: 7300, Loss: 0.11384232342243195
step: 7400, Loss: 0.11566527187824249
step: 7500, Loss: 0.11432188749313354
step: 7600, Loss: 0.11501871049404144
step: 7700, Loss: 0.11432535201311111
step: 7800, Loss: 0.11387475579977036
step: 7900, Loss: 0.11410318315029144
step: 8000, Loss: 0.1141737550497055
step: 8100, Loss: 0.1133095994591713
step: 8200, Loss: 0.11389462649822235
step: 8300, Loss: 0.11295673251152039
step: 8400, Loss: 0.1133979856967926
step: 8500, Loss: 0.11249443888664246
step: 8600, Loss: 0.11271557211875916
step: 8700, Loss: 0.11302491277456284
step: 8800, Loss: 0.11336221545934677
step: 8900, Loss: 0.11432739347219467
step: 9000, Loss: 0.11399687826633453
step: 9100, Loss: 0.1154908686876297
step: 9200, Loss: 0.11488424986600876
step: 9300, Loss: 0.11284203827381134
step: 9400, Loss: 0.11265533417463303
step: 9500, Loss: 0.1135464683175087
step: 9600, Loss: 0.11508950591087341
step: 9700, Loss: 0.11289852112531662
step: 9800, Loss: 0.11394147574901581
step: 9900, Loss: 0.1142016053199768
training successfully ended.
validating...
validate data length:114
acc: 0.9910714285714286
precision: 0.9827586206896551
recall: 1.0
F_score: 0.9913043478260869
******fold 7******

Training... train_data length:1026
step: 0, Loss: 0.11737240105867386
step: 100, Loss: 0.11438968777656555
step: 200, Loss: 0.11720927059650421
step: 300, Loss: 0.11316151171922684
step: 400, Loss: 0.11389823257923126
step: 500, Loss: 0.11230355501174927
step: 600, Loss: 0.1135258600115776
step: 700, Loss: 0.11341624706983566
step: 800, Loss: 0.11350814998149872
step: 900, Loss: 0.11343701183795929
step: 1000, Loss: 0.11420322209596634
step: 1100, Loss: 0.11345980316400528
step: 1200, Loss: 0.11406311392784119
step: 1300, Loss: 0.12937742471694946
step: 1400, Loss: 0.1197967678308487
step: 1500, Loss: 0.12332897633314133
step: 1600, Loss: 0.1254655420780182
step: 1700, Loss: 0.11684583127498627
step: 1800, Loss: 0.11896134167909622
step: 1900, Loss: 0.119269959628582
step: 2000, Loss: 0.11583656817674637
step: 2100, Loss: 0.11662846058607101
step: 2200, Loss: 0.11557155102491379
step: 2300, Loss: 0.11709867417812347
step: 2400, Loss: 0.11515693366527557
step: 2500, Loss: 0.1176014244556427
step: 2600, Loss: 0.1154894307255745
step: 2700, Loss: 0.11374394595623016
step: 2800, Loss: 0.11469457298517227
step: 2900, Loss: 0.11497868597507477
step: 3000, Loss: 0.11511745303869247
step: 3100, Loss: 0.1145503968000412
step: 3200, Loss: 0.11505816876888275
step: 3300, Loss: 0.1147642582654953
step: 3400, Loss: 0.11424045264720917
step: 3500, Loss: 0.11385606229305267
step: 3600, Loss: 0.11521205306053162
step: 3700, Loss: 0.11428119242191315
step: 3800, Loss: 0.11401207745075226
step: 3900, Loss: 0.11558404564857483
step: 4000, Loss: 0.11362865567207336
step: 4100, Loss: 0.11489508301019669
step: 4200, Loss: 0.11386074125766754
step: 4300, Loss: 0.11271978169679642
step: 4400, Loss: 0.11434000730514526
step: 4500, Loss: 0.1144399642944336
step: 4600, Loss: 0.11466750502586365
step: 4700, Loss: 0.11362523585557938
step: 4800, Loss: 0.11396758258342743
step: 4900, Loss: 0.11354938894510269
step: 5000, Loss: 0.11440706253051758
step: 5100, Loss: 0.11314447224140167
step: 5200, Loss: 0.1136942058801651
step: 5300, Loss: 0.1143118292093277
step: 5400, Loss: 0.11358579993247986
step: 5500, Loss: 0.11586921662092209
step: 5600, Loss: 0.1134982705116272
step: 5700, Loss: 0.1137268990278244
step: 5800, Loss: 0.1128537580370903
step: 5900, Loss: 0.11472493410110474
step: 6000, Loss: 0.11398427188396454
step: 6100, Loss: 0.11436383426189423
step: 6200, Loss: 0.11499194800853729
step: 6300, Loss: 0.11246064305305481
step: 6400, Loss: 0.11377217620611191
step: 6500, Loss: 0.11342113465070724
step: 6600, Loss: 0.11298030614852905
step: 6700, Loss: 0.1129973828792572
step: 6800, Loss: 0.11239344626665115
step: 6900, Loss: 0.1131117045879364
step: 7000, Loss: 0.1127982810139656
step: 7100, Loss: 0.11287377774715424
step: 7200, Loss: 0.11301097273826599
step: 7300, Loss: 0.1141262948513031
step: 7400, Loss: 0.11301842331886292
step: 7500, Loss: 0.11323286592960358
step: 7600, Loss: 0.11321469396352768
step: 7700, Loss: 0.11297418177127838
step: 7800, Loss: 0.11397700756788254
step: 7900, Loss: 0.11343858391046524
step: 8000, Loss: 0.11448585987091064
step: 8100, Loss: 0.11431184411048889
step: 8200, Loss: 0.11369294673204422
step: 8300, Loss: 0.11332256346940994
step: 8400, Loss: 0.11426089704036713
step: 8500, Loss: 0.11469573527574539
step: 8600, Loss: 0.11361744999885559
step: 8700, Loss: 0.11322932690382004
step: 8800, Loss: 0.11384502053260803
step: 8900, Loss: 0.11247766017913818
step: 9000, Loss: 0.11323685944080353
step: 9100, Loss: 0.11369321495294571
step: 9200, Loss: 0.1128859668970108
step: 9300, Loss: 0.11273431777954102
step: 9400, Loss: 0.11428044736385345
step: 9500, Loss: 0.11317543685436249
step: 9600, Loss: 0.11422382295131683
step: 9700, Loss: 0.11445286870002747
step: 9800, Loss: 0.11516930907964706
step: 9900, Loss: 0.11499674618244171
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:1026
step: 0, Loss: 0.11591774225234985
step: 100, Loss: 0.11434850096702576
step: 200, Loss: 0.11428356170654297
step: 300, Loss: 0.11352407187223434
step: 400, Loss: 0.11527394503355026
step: 500, Loss: 0.11420479416847229
step: 600, Loss: 0.11297845840454102
step: 700, Loss: 0.1163511872291565
step: 800, Loss: 0.11355177313089371
step: 900, Loss: 0.11330011487007141
step: 1000, Loss: 0.11384738981723785
step: 1100, Loss: 0.11433565616607666
step: 1200, Loss: 0.11398904025554657
step: 1300, Loss: 0.11348633468151093
step: 1400, Loss: 0.11311551928520203
step: 1500, Loss: 0.1129748523235321
step: 1600, Loss: 0.11298426985740662
step: 1700, Loss: 0.11643233895301819
step: 1800, Loss: 0.11382253468036652
step: 1900, Loss: 0.1141677275300026
step: 2000, Loss: 0.11294730752706528
step: 2100, Loss: 0.11338245868682861
step: 2200, Loss: 0.11310598254203796
step: 2300, Loss: 0.114033542573452
step: 2400, Loss: 0.11509344726800919
step: 2500, Loss: 0.11372561007738113
step: 2600, Loss: 0.11335593461990356
step: 2700, Loss: 0.11297771334648132
step: 2800, Loss: 0.11362326145172119
step: 2900, Loss: 0.11514026671648026
step: 3000, Loss: 0.11411701887845993
step: 3100, Loss: 0.11281166970729828
step: 3200, Loss: 0.11250271648168564
step: 3300, Loss: 0.1134568303823471
step: 3400, Loss: 0.11325917392969131
step: 3500, Loss: 0.11355551332235336
step: 3600, Loss: 0.11422479152679443
step: 3700, Loss: 0.11334007233381271
step: 3800, Loss: 0.11356344074010849
step: 3900, Loss: 0.7765898704528809
step: 4000, Loss: 0.1518065333366394
step: 4100, Loss: 0.1278531700372696
step: 4200, Loss: 0.12098884582519531
step: 4300, Loss: 0.1197526678442955
step: 4400, Loss: 0.11802339553833008
step: 4500, Loss: 0.11835432052612305
step: 4600, Loss: 0.12014930695295334
step: 4700, Loss: 0.11859894543886185
step: 4800, Loss: 0.11940266191959381
step: 4900, Loss: 0.11686816811561584
step: 5000, Loss: 0.11615219712257385
step: 5100, Loss: 0.11587923765182495
step: 5200, Loss: 0.11575306951999664
step: 5300, Loss: 0.11781035363674164
step: 5400, Loss: 0.11503429710865021
step: 5500, Loss: 0.1145646870136261
step: 5600, Loss: 0.11637303978204727
step: 5700, Loss: 0.11602277308702469
step: 5800, Loss: 0.11516246944665909
step: 5900, Loss: 0.11471164971590042
step: 6000, Loss: 0.11792337149381638
step: 6100, Loss: 0.11379721015691757
step: 6200, Loss: 0.115028515458107
step: 6300, Loss: 0.11639443784952164
step: 6400, Loss: 0.11505284160375595
step: 6500, Loss: 0.11410758644342422
step: 6600, Loss: 0.11399054527282715
step: 6700, Loss: 0.11474418640136719
step: 6800, Loss: 0.11495324969291687
step: 6900, Loss: 0.1141500174999237
step: 7000, Loss: 0.11363736540079117
step: 7100, Loss: 0.11367382854223251
step: 7200, Loss: 0.11515249311923981
step: 7300, Loss: 0.11339901387691498
step: 7400, Loss: 0.11403149366378784
step: 7500, Loss: 0.11427656561136246
step: 7600, Loss: 0.114261694252491
step: 7700, Loss: 0.11316836625337601
step: 7800, Loss: 0.11284598708152771
step: 7900, Loss: 0.11376941949129105
step: 8000, Loss: 0.11281978338956833
step: 8100, Loss: 0.11380279064178467
step: 8200, Loss: 0.11532997339963913
step: 8300, Loss: 0.11374989151954651
step: 8400, Loss: 0.11405134201049805
step: 8500, Loss: 0.11401136219501495
step: 8600, Loss: 0.1132519319653511
step: 8700, Loss: 0.1142268180847168
step: 8800, Loss: 0.1137097105383873
step: 8900, Loss: 0.11327413469552994
step: 9000, Loss: 0.11427220702171326
step: 9100, Loss: 0.11239063739776611
step: 9200, Loss: 0.11312058568000793
step: 9300, Loss: 0.11384031176567078
step: 9400, Loss: 0.11354144662618637
step: 9500, Loss: 0.1129523366689682
step: 9600, Loss: 0.11272554099559784
step: 9700, Loss: 0.1136934757232666
step: 9800, Loss: 0.1135454848408699
step: 9900, Loss: 0.11399880796670914
training successfully ended.
validating...
validate data length:114
acc: 0.9732142857142857
precision: 0.9444444444444444
recall: 1.0
F_score: 0.9714285714285714
******fold 9******

Training... train_data length:1026
step: 0, Loss: 0.1165386214852333
step: 100, Loss: 0.11396923661231995
step: 200, Loss: 0.11357250064611435
step: 300, Loss: 0.11306805163621902
step: 400, Loss: 0.11495282500982285
step: 500, Loss: 0.11326683312654495
step: 600, Loss: 0.11282846331596375
step: 700, Loss: 0.11560551822185516
step: 800, Loss: 0.1137414202094078
step: 900, Loss: 0.6572809219360352
step: 1000, Loss: 0.13664977252483368
step: 1100, Loss: 0.12182167172431946
step: 1200, Loss: 0.1238672062754631
step: 1300, Loss: 0.1197090893983841
step: 1400, Loss: 0.1200418472290039
step: 1500, Loss: 0.11668016761541367
step: 1600, Loss: 0.12083026021718979
step: 1700, Loss: 0.11573411524295807
step: 1800, Loss: 0.1167379692196846
step: 1900, Loss: 0.11848605424165726
step: 2000, Loss: 0.1152607649564743
step: 2100, Loss: 0.11553515493869781
step: 2200, Loss: 0.11578486114740372
step: 2300, Loss: 0.11651518195867538
step: 2400, Loss: 0.11535468697547913
step: 2500, Loss: 0.11583061516284943
step: 2600, Loss: 0.11622485518455505
step: 2700, Loss: 0.11631999909877777
step: 2800, Loss: 0.11644306778907776
step: 2900, Loss: 0.11446929723024368
step: 3000, Loss: 0.11512094736099243
step: 3100, Loss: 0.11662130802869797
step: 3200, Loss: 0.1137940064072609
step: 3300, Loss: 0.11593521386384964
step: 3400, Loss: 0.11527443677186966
step: 3500, Loss: 0.11384101957082748
step: 3600, Loss: 0.11437590420246124
step: 3700, Loss: 0.11334548890590668
step: 3800, Loss: 0.11433214694261551
step: 3900, Loss: 0.11348676681518555
step: 4000, Loss: 0.11354628205299377
step: 4100, Loss: 0.11414637416601181
step: 4200, Loss: 0.11449428647756577
step: 4300, Loss: 0.11355693638324738
step: 4400, Loss: 0.11401593685150146
step: 4500, Loss: 0.114162877202034
step: 4600, Loss: 0.11389249563217163
step: 4700, Loss: 0.11689222604036331
step: 4800, Loss: 0.11312688887119293
step: 4900, Loss: 0.11434819549322128
step: 5000, Loss: 0.11460605263710022
step: 5100, Loss: 0.11465434730052948
step: 5200, Loss: 0.1159047931432724
step: 5300, Loss: 0.11477862298488617
step: 5400, Loss: 0.11325716972351074
step: 5500, Loss: 0.11471180617809296
step: 5600, Loss: 0.11319597065448761
step: 5700, Loss: 0.11418057978153229
step: 5800, Loss: 0.11391796171665192
step: 5900, Loss: 0.11400996893644333
step: 6000, Loss: 0.11548315733671188
step: 6100, Loss: 0.1145501509308815
step: 6200, Loss: 0.1135871559381485
step: 6300, Loss: 0.11343823373317719
step: 6400, Loss: 0.11312639713287354
step: 6500, Loss: 0.11265475302934647
step: 6600, Loss: 0.11317765712738037
step: 6700, Loss: 0.11376093327999115
step: 6800, Loss: 0.11561135947704315
step: 6900, Loss: 0.11430492997169495
step: 7000, Loss: 0.11290031671524048
step: 7100, Loss: 0.11413108557462692
step: 7200, Loss: 0.11313991248607635
step: 7300, Loss: 0.11455661803483963
step: 7400, Loss: 0.11424028128385544
step: 7500, Loss: 0.11414831131696701
step: 7600, Loss: 0.11384118348360062
step: 7700, Loss: 0.11353687942028046
step: 7800, Loss: 0.11567314714193344
step: 7900, Loss: 0.11379969865083694
step: 8000, Loss: 0.11385058611631393
step: 8100, Loss: 2.3409621715545654
step: 8200, Loss: 0.3146229386329651
step: 8300, Loss: 0.13894636929035187
step: 8400, Loss: 0.12117607146501541
step: 8500, Loss: 0.1321345716714859
step: 8600, Loss: 0.12381170690059662
step: 8700, Loss: 0.1215890645980835
step: 8800, Loss: 0.11770955473184586
step: 8900, Loss: 0.11987563967704773
step: 9000, Loss: 0.11693889647722244
step: 9100, Loss: 0.11630755662918091
step: 9200, Loss: 0.11989223957061768
step: 9300, Loss: 0.12181548029184341
step: 9400, Loss: 0.114720419049263
step: 9500, Loss: 0.11545955389738083
step: 9600, Loss: 0.11528762429952621
step: 9700, Loss: 0.11461666226387024
step: 9800, Loss: 0.1165194883942604
step: 9900, Loss: 0.11575838923454285
training successfully ended.
validating...
validate data length:114
acc: 0.9821428571428571
precision: 0.9615384615384616
recall: 1.0
F_score: 0.9803921568627451
******fold 10******

Training... train_data length:1026
step: 0, Loss: 0.11693678796291351
step: 100, Loss: 0.11562014371156693
step: 200, Loss: 0.1164526715874672
step: 300, Loss: 0.11464173346757889
step: 400, Loss: 0.11294401437044144
step: 500, Loss: 0.11324835568666458
step: 600, Loss: 0.1137087419629097
step: 700, Loss: 0.11528100073337555
step: 800, Loss: 0.11469794064760208
step: 900, Loss: 0.11372144520282745
step: 1000, Loss: 0.11397503316402435
step: 1100, Loss: 0.11360599100589752
step: 1200, Loss: 0.11290456354618073
step: 1300, Loss: 0.11362013965845108
step: 1400, Loss: 0.11324165761470795
step: 1500, Loss: 0.11303921043872833
step: 1600, Loss: 0.11321365088224411
step: 1700, Loss: 0.11415093392133713
step: 1800, Loss: 0.11445564776659012
step: 1900, Loss: 0.11356902122497559
step: 2000, Loss: 0.11318393051624298
step: 2100, Loss: 0.11415277421474457
step: 2200, Loss: 0.11379143595695496
step: 2300, Loss: 0.11368262767791748
step: 2400, Loss: 0.11472271382808685
step: 2500, Loss: 0.11425060033798218
step: 2600, Loss: 0.11334691196680069
step: 2700, Loss: 0.11395292729139328
step: 2800, Loss: 0.11381841450929642
step: 2900, Loss: 0.11256107687950134
step: 3000, Loss: 0.11309318244457245
step: 3100, Loss: 0.11337115615606308
step: 3200, Loss: 0.11450529098510742
step: 3300, Loss: 0.11405612528324127
step: 3400, Loss: 0.11259789019823074
step: 3500, Loss: 0.1149197667837143
step: 3600, Loss: 0.11472803354263306
step: 3700, Loss: 0.11500798165798187
step: 3800, Loss: 0.11433925479650497
step: 3900, Loss: 0.11363287270069122
step: 4000, Loss: 0.15338347852230072
step: 4100, Loss: 0.12679935991764069
step: 4200, Loss: 0.12604093551635742
step: 4300, Loss: 0.11880654096603394
step: 4400, Loss: 0.12020500749349594
step: 4500, Loss: 0.122931107878685
step: 4600, Loss: 0.1163930892944336
step: 4700, Loss: 0.11587689816951752
step: 4800, Loss: 0.11659841239452362
step: 4900, Loss: 0.117041677236557
step: 5000, Loss: 0.11447753757238388
step: 5100, Loss: 0.11652719229459763
step: 5200, Loss: 0.1159549131989479
step: 5300, Loss: 0.11426399648189545
step: 5400, Loss: 0.11567510664463043
step: 5500, Loss: 0.11486343294382095
step: 5600, Loss: 0.11687446385622025
step: 5700, Loss: 0.11546903848648071
step: 5800, Loss: 0.11525936424732208
step: 5900, Loss: 0.1144392117857933
step: 6000, Loss: 0.11448673903942108
step: 6100, Loss: 0.11396355926990509
step: 6200, Loss: 0.1143687292933464
step: 6300, Loss: 0.11450619995594025
step: 6400, Loss: 0.11378522962331772
step: 6500, Loss: 0.1153307631611824
step: 6600, Loss: 0.11483687162399292
step: 6700, Loss: 0.11384140700101852
step: 6800, Loss: 0.11454050987958908
step: 6900, Loss: 0.11389948427677155
step: 7000, Loss: 0.11423563957214355
step: 7100, Loss: 0.11487586796283722
step: 7200, Loss: 0.1137864738702774
step: 7300, Loss: 0.11362576484680176
step: 7400, Loss: 0.11350198835134506
step: 7500, Loss: 0.1141914576292038
step: 7600, Loss: 0.11359452456235886
step: 7700, Loss: 0.11321751028299332
step: 7800, Loss: 0.11425989866256714
step: 7900, Loss: 0.11338914930820465
step: 8000, Loss: 0.11413054913282394
step: 8100, Loss: 0.11314235627651215
step: 8200, Loss: 0.11351466923952103
step: 8300, Loss: 0.11321360617876053
step: 8400, Loss: 0.11478389799594879
step: 8500, Loss: 0.11330457776784897
step: 8600, Loss: 0.11301352828741074
step: 8700, Loss: 0.11396409571170807
step: 8800, Loss: 0.11606317013502121
step: 8900, Loss: 0.11380070447921753
step: 9000, Loss: 0.1140342727303505
step: 9100, Loss: 0.11323494464159012
step: 9200, Loss: 0.11372776329517365
step: 9300, Loss: 0.113576240837574
step: 9400, Loss: 0.11398562043905258
step: 9500, Loss: 0.11267933249473572
step: 9600, Loss: 0.11273596435785294
step: 9700, Loss: 0.11417245119810104
step: 9800, Loss: 0.11438895016908646
step: 9900, Loss: 0.11325956135988235
training successfully ended.
validating...
validate data length:114
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 25 Avgacc: 0.9883928571428571 Avgfscore: 0.9884137740297196 
 Max acc:1.0, Max f score:1.0
******** mix subject_26 ********

[323, 437]
******fold 1******

Training... train_data length:786
step: 0, Loss: 53.17865753173828
step: 100, Loss: 5.74880838394165
step: 200, Loss: 1.1617865562438965
step: 300, Loss: 0.7305214405059814
step: 400, Loss: 1.2992883920669556
step: 500, Loss: 0.5563574433326721
step: 600, Loss: 0.1686922311782837
step: 700, Loss: 0.13973267376422882
step: 800, Loss: 0.12368447333574295
step: 900, Loss: 0.13664111495018005
step: 1000, Loss: 0.13003215193748474
step: 1100, Loss: 0.12724298238754272
step: 1200, Loss: 0.13728351891040802
step: 1300, Loss: 0.12416104972362518
step: 1400, Loss: 0.12283886224031448
step: 1500, Loss: 0.12988319993019104
step: 1600, Loss: 0.12358198314905167
step: 1700, Loss: 0.12872208654880524
step: 1800, Loss: 0.11931736022233963
step: 1900, Loss: 0.11823737621307373
step: 2000, Loss: 0.12135657668113708
step: 2100, Loss: 0.1215074360370636
step: 2200, Loss: 0.11933325231075287
step: 2300, Loss: 0.12277667224407196
step: 2400, Loss: 0.12169839441776276
step: 2500, Loss: 0.11760253459215164
step: 2600, Loss: 0.11951614916324615
step: 2700, Loss: 0.1201031282544136
step: 2800, Loss: 0.12156465649604797
step: 2900, Loss: 0.11746847629547119
step: 3000, Loss: 0.11701010167598724
step: 3100, Loss: 0.11675870418548584
step: 3200, Loss: 0.11879320442676544
step: 3300, Loss: 0.11795651167631149
step: 3400, Loss: 0.11993617564439774
step: 3500, Loss: 0.1175098791718483
step: 3600, Loss: 0.11620768159627914
step: 3700, Loss: 0.11798515915870667
step: 3800, Loss: 0.11559388786554337
step: 3900, Loss: 0.11760012805461884
step: 4000, Loss: 0.1168612465262413
step: 4100, Loss: 0.11533354967832565
step: 4200, Loss: 0.11575863510370255
step: 4300, Loss: 0.11674950271844864
step: 4400, Loss: 0.11559657007455826
step: 4500, Loss: 0.11569305509328842
step: 4600, Loss: 0.11473238468170166
step: 4700, Loss: 0.11498591303825378
step: 4800, Loss: 0.11672009527683258
step: 4900, Loss: 0.11588433384895325
step: 5000, Loss: 0.11616512387990952
step: 5100, Loss: 0.11407467722892761
step: 5200, Loss: 0.1146625429391861
step: 5300, Loss: 0.11437664926052094
step: 5400, Loss: 0.11665336787700653
step: 5500, Loss: 0.1150476261973381
step: 5600, Loss: 0.11593593657016754
step: 5700, Loss: 0.11460134387016296
step: 5800, Loss: 0.11529895663261414
step: 5900, Loss: 0.11403529345989227
step: 6000, Loss: 0.11279921233654022
step: 6100, Loss: 0.11466993391513824
step: 6200, Loss: 0.11491498351097107
step: 6300, Loss: 0.11317270994186401
step: 6400, Loss: 0.11374788731336594
step: 6500, Loss: 0.11585675925016403
step: 6600, Loss: 0.11632800847291946
step: 6700, Loss: 0.11545538902282715
step: 6800, Loss: 0.11342819035053253
step: 6900, Loss: 0.11487001925706863
step: 7000, Loss: 0.11411917954683304
step: 7100, Loss: 0.1163872554898262
step: 7200, Loss: 0.11496120691299438
step: 7300, Loss: 7.121588706970215
step: 7400, Loss: 3.026193141937256
step: 7500, Loss: 0.1553277224302292
step: 7600, Loss: 0.1354203075170517
step: 7700, Loss: 0.12649327516555786
step: 7800, Loss: 0.12896940112113953
step: 7900, Loss: 0.12261234223842621
step: 8000, Loss: 0.12101990729570389
step: 8100, Loss: 0.12314034253358841
step: 8200, Loss: 0.12209272384643555
step: 8300, Loss: 0.12407918274402618
step: 8400, Loss: 0.11978274583816528
step: 8500, Loss: 0.11771148443222046
step: 8600, Loss: 0.12022155523300171
step: 8700, Loss: 0.12123875319957733
step: 8800, Loss: 0.11920072883367538
step: 8900, Loss: 0.11725804209709167
step: 9000, Loss: 0.12004142254590988
step: 9100, Loss: 0.11638714373111725
step: 9200, Loss: 0.12203224003314972
step: 9300, Loss: 0.11663561314344406
step: 9400, Loss: 0.11805827915668488
step: 9500, Loss: 0.11490605026483536
step: 9600, Loss: 0.11787822097539902
step: 9700, Loss: 0.11613187938928604
step: 9800, Loss: 0.11689378321170807
step: 9900, Loss: 0.1192212775349617
training successfully ended.
validating...
validate data length:88
acc: 0.8295454545454546
precision: 0.8333333333333334
recall: 0.851063829787234
F_score: 0.8421052631578947
******fold 2******

Training... train_data length:786
step: 0, Loss: 0.11691980808973312
step: 100, Loss: 0.13569149374961853
step: 200, Loss: 0.12483922392129898
step: 300, Loss: 0.1269250065088272
step: 400, Loss: 0.11983327567577362
step: 500, Loss: 0.1201261356472969
step: 600, Loss: 0.12028118968009949
step: 700, Loss: 0.11800888180732727
step: 800, Loss: 0.11886961758136749
step: 900, Loss: 0.11897285282611847
step: 1000, Loss: 0.11859823763370514
step: 1100, Loss: 0.11658050119876862
step: 1200, Loss: 0.11654314398765564
step: 1300, Loss: 0.113370880484581
step: 1400, Loss: 0.11665565520524979
step: 1500, Loss: 0.11353705823421478
step: 1600, Loss: 0.11469244956970215
step: 1700, Loss: 0.11703942716121674
step: 1800, Loss: 0.11378496885299683
step: 1900, Loss: 0.11689256876707077
step: 2000, Loss: 0.11646738648414612
step: 2100, Loss: 0.11704390496015549
step: 2200, Loss: 0.1171930581331253
step: 2300, Loss: 0.11319544166326523
step: 2400, Loss: 0.11552054435014725
step: 2500, Loss: 0.11416785418987274
step: 2600, Loss: 0.11643882095813751
step: 2700, Loss: 0.1158536821603775
step: 2800, Loss: 0.11417394876480103
step: 2900, Loss: 0.1142461821436882
step: 3000, Loss: 0.1152161955833435
step: 3100, Loss: 0.11344952881336212
step: 3200, Loss: 0.11480655521154404
step: 3300, Loss: 0.11526156961917877
step: 3400, Loss: 0.11608682572841644
step: 3500, Loss: 0.11435012519359589
step: 3600, Loss: 0.11303449422121048
step: 3700, Loss: 0.11419408768415451
step: 3800, Loss: 4.94852352142334
step: 3900, Loss: 0.3153887987136841
step: 4000, Loss: 0.1307188719511032
step: 4100, Loss: 0.11948544532060623
step: 4200, Loss: 0.12951183319091797
step: 4300, Loss: 0.12417127937078476
step: 4400, Loss: 0.11911050975322723
step: 4500, Loss: 0.11616985499858856
step: 4600, Loss: 0.12210281193256378
step: 4700, Loss: 0.12090396136045456
step: 4800, Loss: 0.11916008591651917
step: 4900, Loss: 0.11784535646438599
step: 5000, Loss: 0.11717052757740021
step: 5100, Loss: 0.11770924180746078
step: 5200, Loss: 0.11879220604896545
step: 5300, Loss: 0.11871327459812164
step: 5400, Loss: 0.1176995262503624
step: 5500, Loss: 0.11573082208633423
step: 5600, Loss: 0.11720134317874908
step: 5700, Loss: 0.11528681963682175
step: 5800, Loss: 0.1172066479921341
step: 5900, Loss: 0.11452633142471313
step: 6000, Loss: 0.11714305728673935
step: 6100, Loss: 0.11548985540866852
step: 6200, Loss: 0.11704275757074356
step: 6300, Loss: 0.1157708466053009
step: 6400, Loss: 0.1160489171743393
step: 6500, Loss: 0.11578138172626495
step: 6600, Loss: 0.11427132785320282
step: 6700, Loss: 0.11337508261203766
step: 6800, Loss: 0.11367308348417282
step: 6900, Loss: 0.11422647535800934
step: 7000, Loss: 0.1152612715959549
step: 7100, Loss: 0.1154986023902893
step: 7200, Loss: 0.11664488911628723
step: 7300, Loss: 0.11446654796600342
step: 7400, Loss: 0.1133689433336258
step: 7500, Loss: 0.11505617201328278
step: 7600, Loss: 0.11575688421726227
step: 7700, Loss: 0.11345749348402023
step: 7800, Loss: 0.11392173171043396
step: 7900, Loss: 0.11337046325206757
step: 8000, Loss: 0.11416661739349365
step: 8100, Loss: 0.11375229060649872
step: 8200, Loss: 0.11422675848007202
step: 8300, Loss: 0.11414985358715057
step: 8400, Loss: 0.1138060912489891
step: 8500, Loss: 0.11350205540657043
step: 8600, Loss: 0.11372468620538712
step: 8700, Loss: 0.11443936824798584
step: 8800, Loss: 0.11348772794008255
step: 8900, Loss: 0.11421908438205719
step: 9000, Loss: 0.11273251473903656
step: 9100, Loss: 0.1136309802532196
step: 9200, Loss: 0.11489780247211456
step: 9300, Loss: 0.11405780166387558
step: 9400, Loss: 0.1131567507982254
step: 9500, Loss: 0.11410755664110184
step: 9600, Loss: 0.11455881595611572
step: 9700, Loss: 0.11390668898820877
step: 9800, Loss: 0.11409308016300201
step: 9900, Loss: 0.11296345293521881
training successfully ended.
validating...
validate data length:88
acc: 0.9659090909090909
precision: 0.9761904761904762
recall: 0.9534883720930233
F_score: 0.9647058823529412
******fold 3******

Training... train_data length:786
step: 0, Loss: 0.12075652182102203
step: 100, Loss: 0.11859726905822754
step: 200, Loss: 0.11668377369642258
step: 300, Loss: 0.11557363718748093
step: 400, Loss: 0.1136036217212677
step: 500, Loss: 0.11472098529338837
step: 600, Loss: 0.11520847678184509
step: 700, Loss: 0.11437226831912994
step: 800, Loss: 0.11356742680072784
step: 900, Loss: 0.11322580277919769
step: 1000, Loss: 0.11278646439313889
step: 1100, Loss: 0.11444205045700073
step: 1200, Loss: 0.11410374939441681
step: 1300, Loss: 0.11373604089021683
step: 1400, Loss: 0.11380967497825623
step: 1500, Loss: 0.11493608355522156
step: 1600, Loss: 0.11408904939889908
step: 1700, Loss: 2.0431132316589355
step: 1800, Loss: 0.43902844190597534
step: 1900, Loss: 0.1305036097764969
step: 2000, Loss: 0.12383439391851425
step: 2100, Loss: 0.1197815090417862
step: 2200, Loss: 0.11583869159221649
step: 2300, Loss: 0.1207844465970993
step: 2400, Loss: 0.11888142675161362
step: 2500, Loss: 0.1211286336183548
step: 2600, Loss: 0.11858925968408585
step: 2700, Loss: 0.1164669394493103
step: 2800, Loss: 0.11744814366102219
step: 2900, Loss: 0.11685197800397873
step: 3000, Loss: 0.11742809414863586
step: 3100, Loss: 0.11631771177053452
step: 3200, Loss: 0.11667358130216599
step: 3300, Loss: 0.11462794244289398
step: 3400, Loss: 0.1171526312828064
step: 3500, Loss: 0.11559563130140305
step: 3600, Loss: 0.11635683476924896
step: 3700, Loss: 0.11424639075994492
step: 3800, Loss: 0.11478886753320694
step: 3900, Loss: 0.11564744263887405
step: 4000, Loss: 0.11608735471963882
step: 4100, Loss: 0.11576186865568161
step: 4200, Loss: 0.11433582007884979
step: 4300, Loss: 0.11617736518383026
step: 4400, Loss: 0.1164638102054596
step: 4500, Loss: 0.11529858410358429
step: 4600, Loss: 0.11520397663116455
step: 4700, Loss: 0.11493932455778122
step: 4800, Loss: 0.11384430527687073
step: 4900, Loss: 0.11435732245445251
step: 5000, Loss: 0.11582425236701965
step: 5100, Loss: 0.11567060649394989
step: 5200, Loss: 0.11589322984218597
step: 5300, Loss: 0.11465424299240112
step: 5400, Loss: 0.11467855423688889
step: 5500, Loss: 0.11363109946250916
step: 5600, Loss: 0.1159425675868988
step: 5700, Loss: 0.11416706442832947
step: 5800, Loss: 0.11541321873664856
step: 5900, Loss: 0.11383426189422607
step: 6000, Loss: 0.1143464744091034
step: 6100, Loss: 0.11437027901411057
step: 6200, Loss: 0.11424136906862259
step: 6300, Loss: 0.11354340612888336
step: 6400, Loss: 0.11330760270357132
step: 6500, Loss: 0.11455918848514557
step: 6600, Loss: 0.11412997543811798
step: 6700, Loss: 0.11469046771526337
step: 6800, Loss: 0.11428742110729218
step: 6900, Loss: 0.1140795648097992
step: 7000, Loss: 0.11421814560890198
step: 7100, Loss: 0.11351433396339417
step: 7200, Loss: 0.11333483457565308
step: 7300, Loss: 0.11465930193662643
step: 7400, Loss: 0.11373648047447205
step: 7500, Loss: 0.11360465735197067
step: 7600, Loss: 0.11372929811477661
step: 7700, Loss: 0.11448870599269867
step: 7800, Loss: 0.11300725489854813
step: 7900, Loss: 0.1139751672744751
step: 8000, Loss: 0.11338695883750916
step: 8100, Loss: 0.11357906460762024
step: 8200, Loss: 0.1134062260389328
step: 8300, Loss: 0.34482255578041077
step: 8400, Loss: 3.670205593109131
step: 8500, Loss: 0.18735376000404358
step: 8600, Loss: 0.12913402915000916
step: 8700, Loss: 0.12629565596580505
step: 8800, Loss: 0.1201583594083786
step: 8900, Loss: 0.11600685119628906
step: 9000, Loss: 0.11951522529125214
step: 9100, Loss: 0.11924360692501068
step: 9200, Loss: 0.11783139407634735
step: 9300, Loss: 0.11603595316410065
step: 9400, Loss: 0.11725808680057526
step: 9500, Loss: 0.11924000084400177
step: 9600, Loss: 0.11689333617687225
step: 9700, Loss: 0.11770239472389221
step: 9800, Loss: 0.12077891826629639
step: 9900, Loss: 0.1159762293100357
training successfully ended.
validating...
validate data length:88
acc: 0.9545454545454546
precision: 0.9534883720930233
recall: 0.9534883720930233
F_score: 0.9534883720930233
******fold 4******

Training... train_data length:786
step: 0, Loss: 0.11729568243026733
step: 100, Loss: 0.11716057360172272
step: 200, Loss: 0.11472028493881226
step: 300, Loss: 0.11408788710832596
step: 400, Loss: 0.11539649218320847
step: 500, Loss: 0.11393789947032928
step: 600, Loss: 0.11346855014562607
step: 700, Loss: 0.11390610784292221
step: 800, Loss: 0.11463706195354462
step: 900, Loss: 0.11534824222326279
step: 1000, Loss: 0.11450262367725372
step: 1100, Loss: 0.11354613304138184
step: 1200, Loss: 0.11422503739595413
step: 1300, Loss: 0.11486497521400452
step: 1400, Loss: 0.11469447612762451
step: 1500, Loss: 0.11397912353277206
step: 1600, Loss: 0.11356917768716812
step: 1700, Loss: 0.11361861974000931
step: 1800, Loss: 0.11295954138040543
step: 1900, Loss: 0.113386370241642
step: 2000, Loss: 0.1132308691740036
step: 2100, Loss: 0.11664630472660065
step: 2200, Loss: 0.11358950287103653
step: 2300, Loss: 0.11375296115875244
step: 2400, Loss: 0.11348366737365723
step: 2500, Loss: 0.113872230052948
step: 2600, Loss: 0.11313290894031525
step: 2700, Loss: 0.11425641179084778
step: 2800, Loss: 0.11386404931545258
step: 2900, Loss: 0.11418271064758301
step: 3000, Loss: 0.11355958878993988
step: 3100, Loss: 0.11388374865055084
step: 3200, Loss: 0.11746524274349213
step: 3300, Loss: 0.11476543545722961
step: 3400, Loss: 0.11476801335811615
step: 3500, Loss: 0.14055505394935608
step: 3600, Loss: 0.1734931468963623
step: 3700, Loss: 0.15171265602111816
step: 3800, Loss: 0.13700830936431885
step: 3900, Loss: 0.13176359236240387
step: 4000, Loss: 0.12941130995750427
step: 4100, Loss: 0.11869322508573532
step: 4200, Loss: 0.11948534101247787
step: 4300, Loss: 0.1184142529964447
step: 4400, Loss: 0.11815148591995239
step: 4500, Loss: 0.11904092133045197
step: 4600, Loss: 0.12037505209445953
step: 4700, Loss: 0.11669906973838806
step: 4800, Loss: 0.11838524788618088
step: 4900, Loss: 0.11548156291246414
step: 5000, Loss: 0.11748383939266205
step: 5100, Loss: 0.117542564868927
step: 5200, Loss: 0.11711669713258743
step: 5300, Loss: 0.11714296042919159
step: 5400, Loss: 0.11605638265609741
step: 5500, Loss: 0.11636405438184738
step: 5600, Loss: 0.11707665026187897
step: 5700, Loss: 0.11594177782535553
step: 5800, Loss: 0.11597983539104462
step: 5900, Loss: 0.11551736295223236
step: 6000, Loss: 0.11561760306358337
step: 6100, Loss: 0.1150679662823677
step: 6200, Loss: 0.11484594643115997
step: 6300, Loss: 0.11494053900241852
step: 6400, Loss: 0.11428551375865936
step: 6500, Loss: 0.11525564640760422
step: 6600, Loss: 0.11555647850036621
step: 6700, Loss: 0.11428363621234894
step: 6800, Loss: 0.1133054718375206
step: 6900, Loss: 0.11481429636478424
step: 7000, Loss: 0.11458682268857956
step: 7100, Loss: 0.11520031094551086
step: 7200, Loss: 0.11457133293151855
step: 7300, Loss: 0.11468401551246643
step: 7400, Loss: 0.1137452945113182
step: 7500, Loss: 0.11340442299842834
step: 7600, Loss: 0.11438817530870438
step: 7700, Loss: 0.11458682268857956
step: 7800, Loss: 0.11421951651573181
step: 7900, Loss: 0.11315692961215973
step: 8000, Loss: 0.11384638398885727
step: 8100, Loss: 0.11567536741495132
step: 8200, Loss: 0.11394169926643372
step: 8300, Loss: 0.11306115984916687
step: 8400, Loss: 0.11494360864162445
step: 8500, Loss: 0.11296681314706802
step: 8600, Loss: 0.11272597312927246
step: 8700, Loss: 0.11389406770467758
step: 8800, Loss: 0.1130535677075386
step: 8900, Loss: 0.11395781487226486
step: 9000, Loss: 0.11296553164720535
step: 9100, Loss: 0.11365626752376556
step: 9200, Loss: 0.11297514289617538
step: 9300, Loss: 0.11317676305770874
step: 9400, Loss: 0.11303652077913284
step: 9500, Loss: 0.11387861520051956
step: 9600, Loss: 0.11352621763944626
step: 9700, Loss: 0.11697986721992493
step: 9800, Loss: 0.11440001428127289
step: 9900, Loss: 0.1136995330452919
training successfully ended.
validating...
validate data length:88
acc: 0.9886363636363636
precision: 0.9787234042553191
recall: 1.0
F_score: 0.989247311827957
******fold 5******

Training... train_data length:787
step: 0, Loss: 0.1291678547859192
step: 100, Loss: 0.11449312418699265
step: 200, Loss: 0.11673372983932495
step: 300, Loss: 0.1157405897974968
step: 400, Loss: 0.11481140553951263
step: 500, Loss: 0.11301583051681519
step: 600, Loss: 0.11410687863826752
step: 700, Loss: 0.11488671600818634
step: 800, Loss: 0.1134251058101654
step: 900, Loss: 0.11445706337690353
step: 1000, Loss: 0.1129971519112587
step: 1100, Loss: 0.11415417492389679
step: 1200, Loss: 0.1153835654258728
step: 1300, Loss: 0.11402929574251175
step: 1400, Loss: 0.113264299929142
step: 1500, Loss: 0.11335872858762741
step: 1600, Loss: 0.11420479416847229
step: 1700, Loss: 0.11354511976242065
step: 1800, Loss: 0.1134994700551033
step: 1900, Loss: 0.11325953155755997
step: 2000, Loss: 0.11490345746278763
step: 2100, Loss: 0.11348224431276321
step: 2200, Loss: 0.11341747641563416
step: 2300, Loss: 0.11353742331266403
step: 2400, Loss: 0.11509402096271515
step: 2500, Loss: 0.11363627016544342
step: 2600, Loss: 0.11243608593940735
step: 2700, Loss: 0.11289859563112259
step: 2800, Loss: 0.11376608908176422
step: 2900, Loss: 0.1142977774143219
step: 3000, Loss: 0.1129034236073494
step: 3100, Loss: 0.11484961211681366
step: 3200, Loss: 0.11358316987752914
step: 3300, Loss: 0.11370868235826492
step: 3400, Loss: 0.11577044427394867
step: 3500, Loss: 0.11328724771738052
step: 3600, Loss: 0.11360760033130646
step: 3700, Loss: 0.11304565519094467
step: 3800, Loss: 0.11379209160804749
step: 3900, Loss: 0.11365418136119843
step: 4000, Loss: 0.11576154083013535
step: 4100, Loss: 0.1131812110543251
step: 4200, Loss: 1.2746227979660034
step: 4300, Loss: 0.15832455456256866
step: 4400, Loss: 0.1314021646976471
step: 4500, Loss: 0.12253132462501526
step: 4600, Loss: 0.12088151276111603
step: 4700, Loss: 0.12446877360343933
step: 4800, Loss: 0.11832719296216965
step: 4900, Loss: 0.11848972737789154
step: 5000, Loss: 0.1251232922077179
step: 5100, Loss: 0.11833591759204865
step: 5200, Loss: 0.12071375548839569
step: 5300, Loss: 0.11902043223381042
step: 5400, Loss: 0.11585123836994171
step: 5500, Loss: 0.1155194491147995
step: 5600, Loss: 0.11466865241527557
step: 5700, Loss: 0.11616133898496628
step: 5800, Loss: 0.11750293523073196
step: 5900, Loss: 0.11528836935758591
step: 6000, Loss: 0.11594530940055847
step: 6100, Loss: 0.11581604182720184
step: 6200, Loss: 0.11447973549365997
step: 6300, Loss: 0.11619946360588074
step: 6400, Loss: 0.11676383018493652
step: 6500, Loss: 0.1141788512468338
step: 6600, Loss: 0.11774645000696182
step: 6700, Loss: 0.11498689651489258
step: 6800, Loss: 0.11470621079206467
step: 6900, Loss: 0.11488187313079834
step: 7000, Loss: 0.11467868834733963
step: 7100, Loss: 0.11454498022794724
step: 7200, Loss: 0.1142919659614563
step: 7300, Loss: 0.11579994857311249
step: 7400, Loss: 0.11424453556537628
step: 7500, Loss: 0.11432540416717529
step: 7600, Loss: 0.11391481757164001
step: 7700, Loss: 0.11420860886573792
step: 7800, Loss: 0.11618580669164658
step: 7900, Loss: 0.11421692371368408
step: 8000, Loss: 0.11473900079727173
step: 8100, Loss: 0.11558808386325836
step: 8200, Loss: 0.11368362605571747
step: 8300, Loss: 0.11311143636703491
step: 8400, Loss: 0.11323940008878708
step: 8500, Loss: 0.11489753425121307
step: 8600, Loss: 0.11465926468372345
step: 8700, Loss: 0.11473174393177032
step: 8800, Loss: 0.11361363530158997
step: 8900, Loss: 0.11424648761749268
step: 9000, Loss: 0.11357609927654266
step: 9100, Loss: 0.11377790570259094
step: 9200, Loss: 0.11433456838130951
step: 9300, Loss: 0.11509358882904053
step: 9400, Loss: 0.11375641077756882
step: 9500, Loss: 0.11398138105869293
step: 9600, Loss: 0.11339356005191803
step: 9700, Loss: 0.11376960575580597
step: 9800, Loss: 0.11370949447154999
step: 9900, Loss: 0.11513972282409668
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

Training... train_data length:787
step: 0, Loss: 0.11327793449163437
step: 100, Loss: 0.12059001624584198
step: 200, Loss: 0.11752959340810776
step: 300, Loss: 0.11476919800043106
step: 400, Loss: 0.1145479753613472
step: 500, Loss: 0.11391299217939377
step: 600, Loss: 0.11492665112018585
step: 700, Loss: 0.11410969495773315
step: 800, Loss: 0.11342742294073105
step: 900, Loss: 0.11354590207338333
step: 1000, Loss: 0.11285077780485153
step: 1100, Loss: 0.11354059725999832
step: 1200, Loss: 0.11400521546602249
step: 1300, Loss: 0.11369654536247253
step: 1400, Loss: 0.11305150389671326
step: 1500, Loss: 0.11466451734304428
step: 1600, Loss: 0.1142200231552124
step: 1700, Loss: 0.11378023773431778
step: 1800, Loss: 0.11301524937152863
step: 1900, Loss: 0.11407510191202164
step: 2000, Loss: 0.11376139521598816
step: 2100, Loss: 0.11336621642112732
step: 2200, Loss: 0.11445280909538269
step: 2300, Loss: 0.11611241102218628
step: 2400, Loss: 0.11359810829162598
step: 2500, Loss: 0.11379705369472504
step: 2600, Loss: 0.11431971937417984
step: 2700, Loss: 0.11686811596155167
step: 2800, Loss: 0.1138506829738617
step: 2900, Loss: 0.11373909562826157
step: 3000, Loss: 0.11284810304641724
step: 3100, Loss: 0.11267458647489548
step: 3200, Loss: 0.11312204599380493
step: 3300, Loss: 0.11393415927886963
step: 3400, Loss: 5.37830924987793
step: 3500, Loss: 0.22225800156593323
step: 3600, Loss: 0.13072781264781952
step: 3700, Loss: 0.12430960685014725
step: 3800, Loss: 0.12197895348072052
step: 3900, Loss: 0.11607226729393005
step: 4000, Loss: 0.11784763634204865
step: 4100, Loss: 0.1218222826719284
step: 4200, Loss: 0.12173184007406235
step: 4300, Loss: 0.11523357033729553
step: 4400, Loss: 0.11654116958379745
step: 4500, Loss: 0.11794790625572205
step: 4600, Loss: 0.1175553947687149
step: 4700, Loss: 0.1183655709028244
step: 4800, Loss: 0.11508753895759583
step: 4900, Loss: 0.11607675999403
step: 5000, Loss: 0.11529520153999329
step: 5100, Loss: 0.1162237823009491
step: 5200, Loss: 0.1134786605834961
step: 5300, Loss: 0.1141153872013092
step: 5400, Loss: 0.11481176316738129
step: 5500, Loss: 0.1152290552854538
step: 5600, Loss: 0.11550082266330719
step: 5700, Loss: 0.1164267510175705
step: 5800, Loss: 0.11587951332330704
step: 5900, Loss: 0.1159854531288147
step: 6000, Loss: 0.11556319892406464
step: 6100, Loss: 0.11518656462430954
step: 6200, Loss: 0.1147703304886818
step: 6300, Loss: 0.1142585277557373
step: 6400, Loss: 0.11313536763191223
step: 6500, Loss: 0.11418912559747696
step: 6600, Loss: 0.11432720720767975
step: 6700, Loss: 0.1134471744298935
step: 6800, Loss: 0.1140180379152298
step: 6900, Loss: 0.1144053265452385
step: 7000, Loss: 0.11393799632787704
step: 7100, Loss: 0.11484894901514053
step: 7200, Loss: 0.11613771319389343
step: 7300, Loss: 0.11395956575870514
step: 7400, Loss: 0.11488741636276245
step: 7500, Loss: 0.1136566624045372
step: 7600, Loss: 0.11429477483034134
step: 7700, Loss: 0.11548423767089844
step: 7800, Loss: 0.11407198011875153
step: 7900, Loss: 0.11381896585226059
step: 8000, Loss: 0.1151525229215622
step: 8100, Loss: 0.11313100904226303
step: 8200, Loss: 0.11347271502017975
step: 8300, Loss: 0.11400535702705383
step: 8400, Loss: 0.11567129194736481
step: 8500, Loss: 0.11315805464982986
step: 8600, Loss: 0.11324858665466309
step: 8700, Loss: 0.11305785179138184
step: 8800, Loss: 0.1140141710639
step: 8900, Loss: 0.11496828496456146
step: 9000, Loss: 0.11357289552688599
step: 9100, Loss: 0.11418715864419937
step: 9200, Loss: 0.11419672518968582
step: 9300, Loss: 0.11329688131809235
step: 9400, Loss: 0.11378154903650284
step: 9500, Loss: 0.11352286487817764
step: 9600, Loss: 0.11413466930389404
step: 9700, Loss: 0.11300347745418549
step: 9800, Loss: 0.11259425431489944
step: 9900, Loss: 0.11416619271039963
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:787
step: 0, Loss: 0.1151336133480072
step: 100, Loss: 0.11757625639438629
step: 200, Loss: 0.11438769102096558
step: 300, Loss: 0.11557137966156006
step: 400, Loss: 0.11480048298835754
step: 500, Loss: 0.11414363235235214
step: 600, Loss: 0.11421464383602142
step: 700, Loss: 0.115147665143013
step: 800, Loss: 0.1133880466222763
step: 900, Loss: 0.1137835681438446
step: 1000, Loss: 0.11386711895465851
step: 1100, Loss: 0.11280509829521179
step: 1200, Loss: 0.11355461180210114
step: 1300, Loss: 0.11405978351831436
step: 1400, Loss: 0.11393493413925171
step: 1500, Loss: 0.11380604654550552
step: 1600, Loss: 0.11407309770584106
step: 1700, Loss: 0.11451472342014313
step: 1800, Loss: 0.11391942203044891
step: 1900, Loss: 0.11319246888160706
step: 2000, Loss: 0.11306783556938171
step: 2100, Loss: 0.11290240287780762
step: 2200, Loss: 0.1134234368801117
step: 2300, Loss: 0.1135053038597107
step: 2400, Loss: 0.11276984214782715
step: 2500, Loss: 0.1142372414469719
step: 2600, Loss: 0.11351180076599121
step: 2700, Loss: 0.114418625831604
step: 2800, Loss: 0.1136193498969078
step: 2900, Loss: 3.107055425643921
step: 3000, Loss: 0.7512678503990173
step: 3100, Loss: 0.12754285335540771
step: 3200, Loss: 0.11999475955963135
step: 3300, Loss: 0.12247815728187561
step: 3400, Loss: 0.12103250622749329
step: 3500, Loss: 0.12053007632493973
step: 3600, Loss: 0.12072593718767166
step: 3700, Loss: 0.11574792116880417
step: 3800, Loss: 0.11742919683456421
step: 3900, Loss: 0.12171828746795654
step: 4000, Loss: 0.11922989040613174
step: 4100, Loss: 0.1161937415599823
step: 4200, Loss: 0.11751872301101685
step: 4300, Loss: 0.11644111573696136
step: 4400, Loss: 0.11677698791027069
step: 4500, Loss: 0.1194704994559288
step: 4600, Loss: 0.11714356392621994
step: 4700, Loss: 0.11552464962005615
step: 4800, Loss: 0.11385144293308258
step: 4900, Loss: 0.11609503626823425
step: 5000, Loss: 0.11691461503505707
step: 5100, Loss: 0.11546985805034637
step: 5200, Loss: 0.11522693932056427
step: 5300, Loss: 0.11704263091087341
step: 5400, Loss: 0.11504962295293808
step: 5500, Loss: 0.11450646817684174
step: 5600, Loss: 0.11426608264446259
step: 5700, Loss: 0.11635833978652954
step: 5800, Loss: 0.11564324796199799
step: 5900, Loss: 0.11417838931083679
step: 6000, Loss: 0.11457739025354385
step: 6100, Loss: 0.11608783155679703
step: 6200, Loss: 0.11524302512407303
step: 6300, Loss: 0.11382609605789185
step: 6400, Loss: 0.11358349770307541
step: 6500, Loss: 0.11359993368387222
step: 6600, Loss: 0.11265488713979721
step: 6700, Loss: 0.1146581843495369
step: 6800, Loss: 0.11463423073291779
step: 6900, Loss: 0.11452661454677582
step: 7000, Loss: 0.11555637419223785
step: 7100, Loss: 0.1135084331035614
step: 7200, Loss: 0.11420445144176483
step: 7300, Loss: 0.11439502984285355
step: 7400, Loss: 0.114075668156147
step: 7500, Loss: 0.11334240436553955
step: 7600, Loss: 0.11429192125797272
step: 7700, Loss: 0.11285658180713654
step: 7800, Loss: 0.11516775190830231
step: 7900, Loss: 0.11456307768821716
step: 8000, Loss: 0.11342323571443558
step: 8100, Loss: 0.11481081694364548
step: 8200, Loss: 0.1127350702881813
step: 8300, Loss: 0.11322854459285736
step: 8400, Loss: 0.1149950921535492
step: 8500, Loss: 0.11417626589536667
step: 8600, Loss: 0.11308296769857407
step: 8700, Loss: 0.11347370594739914
step: 8800, Loss: 0.11377933621406555
step: 8900, Loss: 0.11267012357711792
step: 9000, Loss: 0.11419206112623215
step: 9100, Loss: 0.11527025699615479
step: 9200, Loss: 0.11371608823537827
step: 9300, Loss: 0.11403120309114456
step: 9400, Loss: 0.11527419090270996
step: 9500, Loss: 0.11271024495363235
step: 9600, Loss: 0.11298129707574844
step: 9700, Loss: 0.1134297251701355
step: 9800, Loss: 0.1138782650232315
step: 9900, Loss: 0.1135660707950592
training successfully ended.
validating...
validate data length:87
acc: 0.975
precision: 0.9714285714285714
recall: 0.9714285714285714
F_score: 0.9714285714285714
******fold 8******

Training... train_data length:787
step: 0, Loss: 0.11405856162309647
step: 100, Loss: 0.11705611646175385
step: 200, Loss: 0.11481067538261414
step: 300, Loss: 0.11423605680465698
step: 400, Loss: 0.11537647992372513
step: 500, Loss: 0.11311480402946472
step: 600, Loss: 0.1150859072804451
step: 700, Loss: 0.11388969421386719
step: 800, Loss: 0.11400547623634338
step: 900, Loss: 0.1137271374464035
step: 1000, Loss: 0.11299801617860794
step: 1100, Loss: 0.11379123479127884
step: 1200, Loss: 0.11375632882118225
step: 1300, Loss: 0.11583408713340759
step: 1400, Loss: 0.11347879469394684
step: 1500, Loss: 0.11474654078483582
step: 1600, Loss: 0.11285942792892456
step: 1700, Loss: 0.11494042724370956
step: 1800, Loss: 0.1145857721567154
step: 1900, Loss: 0.11457085609436035
step: 2000, Loss: 0.11425629258155823
step: 2100, Loss: 0.11445249617099762
step: 2200, Loss: 0.1152801513671875
step: 2300, Loss: 0.1131216362118721
step: 2400, Loss: 0.11387911438941956
step: 2500, Loss: 0.11400157958269119
step: 2600, Loss: 0.11278584599494934
step: 2700, Loss: 0.1138014867901802
step: 2800, Loss: 0.11334897577762604
step: 2900, Loss: 0.11348792165517807
step: 3000, Loss: 0.1131509393453598
step: 3100, Loss: 0.11334557831287384
step: 3200, Loss: 0.11328443884849548
step: 3300, Loss: 0.11266201734542847
step: 3400, Loss: 0.11350353807210922
step: 3500, Loss: 0.11592196673154831
step: 3600, Loss: 1.0226824283599854
step: 3700, Loss: 0.13701529800891876
step: 3800, Loss: 0.12451204657554626
step: 3900, Loss: 0.11994723975658417
step: 4000, Loss: 0.11952369660139084
step: 4100, Loss: 0.11870332062244415
step: 4200, Loss: 0.11737041175365448
step: 4300, Loss: 0.11629998683929443
step: 4400, Loss: 0.11775390058755875
step: 4500, Loss: 0.11691873520612717
step: 4600, Loss: 0.11858458071947098
step: 4700, Loss: 0.11469970643520355
step: 4800, Loss: 0.11648362874984741
step: 4900, Loss: 0.11748320609331131
step: 5000, Loss: 0.11644886434078217
step: 5100, Loss: 0.11514189839363098
step: 5200, Loss: 0.1167217344045639
step: 5300, Loss: 0.11740005016326904
step: 5400, Loss: 0.11579311639070511
step: 5500, Loss: 0.11487950384616852
step: 5600, Loss: 0.11551843583583832
step: 5700, Loss: 0.11560891568660736
step: 5800, Loss: 0.1137973815202713
step: 5900, Loss: 0.11335214227437973
step: 6000, Loss: 0.11534909904003143
step: 6100, Loss: 0.11526666581630707
step: 6200, Loss: 0.1145985871553421
step: 6300, Loss: 0.11510396003723145
step: 6400, Loss: 0.11575794219970703
step: 6500, Loss: 0.1156185194849968
step: 6600, Loss: 0.11430687457323074
step: 6700, Loss: 0.1141626238822937
step: 6800, Loss: 0.11368005722761154
step: 6900, Loss: 0.11406542360782623
step: 7000, Loss: 0.11362171918153763
step: 7100, Loss: 0.11502138525247574
step: 7200, Loss: 0.11588460206985474
step: 7300, Loss: 0.11448512226343155
step: 7400, Loss: 0.11412990838289261
step: 7500, Loss: 0.11453035473823547
step: 7600, Loss: 0.11384842544794083
step: 7700, Loss: 0.11366492509841919
step: 7800, Loss: 0.11462442576885223
step: 7900, Loss: 0.11453542113304138
step: 8000, Loss: 0.11304255574941635
step: 8100, Loss: 0.11372962594032288
step: 8200, Loss: 0.11500249058008194
step: 8300, Loss: 0.11320873349905014
step: 8400, Loss: 0.11363095790147781
step: 8500, Loss: 0.11391592025756836
step: 8600, Loss: 0.1149945855140686
step: 8700, Loss: 0.11359447240829468
step: 8800, Loss: 0.11310411989688873
step: 8900, Loss: 0.11425688117742538
step: 9000, Loss: 0.1126822680234909
step: 9100, Loss: 0.11337681114673615
step: 9200, Loss: 0.1137896180152893
step: 9300, Loss: 0.1128416433930397
step: 9400, Loss: 0.11352310329675674
step: 9500, Loss: 0.11338257044553757
step: 9600, Loss: 0.1127367839217186
step: 9700, Loss: 0.11323753744363785
step: 9800, Loss: 0.1142110824584961
step: 9900, Loss: 0.11393778771162033
training successfully ended.
validating...
validate data length:87
acc: 0.9375
precision: 0.9111111111111111
recall: 0.9761904761904762
F_score: 0.9425287356321839
******fold 9******

Training... train_data length:787
step: 0, Loss: 0.11299076676368713
step: 100, Loss: 0.11496488004922867
step: 200, Loss: 0.11539630591869354
step: 300, Loss: 0.11432917416095734
step: 400, Loss: 0.11363327503204346
step: 500, Loss: 0.11345641314983368
step: 600, Loss: 0.11518003046512604
step: 700, Loss: 0.11361023783683777
step: 800, Loss: 0.11529755592346191
step: 900, Loss: 0.11302971094846725
step: 1000, Loss: 0.11389646679162979
step: 1100, Loss: 0.11301697045564651
step: 1200, Loss: 0.11344367265701294
step: 1300, Loss: 0.11655472964048386
step: 1400, Loss: 0.1147819459438324
step: 1500, Loss: 0.11373075842857361
step: 1600, Loss: 0.11366146802902222
step: 1700, Loss: 0.11395083367824554
step: 1800, Loss: 0.11348050087690353
step: 1900, Loss: 0.11412031203508377
step: 2000, Loss: 0.11301237344741821
step: 2100, Loss: 0.11302272975444794
step: 2200, Loss: 0.11468224227428436
step: 2300, Loss: 0.11340007185935974
step: 2400, Loss: 0.1144360601902008
step: 2500, Loss: 0.1145770251750946
step: 2600, Loss: 0.11446954309940338
step: 2700, Loss: 0.11287873983383179
step: 2800, Loss: 0.11379273235797882
step: 2900, Loss: 0.11804699152708054
step: 3000, Loss: 0.1141304075717926
step: 3100, Loss: 0.11355528235435486
step: 3200, Loss: 0.11316695809364319
step: 3300, Loss: 0.11460427939891815
step: 3400, Loss: 0.11401813477277756
step: 3500, Loss: 0.11415376514196396
step: 3600, Loss: 0.11390835046768188
step: 3700, Loss: 0.11411306262016296
step: 3800, Loss: 0.9810596704483032
step: 3900, Loss: 0.1405414342880249
step: 4000, Loss: 0.13093003630638123
step: 4100, Loss: 0.12030665576457977
step: 4200, Loss: 0.11918903887271881
step: 4300, Loss: 0.1218414306640625
step: 4400, Loss: 0.11871209740638733
step: 4500, Loss: 0.11872096359729767
step: 4600, Loss: 0.12221912294626236
step: 4700, Loss: 0.12301448732614517
step: 4800, Loss: 0.11637725681066513
step: 4900, Loss: 0.11695965379476547
step: 5000, Loss: 0.11795872449874878
step: 5100, Loss: 0.11897744983434677
step: 5200, Loss: 0.11765722930431366
step: 5300, Loss: 0.11536718904972076
step: 5400, Loss: 0.11544017493724823
step: 5500, Loss: 0.1159428283572197
step: 5600, Loss: 0.11414274573326111
step: 5700, Loss: 0.11740557104349136
step: 5800, Loss: 0.1159941777586937
step: 5900, Loss: 0.1146402582526207
step: 6000, Loss: 0.11615973711013794
step: 6100, Loss: 0.1154673844575882
step: 6200, Loss: 0.11345085501670837
step: 6300, Loss: 0.11450913548469543
step: 6400, Loss: 0.11471405625343323
step: 6500, Loss: 0.11371655017137527
step: 6600, Loss: 0.11344282329082489
step: 6700, Loss: 0.11422562599182129
step: 6800, Loss: 0.11432218551635742
step: 6900, Loss: 0.11396851390600204
step: 7000, Loss: 0.11419603228569031
step: 7100, Loss: 0.11366183310747147
step: 7200, Loss: 0.11409104615449905
step: 7300, Loss: 0.11483117192983627
step: 7400, Loss: 0.11373434960842133
step: 7500, Loss: 0.11407333612442017
step: 7600, Loss: 0.11337966471910477
step: 7700, Loss: 0.11395251005887985
step: 7800, Loss: 0.11404520273208618
step: 7900, Loss: 0.11469960957765579
step: 8000, Loss: 0.11325134336948395
step: 8100, Loss: 0.11406611651182175
step: 8200, Loss: 0.11298913508653641
step: 8300, Loss: 0.1130332201719284
step: 8400, Loss: 0.11462613940238953
step: 8500, Loss: 0.1142059937119484
step: 8600, Loss: 0.1136784479022026
step: 8700, Loss: 0.11368842422962189
step: 8800, Loss: 0.11377140134572983
step: 8900, Loss: 0.11527242511510849
step: 9000, Loss: 0.11512460559606552
step: 9100, Loss: 0.11298462003469467
step: 9200, Loss: 0.11430215835571289
step: 9300, Loss: 0.11500123143196106
step: 9400, Loss: 0.11322192847728729
step: 9500, Loss: 0.11472570151090622
step: 9600, Loss: 0.1132395938038826
step: 9700, Loss: 0.11239684373140335
step: 9800, Loss: 0.11344185471534729
step: 9900, Loss: 0.1138736754655838
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

Training... train_data length:787
step: 0, Loss: 0.11557638645172119
step: 100, Loss: 0.11442768573760986
step: 200, Loss: 0.11604548990726471
step: 300, Loss: 0.1153121143579483
step: 400, Loss: 0.114250548183918
step: 500, Loss: 0.11364610493183136
step: 600, Loss: 0.11422037333250046
step: 700, Loss: 0.11370646953582764
step: 800, Loss: 0.11390922963619232
step: 900, Loss: 0.11487042903900146
step: 1000, Loss: 0.1130441427230835
step: 1100, Loss: 0.1147318184375763
step: 1200, Loss: 0.11326341331005096
step: 1300, Loss: 0.11297328025102615
step: 1400, Loss: 0.11409561336040497
step: 1500, Loss: 0.11441673338413239
step: 1600, Loss: 0.11416053771972656
step: 1700, Loss: 0.11443405598402023
step: 1800, Loss: 0.11425407975912094
step: 1900, Loss: 0.11290200799703598
step: 2000, Loss: 0.11311765015125275
step: 2100, Loss: 0.11369119584560394
step: 2200, Loss: 0.11465517431497574
step: 2300, Loss: 0.11513371020555496
step: 2400, Loss: 0.11356687545776367
step: 2500, Loss: 0.11376049369573593
step: 2600, Loss: 0.11321892589330673
step: 2700, Loss: 0.11352355778217316
step: 2800, Loss: 3.2039475440979004
step: 2900, Loss: 0.1370762437582016
step: 3000, Loss: 0.12342074513435364
step: 3100, Loss: 0.12506625056266785
step: 3200, Loss: 0.12019526958465576
step: 3300, Loss: 0.12103152275085449
step: 3400, Loss: 0.11885736882686615
step: 3500, Loss: 0.12070407718420029
step: 3600, Loss: 0.12076637148857117
step: 3700, Loss: 0.11737990379333496
step: 3800, Loss: 0.12032920122146606
step: 3900, Loss: 0.11821053922176361
step: 4000, Loss: 0.11533544957637787
step: 4100, Loss: 0.11789953708648682
step: 4200, Loss: 0.11614358425140381
step: 4300, Loss: 0.11473038792610168
step: 4400, Loss: 0.11744029074907303
step: 4500, Loss: 0.11496220529079437
step: 4600, Loss: 0.11472038924694061
step: 4700, Loss: 0.1162685751914978
step: 4800, Loss: 0.11376743018627167
step: 4900, Loss: 0.11549411714076996
step: 5000, Loss: 0.11505473405122757
step: 5100, Loss: 0.1146780252456665
step: 5200, Loss: 0.11623411625623703
step: 5300, Loss: 0.11590801179409027
step: 5400, Loss: 0.11350138485431671
step: 5500, Loss: 0.11474643647670746
step: 5600, Loss: 0.11517977714538574
step: 5700, Loss: 0.11463044583797455
step: 5800, Loss: 0.11604231595993042
step: 5900, Loss: 0.11386612057685852
step: 6000, Loss: 0.11507871747016907
step: 6100, Loss: 0.11447297781705856
step: 6200, Loss: 0.11366456001996994
step: 6300, Loss: 0.11336306482553482
step: 6400, Loss: 0.11459069699048996
step: 6500, Loss: 0.11356202512979507
step: 6600, Loss: 0.1138351708650589
step: 6700, Loss: 0.11405839771032333
step: 6800, Loss: 0.11358606815338135
step: 6900, Loss: 0.11620043218135834
step: 7000, Loss: 0.11400124430656433
step: 7100, Loss: 0.11416380107402802
step: 7200, Loss: 0.1151810884475708
step: 7300, Loss: 0.11446130275726318
step: 7400, Loss: 0.11333105713129044
step: 7500, Loss: 0.11417106539011002
step: 7600, Loss: 0.11545543372631073
step: 7700, Loss: 0.11336006969213486
step: 7800, Loss: 0.11408470571041107
step: 7900, Loss: 0.11333464086055756
step: 8000, Loss: 0.11382244527339935
step: 8100, Loss: 0.11450386047363281
step: 8200, Loss: 0.11257971078157425
step: 8300, Loss: 0.11487771570682526
step: 8400, Loss: 0.1126277968287468
step: 8500, Loss: 0.1148432195186615
step: 8600, Loss: 0.11403099447488785
step: 8700, Loss: 0.1132194846868515
step: 8800, Loss: 0.11305917054414749
step: 8900, Loss: 0.11370223015546799
step: 9000, Loss: 0.1134999617934227
step: 9100, Loss: 0.11335401237010956
step: 9200, Loss: 0.11399074643850327
step: 9300, Loss: 0.11301688104867935
step: 9400, Loss: 0.11377277970314026
step: 9500, Loss: 0.11416076123714447
step: 9600, Loss: 0.11341039836406708
step: 9700, Loss: 0.11335525661706924
step: 9800, Loss: 0.11401000618934631
step: 9900, Loss: 0.112650066614151
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 26 Avgacc: 0.9651136363636365 Avgfscore: 0.9663504136492571 
 Max acc:1.0, Max f score:1.0
******** mix subject_27 ********

[380, 380]
******fold 1******

Training... train_data length:684
step: 0, Loss: 40.519351959228516
step: 100, Loss: 4.510749816894531
step: 200, Loss: 1.662622332572937
step: 300, Loss: 2.2139155864715576
step: 400, Loss: 1.9519661664962769
step: 500, Loss: 2.7069787979125977
step: 600, Loss: 0.36921197175979614
step: 700, Loss: 0.41244205832481384
step: 800, Loss: 0.27172982692718506
step: 900, Loss: 0.1640702188014984
step: 1000, Loss: 1.7592228651046753
step: 1100, Loss: 0.187310129404068
step: 1200, Loss: 0.20287726819515228
step: 1300, Loss: 0.1641540229320526
step: 1400, Loss: 0.1418786644935608
step: 1500, Loss: 0.23102018237113953
step: 1600, Loss: 0.1394154131412506
step: 1700, Loss: 0.15014199912548065
step: 1800, Loss: 0.14729544520378113
step: 1900, Loss: 0.1357906460762024
step: 2000, Loss: 0.14239922165870667
step: 2100, Loss: 0.140043705701828
step: 2200, Loss: 0.14942002296447754
step: 2300, Loss: 0.1448671817779541
step: 2400, Loss: 0.1413634717464447
step: 2500, Loss: 0.13749979436397552
step: 2600, Loss: 0.13232089579105377
step: 2700, Loss: 0.13722819089889526
step: 2800, Loss: 0.12881886959075928
step: 2900, Loss: 0.13630595803260803
step: 3000, Loss: 0.1314576119184494
step: 3100, Loss: 0.1332808881998062
step: 3200, Loss: 0.12945887446403503
step: 3300, Loss: 0.12271060794591904
step: 3400, Loss: 0.20977473258972168
step: 3500, Loss: 0.12257051467895508
step: 3600, Loss: 0.12123841047286987
step: 3700, Loss: 0.12141978740692139
step: 3800, Loss: 0.12224245816469193
step: 3900, Loss: 0.1251755803823471
step: 4000, Loss: 0.11903820186853409
step: 4100, Loss: 0.12299153208732605
step: 4200, Loss: 0.12201592326164246
step: 4300, Loss: 0.11984562873840332
step: 4400, Loss: 0.12296745926141739
step: 4500, Loss: 0.13685408234596252
step: 4600, Loss: 5.281776428222656
step: 4700, Loss: 0.9867838025093079
step: 4800, Loss: 0.689138650894165
step: 4900, Loss: 2.392577886581421
step: 5000, Loss: 0.3517753779888153
step: 5100, Loss: 0.1555059552192688
step: 5200, Loss: 0.15060576796531677
step: 5300, Loss: 0.21574373543262482
step: 5400, Loss: 0.13297119736671448
step: 5500, Loss: 0.1379711776971817
step: 5600, Loss: 0.12874239683151245
step: 5700, Loss: 0.13931342959403992
step: 5800, Loss: 0.13509811460971832
step: 5900, Loss: 0.1316496878862381
step: 6000, Loss: 0.13346132636070251
step: 6100, Loss: 0.1374060958623886
step: 6200, Loss: 0.13310606777668
step: 6300, Loss: 0.12794016301631927
step: 6400, Loss: 0.12707293033599854
step: 6500, Loss: 0.12505865097045898
step: 6600, Loss: 0.12335503101348877
step: 6700, Loss: 0.12138201296329498
step: 6800, Loss: 0.1266220360994339
step: 6900, Loss: 0.12214008718729019
step: 7000, Loss: 0.11825163662433624
step: 7100, Loss: 0.11866843700408936
step: 7200, Loss: 0.20119014382362366
step: 7300, Loss: 0.12121932953596115
step: 7400, Loss: 0.1199311688542366
step: 7500, Loss: 0.12088562548160553
step: 7600, Loss: 0.12346801161766052
step: 7700, Loss: 0.11996110528707504
step: 7800, Loss: 0.11856511980295181
step: 7900, Loss: 0.12065471708774567
step: 8000, Loss: 0.1200852170586586
step: 8100, Loss: 0.11846064031124115
step: 8200, Loss: 0.12052711844444275
step: 8300, Loss: 0.11803989112377167
step: 8400, Loss: 0.11922895908355713
step: 8500, Loss: 0.1191263496875763
step: 8600, Loss: 0.1181594654917717
step: 8700, Loss: 0.11917287111282349
step: 8800, Loss: 0.1168266162276268
step: 8900, Loss: 0.11617660522460938
step: 9000, Loss: 0.11413076519966125
step: 9100, Loss: 0.19523462653160095
step: 9200, Loss: 0.11944545805454254
step: 9300, Loss: 0.11732438206672668
step: 9400, Loss: 0.1153779998421669
step: 9500, Loss: 0.11511677503585815
step: 9600, Loss: 0.11621551215648651
step: 9700, Loss: 0.11603229492902756
step: 9800, Loss: 0.11458136886358261
step: 9900, Loss: 0.11646869778633118
training successfully ended.
validating...
validate data length:76
acc: 0.8611111111111112
precision: 0.8484848484848485
recall: 0.8484848484848485
F_score: 0.8484848484848486
******fold 2******

Training... train_data length:684
step: 0, Loss: 2.5786123275756836
step: 100, Loss: 0.21533559262752533
step: 200, Loss: 0.1691199243068695
step: 300, Loss: 0.6586349010467529
step: 400, Loss: 0.9072056412696838
step: 500, Loss: 0.14608174562454224
step: 600, Loss: 0.12673425674438477
step: 700, Loss: 0.131718248128891
step: 800, Loss: 0.13063785433769226
step: 900, Loss: 0.1204594299197197
step: 1000, Loss: 0.12054726481437683
step: 1100, Loss: 0.12486082315444946
step: 1200, Loss: 0.1211465448141098
step: 1300, Loss: 0.12128783762454987
step: 1400, Loss: 0.1196255087852478
step: 1500, Loss: 0.20011679828166962
step: 1600, Loss: 0.1190902441740036
step: 1700, Loss: 0.11829423904418945
step: 1800, Loss: 0.12303642928600311
step: 1900, Loss: 0.11988772451877594
step: 2000, Loss: 0.11857910454273224
step: 2100, Loss: 0.11765439808368683
step: 2200, Loss: 0.11735370755195618
step: 2300, Loss: 0.11932986974716187
step: 2400, Loss: 0.11515404284000397
step: 2500, Loss: 0.11538313329219818
step: 2600, Loss: 0.1174713671207428
step: 2700, Loss: 0.11683908104896545
step: 2800, Loss: 0.11658166348934174
step: 2900, Loss: 0.11518479138612747
step: 3000, Loss: 0.11563849449157715
step: 3100, Loss: 0.11536672711372375
step: 3200, Loss: 0.11622604727745056
step: 3300, Loss: 0.11567313969135284
step: 3400, Loss: 0.20028792321681976
step: 3500, Loss: 0.11696682125329971
step: 3600, Loss: 0.11559247970581055
step: 3700, Loss: 0.11523541063070297
step: 3800, Loss: 0.11516517400741577
step: 3900, Loss: 0.11614503711462021
step: 4000, Loss: 0.11728952080011368
step: 4100, Loss: 0.11500285565853119
step: 4200, Loss: 0.11452147364616394
step: 4300, Loss: 0.1133732944726944
step: 4400, Loss: 0.11323603987693787
step: 4500, Loss: 0.11410149931907654
step: 4600, Loss: 0.11450577527284622
step: 4700, Loss: 0.1154560074210167
step: 4800, Loss: 0.11786912381649017
step: 4900, Loss: 0.1137053444981575
step: 5000, Loss: 0.11572667211294174
step: 5100, Loss: 0.11638683080673218
step: 5200, Loss: 0.11450210958719254
step: 5300, Loss: 0.194868266582489
step: 5400, Loss: 0.11543304473161697
step: 5500, Loss: 0.11498890072107315
step: 5600, Loss: 0.11615172028541565
step: 5700, Loss: 0.11467158794403076
step: 5800, Loss: 2.3700673580169678
step: 5900, Loss: 0.27089831233024597
step: 6000, Loss: 0.39169853925704956
step: 6100, Loss: 0.18793484568595886
step: 6200, Loss: 0.1309329867362976
step: 6300, Loss: 0.14083632826805115
step: 6400, Loss: 0.14135313034057617
step: 6500, Loss: 0.1321527659893036
step: 6600, Loss: 0.12330800294876099
step: 6700, Loss: 0.12636128067970276
step: 6800, Loss: 0.12532272934913635
step: 6900, Loss: 0.13297626376152039
step: 7000, Loss: 0.12467993050813675
step: 7100, Loss: 0.11722204089164734
step: 7200, Loss: 0.2016603797674179
step: 7300, Loss: 0.1205889880657196
step: 7400, Loss: 0.11875896155834198
step: 7500, Loss: 0.12105455249547958
step: 7600, Loss: 0.11887907236814499
step: 7700, Loss: 0.11984400451183319
step: 7800, Loss: 0.12121021747589111
step: 7900, Loss: 0.12076050043106079
step: 8000, Loss: 0.11657896637916565
step: 8100, Loss: 0.11817377060651779
step: 8200, Loss: 0.11783664673566818
step: 8300, Loss: 0.12074834853410721
step: 8400, Loss: 0.12083686888217926
step: 8500, Loss: 0.1154477447271347
step: 8600, Loss: 0.11788361519575119
step: 8700, Loss: 0.11692420393228531
step: 8800, Loss: 0.1176345944404602
step: 8900, Loss: 0.11547365039587021
step: 9000, Loss: 0.11597272753715515
step: 9100, Loss: 0.20114794373512268
step: 9200, Loss: 0.11732803285121918
step: 9300, Loss: 0.1147104799747467
step: 9400, Loss: 0.11520460247993469
step: 9500, Loss: 0.1146814376115799
step: 9600, Loss: 0.11683113873004913
step: 9700, Loss: 0.11522860080003738
step: 9800, Loss: 0.11523499339818954
step: 9900, Loss: 0.1157531887292862
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 1.0
recall: 0.9487179487179487
F_score: 0.9736842105263158
******fold 3******

Training... train_data length:684
step: 0, Loss: 0.1353195160627365
step: 100, Loss: 0.12200379371643066
step: 200, Loss: 0.1171727403998375
step: 300, Loss: 0.12319552898406982
step: 400, Loss: 0.11772020161151886
step: 500, Loss: 0.11559537798166275
step: 600, Loss: 0.11987603455781937
step: 700, Loss: 0.11886680126190186
step: 800, Loss: 0.11939734220504761
step: 900, Loss: 0.11452462524175644
step: 1000, Loss: 0.11674614250659943
step: 1100, Loss: 0.11708126217126846
step: 1200, Loss: 0.11615649610757828
step: 1300, Loss: 0.1147029921412468
step: 1400, Loss: 0.11426364630460739
step: 1500, Loss: 0.19580839574337006
step: 1600, Loss: 0.11650370061397552
step: 1700, Loss: 0.11606606096029282
step: 1800, Loss: 0.1139836311340332
step: 1900, Loss: 0.11437905579805374
step: 2000, Loss: 0.11590221524238586
step: 2100, Loss: 0.11542881280183792
step: 2200, Loss: 0.11553897708654404
step: 2300, Loss: 0.1150830015540123
step: 2400, Loss: 0.11493941396474838
step: 2500, Loss: 0.11442999541759491
step: 2600, Loss: 0.11446686089038849
step: 2700, Loss: 0.1149924024939537
step: 2800, Loss: 0.11440055072307587
step: 2900, Loss: 0.11593516170978546
step: 3000, Loss: 0.11720749735832214
step: 3100, Loss: 0.11733060330152512
step: 3200, Loss: 1.4298441410064697
step: 3300, Loss: 0.1964883953332901
step: 3400, Loss: 0.2338552325963974
step: 3500, Loss: 0.13047268986701965
step: 3600, Loss: 0.12290089577436447
step: 3700, Loss: 0.12553457915782928
step: 3800, Loss: 0.12087225914001465
step: 3900, Loss: 0.12364523112773895
step: 4000, Loss: 0.1201111301779747
step: 4100, Loss: 0.1241665631532669
step: 4200, Loss: 0.12340568751096725
step: 4300, Loss: 0.11783827096223831
step: 4400, Loss: 0.12025929987430573
step: 4500, Loss: 0.12498843669891357
step: 4600, Loss: 0.11960145086050034
step: 4700, Loss: 0.11480607092380524
step: 4800, Loss: 0.11852270364761353
step: 4900, Loss: 0.11772128939628601
step: 5000, Loss: 0.11741545796394348
step: 5100, Loss: 0.11531294137239456
step: 5200, Loss: 0.11563920974731445
step: 5300, Loss: 0.19559085369110107
step: 5400, Loss: 0.11840737611055374
step: 5500, Loss: 0.11565186083316803
step: 5600, Loss: 0.11761900037527084
step: 5700, Loss: 0.11379352957010269
step: 5800, Loss: 0.11766503751277924
step: 5900, Loss: 0.11523900926113129
step: 6000, Loss: 0.11875084042549133
step: 6100, Loss: 0.11695787310600281
step: 6200, Loss: 0.1155884712934494
step: 6300, Loss: 0.11616375297307968
step: 6400, Loss: 0.11640536040067673
step: 6500, Loss: 0.1172184944152832
step: 6600, Loss: 0.11520843207836151
step: 6700, Loss: 0.11478009819984436
step: 6800, Loss: 0.11570052802562714
step: 6900, Loss: 0.1136295348405838
step: 7000, Loss: 0.11630180478096008
step: 7100, Loss: 0.1147027388215065
step: 7200, Loss: 0.19424714148044586
step: 7300, Loss: 0.11525729298591614
step: 7400, Loss: 0.11375679075717926
step: 7500, Loss: 0.11419472098350525
step: 7600, Loss: 0.11367812007665634
step: 7700, Loss: 0.11412785202264786
step: 7800, Loss: 0.11376349627971649
step: 7900, Loss: 0.114991694688797
step: 8000, Loss: 0.11386671662330627
step: 8100, Loss: 0.11491139978170395
step: 8200, Loss: 0.11516842246055603
step: 8300, Loss: 0.11364221572875977
step: 8400, Loss: 0.11566884815692902
step: 8500, Loss: 0.11608409136533737
step: 8600, Loss: 0.1150546446442604
step: 8700, Loss: 0.11485178023576736
step: 8800, Loss: 0.11502958834171295
step: 8900, Loss: 0.11397527158260345
step: 9000, Loss: 0.114279605448246
step: 9100, Loss: 0.19659505784511566
step: 9200, Loss: 0.11476173996925354
step: 9300, Loss: 0.11508393287658691
step: 9400, Loss: 0.11450467258691788
step: 9500, Loss: 0.11393889039754868
step: 9600, Loss: 0.11393017321825027
step: 9700, Loss: 0.11356379091739655
step: 9800, Loss: 0.11397863924503326
step: 9900, Loss: 0.11376655101776123
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9743589743589743
recall: 1.0
F_score: 0.9870129870129869
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.12663255631923676
step: 100, Loss: 0.1382022500038147
step: 200, Loss: 0.1172390803694725
step: 300, Loss: 0.1169101819396019
step: 400, Loss: 0.115057572722435
step: 500, Loss: 0.11614348739385605
step: 600, Loss: 0.11859878897666931
step: 700, Loss: 0.11539196223020554
step: 800, Loss: 0.11791291832923889
step: 900, Loss: 0.11531302332878113
step: 1000, Loss: 0.11571277678012848
step: 1100, Loss: 0.11430850625038147
step: 1200, Loss: 0.1145678460597992
step: 1300, Loss: 0.11606759577989578
step: 1400, Loss: 0.11469101160764694
step: 1500, Loss: 0.19359759986400604
step: 1600, Loss: 0.11532661318778992
step: 1700, Loss: 0.11532066017389297
step: 1800, Loss: 0.1137758195400238
step: 1900, Loss: 0.11417961120605469
step: 2000, Loss: 0.11306620389223099
step: 2100, Loss: 0.11315332353115082
step: 2200, Loss: 0.11410874873399734
step: 2300, Loss: 0.11365556716918945
step: 2400, Loss: 0.11500191688537598
step: 2500, Loss: 0.11416357010602951
step: 2600, Loss: 0.11517538130283356
step: 2700, Loss: 0.11743450164794922
step: 2800, Loss: 0.11336119472980499
step: 2900, Loss: 0.1143728643655777
step: 3000, Loss: 0.11433771252632141
step: 3100, Loss: 0.11433601379394531
step: 3200, Loss: 0.11349654197692871
step: 3300, Loss: 0.113755002617836
step: 3400, Loss: 0.19264860451221466
step: 3500, Loss: 0.11425960063934326
step: 3600, Loss: 0.11412458121776581
step: 3700, Loss: 0.11357970535755157
step: 3800, Loss: 0.11474966257810593
step: 3900, Loss: 0.11304792016744614
step: 4000, Loss: 0.11341607570648193
step: 4100, Loss: 0.11399082839488983
step: 4200, Loss: 0.11474382877349854
step: 4300, Loss: 0.11648398637771606
step: 4400, Loss: 2.4255268573760986
step: 4500, Loss: 1.4939918518066406
step: 4600, Loss: 0.8141324520111084
step: 4700, Loss: 0.12905330955982208
step: 4800, Loss: 0.12722818553447723
step: 4900, Loss: 0.12324244529008865
step: 5000, Loss: 0.1247839629650116
step: 5100, Loss: 0.1175854429602623
step: 5200, Loss: 0.11572498083114624
step: 5300, Loss: 0.19828510284423828
step: 5400, Loss: 0.1164519190788269
step: 5500, Loss: 0.12299227714538574
step: 5600, Loss: 0.11804549396038055
step: 5700, Loss: 0.1193687841296196
step: 5800, Loss: 0.11808939278125763
step: 5900, Loss: 0.11908185482025146
step: 6000, Loss: 0.11671293526887894
step: 6100, Loss: 0.11567184329032898
step: 6200, Loss: 0.11640272289514542
step: 6300, Loss: 0.11968514323234558
step: 6400, Loss: 0.1161855012178421
step: 6500, Loss: 0.12056595087051392
step: 6600, Loss: 0.11401525884866714
step: 6700, Loss: 0.11576806753873825
step: 6800, Loss: 0.11610252410173416
step: 6900, Loss: 0.1178496927022934
step: 7000, Loss: 0.11886830627918243
step: 7100, Loss: 0.11371398717164993
step: 7200, Loss: 0.19523462653160095
step: 7300, Loss: 0.1148180142045021
step: 7400, Loss: 0.11560345441102982
step: 7500, Loss: 0.11576595157384872
step: 7600, Loss: 0.1148868203163147
step: 7700, Loss: 0.1154848039150238
step: 7800, Loss: 0.11437544971704483
step: 7900, Loss: 0.11510872840881348
step: 8000, Loss: 0.1141633540391922
step: 8100, Loss: 0.11404696851968765
step: 8200, Loss: 0.1158028319478035
step: 8300, Loss: 0.11688818782567978
step: 8400, Loss: 0.1151176393032074
step: 8500, Loss: 0.11513165384531021
step: 8600, Loss: 0.11356313526630402
step: 8700, Loss: 0.11334800720214844
step: 8800, Loss: 0.1150292232632637
step: 8900, Loss: 0.11338288336992264
step: 9000, Loss: 0.11436596512794495
step: 9100, Loss: 0.1922622174024582
step: 9200, Loss: 0.11446395516395569
step: 9300, Loss: 0.11407022178173065
step: 9400, Loss: 0.11393970996141434
step: 9500, Loss: 0.11491705477237701
step: 9600, Loss: 0.1160852238535881
step: 9700, Loss: 0.11272954940795898
step: 9800, Loss: 0.11565446853637695
step: 9900, Loss: 0.11404278874397278
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.1202419251203537
step: 100, Loss: 0.17701297998428345
step: 200, Loss: 0.11758272349834442
step: 300, Loss: 0.11634346097707748
step: 400, Loss: 0.11828210949897766
step: 500, Loss: 0.11527958512306213
step: 600, Loss: 0.11863252520561218
step: 700, Loss: 0.11447786539793015
step: 800, Loss: 0.11697835475206375
step: 900, Loss: 0.11480577290058136
step: 1000, Loss: 0.11401169747114182
step: 1100, Loss: 0.11581385135650635
step: 1200, Loss: 0.11528155207633972
step: 1300, Loss: 0.11806463450193405
step: 1400, Loss: 0.11397189646959305
step: 1500, Loss: 0.19626569747924805
step: 1600, Loss: 0.11368361115455627
step: 1700, Loss: 0.11482305824756622
step: 1800, Loss: 0.11487049609422684
step: 1900, Loss: 0.11467857658863068
step: 2000, Loss: 0.11389149725437164
step: 2100, Loss: 0.11435985565185547
step: 2200, Loss: 0.11348086595535278
step: 2300, Loss: 0.1134575828909874
step: 2400, Loss: 0.11319582164287567
step: 2500, Loss: 0.11484373360872269
step: 2600, Loss: 0.11398454010486603
step: 2700, Loss: 0.11381089687347412
step: 2800, Loss: 0.11417300254106522
step: 2900, Loss: 0.11425679922103882
step: 3000, Loss: 0.11550721526145935
step: 3100, Loss: 0.11414378881454468
step: 3200, Loss: 0.11461381614208221
step: 3300, Loss: 0.11423268914222717
step: 3400, Loss: 0.19205884635448456
step: 3500, Loss: 0.11381131410598755
step: 3600, Loss: 0.11354869604110718
step: 3700, Loss: 0.11483804881572723
step: 3800, Loss: 0.11498475074768066
step: 3900, Loss: 0.11400134116411209
step: 4000, Loss: 0.11344575136899948
step: 4100, Loss: 0.11292694509029388
step: 4200, Loss: 0.11409235000610352
step: 4300, Loss: 0.11388738453388214
step: 4400, Loss: 0.11379460990428925
step: 4500, Loss: 0.1155962347984314
step: 4600, Loss: 0.11369279026985168
step: 4700, Loss: 0.11370645463466644
step: 4800, Loss: 0.11519773304462433
step: 4900, Loss: 0.1154622882604599
step: 5000, Loss: 0.1140463575720787
step: 5100, Loss: 0.11350677907466888
step: 5200, Loss: 0.11322705447673798
step: 5300, Loss: 0.1919133961200714
step: 5400, Loss: 0.11368729174137115
step: 5500, Loss: 0.11553438007831573
step: 5600, Loss: 4.206507682800293
step: 5700, Loss: 1.0171937942504883
step: 5800, Loss: 0.1719241738319397
step: 5900, Loss: 2.719170570373535
step: 6000, Loss: 0.12773631513118744
step: 6100, Loss: 0.12376546859741211
step: 6200, Loss: 0.12001258134841919
step: 6300, Loss: 0.12076853215694427
step: 6400, Loss: 0.12069027125835419
step: 6500, Loss: 0.12561486661434174
step: 6600, Loss: 0.11595454066991806
step: 6700, Loss: 0.1217581257224083
step: 6800, Loss: 0.11949136108160019
step: 6900, Loss: 0.11776726692914963
step: 7000, Loss: 0.11772046983242035
step: 7100, Loss: 0.11707121133804321
step: 7200, Loss: 0.20189526677131653
step: 7300, Loss: 0.11676748096942902
step: 7400, Loss: 0.11517829447984695
step: 7500, Loss: 0.11783821880817413
step: 7600, Loss: 0.11509321630001068
step: 7700, Loss: 0.1185462549328804
step: 7800, Loss: 0.11633850634098053
step: 7900, Loss: 0.11753789335489273
step: 8000, Loss: 0.11353956162929535
step: 8100, Loss: 0.11526225507259369
step: 8200, Loss: 0.11533845216035843
step: 8300, Loss: 0.11615227907896042
step: 8400, Loss: 0.11839067935943604
step: 8500, Loss: 0.11376304179430008
step: 8600, Loss: 0.11584620177745819
step: 8700, Loss: 0.11587455123662949
step: 8800, Loss: 0.11685972660779953
step: 8900, Loss: 0.11640554666519165
step: 9000, Loss: 0.1139477789402008
step: 9100, Loss: 0.19490636885166168
step: 9200, Loss: 0.11384069919586182
step: 9300, Loss: 0.11401337385177612
step: 9400, Loss: 0.1155962198972702
step: 9500, Loss: 0.1137777641415596
step: 9600, Loss: 0.1150214895606041
step: 9700, Loss: 0.11295013874769211
step: 9800, Loss: 0.11332809180021286
step: 9900, Loss: 0.11395858973264694
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.11578292399644852
step: 100, Loss: 0.1157301515340805
step: 200, Loss: 0.11540467292070389
step: 300, Loss: 0.11414441466331482
step: 400, Loss: 0.11514408886432648
step: 500, Loss: 0.11588012427091599
step: 600, Loss: 0.11342844367027283
step: 700, Loss: 0.11396539211273193
step: 800, Loss: 0.11588685214519501
step: 900, Loss: 0.11426776647567749
step: 1000, Loss: 0.11495310068130493
step: 1100, Loss: 0.11433668434619904
step: 1200, Loss: 0.11403853446245193
step: 1300, Loss: 0.11402629315853119
step: 1400, Loss: 0.11393818259239197
step: 1500, Loss: 0.19500645995140076
step: 1600, Loss: 0.11375642567873001
step: 1700, Loss: 0.11411821842193604
step: 1800, Loss: 0.11415696889162064
step: 1900, Loss: 0.11497409641742706
step: 2000, Loss: 0.11422757804393768
step: 2100, Loss: 0.11283545941114426
step: 2200, Loss: 0.11494678258895874
step: 2300, Loss: 0.11371029168367386
step: 2400, Loss: 0.11510035395622253
step: 2500, Loss: 0.11512944847345352
step: 2600, Loss: 0.11320403963327408
step: 2700, Loss: 0.11433093249797821
step: 2800, Loss: 2.833320140838623
step: 2900, Loss: 2.316439390182495
step: 3000, Loss: 0.14341828227043152
step: 3100, Loss: 0.13193246722221375
step: 3200, Loss: 0.12284619361162186
step: 3300, Loss: 0.12095054984092712
step: 3400, Loss: 0.20097513496875763
step: 3500, Loss: 0.11835092306137085
step: 3600, Loss: 0.11877649277448654
step: 3700, Loss: 0.11737452447414398
step: 3800, Loss: 0.11708562076091766
step: 3900, Loss: 0.11858446151018143
step: 4000, Loss: 0.11626455187797546
step: 4100, Loss: 0.11681199818849564
step: 4200, Loss: 0.11687298864126205
step: 4300, Loss: 0.11638299375772476
step: 4400, Loss: 0.11716149747371674
step: 4500, Loss: 0.11744656413793564
step: 4600, Loss: 0.1205713078379631
step: 4700, Loss: 0.11612964421510696
step: 4800, Loss: 0.11856772750616074
step: 4900, Loss: 0.11517202854156494
step: 5000, Loss: 0.11609034240245819
step: 5100, Loss: 0.11604678630828857
step: 5200, Loss: 0.11559000611305237
step: 5300, Loss: 0.19471703469753265
step: 5400, Loss: 0.11413655430078506
step: 5500, Loss: 0.1141047328710556
step: 5600, Loss: 0.11546585708856583
step: 5700, Loss: 0.11644165217876434
step: 5800, Loss: 0.11510968953371048
step: 5900, Loss: 0.11396797001361847
step: 6000, Loss: 0.11415036767721176
step: 6100, Loss: 0.11465025693178177
step: 6200, Loss: 0.11457100510597229
step: 6300, Loss: 0.11431436240673065
step: 6400, Loss: 0.11420756578445435
step: 6500, Loss: 0.11513153463602066
step: 6600, Loss: 0.11474797874689102
step: 6700, Loss: 0.1155240535736084
step: 6800, Loss: 0.1155480220913887
step: 6900, Loss: 0.11350934207439423
step: 7000, Loss: 0.11413095891475677
step: 7100, Loss: 0.11424407362937927
step: 7200, Loss: 0.1961100548505783
step: 7300, Loss: 0.11480170488357544
step: 7400, Loss: 0.11352553963661194
step: 7500, Loss: 0.11429736018180847
step: 7600, Loss: 0.1140085831284523
step: 7700, Loss: 0.11503517627716064
step: 7800, Loss: 0.11409048736095428
step: 7900, Loss: 0.11510369181632996
step: 8000, Loss: 0.11421003192663193
step: 8100, Loss: 0.11388474702835083
step: 8200, Loss: 0.11544838547706604
step: 8300, Loss: 0.1141139343380928
step: 8400, Loss: 0.11493661254644394
step: 8500, Loss: 0.11325548589229584
step: 8600, Loss: 0.11504082381725311
step: 8700, Loss: 0.11403845250606537
step: 8800, Loss: 0.11470376700162888
step: 8900, Loss: 0.11393947899341583
step: 9000, Loss: 0.11426743119955063
step: 9100, Loss: 0.19434992969036102
step: 9200, Loss: 0.11408776044845581
step: 9300, Loss: 1.9659879207611084
step: 9400, Loss: 0.6577428579330444
step: 9500, Loss: 0.12687945365905762
step: 9600, Loss: 0.1211586520075798
step: 9700, Loss: 0.12313062697649002
step: 9800, Loss: 0.12055416405200958
step: 9900, Loss: 0.12026140093803406
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 1.0
recall: 0.9459459459459459
F_score: 0.9722222222222222
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11469915509223938
step: 100, Loss: 0.11991102993488312
step: 200, Loss: 0.11506388336420059
step: 300, Loss: 0.11502338945865631
step: 400, Loss: 0.11590316146612167
step: 500, Loss: 0.11485268920660019
step: 600, Loss: 0.11492079496383667
step: 700, Loss: 0.11605220288038254
step: 800, Loss: 0.11461471021175385
step: 900, Loss: 0.11389195173978806
step: 1000, Loss: 0.11344577372074127
step: 1100, Loss: 0.11504266411066055
step: 1200, Loss: 0.11397290974855423
step: 1300, Loss: 0.11544008553028107
step: 1400, Loss: 0.1134866327047348
step: 1500, Loss: 0.1936199963092804
step: 1600, Loss: 0.11386626213788986
step: 1700, Loss: 0.1141432374715805
step: 1800, Loss: 0.11448777467012405
step: 1900, Loss: 0.11472420394420624
step: 2000, Loss: 0.11415820568799973
step: 2100, Loss: 0.11528225243091583
step: 2200, Loss: 0.11416012793779373
step: 2300, Loss: 0.1138882040977478
step: 2400, Loss: 0.11430955678224564
step: 2500, Loss: 0.11421050876379013
step: 2600, Loss: 0.1189054548740387
step: 2700, Loss: 0.11375341564416885
step: 2800, Loss: 0.11303497105836868
step: 2900, Loss: 0.11447588354349136
step: 3000, Loss: 0.11400524526834488
step: 3100, Loss: 0.11266370117664337
step: 3200, Loss: 0.11460001021623611
step: 3300, Loss: 3.6878349781036377
step: 3400, Loss: 0.24837423861026764
step: 3500, Loss: 1.167807698249817
step: 3600, Loss: 0.1342839151620865
step: 3700, Loss: 0.12260349094867706
step: 3800, Loss: 0.12068279832601547
step: 3900, Loss: 0.12112902849912643
step: 4000, Loss: 0.12460541725158691
step: 4100, Loss: 0.11851783841848373
step: 4200, Loss: 0.12043455988168716
step: 4300, Loss: 0.11862649023532867
step: 4400, Loss: 0.1192135363817215
step: 4500, Loss: 0.1190846711397171
step: 4600, Loss: 0.11857409030199051
step: 4700, Loss: 0.11839534342288971
step: 4800, Loss: 0.11901301145553589
step: 4900, Loss: 0.11667463928461075
step: 5000, Loss: 0.11785945296287537
step: 5100, Loss: 0.11750279366970062
step: 5200, Loss: 0.11445534229278564
step: 5300, Loss: 0.19835440814495087
step: 5400, Loss: 0.11777042597532272
step: 5500, Loss: 0.11493091285228729
step: 5600, Loss: 0.11549399793148041
step: 5700, Loss: 0.1162770614027977
step: 5800, Loss: 0.11528186500072479
step: 5900, Loss: 0.11646214127540588
step: 6000, Loss: 0.11617878079414368
step: 6100, Loss: 0.11384962499141693
step: 6200, Loss: 0.11688559502363205
step: 6300, Loss: 0.11599994450807571
step: 6400, Loss: 0.1171218752861023
step: 6500, Loss: 0.11498802155256271
step: 6600, Loss: 0.11462008208036423
step: 6700, Loss: 0.11482968926429749
step: 6800, Loss: 0.11357246339321136
step: 6900, Loss: 0.11512666195631027
step: 7000, Loss: 0.11531248688697815
step: 7100, Loss: 0.1134578213095665
step: 7200, Loss: 0.19495654106140137
step: 7300, Loss: 0.11480751633644104
step: 7400, Loss: 0.11332525312900543
step: 7500, Loss: 0.11377619951963425
step: 7600, Loss: 0.11372287571430206
step: 7700, Loss: 0.11317820847034454
step: 7800, Loss: 0.113326296210289
step: 7900, Loss: 0.11323835700750351
step: 8000, Loss: 0.11366389691829681
step: 8100, Loss: 0.11456350237131119
step: 8200, Loss: 0.1144917756319046
step: 8300, Loss: 0.11441750824451447
step: 8400, Loss: 0.11508921533823013
step: 8500, Loss: 0.11346743255853653
step: 8600, Loss: 0.11491389572620392
step: 8700, Loss: 0.11361788213253021
step: 8800, Loss: 0.1132674515247345
step: 8900, Loss: 0.11411530524492264
step: 9000, Loss: 0.11448894441127777
step: 9100, Loss: 0.1930866539478302
step: 9200, Loss: 0.11393212527036667
step: 9300, Loss: 0.11325301975011826
step: 9400, Loss: 0.11322850733995438
step: 9500, Loss: 0.11386175453662872
step: 9600, Loss: 0.11369092762470245
step: 9700, Loss: 0.11515199393033981
step: 9800, Loss: 0.11396912485361099
step: 9900, Loss: 0.1139066219329834
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9722222222222222
recall: 1.0
F_score: 0.9859154929577464
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11615654826164246
step: 100, Loss: 0.11699570715427399
step: 200, Loss: 0.11667753756046295
step: 300, Loss: 0.11327268183231354
step: 400, Loss: 0.11551205813884735
step: 500, Loss: 0.11414875090122223
step: 600, Loss: 0.11422505974769592
step: 700, Loss: 0.11500342935323715
step: 800, Loss: 0.114583320915699
step: 900, Loss: 0.1137273982167244
step: 1000, Loss: 0.1143559142947197
step: 1100, Loss: 0.11320962756872177
step: 1200, Loss: 0.11409920454025269
step: 1300, Loss: 0.11412718892097473
step: 1400, Loss: 0.11451506614685059
step: 1500, Loss: 0.19303996860980988
step: 1600, Loss: 0.11399555951356888
step: 1700, Loss: 0.11365130543708801
step: 1800, Loss: 0.11344954371452332
step: 1900, Loss: 0.11365905404090881
step: 2000, Loss: 0.11319147795438766
step: 2100, Loss: 2.348220109939575
step: 2200, Loss: 0.1929156482219696
step: 2300, Loss: 2.5269064903259277
step: 2400, Loss: 0.14821580052375793
step: 2500, Loss: 0.1516166776418686
step: 2600, Loss: 0.12851832807064056
step: 2700, Loss: 0.12684717774391174
step: 2800, Loss: 0.11958397924900055
step: 2900, Loss: 0.1254601925611496
step: 3000, Loss: 0.11649207770824432
step: 3100, Loss: 0.11891084164381027
step: 3200, Loss: 0.11686816811561584
step: 3300, Loss: 0.11543741077184677
step: 3400, Loss: 0.20134252309799194
step: 3500, Loss: 0.11563419550657272
step: 3600, Loss: 0.11742141842842102
step: 3700, Loss: 0.11750654876232147
step: 3800, Loss: 0.11715441942214966
step: 3900, Loss: 0.1172117292881012
step: 4000, Loss: 0.11776769906282425
step: 4100, Loss: 0.11814261227846146
step: 4200, Loss: 0.11818414181470871
step: 4300, Loss: 0.11758490651845932
step: 4400, Loss: 0.11762959510087967
step: 4500, Loss: 0.11395253986120224
step: 4600, Loss: 0.11787261068820953
step: 4700, Loss: 0.11509807407855988
step: 4800, Loss: 0.11736501753330231
step: 4900, Loss: 0.11628775298595428
step: 5000, Loss: 0.1161019504070282
step: 5100, Loss: 0.11528678238391876
step: 5200, Loss: 0.1144334152340889
step: 5300, Loss: 0.19783970713615417
step: 5400, Loss: 0.11488254368305206
step: 5500, Loss: 0.11427594721317291
step: 5600, Loss: 0.11461202800273895
step: 5700, Loss: 0.11476248502731323
step: 5800, Loss: 0.11536022275686264
step: 5900, Loss: 0.1143970862030983
step: 6000, Loss: 0.11493349075317383
step: 6100, Loss: 0.11457569152116776
step: 6200, Loss: 0.11382082849740982
step: 6300, Loss: 0.11591333150863647
step: 6400, Loss: 0.11449608206748962
step: 6500, Loss: 0.11551471054553986
step: 6600, Loss: 0.11343058943748474
step: 6700, Loss: 0.11515060067176819
step: 6800, Loss: 0.11403734236955643
step: 6900, Loss: 0.11351900547742844
step: 7000, Loss: 0.11379119008779526
step: 7100, Loss: 0.11387522518634796
step: 7200, Loss: 0.19697846472263336
step: 7300, Loss: 0.1134723350405693
step: 7400, Loss: 0.11414961516857147
step: 7500, Loss: 0.1148577481508255
step: 7600, Loss: 0.11551745980978012
step: 7700, Loss: 0.11328750848770142
step: 7800, Loss: 0.11372889578342438
step: 7900, Loss: 0.11337315291166306
step: 8000, Loss: 0.11388424038887024
step: 8100, Loss: 0.11478392779827118
step: 8200, Loss: 0.11529885977506638
step: 8300, Loss: 0.11433503776788712
step: 8400, Loss: 0.11321411281824112
step: 8500, Loss: 0.11431707441806793
step: 8600, Loss: 0.11274794489145279
step: 8700, Loss: 0.11309228837490082
step: 8800, Loss: 0.11514829099178314
step: 8900, Loss: 5.649452209472656
step: 9000, Loss: 1.013698697090149
step: 9100, Loss: 0.2360411137342453
step: 9200, Loss: 0.1343260109424591
step: 9300, Loss: 0.12738412618637085
step: 9400, Loss: 0.12047334015369415
step: 9500, Loss: 0.12049795687198639
step: 9600, Loss: 0.11646407842636108
step: 9700, Loss: 0.11845244467258453
step: 9800, Loss: 0.12216520309448242
step: 9900, Loss: 0.11736408621072769
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9666666666666667
recall: 1.0
F_score: 0.983050847457627
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.1166793704032898
step: 100, Loss: 0.11972963809967041
step: 200, Loss: 0.1155809760093689
step: 300, Loss: 0.11605808138847351
step: 400, Loss: 0.1145627349615097
step: 500, Loss: 0.1139959767460823
step: 600, Loss: 0.11479988694190979
step: 700, Loss: 0.11587975174188614
step: 800, Loss: 0.11679044365882874
step: 900, Loss: 0.11316517740488052
step: 1000, Loss: 0.11479833722114563
step: 1100, Loss: 0.11494241654872894
step: 1200, Loss: 0.11524935811758041
step: 1300, Loss: 0.11678314208984375
step: 1400, Loss: 0.11523733288049698
step: 1500, Loss: 0.19602076709270477
step: 1600, Loss: 0.1130494624376297
step: 1700, Loss: 0.11537531018257141
step: 1800, Loss: 0.11472143232822418
step: 1900, Loss: 0.11309390515089035
step: 2000, Loss: 0.11404559761285782
step: 2100, Loss: 0.11616257578134537
step: 2200, Loss: 0.11396517604589462
step: 2300, Loss: 0.11330007016658783
step: 2400, Loss: 0.1135360449552536
step: 2500, Loss: 0.11373376846313477
step: 2600, Loss: 0.11433961987495422
step: 2700, Loss: 0.1139131411910057
step: 2800, Loss: 0.1136416494846344
step: 2900, Loss: 0.11443795263767242
step: 3000, Loss: 0.11378113180398941
step: 3100, Loss: 8.081445693969727
step: 3200, Loss: 0.3573295474052429
step: 3300, Loss: 0.21864444017410278
step: 3400, Loss: 0.21296292543411255
step: 3500, Loss: 0.12171836197376251
step: 3600, Loss: 0.12165755033493042
step: 3700, Loss: 0.11682961881160736
step: 3800, Loss: 0.12042827904224396
step: 3900, Loss: 0.1212080717086792
step: 4000, Loss: 0.11775735020637512
step: 4100, Loss: 0.11558521538972855
step: 4200, Loss: 0.11672496050596237
step: 4300, Loss: 0.11722668260335922
step: 4400, Loss: 0.11702217906713486
step: 4500, Loss: 0.11819777637720108
step: 4600, Loss: 0.11858236789703369
step: 4700, Loss: 0.11726337671279907
step: 4800, Loss: 0.11830542236566544
step: 4900, Loss: 0.115205317735672
step: 5000, Loss: 0.1154332384467125
step: 5100, Loss: 0.11579151451587677
step: 5200, Loss: 0.114281564950943
step: 5300, Loss: 0.19942380487918854
step: 5400, Loss: 0.11398408561944962
step: 5500, Loss: 0.11395598948001862
step: 5600, Loss: 0.11553706973791122
step: 5700, Loss: 0.11554928123950958
step: 5800, Loss: 0.1139557957649231
step: 5900, Loss: 0.11376194655895233
step: 6000, Loss: 0.11397912353277206
step: 6100, Loss: 0.11352331936359406
step: 6200, Loss: 0.11300021409988403
step: 6300, Loss: 0.11330018937587738
step: 6400, Loss: 0.11500339210033417
step: 6500, Loss: 0.1172306090593338
step: 6600, Loss: 0.1147335097193718
step: 6700, Loss: 0.11675363034009933
step: 6800, Loss: 0.11365605890750885
step: 6900, Loss: 0.11352309584617615
step: 7000, Loss: 0.11564696580171585
step: 7100, Loss: 0.11455985903739929
step: 7200, Loss: 0.1955433487892151
step: 7300, Loss: 0.11556009203195572
step: 7400, Loss: 0.11388009041547775
step: 7500, Loss: 0.11405788362026215
step: 7600, Loss: 0.11447189748287201
step: 7700, Loss: 0.11359629034996033
step: 7800, Loss: 0.11365143954753876
step: 7900, Loss: 0.1136295422911644
step: 8000, Loss: 0.11472231149673462
step: 8100, Loss: 0.1135500892996788
step: 8200, Loss: 0.11549843102693558
step: 8300, Loss: 0.11373716592788696
step: 8400, Loss: 0.11364768445491791
step: 8500, Loss: 0.11335994303226471
step: 8600, Loss: 0.11374346911907196
step: 8700, Loss: 0.11378122866153717
step: 8800, Loss: 0.11488131433725357
step: 8900, Loss: 0.11431211233139038
step: 9000, Loss: 0.11329929530620575
step: 9100, Loss: 0.19334140419960022
step: 9200, Loss: 0.11545641720294952
step: 9300, Loss: 0.11355013400316238
step: 9400, Loss: 0.11476340889930725
step: 9500, Loss: 0.11338157951831818
step: 9600, Loss: 0.11544394493103027
step: 9700, Loss: 0.11326026171445847
step: 9800, Loss: 0.11376572400331497
step: 9900, Loss: 0.11431913822889328
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.975609756097561
F_score: 0.9876543209876543
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11759326606988907
step: 100, Loss: 0.17952436208724976
step: 200, Loss: 0.11904862523078918
step: 300, Loss: 0.11735048145055771
step: 400, Loss: 0.116887666285038
step: 500, Loss: 0.11397212743759155
step: 600, Loss: 0.11447499692440033
step: 700, Loss: 0.11458031833171844
step: 800, Loss: 0.11724215745925903
step: 900, Loss: 0.11469116806983948
step: 1000, Loss: 0.11443063616752625
step: 1100, Loss: 0.11575828492641449
step: 1200, Loss: 0.1140189841389656
step: 1300, Loss: 0.11365719139575958
step: 1400, Loss: 0.11329834908246994
step: 1500, Loss: 0.1954215168952942
step: 1600, Loss: 0.11565349251031876
step: 1700, Loss: 0.11301440745592117
step: 1800, Loss: 0.11322197318077087
step: 1900, Loss: 0.1142909973859787
step: 2000, Loss: 0.11479900777339935
step: 2100, Loss: 0.11517947912216187
step: 2200, Loss: 0.11526645720005035
step: 2300, Loss: 0.11420830339193344
step: 2400, Loss: 0.11569378525018692
step: 2500, Loss: 0.11389763653278351
step: 2600, Loss: 0.11339564621448517
step: 2700, Loss: 0.11474110186100006
step: 2800, Loss: 0.113736592233181
step: 2900, Loss: 0.11283637583255768
step: 3000, Loss: 0.11510176211595535
step: 3100, Loss: 0.11371671408414841
step: 3200, Loss: 0.11525862663984299
step: 3300, Loss: 0.11387071758508682
step: 3400, Loss: 0.19453129172325134
step: 3500, Loss: 0.11376732587814331
step: 3600, Loss: 0.11349047720432281
step: 3700, Loss: 0.11601535975933075
step: 3800, Loss: 0.11337623000144958
step: 3900, Loss: 0.11373770236968994
step: 4000, Loss: 0.11364267021417618
step: 4100, Loss: 0.11404821276664734
step: 4200, Loss: 0.11377045512199402
step: 4300, Loss: 0.11498022079467773
step: 4400, Loss: 0.11391986906528473
step: 4500, Loss: 0.11422786116600037
step: 4600, Loss: 0.11460645496845245
step: 4700, Loss: 0.11409036815166473
step: 4800, Loss: 0.1132889837026596
step: 4900, Loss: 0.11369405686855316
step: 5000, Loss: 0.11386650800704956
step: 5100, Loss: 0.11555483937263489
step: 5200, Loss: 0.11314598470926285
step: 5300, Loss: 0.19794639945030212
step: 5400, Loss: 0.11547517776489258
step: 5500, Loss: 1.221904993057251
step: 5600, Loss: 0.17045238614082336
step: 5700, Loss: 0.12456147372722626
step: 5800, Loss: 0.1269279420375824
step: 5900, Loss: 0.11952473223209381
step: 6000, Loss: 0.12432435154914856
step: 6100, Loss: 0.1210193857550621
step: 6200, Loss: 0.1200886070728302
step: 6300, Loss: 0.11907696723937988
step: 6400, Loss: 0.12227201461791992
step: 6500, Loss: 0.11616693437099457
step: 6600, Loss: 0.11732348799705505
step: 6700, Loss: 0.11788074672222137
step: 6800, Loss: 0.11553902179002762
step: 6900, Loss: 0.11891588568687439
step: 7000, Loss: 0.11872786283493042
step: 7100, Loss: 0.11484133452177048
step: 7200, Loss: 0.1971818208694458
step: 7300, Loss: 0.11575443297624588
step: 7400, Loss: 0.11416004598140717
step: 7500, Loss: 0.11565540730953217
step: 7600, Loss: 0.11368406563997269
step: 7700, Loss: 0.11636468768119812
step: 7800, Loss: 0.11526114493608475
step: 7900, Loss: 0.1175864040851593
step: 8000, Loss: 0.11565715819597244
step: 8100, Loss: 0.11618287861347198
step: 8200, Loss: 0.1152813583612442
step: 8300, Loss: 0.11521875858306885
step: 8400, Loss: 0.11359885334968567
step: 8500, Loss: 0.11449433863162994
step: 8600, Loss: 0.11429891735315323
step: 8700, Loss: 0.11480191349983215
step: 8800, Loss: 0.11417362838983536
step: 8900, Loss: 0.11523975431919098
step: 9000, Loss: 0.11654317378997803
step: 9100, Loss: 0.19308902323246002
step: 9200, Loss: 0.1133875846862793
step: 9300, Loss: 0.11459841579198837
step: 9400, Loss: 0.1135682463645935
step: 9500, Loss: 0.11333505064249039
step: 9600, Loss: 0.11450959742069244
step: 9700, Loss: 0.11501849442720413
step: 9800, Loss: 0.11698572337627411
step: 9900, Loss: 0.11415687203407288
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.972972972972973
recall: 1.0
F_score: 0.9863013698630138
subject 27 Avgacc: 0.9736111111111111 Avgfscore: 0.9724326299512416 
 Max acc:1.0, Max f score:1.0
******** mix subject_28 ********

[323, 437]
******fold 1******

Training... train_data length:786
step: 0, Loss: 33.8990364074707
step: 100, Loss: 8.299057006835938
step: 200, Loss: 1.8543812036514282
step: 300, Loss: 1.3499053716659546
step: 400, Loss: 1.121983528137207
step: 500, Loss: 0.3424149453639984
step: 600, Loss: 2.0868031978607178
step: 700, Loss: 0.19442452490329742
step: 800, Loss: 0.21198730170726776
step: 900, Loss: 0.1517307013273239
step: 1000, Loss: 0.16993391513824463
step: 1100, Loss: 0.1569444239139557
step: 1200, Loss: 0.14500202238559723
step: 1300, Loss: 0.13989298045635223
step: 1400, Loss: 0.1624661535024643
step: 1500, Loss: 0.14467747509479523
step: 1600, Loss: 0.15387801826000214
step: 1700, Loss: 0.15278373658657074
step: 1800, Loss: 0.15786148607730865
step: 1900, Loss: 0.13594889640808105
step: 2000, Loss: 0.13280801475048065
step: 2100, Loss: 0.13914746046066284
step: 2200, Loss: 0.14011767506599426
step: 2300, Loss: 0.13940051198005676
step: 2400, Loss: 0.1298469752073288
step: 2500, Loss: 0.13460059463977814
step: 2600, Loss: 0.1277603805065155
step: 2700, Loss: 0.13667501509189606
step: 2800, Loss: 0.13238675892353058
step: 2900, Loss: 0.13861271739006042
step: 3000, Loss: 0.1229279488325119
step: 3100, Loss: 0.12700685858726501
step: 3200, Loss: 0.1317192167043686
step: 3300, Loss: 0.12218108773231506
step: 3400, Loss: 0.1283503770828247
step: 3500, Loss: 0.11991992592811584
step: 3600, Loss: 0.11958205699920654
step: 3700, Loss: 0.12167467176914215
step: 3800, Loss: 0.12989675998687744
step: 3900, Loss: 0.1253160685300827
step: 4000, Loss: 0.12722483277320862
step: 4100, Loss: 0.11783263087272644
step: 4200, Loss: 0.11962583661079407
step: 4300, Loss: 0.12327326834201813
step: 4400, Loss: 0.12056200206279755
step: 4500, Loss: 0.1291261464357376
step: 4600, Loss: 0.12021704018115997
step: 4700, Loss: 0.12062069028615952
step: 4800, Loss: 0.12036627531051636
step: 4900, Loss: 0.12259306013584137
step: 5000, Loss: 0.11881870031356812
step: 5100, Loss: 0.12239294499158859
step: 5200, Loss: 0.11733570694923401
step: 5300, Loss: 0.11794774234294891
step: 5400, Loss: 0.12282422184944153
step: 5500, Loss: 0.11777593195438385
step: 5600, Loss: 0.12039293348789215
step: 5700, Loss: 0.11935637146234512
step: 5800, Loss: 4.086837291717529
step: 5900, Loss: 0.9098004698753357
step: 6000, Loss: 0.22943031787872314
step: 6100, Loss: 0.45286238193511963
step: 6200, Loss: 0.17386819422245026
step: 6300, Loss: 0.16494357585906982
step: 6400, Loss: 0.1492888182401657
step: 6500, Loss: 0.1421007513999939
step: 6600, Loss: 0.14433342218399048
step: 6700, Loss: 0.1370825320482254
step: 6800, Loss: 0.12712158262729645
step: 6900, Loss: 0.13211175799369812
step: 7000, Loss: 0.1304648220539093
step: 7100, Loss: 0.14360758662223816
step: 7200, Loss: 0.13226629793643951
step: 7300, Loss: 0.14012786746025085
step: 7400, Loss: 0.1241009533405304
step: 7500, Loss: 0.12983693182468414
step: 7600, Loss: 0.1258106529712677
step: 7700, Loss: 0.12884150445461273
step: 7800, Loss: 0.12539097666740417
step: 7900, Loss: 0.12507903575897217
step: 8000, Loss: 0.12730754911899567
step: 8100, Loss: 0.12240596115589142
step: 8200, Loss: 0.1367816925048828
step: 8300, Loss: 0.12778803706169128
step: 8400, Loss: 0.13209043443202972
step: 8500, Loss: 0.12290837615728378
step: 8600, Loss: 0.12463880330324173
step: 8700, Loss: 0.12904395163059235
step: 8800, Loss: 0.12853530049324036
step: 8900, Loss: 0.12218888103961945
step: 9000, Loss: 0.1189337819814682
step: 9100, Loss: 0.12216155230998993
step: 9200, Loss: 0.11830807477235794
step: 9300, Loss: 0.13149049878120422
step: 9400, Loss: 0.11775071173906326
step: 9500, Loss: 0.127131387591362
step: 9600, Loss: 0.11617580056190491
step: 9700, Loss: 0.12117522209882736
step: 9800, Loss: 0.1227826178073883
step: 9900, Loss: 0.11866094172000885
training successfully ended.
validating...
validate data length:88
acc: 0.7954545454545454
precision: 0.7636363636363637
recall: 0.8936170212765957
F_score: 0.8235294117647058
******fold 2******

Training... train_data length:786
step: 0, Loss: 3.653146266937256
step: 100, Loss: 0.24700695276260376
step: 200, Loss: 0.1313292235136032
step: 300, Loss: 0.13377101719379425
step: 400, Loss: 0.1232805997133255
step: 500, Loss: 0.13474883139133453
step: 600, Loss: 0.13114117085933685
step: 700, Loss: 0.1284351646900177
step: 800, Loss: 0.12406567484140396
step: 900, Loss: 0.1300399899482727
step: 1000, Loss: 0.12509126961231232
step: 1100, Loss: 0.12332887947559357
step: 1200, Loss: 0.12433207035064697
step: 1300, Loss: 0.11898487061262131
step: 1400, Loss: 0.12348790466785431
step: 1500, Loss: 0.11733641475439072
step: 1600, Loss: 0.12338066101074219
step: 1700, Loss: 0.12206827104091644
step: 1800, Loss: 0.12239474058151245
step: 1900, Loss: 0.11998787522315979
step: 2000, Loss: 0.12034697830677032
step: 2100, Loss: 0.1196204274892807
step: 2200, Loss: 0.1154216006398201
step: 2300, Loss: 0.28915318846702576
step: 2400, Loss: 0.3869055211544037
step: 2500, Loss: 0.1466626524925232
step: 2600, Loss: 0.1313270926475525
step: 2700, Loss: 0.1457316279411316
step: 2800, Loss: 0.13469108939170837
step: 2900, Loss: 0.15207381546497345
step: 3000, Loss: 0.13436010479927063
step: 3100, Loss: 0.13798190653324127
step: 3200, Loss: 0.13135921955108643
step: 3300, Loss: 0.13777805864810944
step: 3400, Loss: 0.12438639998435974
step: 3500, Loss: 0.12454750388860703
step: 3600, Loss: 0.1211114376783371
step: 3700, Loss: 0.12361746281385422
step: 3800, Loss: 0.1364825814962387
step: 3900, Loss: 0.1270006150007248
step: 4000, Loss: 0.13383524119853973
step: 4100, Loss: 0.1255435049533844
step: 4200, Loss: 0.1331738829612732
step: 4300, Loss: 0.12202846258878708
step: 4400, Loss: 0.1174161434173584
step: 4500, Loss: 0.12137255817651749
step: 4600, Loss: 0.11735723912715912
step: 4700, Loss: 0.11869153380393982
step: 4800, Loss: 0.11742603778839111
step: 4900, Loss: 0.12722283601760864
step: 5000, Loss: 0.11853138357400894
step: 5100, Loss: 0.12517140805721283
step: 5200, Loss: 0.1165853887796402
step: 5300, Loss: 0.12233558297157288
step: 5400, Loss: 0.11651705205440521
step: 5500, Loss: 0.11733689159154892
step: 5600, Loss: 0.11619820445775986
step: 5700, Loss: 0.11588044464588165
step: 5800, Loss: 0.11879280209541321
step: 5900, Loss: 0.11590145528316498
step: 6000, Loss: 0.12008292973041534
step: 6100, Loss: 0.11746467649936676
step: 6200, Loss: 0.11703521013259888
step: 6300, Loss: 0.11806150525808334
step: 6400, Loss: 0.11729198694229126
step: 6500, Loss: 0.11633635312318802
step: 6600, Loss: 0.1153511255979538
step: 6700, Loss: 0.11613964289426804
step: 6800, Loss: 0.11491159349679947
step: 6900, Loss: 0.1159207671880722
step: 7000, Loss: 0.11643089354038239
step: 7100, Loss: 0.11613000929355621
step: 7200, Loss: 0.1161433681845665
step: 7300, Loss: 0.11613369733095169
step: 7400, Loss: 0.11685710400342941
step: 7500, Loss: 0.1199379488825798
step: 7600, Loss: 0.11844570934772491
step: 7700, Loss: 0.12093056738376617
step: 7800, Loss: 0.11792111396789551
step: 7900, Loss: 0.12038049101829529
step: 8000, Loss: 2.5146100521087646
step: 8100, Loss: 0.16322144865989685
step: 8200, Loss: 0.16639770567417145
step: 8300, Loss: 0.1377321183681488
step: 8400, Loss: 0.13861286640167236
step: 8500, Loss: 0.13968932628631592
step: 8600, Loss: 0.13527143001556396
step: 8700, Loss: 0.13489829003810883
step: 8800, Loss: 0.13082095980644226
step: 8900, Loss: 0.12931378185749054
step: 9000, Loss: 0.12339141219854355
step: 9100, Loss: 0.12909942865371704
step: 9200, Loss: 0.11875934898853302
step: 9300, Loss: 0.12796235084533691
step: 9400, Loss: 0.12804651260375977
step: 9500, Loss: 0.12312737852334976
step: 9600, Loss: 0.12015287578105927
step: 9700, Loss: 0.12220853567123413
step: 9800, Loss: 0.12511129677295685
step: 9900, Loss: 0.11930243670940399
training successfully ended.
validating...
validate data length:88
acc: 0.8977272727272727
precision: 0.8269230769230769
recall: 1.0
F_score: 0.9052631578947368
******fold 3******

Training... train_data length:786
step: 0, Loss: 2.926637649536133
step: 100, Loss: 0.14414092898368835
step: 200, Loss: 0.12769290804862976
step: 300, Loss: 0.12653279304504395
step: 400, Loss: 0.11840283870697021
step: 500, Loss: 0.12671621143817902
step: 600, Loss: 0.12195566296577454
step: 700, Loss: 0.12636429071426392
step: 800, Loss: 0.12311964482069016
step: 900, Loss: 0.12430686503648758
step: 1000, Loss: 0.11994633078575134
step: 1100, Loss: 0.11833065748214722
step: 1200, Loss: 0.11636507511138916
step: 1300, Loss: 0.11764901876449585
step: 1400, Loss: 0.11656323075294495
step: 1500, Loss: 0.11786676943302155
step: 1600, Loss: 0.11767749488353729
step: 1700, Loss: 0.11857408285140991
step: 1800, Loss: 0.1165730208158493
step: 1900, Loss: 0.1174452006816864
step: 2000, Loss: 0.11758589744567871
step: 2100, Loss: 0.11832840740680695
step: 2200, Loss: 0.1146169975399971
step: 2300, Loss: 0.11504317820072174
step: 2400, Loss: 0.1144460141658783
step: 2500, Loss: 0.115350641310215
step: 2600, Loss: 0.1161520928144455
step: 2700, Loss: 0.11788725107908249
step: 2800, Loss: 0.12207619845867157
step: 2900, Loss: 0.11501996219158173
step: 3000, Loss: 0.11504378914833069
step: 3100, Loss: 0.12124748528003693
step: 3200, Loss: 0.11720003187656403
step: 3300, Loss: 0.1177491843700409
step: 3400, Loss: 0.12031719088554382
step: 3500, Loss: 0.34095752239227295
step: 3600, Loss: 0.14300549030303955
step: 3700, Loss: 0.14038127660751343
step: 3800, Loss: 0.13849398493766785
step: 3900, Loss: 0.13045528531074524
step: 4000, Loss: 0.1389322131872177
step: 4100, Loss: 0.13019496202468872
step: 4200, Loss: 0.12573716044425964
step: 4300, Loss: 0.13444040715694427
step: 4400, Loss: 0.13010238111019135
step: 4500, Loss: 0.12454927712678909
step: 4600, Loss: 0.12103231996297836
step: 4700, Loss: 0.12282514572143555
step: 4800, Loss: 0.11982675641775131
step: 4900, Loss: 0.12235681712627411
step: 5000, Loss: 0.1191253587603569
step: 5100, Loss: 0.12491718679666519
step: 5200, Loss: 0.12117008864879608
step: 5300, Loss: 0.12067179381847382
step: 5400, Loss: 0.12471917271614075
step: 5500, Loss: 0.11807572841644287
step: 5600, Loss: 0.1237233579158783
step: 5700, Loss: 0.11677558720111847
step: 5800, Loss: 0.11892901360988617
step: 5900, Loss: 0.11550502479076385
step: 6000, Loss: 0.11746123433113098
step: 6100, Loss: 0.1177443116903305
step: 6200, Loss: 0.11917828023433685
step: 6300, Loss: 0.11437418311834335
step: 6400, Loss: 0.11961531639099121
step: 6500, Loss: 0.11961537599563599
step: 6600, Loss: 0.11601714044809341
step: 6700, Loss: 0.11558566242456436
step: 6800, Loss: 0.11443516612052917
step: 6900, Loss: 0.11583428084850311
step: 7000, Loss: 0.11535518616437912
step: 7100, Loss: 0.11545495688915253
step: 7200, Loss: 0.11704094707965851
step: 7300, Loss: 0.11652565747499466
step: 7400, Loss: 0.1152617409825325
step: 7500, Loss: 0.11619829386472702
step: 7600, Loss: 0.11725501716136932
step: 7700, Loss: 0.11489849537611008
step: 7800, Loss: 0.11403851211071014
step: 7900, Loss: 0.114117830991745
step: 8000, Loss: 0.11609019339084625
step: 8100, Loss: 0.11483833193778992
step: 8200, Loss: 0.11585037410259247
step: 8300, Loss: 0.11536003649234772
step: 8400, Loss: 0.11475645750761032
step: 8500, Loss: 0.11510570347309113
step: 8600, Loss: 0.1148044615983963
step: 8700, Loss: 0.11359144002199173
step: 8800, Loss: 0.11451322585344315
step: 8900, Loss: 0.11563801765441895
step: 9000, Loss: 0.11454281955957413
step: 9100, Loss: 0.11323962360620499
step: 9200, Loss: 1.4465736150741577
step: 9300, Loss: 0.31596213579177856
step: 9400, Loss: 0.1418323814868927
step: 9500, Loss: 0.14565777778625488
step: 9600, Loss: 0.13261078298091888
step: 9700, Loss: 0.1322406381368637
step: 9800, Loss: 0.12907643616199493
step: 9900, Loss: 0.1317102164030075
training successfully ended.
validating...
validate data length:88
acc: 0.9772727272727273
precision: 0.9555555555555556
recall: 1.0
F_score: 0.9772727272727273
******fold 4******

Training... train_data length:786
step: 0, Loss: 0.12898610532283783
step: 100, Loss: 0.12911157310009003
step: 200, Loss: 0.12591589987277985
step: 300, Loss: 0.1302616000175476
step: 400, Loss: 0.12055365741252899
step: 500, Loss: 0.12571415305137634
step: 600, Loss: 0.11911416053771973
step: 700, Loss: 0.11996837705373764
step: 800, Loss: 0.11785174161195755
step: 900, Loss: 0.11797955632209778
step: 1000, Loss: 0.11729732155799866
step: 1100, Loss: 0.11517323553562164
step: 1200, Loss: 0.11964363604784012
step: 1300, Loss: 0.11636483669281006
step: 1400, Loss: 0.11682666838169098
step: 1500, Loss: 0.11518336087465286
step: 1600, Loss: 0.11504273861646652
step: 1700, Loss: 0.11666697263717651
step: 1800, Loss: 0.11735380440950394
step: 1900, Loss: 0.1165512204170227
step: 2000, Loss: 0.11445294320583344
step: 2100, Loss: 0.11407120525836945
step: 2200, Loss: 0.11697861552238464
step: 2300, Loss: 0.11453080177307129
step: 2400, Loss: 0.11658559739589691
step: 2500, Loss: 0.11461187899112701
step: 2600, Loss: 0.11779491603374481
step: 2700, Loss: 0.11475631594657898
step: 2800, Loss: 0.11549000442028046
step: 2900, Loss: 0.11864939332008362
step: 3000, Loss: 0.11762161552906036
step: 3100, Loss: 0.11569637060165405
step: 3200, Loss: 0.11823292076587677
step: 3300, Loss: 0.11574119329452515
step: 3400, Loss: 0.11690957844257355
step: 3500, Loss: 0.1160397082567215
step: 3600, Loss: 0.12781192362308502
step: 3700, Loss: 0.11745932698249817
step: 3800, Loss: 0.12256038188934326
step: 3900, Loss: 0.11362913250923157
step: 4000, Loss: 0.11445199698209763
step: 4100, Loss: 0.1180260181427002
step: 4200, Loss: 0.11638254672288895
step: 4300, Loss: 0.11402445286512375
step: 4400, Loss: 0.11588175594806671
step: 4500, Loss: 0.11530066281557083
step: 4600, Loss: 0.11483868211507797
step: 4700, Loss: 0.1320466250181198
step: 4800, Loss: 0.3169863224029541
step: 4900, Loss: 0.14577950537204742
step: 5000, Loss: 0.1366339772939682
step: 5100, Loss: 0.13071990013122559
step: 5200, Loss: 0.12590345740318298
step: 5300, Loss: 0.12022299319505692
step: 5400, Loss: 0.1226540058851242
step: 5500, Loss: 0.12594303488731384
step: 5600, Loss: 0.12461934983730316
step: 5700, Loss: 0.1236305981874466
step: 5800, Loss: 0.12244343012571335
step: 5900, Loss: 0.12196255475282669
step: 6000, Loss: 0.12002589553594589
step: 6100, Loss: 0.11956584453582764
step: 6200, Loss: 0.11820589751005173
step: 6300, Loss: 0.12000207602977753
step: 6400, Loss: 0.1180349588394165
step: 6500, Loss: 0.11610502004623413
step: 6600, Loss: 0.11680736392736435
step: 6700, Loss: 0.11701454967260361
step: 6800, Loss: 0.11902900785207748
step: 6900, Loss: 0.11955714225769043
step: 7000, Loss: 0.11888779699802399
step: 7100, Loss: 0.11463428288698196
step: 7200, Loss: 0.11660260707139969
step: 7300, Loss: 0.11485479772090912
step: 7400, Loss: 0.11779093742370605
step: 7500, Loss: 0.1172202080488205
step: 7600, Loss: 0.11506122350692749
step: 7700, Loss: 0.11524385213851929
step: 7800, Loss: 0.11562523990869522
step: 7900, Loss: 0.11575983464717865
step: 8000, Loss: 0.11517955362796783
step: 8100, Loss: 0.1154923066496849
step: 8200, Loss: 0.11500009894371033
step: 8300, Loss: 0.11450579762458801
step: 8400, Loss: 0.11571920663118362
step: 8500, Loss: 0.1140793189406395
step: 8600, Loss: 0.11425279080867767
step: 8700, Loss: 0.11366268992424011
step: 8800, Loss: 0.1149539202451706
step: 8900, Loss: 0.11420212686061859
step: 9000, Loss: 0.11391739547252655
step: 9100, Loss: 0.11393546313047409
step: 9200, Loss: 0.11493217945098877
step: 9300, Loss: 0.11361727863550186
step: 9400, Loss: 0.11516303569078445
step: 9500, Loss: 0.1152125746011734
step: 9600, Loss: 0.11525405198335648
step: 9700, Loss: 0.11341378837823868
step: 9800, Loss: 0.11469176411628723
step: 9900, Loss: 0.11367479711771011
training successfully ended.
validating...
validate data length:88
acc: 0.9772727272727273
precision: 0.9583333333333334
recall: 1.0
F_score: 0.9787234042553191
******fold 5******

Training... train_data length:787
step: 0, Loss: 0.1316784769296646
step: 100, Loss: 0.13717469573020935
step: 200, Loss: 0.12434208393096924
step: 300, Loss: 0.12331981956958771
step: 400, Loss: 0.12264293432235718
step: 500, Loss: 0.12957531213760376
step: 600, Loss: 0.12242933362722397
step: 700, Loss: 0.12064474821090698
step: 800, Loss: 0.11835917085409164
step: 900, Loss: 0.11776787042617798
step: 1000, Loss: 0.11478718370199203
step: 1100, Loss: 0.11954299360513687
step: 1200, Loss: 0.11768775433301926
step: 1300, Loss: 0.11706166714429855
step: 1400, Loss: 0.1168348491191864
step: 1500, Loss: 0.11655671894550323
step: 1600, Loss: 0.11688072979450226
step: 1700, Loss: 0.11689113080501556
step: 1800, Loss: 0.11424486339092255
step: 1900, Loss: 0.11575473099946976
step: 2000, Loss: 0.11561565846204758
step: 2100, Loss: 0.11457791179418564
step: 2200, Loss: 0.11699357628822327
step: 2300, Loss: 0.1159723550081253
step: 2400, Loss: 0.11353615671396255
step: 2500, Loss: 0.11492520570755005
step: 2600, Loss: 0.11367020010948181
step: 2700, Loss: 0.11586406081914902
step: 2800, Loss: 0.11347775161266327
step: 2900, Loss: 0.1158166229724884
step: 3000, Loss: 0.11600285768508911
step: 3100, Loss: 0.11774399876594543
step: 3200, Loss: 0.11838804930448532
step: 3300, Loss: 0.11493590474128723
step: 3400, Loss: 0.11345966905355453
step: 3500, Loss: 0.11585783958435059
step: 3600, Loss: 0.11773290485143661
step: 3700, Loss: 0.11728724837303162
step: 3800, Loss: 0.1234150230884552
step: 3900, Loss: 0.24314001202583313
step: 4000, Loss: 0.15612606704235077
step: 4100, Loss: 0.1356564164161682
step: 4200, Loss: 0.13998912274837494
step: 4300, Loss: 0.12632830440998077
step: 4400, Loss: 0.12687546014785767
step: 4500, Loss: 0.12652046978473663
step: 4600, Loss: 0.12006739526987076
step: 4700, Loss: 0.12617835402488708
step: 4800, Loss: 0.1249627098441124
step: 4900, Loss: 0.1300240457057953
step: 5000, Loss: 0.11710311472415924
step: 5100, Loss: 0.12172718346118927
step: 5200, Loss: 0.12152035534381866
step: 5300, Loss: 0.12058483064174652
step: 5400, Loss: 0.1262013465166092
step: 5500, Loss: 0.12018509209156036
step: 5600, Loss: 0.1185397133231163
step: 5700, Loss: 0.1162281185388565
step: 5800, Loss: 0.11740939319133759
step: 5900, Loss: 0.11759819090366364
step: 6000, Loss: 0.12021689862012863
step: 6100, Loss: 0.11610405147075653
step: 6200, Loss: 0.11708903312683105
step: 6300, Loss: 0.11797645688056946
step: 6400, Loss: 0.11874545365571976
step: 6500, Loss: 0.1185644119977951
step: 6600, Loss: 0.1170639842748642
step: 6700, Loss: 0.11682797968387604
step: 6800, Loss: 0.11558394879102707
step: 6900, Loss: 0.11578582972288132
step: 7000, Loss: 0.11815792322158813
step: 7100, Loss: 0.11842530965805054
step: 7200, Loss: 0.11576106399297714
step: 7300, Loss: 0.11545759439468384
step: 7400, Loss: 0.11532656103372574
step: 7500, Loss: 0.11434213817119598
step: 7600, Loss: 0.1168043315410614
step: 7700, Loss: 0.11658558249473572
step: 7800, Loss: 0.11501748859882355
step: 7900, Loss: 0.11304674297571182
step: 8000, Loss: 0.1153993308544159
step: 8100, Loss: 0.11412130296230316
step: 8200, Loss: 0.11651813983917236
step: 8300, Loss: 0.11530281603336334
step: 8400, Loss: 0.11430290341377258
step: 8500, Loss: 0.11523802578449249
step: 8600, Loss: 0.11402146518230438
step: 8700, Loss: 0.11357106268405914
step: 8800, Loss: 0.11580595374107361
step: 8900, Loss: 0.11420393735170364
step: 9000, Loss: 0.11371271312236786
step: 9100, Loss: 0.11416558176279068
step: 9200, Loss: 0.11444666981697083
step: 9300, Loss: 0.11387351155281067
step: 9400, Loss: 0.11441056430339813
step: 9500, Loss: 0.11370722204446793
step: 9600, Loss: 0.11366407573223114
step: 9700, Loss: 0.1138526201248169
step: 9800, Loss: 0.11515020579099655
step: 9900, Loss: 0.11386732757091522
training successfully ended.
validating...
validate data length:87
acc: 0.975
precision: 0.9444444444444444
recall: 1.0
F_score: 0.9714285714285714
******fold 6******

Training... train_data length:787
step: 0, Loss: 0.14127857983112335
step: 100, Loss: 0.1346207857131958
step: 200, Loss: 0.12026125937700272
step: 300, Loss: 0.1251269429922104
step: 400, Loss: 0.1212863102555275
step: 500, Loss: 0.13126659393310547
step: 600, Loss: 0.12034212052822113
step: 700, Loss: 0.1214541345834732
step: 800, Loss: 0.12040729820728302
step: 900, Loss: 0.11970636993646622
step: 1000, Loss: 0.11684686690568924
step: 1100, Loss: 0.11607815325260162
step: 1200, Loss: 0.11708604544401169
step: 1300, Loss: 0.11607204377651215
step: 1400, Loss: 0.11623960733413696
step: 1500, Loss: 0.11657989025115967
step: 1600, Loss: 0.11645236611366272
step: 1700, Loss: 0.11602983623743057
step: 1800, Loss: 0.11643467843532562
step: 1900, Loss: 0.11783774942159653
step: 2000, Loss: 0.11545955389738083
step: 2100, Loss: 0.11469880491495132
step: 2200, Loss: 0.11567889153957367
step: 2300, Loss: 0.11541034281253815
step: 2400, Loss: 0.11404279619455338
step: 2500, Loss: 0.11562850326299667
step: 2600, Loss: 0.11451254040002823
step: 2700, Loss: 0.11534418910741806
step: 2800, Loss: 0.11590941250324249
step: 2900, Loss: 0.11479933559894562
step: 3000, Loss: 0.11486326903104782
step: 3100, Loss: 0.11443217098712921
step: 3200, Loss: 0.11499114334583282
step: 3300, Loss: 0.11633743345737457
step: 3400, Loss: 0.11569852381944656
step: 3500, Loss: 2.5139033794403076
step: 3600, Loss: 0.24715165793895721
step: 3700, Loss: 0.14602786302566528
step: 3800, Loss: 0.14228487014770508
step: 3900, Loss: 0.12629537284374237
step: 4000, Loss: 0.13575541973114014
step: 4100, Loss: 0.13842742145061493
step: 4200, Loss: 0.12505307793617249
step: 4300, Loss: 0.12129075825214386
step: 4400, Loss: 0.12430182099342346
step: 4500, Loss: 0.12254169583320618
step: 4600, Loss: 0.11961983144283295
step: 4700, Loss: 0.12575478851795197
step: 4800, Loss: 0.12574954330921173
step: 4900, Loss: 0.12527494132518768
step: 5000, Loss: 0.12205475568771362
step: 5100, Loss: 0.12168790400028229
step: 5200, Loss: 0.12429189682006836
step: 5300, Loss: 0.11823400855064392
step: 5400, Loss: 0.1186235249042511
step: 5500, Loss: 0.1188914105296135
step: 5600, Loss: 0.1193002387881279
step: 5700, Loss: 0.11511704325675964
step: 5800, Loss: 0.11938147246837616
step: 5900, Loss: 0.12018119543790817
step: 6000, Loss: 0.11727233231067657
step: 6100, Loss: 0.11673927307128906
step: 6200, Loss: 0.11849768459796906
step: 6300, Loss: 0.11857310682535172
step: 6400, Loss: 0.11661425232887268
step: 6500, Loss: 0.11756911128759384
step: 6600, Loss: 0.11578497290611267
step: 6700, Loss: 0.11594676226377487
step: 6800, Loss: 0.11423349380493164
step: 6900, Loss: 0.11684823781251907
step: 7000, Loss: 0.11498082429170609
step: 7100, Loss: 0.11675047874450684
step: 7200, Loss: 0.11464627087116241
step: 7300, Loss: 0.11403416842222214
step: 7400, Loss: 0.11727509647607803
step: 7500, Loss: 0.11400763690471649
step: 7600, Loss: 0.11504682153463364
step: 7700, Loss: 0.11464401334524155
step: 7800, Loss: 0.11441896855831146
step: 7900, Loss: 0.11494432389736176
step: 8000, Loss: 0.11413352191448212
step: 8100, Loss: 0.11511435359716415
step: 8200, Loss: 0.11468590050935745
step: 8300, Loss: 0.11511432379484177
step: 8400, Loss: 0.11465135216712952
step: 8500, Loss: 0.11283112317323685
step: 8600, Loss: 0.11474887281656265
step: 8700, Loss: 0.11577332019805908
step: 8800, Loss: 0.11528612673282623
step: 8900, Loss: 0.11440452933311462
step: 9000, Loss: 0.11451840400695801
step: 9100, Loss: 0.11394984275102615
step: 9200, Loss: 0.11810507625341415
step: 9300, Loss: 0.11450568586587906
step: 9400, Loss: 0.11450280249118805
step: 9500, Loss: 0.1145341768860817
step: 9600, Loss: 0.11346247792243958
step: 9700, Loss: 0.1143111139535904
step: 9800, Loss: 0.11505631357431412
step: 9900, Loss: 0.11505649983882904
training successfully ended.
validating...
validate data length:87
acc: 0.975
precision: 0.9777777777777777
recall: 0.9777777777777777
F_score: 0.9777777777777777
******fold 7******

Training... train_data length:787
step: 0, Loss: 0.12983699142932892
step: 100, Loss: 0.1322033703327179
step: 200, Loss: 0.12700417637825012
step: 300, Loss: 0.12271716445684433
step: 400, Loss: 0.12484809756278992
step: 500, Loss: 0.12538737058639526
step: 600, Loss: 0.11973890662193298
step: 700, Loss: 0.11963874846696854
step: 800, Loss: 0.11950655281543732
step: 900, Loss: 0.11611754447221756
step: 1000, Loss: 0.1198062002658844
step: 1100, Loss: 0.1171523779630661
step: 1200, Loss: 0.11803418397903442
step: 1300, Loss: 0.11616301536560059
step: 1400, Loss: 0.11570318043231964
step: 1500, Loss: 0.11800503730773926
step: 1600, Loss: 0.11823219060897827
step: 1700, Loss: 0.11512473970651627
step: 1800, Loss: 0.11377641558647156
step: 1900, Loss: 0.1163819208741188
step: 2000, Loss: 0.11455969512462616
step: 2100, Loss: 0.11629438400268555
step: 2200, Loss: 0.11590954661369324
step: 2300, Loss: 0.1154986172914505
step: 2400, Loss: 0.11423084139823914
step: 2500, Loss: 0.11637207865715027
step: 2600, Loss: 0.1154593899846077
step: 2700, Loss: 0.11579890549182892
step: 2800, Loss: 0.1153116449713707
step: 2900, Loss: 0.11568402498960495
step: 3000, Loss: 0.11500399559736252
step: 3100, Loss: 0.11466396600008011
step: 3200, Loss: 0.11711935698986053
step: 3300, Loss: 0.11820846796035767
step: 3400, Loss: 2.9622621536254883
step: 3500, Loss: 0.15857690572738647
step: 3600, Loss: 0.13916152715682983
step: 3700, Loss: 0.12476728856563568
step: 3800, Loss: 0.12803542613983154
step: 3900, Loss: 0.13012054562568665
step: 4000, Loss: 0.13786257803440094
step: 4100, Loss: 0.13227619230747223
step: 4200, Loss: 0.12507084012031555
step: 4300, Loss: 0.12374372035264969
step: 4400, Loss: 0.12566740810871124
step: 4500, Loss: 0.12165912985801697
step: 4600, Loss: 0.12674760818481445
step: 4700, Loss: 0.1194358691573143
step: 4800, Loss: 0.11790009588003159
step: 4900, Loss: 0.11978478729724884
step: 5000, Loss: 0.11850637197494507
step: 5100, Loss: 0.12129414081573486
step: 5200, Loss: 0.12053250521421432
step: 5300, Loss: 0.1222485825419426
step: 5400, Loss: 0.11774920672178268
step: 5500, Loss: 0.11783669888973236
step: 5600, Loss: 0.1210608258843422
step: 5700, Loss: 0.11852675676345825
step: 5800, Loss: 0.11634811758995056
step: 5900, Loss: 0.11482257395982742
step: 6000, Loss: 0.11639009416103363
step: 6100, Loss: 0.11449465900659561
step: 6200, Loss: 0.11666572093963623
step: 6300, Loss: 0.11777310073375702
step: 6400, Loss: 0.11809305846691132
step: 6500, Loss: 0.1153077483177185
step: 6600, Loss: 0.11673974990844727
step: 6700, Loss: 0.115817591547966
step: 6800, Loss: 0.11641237139701843
step: 6900, Loss: 0.11646585166454315
step: 7000, Loss: 0.11349529027938843
step: 7100, Loss: 0.11472281068563461
step: 7200, Loss: 0.11569910496473312
step: 7300, Loss: 0.11480710655450821
step: 7400, Loss: 0.11549130082130432
step: 7500, Loss: 0.11557883769273758
step: 7600, Loss: 0.11560673266649246
step: 7700, Loss: 0.11509908735752106
step: 7800, Loss: 0.11489736288785934
step: 7900, Loss: 0.11563180387020111
step: 8000, Loss: 0.11420653760433197
step: 8100, Loss: 0.1142372190952301
step: 8200, Loss: 0.11435509473085403
step: 8300, Loss: 0.11505775153636932
step: 8400, Loss: 0.11367063969373703
step: 8500, Loss: 0.11359262466430664
step: 8600, Loss: 0.11421340703964233
step: 8700, Loss: 0.11539289355278015
step: 8800, Loss: 0.11403390765190125
step: 8900, Loss: 0.11405132710933685
step: 9000, Loss: 0.1184082180261612
step: 9100, Loss: 0.11619336158037186
step: 9200, Loss: 0.11343803256750107
step: 9300, Loss: 0.11458922177553177
step: 9400, Loss: 0.11474645882844925
step: 9500, Loss: 0.1217813491821289
step: 9600, Loss: 0.4478594660758972
step: 9700, Loss: 0.15871083736419678
step: 9800, Loss: 0.14040742814540863
step: 9900, Loss: 0.12886014580726624
training successfully ended.
validating...
validate data length:87
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:787
step: 0, Loss: 0.23262253403663635
step: 100, Loss: 0.13634184002876282
step: 200, Loss: 0.1261528581380844
step: 300, Loss: 0.1275559812784195
step: 400, Loss: 0.12001952528953552
step: 500, Loss: 0.12544682621955872
step: 600, Loss: 0.11605225503444672
step: 700, Loss: 0.12122806906700134
step: 800, Loss: 0.11640749871730804
step: 900, Loss: 0.11755014210939407
step: 1000, Loss: 0.11842454969882965
step: 1100, Loss: 0.11761042475700378
step: 1200, Loss: 0.11632546782493591
step: 1300, Loss: 0.11543894559144974
step: 1400, Loss: 0.11541205644607544
step: 1500, Loss: 0.11430302262306213
step: 1600, Loss: 0.11567424237728119
step: 1700, Loss: 0.11454339325428009
step: 1800, Loss: 0.11507219076156616
step: 1900, Loss: 0.11460493505001068
step: 2000, Loss: 0.11477862298488617
step: 2100, Loss: 0.11500753462314606
step: 2200, Loss: 0.1140672117471695
step: 2300, Loss: 0.115346260368824
step: 2400, Loss: 0.11415879428386688
step: 2500, Loss: 0.11640895903110504
step: 2600, Loss: 0.11471078544855118
step: 2700, Loss: 0.11649228632450104
step: 2800, Loss: 0.18398956954479218
step: 2900, Loss: 0.13327082991600037
step: 3000, Loss: 0.14682886004447937
step: 3100, Loss: 0.12859812378883362
step: 3200, Loss: 0.12485551834106445
step: 3300, Loss: 0.1279849261045456
step: 3400, Loss: 0.122480109333992
step: 3500, Loss: 0.11712981015443802
step: 3600, Loss: 0.12317347526550293
step: 3700, Loss: 0.12576346099376678
step: 3800, Loss: 0.1200186014175415
step: 3900, Loss: 0.1166796013712883
step: 4000, Loss: 0.11895090341567993
step: 4100, Loss: 0.12388086318969727
step: 4200, Loss: 0.1199498325586319
step: 4300, Loss: 0.11562792956829071
step: 4400, Loss: 0.12118358165025711
step: 4500, Loss: 0.11810758709907532
step: 4600, Loss: 0.11558236181735992
step: 4700, Loss: 0.1194818913936615
step: 4800, Loss: 0.1186138466000557
step: 4900, Loss: 0.11745929718017578
step: 5000, Loss: 0.11798045039176941
step: 5100, Loss: 0.11584009975194931
step: 5200, Loss: 0.11654993891716003
step: 5300, Loss: 0.11742932349443436
step: 5400, Loss: 0.1153397187590599
step: 5500, Loss: 0.11569049209356308
step: 5600, Loss: 0.11443204432725906
step: 5700, Loss: 0.11543688923120499
step: 5800, Loss: 0.11450649052858353
step: 5900, Loss: 0.11395226418972015
step: 6000, Loss: 0.11435744166374207
step: 6100, Loss: 0.11438660323619843
step: 6200, Loss: 0.11458354443311691
step: 6300, Loss: 0.113169364631176
step: 6400, Loss: 0.1141209602355957
step: 6500, Loss: 0.11426155269145966
step: 6600, Loss: 0.11522357910871506
step: 6700, Loss: 0.11407342553138733
step: 6800, Loss: 0.11485963314771652
step: 6900, Loss: 0.11444122344255447
step: 7000, Loss: 0.11413219571113586
step: 7100, Loss: 0.11318691819906235
step: 7200, Loss: 0.11329678446054459
step: 7300, Loss: 0.11412699520587921
step: 7400, Loss: 0.11401914060115814
step: 7500, Loss: 0.11315255612134933
step: 7600, Loss: 0.11336968839168549
step: 7700, Loss: 0.11403455585241318
step: 7800, Loss: 0.11424250900745392
step: 7900, Loss: 0.11433111131191254
step: 8000, Loss: 0.11306274682283401
step: 8100, Loss: 0.1150505393743515
step: 8200, Loss: 0.11522729694843292
step: 8300, Loss: 0.1124446913599968
step: 8400, Loss: 0.11424762010574341
step: 8500, Loss: 0.11552433669567108
step: 8600, Loss: 0.11343703418970108
step: 8700, Loss: 0.11628879606723785
step: 8800, Loss: 0.11286920309066772
step: 8900, Loss: 0.11368083953857422
step: 9000, Loss: 0.11420327425003052
step: 9100, Loss: 0.11390858143568039
step: 9200, Loss: 0.11385302245616913
step: 9300, Loss: 0.11480258405208588
step: 9400, Loss: 0.11523697525262833
step: 9500, Loss: 0.11626303195953369
step: 9600, Loss: 0.11839215457439423
step: 9700, Loss: 0.12604285776615143
step: 9800, Loss: 0.6629234552383423
step: 9900, Loss: 0.20603670179843903
training successfully ended.
validating...
validate data length:87
acc: 0.95
precision: 0.9523809523809523
recall: 0.9523809523809523
F_score: 0.9523809523809523
******fold 9******

Training... train_data length:787
step: 0, Loss: 0.33910441398620605
step: 100, Loss: 0.1395796835422516
step: 200, Loss: 0.12129153311252594
step: 300, Loss: 0.13179920613765717
step: 400, Loss: 0.12023604661226273
step: 500, Loss: 0.12432756274938583
step: 600, Loss: 0.11599452793598175
step: 700, Loss: 0.11762235313653946
step: 800, Loss: 0.11758889257907867
step: 900, Loss: 0.11654242128133774
step: 1000, Loss: 0.11715690791606903
step: 1100, Loss: 0.11640997976064682
step: 1200, Loss: 0.1158733069896698
step: 1300, Loss: 0.11418291181325912
step: 1400, Loss: 0.11495938152074814
step: 1500, Loss: 0.11426613479852676
step: 1600, Loss: 0.1149764135479927
step: 1700, Loss: 0.11378751695156097
step: 1800, Loss: 0.11429375410079956
step: 1900, Loss: 0.11540264636278152
step: 2000, Loss: 0.11525391787290573
step: 2100, Loss: 0.11493601649999619
step: 2200, Loss: 0.11505353450775146
step: 2300, Loss: 0.11487148702144623
step: 2400, Loss: 0.11814284324645996
step: 2500, Loss: 0.11505880206823349
step: 2600, Loss: 0.1215469092130661
step: 2700, Loss: 1.8301854133605957
step: 2800, Loss: 0.1415698081254959
step: 2900, Loss: 0.137851744890213
step: 3000, Loss: 0.1281714141368866
step: 3100, Loss: 0.12661978602409363
step: 3200, Loss: 0.12573690712451935
step: 3300, Loss: 0.12711629271507263
step: 3400, Loss: 0.12096793204545975
step: 3500, Loss: 0.11959991604089737
step: 3600, Loss: 0.12694014608860016
step: 3700, Loss: 0.11779389530420303
step: 3800, Loss: 0.12446766346693039
step: 3900, Loss: 0.12237460166215897
step: 4000, Loss: 0.12470423430204391
step: 4100, Loss: 0.11742807924747467
step: 4200, Loss: 0.11925546079874039
step: 4300, Loss: 0.12047642469406128
step: 4400, Loss: 0.11931788921356201
step: 4500, Loss: 0.1148415058851242
step: 4600, Loss: 0.11733019351959229
step: 4700, Loss: 0.11848140507936478
step: 4800, Loss: 0.11506769806146622
step: 4900, Loss: 0.11985021829605103
step: 5000, Loss: 0.11564801633358002
step: 5100, Loss: 0.11925099790096283
step: 5200, Loss: 0.11771821230649948
step: 5300, Loss: 0.11867047101259232
step: 5400, Loss: 0.11603768914937973
step: 5500, Loss: 0.11676982045173645
step: 5600, Loss: 0.11333651840686798
step: 5700, Loss: 0.11552288383245468
step: 5800, Loss: 0.11627261340618134
step: 5900, Loss: 0.11486639827489853
step: 6000, Loss: 0.11637598276138306
step: 6100, Loss: 0.11544760316610336
step: 6200, Loss: 0.11578880250453949
step: 6300, Loss: 0.11408238112926483
step: 6400, Loss: 0.11431638151407242
step: 6500, Loss: 0.11375011503696442
step: 6600, Loss: 0.11512190103530884
step: 6700, Loss: 0.11457470804452896
step: 6800, Loss: 0.114718496799469
step: 6900, Loss: 0.11597461253404617
step: 7000, Loss: 0.11329340189695358
step: 7100, Loss: 0.1149037703871727
step: 7200, Loss: 0.11539244651794434
step: 7300, Loss: 0.11531763523817062
step: 7400, Loss: 0.11354712396860123
step: 7500, Loss: 0.114618219435215
step: 7600, Loss: 0.11406970024108887
step: 7700, Loss: 0.11391288787126541
step: 7800, Loss: 0.11345730721950531
step: 7900, Loss: 0.11419116705656052
step: 8000, Loss: 0.11448931694030762
step: 8100, Loss: 0.11528167128562927
step: 8200, Loss: 0.11490131914615631
step: 8300, Loss: 0.11469593644142151
step: 8400, Loss: 0.11510913074016571
step: 8500, Loss: 0.11425407975912094
step: 8600, Loss: 0.11304548382759094
step: 8700, Loss: 0.11374570429325104
step: 8800, Loss: 0.11442539095878601
step: 8900, Loss: 0.1133769229054451
step: 9000, Loss: 0.11836579442024231
step: 9100, Loss: 4.225753307342529
step: 9200, Loss: 0.3152872920036316
step: 9300, Loss: 0.15800867974758148
step: 9400, Loss: 0.1246568113565445
step: 9500, Loss: 0.13163518905639648
step: 9600, Loss: 0.1282685250043869
step: 9700, Loss: 0.12892338633537292
step: 9800, Loss: 0.12257711589336395
step: 9900, Loss: 0.12408207356929779
training successfully ended.
validating...
validate data length:87
acc: 0.95
precision: 0.9761904761904762
recall: 0.9318181818181818
F_score: 0.9534883720930233
******fold 10******

Training... train_data length:787
step: 0, Loss: 0.243022158741951
step: 100, Loss: 0.14039388298988342
step: 200, Loss: 0.12416979670524597
step: 300, Loss: 0.12733101844787598
step: 400, Loss: 0.12193173170089722
step: 500, Loss: 0.1250578910112381
step: 600, Loss: 0.1162516176700592
step: 700, Loss: 0.11908116191625595
step: 800, Loss: 0.11747253686189651
step: 900, Loss: 0.11629052460193634
step: 1000, Loss: 0.11663717031478882
step: 1100, Loss: 0.11583911627531052
step: 1200, Loss: 0.1183578372001648
step: 1300, Loss: 0.11564008891582489
step: 1400, Loss: 0.11524239182472229
step: 1500, Loss: 0.11543658375740051
step: 1600, Loss: 0.11603417992591858
step: 1700, Loss: 0.1163412407040596
step: 1800, Loss: 0.11530241370201111
step: 1900, Loss: 0.11484810709953308
step: 2000, Loss: 0.11392303556203842
step: 2100, Loss: 0.1147601529955864
step: 2200, Loss: 0.11470288783311844
step: 2300, Loss: 0.11322277784347534
step: 2400, Loss: 0.11663071066141129
step: 2500, Loss: 0.11536809056997299
step: 2600, Loss: 2.2588541507720947
step: 2700, Loss: 0.16924364864826202
step: 2800, Loss: 0.14348888397216797
step: 2900, Loss: 0.12685567140579224
step: 3000, Loss: 0.1329392045736313
step: 3100, Loss: 0.1237655058503151
step: 3200, Loss: 0.12105375528335571
step: 3300, Loss: 0.12164884805679321
step: 3400, Loss: 0.12486134469509125
step: 3500, Loss: 0.11684749275445938
step: 3600, Loss: 0.12151505798101425
step: 3700, Loss: 0.1218809187412262
step: 3800, Loss: 0.1218881905078888
step: 3900, Loss: 0.11748476326465607
step: 4000, Loss: 0.1186336874961853
step: 4100, Loss: 0.11897546052932739
step: 4200, Loss: 0.11711858958005905
step: 4300, Loss: 0.11690063774585724
step: 4400, Loss: 0.11638196557760239
step: 4500, Loss: 0.11772872507572174
step: 4600, Loss: 0.114617258310318
step: 4700, Loss: 0.11648155748844147
step: 4800, Loss: 0.11893320083618164
step: 4900, Loss: 0.11447744816541672
step: 5000, Loss: 0.11764740198850632
step: 5100, Loss: 0.11554747819900513
step: 5200, Loss: 0.11533235013484955
step: 5300, Loss: 0.11652635037899017
step: 5400, Loss: 0.11542965471744537
step: 5500, Loss: 0.11622372269630432
step: 5600, Loss: 0.11486677825450897
step: 5700, Loss: 0.11468936502933502
step: 5800, Loss: 0.11624012887477875
step: 5900, Loss: 0.11477959901094437
step: 6000, Loss: 0.11548954248428345
step: 6100, Loss: 0.11479080468416214
step: 6200, Loss: 0.11437653750181198
step: 6300, Loss: 0.11413790285587311
step: 6400, Loss: 0.11422885954380035
step: 6500, Loss: 0.11407360434532166
step: 6600, Loss: 0.1148984432220459
step: 6700, Loss: 0.11476212739944458
step: 6800, Loss: 0.11337678879499435
step: 6900, Loss: 0.11380545794963837
step: 7000, Loss: 0.11435296386480331
step: 7100, Loss: 0.11422392725944519
step: 7200, Loss: 0.11326111853122711
step: 7300, Loss: 0.11487041413784027
step: 7400, Loss: 0.11534959077835083
step: 7500, Loss: 0.11405942589044571
step: 7600, Loss: 0.1150166317820549
step: 7700, Loss: 0.11493006348609924
step: 7800, Loss: 0.11585045605897903
step: 7900, Loss: 0.1162051260471344
step: 8000, Loss: 0.11482135951519012
step: 8100, Loss: 0.11355143040418625
step: 8200, Loss: 0.11382371187210083
step: 8300, Loss: 0.11355476081371307
step: 8400, Loss: 0.11451081931591034
step: 8500, Loss: 0.11473353207111359
step: 8600, Loss: 0.11543678492307663
step: 8700, Loss: 0.11349433660507202
step: 8800, Loss: 0.11501049250364304
step: 8900, Loss: 0.11374744772911072
step: 9000, Loss: 0.11553679406642914
step: 9100, Loss: 0.11622633039951324
step: 9200, Loss: 0.1203339546918869
step: 9300, Loss: 0.6425184011459351
step: 9400, Loss: 0.13767516613006592
step: 9500, Loss: 0.1359953135251999
step: 9600, Loss: 0.13704730570316315
step: 9700, Loss: 0.12754642963409424
step: 9800, Loss: 0.13596494495868683
step: 9900, Loss: 0.12365342676639557
training successfully ended.
validating...
validate data length:87
acc: 0.9625
precision: 0.9705882352941176
recall: 0.9428571428571428
F_score: 0.9565217391304348
subject 28 Avgacc: 0.9460227272727272 Avgfscore: 0.949638611399825 
 Max acc:1.0, Max f score:1.0
******** mix subject_29 ********

[361, 399]
******fold 1******

Training... train_data length:684
step: 0, Loss: 65.847412109375
step: 100, Loss: 5.342535972595215
step: 200, Loss: 0.8187264204025269
step: 300, Loss: 2.8219077587127686
step: 400, Loss: 0.26744264364242554
step: 500, Loss: 0.1746917963027954
step: 600, Loss: 0.1597452014684677
step: 700, Loss: 0.14022454619407654
step: 800, Loss: 0.13959796726703644
step: 900, Loss: 0.1557759940624237
step: 1000, Loss: 0.1424574851989746
step: 1100, Loss: 0.1414995789527893
step: 1200, Loss: 0.15352967381477356
step: 1300, Loss: 0.1414136290550232
step: 1400, Loss: 0.14566335082054138
step: 1500, Loss: 0.21688102185726166
step: 1600, Loss: 0.133474662899971
step: 1700, Loss: 0.139542356133461
step: 1800, Loss: 0.1482929140329361
step: 1900, Loss: 0.1276094913482666
step: 2000, Loss: 0.14496658742427826
step: 2100, Loss: 0.13339687883853912
step: 2200, Loss: 0.12659776210784912
step: 2300, Loss: 0.12196904420852661
step: 2400, Loss: 0.1284874826669693
step: 2500, Loss: 0.12688684463500977
step: 2600, Loss: 0.12127828598022461
step: 2700, Loss: 0.11830055713653564
step: 2800, Loss: 0.12778611481189728
step: 2900, Loss: 0.12428127229213715
step: 3000, Loss: 0.12150277942419052
step: 3100, Loss: 0.12260453402996063
step: 3200, Loss: 0.12081630527973175
step: 3300, Loss: 0.1244562566280365
step: 3400, Loss: 0.1954878568649292
step: 3500, Loss: 0.11897452920675278
step: 3600, Loss: 0.12093251198530197
step: 3700, Loss: 0.12320242822170258
step: 3800, Loss: 0.11522345244884491
step: 3900, Loss: 0.1214243471622467
step: 4000, Loss: 0.11505070328712463
step: 4100, Loss: 0.11949001997709274
step: 4200, Loss: 0.11385276168584824
step: 4300, Loss: 0.11693235486745834
step: 4400, Loss: 0.11718433350324631
step: 4500, Loss: 0.1148051917552948
step: 4600, Loss: 0.11612313985824585
step: 4700, Loss: 0.11600998044013977
step: 4800, Loss: 0.11557662487030029
step: 4900, Loss: 0.11790226399898529
step: 5000, Loss: 0.11781750619411469
step: 5100, Loss: 0.1160869151353836
step: 5200, Loss: 0.1189025491476059
step: 5300, Loss: 0.19713953137397766
step: 5400, Loss: 0.1177009791135788
step: 5500, Loss: 0.12322540581226349
step: 5600, Loss: 0.11965413391590118
step: 5700, Loss: 0.12011463940143585
step: 5800, Loss: 0.1328253597021103
step: 5900, Loss: 0.11643574386835098
step: 6000, Loss: 0.11916902661323547
step: 6100, Loss: 0.13025599718093872
step: 6200, Loss: 1.4793322086334229
step: 6300, Loss: 0.14783832430839539
step: 6400, Loss: 0.1541818082332611
step: 6500, Loss: 0.14311583340168
step: 6600, Loss: 0.14295083284378052
step: 6700, Loss: 0.1342107653617859
step: 6800, Loss: 0.1298341155052185
step: 6900, Loss: 0.12457980215549469
step: 7000, Loss: 0.12897472083568573
step: 7100, Loss: 0.12309147417545319
step: 7200, Loss: 0.20193766057491302
step: 7300, Loss: 0.12477162480354309
step: 7400, Loss: 0.12770962715148926
step: 7500, Loss: 0.12310519814491272
step: 7600, Loss: 0.11962302029132843
step: 7700, Loss: 0.11793212592601776
step: 7800, Loss: 0.11779564619064331
step: 7900, Loss: 0.11495950818061829
step: 8000, Loss: 0.11817894876003265
step: 8100, Loss: 0.11851537227630615
step: 8200, Loss: 0.1175171509385109
step: 8300, Loss: 0.12318220734596252
step: 8400, Loss: 0.1185273677110672
step: 8500, Loss: 0.12079767137765884
step: 8600, Loss: 0.11843854188919067
step: 8700, Loss: 0.11550144106149673
step: 8800, Loss: 0.11694292724132538
step: 8900, Loss: 0.11681902408599854
step: 9000, Loss: 0.11635103821754456
step: 9100, Loss: 0.19446612894535065
step: 9200, Loss: 0.11677493155002594
step: 9300, Loss: 0.11583296209573746
step: 9400, Loss: 0.11670892685651779
step: 9500, Loss: 0.11539611220359802
step: 9600, Loss: 0.11483410000801086
step: 9700, Loss: 0.11531215161085129
step: 9800, Loss: 0.11430639028549194
step: 9900, Loss: 0.11440782994031906
training successfully ended.
validating...
validate data length:76
acc: 0.7916666666666666
precision: 0.717948717948718
recall: 0.875
F_score: 0.7887323943661971
******fold 2******

Training... train_data length:684
step: 0, Loss: 3.7205862998962402
step: 100, Loss: 0.17161163687705994
step: 200, Loss: 0.13563798367977142
step: 300, Loss: 0.12259244918823242
step: 400, Loss: 0.12433990836143494
step: 500, Loss: 0.11798274517059326
step: 600, Loss: 0.11815553903579712
step: 700, Loss: 0.11881913244724274
step: 800, Loss: 0.1187002956867218
step: 900, Loss: 0.11591757833957672
step: 1000, Loss: 0.11402327567338943
step: 1100, Loss: 0.11472689360380173
step: 1200, Loss: 0.11423905938863754
step: 1300, Loss: 0.11564236879348755
step: 1400, Loss: 0.11554279178380966
step: 1500, Loss: 0.19402457773685455
step: 1600, Loss: 0.11717076599597931
step: 1700, Loss: 0.11435896158218384
step: 1800, Loss: 0.11765316128730774
step: 1900, Loss: 0.11358892917633057
step: 2000, Loss: 0.11358743906021118
step: 2100, Loss: 0.11464960873126984
step: 2200, Loss: 0.11340934038162231
step: 2300, Loss: 0.1143346056342125
step: 2400, Loss: 0.1152791976928711
step: 2500, Loss: 0.1142270565032959
step: 2600, Loss: 0.11385478079319
step: 2700, Loss: 0.1139829233288765
step: 2800, Loss: 0.11631736904382706
step: 2900, Loss: 0.11462301015853882
step: 3000, Loss: 0.11385205388069153
step: 3100, Loss: 0.11444272100925446
step: 3200, Loss: 0.11471233516931534
step: 3300, Loss: 0.11652116477489471
step: 3400, Loss: 0.19590197503566742
step: 3500, Loss: 0.11500588804483414
step: 3600, Loss: 0.11866030097007751
step: 3700, Loss: 0.1160176694393158
step: 3800, Loss: 0.11819025874137878
step: 3900, Loss: 0.11455904692411423
step: 4000, Loss: 0.1225600317120552
step: 4100, Loss: 0.11647606641054153
step: 4200, Loss: 0.11562333256006241
step: 4300, Loss: 0.11473935097455978
step: 4400, Loss: 0.11806564033031464
step: 4500, Loss: 0.11726737767457962
step: 4600, Loss: 0.11568985134363174
step: 4700, Loss: 1.9010272026062012
step: 4800, Loss: 0.3635907769203186
step: 4900, Loss: 0.1375538557767868
step: 5000, Loss: 0.12535658478736877
step: 5100, Loss: 0.13445764780044556
step: 5200, Loss: 0.1294962465763092
step: 5300, Loss: 0.20429135859012604
step: 5400, Loss: 0.12345804274082184
step: 5500, Loss: 0.12440728396177292
step: 5600, Loss: 0.12294991314411163
step: 5700, Loss: 0.1187201738357544
step: 5800, Loss: 0.11904631555080414
step: 5900, Loss: 0.1244068592786789
step: 6000, Loss: 0.11986039578914642
step: 6100, Loss: 0.11922727525234222
step: 6200, Loss: 0.11767563968896866
step: 6300, Loss: 0.11981513351202011
step: 6400, Loss: 0.12081097066402435
step: 6500, Loss: 0.11547421663999557
step: 6600, Loss: 0.11953235417604446
step: 6700, Loss: 0.1198112815618515
step: 6800, Loss: 0.11642605066299438
step: 6900, Loss: 0.11879298835992813
step: 7000, Loss: 0.11605609953403473
step: 7100, Loss: 0.11718783527612686
step: 7200, Loss: 0.20193271338939667
step: 7300, Loss: 0.11623355001211166
step: 7400, Loss: 0.11473008245229721
step: 7500, Loss: 0.113206647336483
step: 7600, Loss: 0.11374273151159286
step: 7700, Loss: 0.11568906158208847
step: 7800, Loss: 0.11723320186138153
step: 7900, Loss: 0.11505154520273209
step: 8000, Loss: 0.11349411308765411
step: 8100, Loss: 0.11311865597963333
step: 8200, Loss: 0.11494384706020355
step: 8300, Loss: 0.11376449465751648
step: 8400, Loss: 0.11458419263362885
step: 8500, Loss: 0.11494262516498566
step: 8600, Loss: 0.1142757311463356
step: 8700, Loss: 0.11431090533733368
step: 8800, Loss: 0.11410503089427948
step: 8900, Loss: 0.11475837230682373
step: 9000, Loss: 0.11409167945384979
step: 9100, Loss: 0.19385355710983276
step: 9200, Loss: 0.11349144577980042
step: 9300, Loss: 0.11326770484447479
step: 9400, Loss: 0.11412886530160904
step: 9500, Loss: 0.11379093676805496
step: 9600, Loss: 0.11332183331251144
step: 9700, Loss: 0.11536653339862823
step: 9800, Loss: 0.11497706174850464
step: 9900, Loss: 0.11430305242538452
training successfully ended.
validating...
validate data length:76
acc: 0.9305555555555556
precision: 0.868421052631579
recall: 1.0
F_score: 0.9295774647887324
******fold 3******

Training... train_data length:684
step: 0, Loss: 1.019416332244873
step: 100, Loss: 0.12148704379796982
step: 200, Loss: 0.12197145819664001
step: 300, Loss: 0.11978268623352051
step: 400, Loss: 0.11505290865898132
step: 500, Loss: 0.11508405208587646
step: 600, Loss: 0.11411277949810028
step: 700, Loss: 0.11453890800476074
step: 800, Loss: 0.11562159657478333
step: 900, Loss: 0.11599087715148926
step: 1000, Loss: 0.1161884218454361
step: 1100, Loss: 0.11370670795440674
step: 1200, Loss: 0.11627376824617386
step: 1300, Loss: 0.11411411315202713
step: 1400, Loss: 0.11380260437726974
step: 1500, Loss: 0.19067034125328064
step: 1600, Loss: 0.11254533380270004
step: 1700, Loss: 0.11364971101284027
step: 1800, Loss: 0.11382369697093964
step: 1900, Loss: 0.1135619729757309
step: 2000, Loss: 0.1129685789346695
step: 2100, Loss: 0.11406116187572479
step: 2200, Loss: 0.11352261155843735
step: 2300, Loss: 0.11403128504753113
step: 2400, Loss: 0.11428001523017883
step: 2500, Loss: 0.11352992057800293
step: 2600, Loss: 0.11514054238796234
step: 2700, Loss: 0.11342180520296097
step: 2800, Loss: 0.11329956352710724
step: 2900, Loss: 0.11357399076223373
step: 3000, Loss: 0.11273451894521713
step: 3100, Loss: 0.1144687607884407
step: 3200, Loss: 0.11410392820835114
step: 3300, Loss: 0.11661218851804733
step: 3400, Loss: 0.1971977949142456
step: 3500, Loss: 0.11410270631313324
step: 3600, Loss: 0.11639879643917084
step: 3700, Loss: 0.11487391591072083
step: 3800, Loss: 0.1137426570057869
step: 3900, Loss: 0.1138828694820404
step: 4000, Loss: 0.11531179398298264
step: 4100, Loss: 0.1179167851805687
step: 4200, Loss: 0.11437708884477615
step: 4300, Loss: 0.11425676941871643
step: 4400, Loss: 0.11596585810184479
step: 4500, Loss: 0.11555672436952591
step: 4600, Loss: 0.12254152446985245
step: 4700, Loss: 0.17082667350769043
step: 4800, Loss: 0.1416640430688858
step: 4900, Loss: 0.13096469640731812
step: 5000, Loss: 0.1315702497959137
step: 5100, Loss: 0.1291802078485489
step: 5200, Loss: 0.1208023726940155
step: 5300, Loss: 0.20669928193092346
step: 5400, Loss: 0.12426106631755829
step: 5500, Loss: 0.12216392904520035
step: 5600, Loss: 0.11876094341278076
step: 5700, Loss: 0.11740484833717346
step: 5800, Loss: 0.118857242166996
step: 5900, Loss: 0.11774817109107971
step: 6000, Loss: 0.1204308271408081
step: 6100, Loss: 0.12111853063106537
step: 6200, Loss: 0.11699049174785614
step: 6300, Loss: 0.11879682540893555
step: 6400, Loss: 0.1159849613904953
step: 6500, Loss: 0.11840304732322693
step: 6600, Loss: 0.11935541033744812
step: 6700, Loss: 0.11590680480003357
step: 6800, Loss: 0.11420739442110062
step: 6900, Loss: 0.11618119478225708
step: 7000, Loss: 0.11563485860824585
step: 7100, Loss: 0.11461108922958374
step: 7200, Loss: 0.19748248159885406
step: 7300, Loss: 0.1153552383184433
step: 7400, Loss: 0.11487990617752075
step: 7500, Loss: 0.11650829762220383
step: 7600, Loss: 0.11361563205718994
step: 7700, Loss: 0.11372753232717514
step: 7800, Loss: 0.11366455256938934
step: 7900, Loss: 0.11326593160629272
step: 8000, Loss: 0.11606765538454056
step: 8100, Loss: 0.11485444009304047
step: 8200, Loss: 0.1139482855796814
step: 8300, Loss: 0.11338132619857788
step: 8400, Loss: 0.11592993885278702
step: 8500, Loss: 0.11411622911691666
step: 8600, Loss: 0.11457929015159607
step: 8700, Loss: 0.11420990526676178
step: 8800, Loss: 0.11352963745594025
step: 8900, Loss: 0.11431697010993958
step: 9000, Loss: 0.11304356902837753
step: 9100, Loss: 0.19580769538879395
step: 9200, Loss: 0.11357368528842926
step: 9300, Loss: 0.11293391138315201
step: 9400, Loss: 0.11367280781269073
step: 9500, Loss: 0.11328083276748657
step: 9600, Loss: 0.11470437049865723
step: 9700, Loss: 0.113446906208992
step: 9800, Loss: 0.11378314346075058
step: 9900, Loss: 0.11328573524951935
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9705882352941176
recall: 1.0
F_score: 0.9850746268656716
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.11728939414024353
step: 100, Loss: 0.1217230036854744
step: 200, Loss: 0.11414174735546112
step: 300, Loss: 0.11779233068227768
step: 400, Loss: 0.11535026133060455
step: 500, Loss: 0.11600936204195023
step: 600, Loss: 0.11420704424381256
step: 700, Loss: 0.11452771723270416
step: 800, Loss: 0.11550289392471313
step: 900, Loss: 0.11414717137813568
step: 1000, Loss: 0.11555066704750061
step: 1100, Loss: 0.11285707354545593
step: 1200, Loss: 0.1143195778131485
step: 1300, Loss: 0.1141858696937561
step: 1400, Loss: 0.11418034136295319
step: 1500, Loss: 0.19407758116722107
step: 1600, Loss: 0.1132434532046318
step: 1700, Loss: 0.11494635045528412
step: 1800, Loss: 0.11341933161020279
step: 1900, Loss: 0.11393919587135315
step: 2000, Loss: 0.11365333199501038
step: 2100, Loss: 0.11393529176712036
step: 2200, Loss: 0.11449822783470154
step: 2300, Loss: 0.11441756784915924
step: 2400, Loss: 0.11392111331224442
step: 2500, Loss: 0.11335904896259308
step: 2600, Loss: 0.1140303760766983
step: 2700, Loss: 0.12084842473268509
step: 2800, Loss: 0.1152527779340744
step: 2900, Loss: 0.11394960433244705
step: 3000, Loss: 0.11462215334177017
step: 3100, Loss: 0.11357752233743668
step: 3200, Loss: 0.11483252048492432
step: 3300, Loss: 0.11793722212314606
step: 3400, Loss: 0.19839370250701904
step: 3500, Loss: 0.11554378271102905
step: 3600, Loss: 0.11441100388765335
step: 3700, Loss: 0.11518798768520355
step: 3800, Loss: 0.11445731669664383
step: 3900, Loss: 0.11434035003185272
step: 4000, Loss: 0.11462849378585815
step: 4100, Loss: 0.11528836190700531
step: 4200, Loss: 0.11562931537628174
step: 4300, Loss: 0.12449049949645996
step: 4400, Loss: 0.3676031529903412
step: 4500, Loss: 0.15961281955242157
step: 4600, Loss: 0.12776149809360504
step: 4700, Loss: 0.12777335941791534
step: 4800, Loss: 0.1290508210659027
step: 4900, Loss: 0.12279729545116425
step: 5000, Loss: 0.12003014236688614
step: 5100, Loss: 0.1228654533624649
step: 5200, Loss: 0.12112920731306076
step: 5300, Loss: 0.2065459042787552
step: 5400, Loss: 0.12286645174026489
step: 5500, Loss: 0.12097953259944916
step: 5600, Loss: 0.11866143345832825
step: 5700, Loss: 0.12003649771213531
step: 5800, Loss: 0.11552676558494568
step: 5900, Loss: 0.11801618337631226
step: 6000, Loss: 0.11689739674329758
step: 6100, Loss: 0.11840225756168365
step: 6200, Loss: 0.11851577460765839
step: 6300, Loss: 0.11660237610340118
step: 6400, Loss: 0.11552616953849792
step: 6500, Loss: 0.11458026617765427
step: 6600, Loss: 0.11741957068443298
step: 6700, Loss: 0.11789065599441528
step: 6800, Loss: 0.11455509066581726
step: 6900, Loss: 0.11591653525829315
step: 7000, Loss: 0.11436010897159576
step: 7100, Loss: 0.11504945904016495
step: 7200, Loss: 0.195939302444458
step: 7300, Loss: 0.11516285687685013
step: 7400, Loss: 0.11518608033657074
step: 7500, Loss: 0.11634994298219681
step: 7600, Loss: 0.11514432728290558
step: 7700, Loss: 0.11574238538742065
step: 7800, Loss: 0.11440970003604889
step: 7900, Loss: 0.11502508819103241
step: 8000, Loss: 0.11505246162414551
step: 8100, Loss: 0.11633247137069702
step: 8200, Loss: 0.1137646958231926
step: 8300, Loss: 0.11476507782936096
step: 8400, Loss: 0.1126801073551178
step: 8500, Loss: 0.11346495151519775
step: 8600, Loss: 0.11464522778987885
step: 8700, Loss: 0.11256635934114456
step: 8800, Loss: 0.11336326599121094
step: 8900, Loss: 0.11459346860647202
step: 9000, Loss: 0.11528517305850983
step: 9100, Loss: 0.19610802829265594
step: 9200, Loss: 0.11437041312456131
step: 9300, Loss: 0.11349626630544662
step: 9400, Loss: 0.1152333915233612
step: 9500, Loss: 0.11406829953193665
step: 9600, Loss: 0.11366245895624161
step: 9700, Loss: 0.1141815334558487
step: 9800, Loss: 0.11417919397354126
step: 9900, Loss: 0.11451595276594162
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.926829268292683
recall: 1.0
F_score: 0.9620253164556963
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.11607919633388519
step: 100, Loss: 0.11813925951719284
step: 200, Loss: 0.11647561192512512
step: 300, Loss: 0.11690431833267212
step: 400, Loss: 0.11454175412654877
step: 500, Loss: 0.11449097096920013
step: 600, Loss: 0.11439904570579529
step: 700, Loss: 0.11392708867788315
step: 800, Loss: 0.11427682638168335
step: 900, Loss: 0.11591536551713943
step: 1000, Loss: 0.11353304982185364
step: 1100, Loss: 0.11326589435338974
step: 1200, Loss: 0.1134926825761795
step: 1300, Loss: 0.11385991424322128
step: 1400, Loss: 0.1129862517118454
step: 1500, Loss: 0.1978074163198471
step: 1600, Loss: 0.11432415246963501
step: 1700, Loss: 0.11439595371484756
step: 1800, Loss: 0.11431501805782318
step: 1900, Loss: 0.11378414183855057
step: 2000, Loss: 0.11324889957904816
step: 2100, Loss: 0.11347562819719315
step: 2200, Loss: 0.11588982492685318
step: 2300, Loss: 0.11490964144468307
step: 2400, Loss: 0.11516977846622467
step: 2500, Loss: 0.11553686112165451
step: 2600, Loss: 0.11309777200222015
step: 2700, Loss: 0.11513657867908478
step: 2800, Loss: 0.11561623960733414
step: 2900, Loss: 0.11589302122592926
step: 3000, Loss: 0.11431902647018433
step: 3100, Loss: 0.11608374118804932
step: 3200, Loss: 0.11401888728141785
step: 3300, Loss: 0.11516303569078445
step: 3400, Loss: 0.1908092200756073
step: 3500, Loss: 0.11484869569540024
step: 3600, Loss: 0.11485278606414795
step: 3700, Loss: 0.11571937799453735
step: 3800, Loss: 0.11382351815700531
step: 3900, Loss: 0.11353994905948639
step: 4000, Loss: 0.11349870264530182
step: 4100, Loss: 0.17240740358829498
step: 4200, Loss: 0.14074349403381348
step: 4300, Loss: 0.13526861369609833
step: 4400, Loss: 0.13598740100860596
step: 4500, Loss: 0.12281526625156403
step: 4600, Loss: 0.1247008740901947
step: 4700, Loss: 0.12541866302490234
step: 4800, Loss: 0.12254343181848526
step: 4900, Loss: 0.12054093182086945
step: 5000, Loss: 0.12400613725185394
step: 5100, Loss: 0.11987648159265518
step: 5200, Loss: 0.11652085185050964
step: 5300, Loss: 0.2121182531118393
step: 5400, Loss: 0.11742731183767319
step: 5500, Loss: 0.11582572013139725
step: 5600, Loss: 0.11677716672420502
step: 5700, Loss: 0.11581788212060928
step: 5800, Loss: 0.11504745483398438
step: 5900, Loss: 0.1172877848148346
step: 6000, Loss: 0.12124155461788177
step: 6100, Loss: 0.11912597715854645
step: 6200, Loss: 0.11562158912420273
step: 6300, Loss: 0.11605198681354523
step: 6400, Loss: 0.1152224987745285
step: 6500, Loss: 0.11591962724924088
step: 6600, Loss: 0.11778874695301056
step: 6700, Loss: 0.11488515138626099
step: 6800, Loss: 0.11385946720838547
step: 6900, Loss: 0.11415785551071167
step: 7000, Loss: 0.11566600948572159
step: 7100, Loss: 0.1129661276936531
step: 7200, Loss: 0.19559240341186523
step: 7300, Loss: 0.11378362029790878
step: 7400, Loss: 0.11628542095422745
step: 7500, Loss: 0.11488841474056244
step: 7600, Loss: 0.11289709806442261
step: 7700, Loss: 0.1138753816485405
step: 7800, Loss: 0.11429890245199203
step: 7900, Loss: 0.11395907402038574
step: 8000, Loss: 0.11465436965227127
step: 8100, Loss: 0.11351998150348663
step: 8200, Loss: 0.11334899067878723
step: 8300, Loss: 0.1145487055182457
step: 8400, Loss: 0.11341547220945358
step: 8500, Loss: 0.11460518091917038
step: 8600, Loss: 0.11293580383062363
step: 8700, Loss: 0.11353825032711029
step: 8800, Loss: 0.11390893906354904
step: 8900, Loss: 0.11341790109872818
step: 9000, Loss: 0.11331192404031754
step: 9100, Loss: 0.1919381320476532
step: 9200, Loss: 0.11406956613063812
step: 9300, Loss: 0.11310546100139618
step: 9400, Loss: 0.11265299469232559
step: 9500, Loss: 0.11459749937057495
step: 9600, Loss: 0.11357424408197403
step: 9700, Loss: 0.11357671022415161
step: 9800, Loss: 0.11423449218273163
step: 9900, Loss: 0.11339549720287323
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 1.0
recall: 0.9722222222222222
F_score: 0.9859154929577464
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.11805274337530136
step: 100, Loss: 0.11694052815437317
step: 200, Loss: 0.11505424231290817
step: 300, Loss: 0.11542622745037079
step: 400, Loss: 0.11414463818073273
step: 500, Loss: 0.11583492159843445
step: 600, Loss: 0.1147158220410347
step: 700, Loss: 0.11317752301692963
step: 800, Loss: 0.11331325769424438
step: 900, Loss: 0.11478904634714127
step: 1000, Loss: 0.11711946129798889
step: 1100, Loss: 0.11472521722316742
step: 1200, Loss: 0.11305050551891327
step: 1300, Loss: 0.11377492547035217
step: 1400, Loss: 0.11464974284172058
step: 1500, Loss: 0.19219286739826202
step: 1600, Loss: 0.11363610625267029
step: 1700, Loss: 0.11471793800592422
step: 1800, Loss: 0.11303045600652695
step: 1900, Loss: 0.11438356339931488
step: 2000, Loss: 0.11447486281394958
step: 2100, Loss: 0.11392058432102203
step: 2200, Loss: 0.11448248475790024
step: 2300, Loss: 0.11589647829532623
step: 2400, Loss: 0.11497309058904648
step: 2500, Loss: 0.11400462687015533
step: 2600, Loss: 0.11388811469078064
step: 2700, Loss: 0.11413779854774475
step: 2800, Loss: 0.11615939438343048
step: 2900, Loss: 0.11812073737382889
step: 3000, Loss: 0.11379922926425934
step: 3100, Loss: 0.11716680228710175
step: 3200, Loss: 0.1139550432562828
step: 3300, Loss: 0.5709951519966125
step: 3400, Loss: 0.2291342318058014
step: 3500, Loss: 0.1318356692790985
step: 3600, Loss: 0.12347462028265
step: 3700, Loss: 0.12105350196361542
step: 3800, Loss: 0.12425874173641205
step: 3900, Loss: 0.1178368628025055
step: 4000, Loss: 0.11993549764156342
step: 4100, Loss: 0.12231654673814774
step: 4200, Loss: 0.11973967403173447
step: 4300, Loss: 0.11606603860855103
step: 4400, Loss: 0.11877581477165222
step: 4500, Loss: 0.1191546842455864
step: 4600, Loss: 0.11897967755794525
step: 4700, Loss: 0.11837508529424667
step: 4800, Loss: 0.12088262289762497
step: 4900, Loss: 0.11977199465036392
step: 5000, Loss: 0.11619867384433746
step: 5100, Loss: 0.11742789298295975
step: 5200, Loss: 0.11612913012504578
step: 5300, Loss: 0.19913873076438904
step: 5400, Loss: 0.1154555082321167
step: 5500, Loss: 0.11648960411548615
step: 5600, Loss: 0.11647610366344452
step: 5700, Loss: 0.11546511948108673
step: 5800, Loss: 0.11419306695461273
step: 5900, Loss: 0.11282084882259369
step: 6000, Loss: 0.11509590595960617
step: 6100, Loss: 0.1143937036395073
step: 6200, Loss: 0.11397622525691986
step: 6300, Loss: 0.11511129140853882
step: 6400, Loss: 0.11580921709537506
step: 6500, Loss: 0.11330965161323547
step: 6600, Loss: 0.11270204931497574
step: 6700, Loss: 0.11432216316461563
step: 6800, Loss: 0.11466791480779648
step: 6900, Loss: 0.11405269801616669
step: 7000, Loss: 0.1149112656712532
step: 7100, Loss: 0.11325845867395401
step: 7200, Loss: 0.1953979730606079
step: 7300, Loss: 0.11395660042762756
step: 7400, Loss: 0.11458715796470642
step: 7500, Loss: 0.11418455094099045
step: 7600, Loss: 0.11359066516160965
step: 7700, Loss: 0.11430759727954865
step: 7800, Loss: 0.11337948590517044
step: 7900, Loss: 0.11286088824272156
step: 8000, Loss: 0.11340200901031494
step: 8100, Loss: 0.11305347830057144
step: 8200, Loss: 0.11395574361085892
step: 8300, Loss: 0.11347182095050812
step: 8400, Loss: 0.11394073814153671
step: 8500, Loss: 0.11439600586891174
step: 8600, Loss: 0.11244265735149384
step: 8700, Loss: 0.11349434405565262
step: 8800, Loss: 0.11381644010543823
step: 8900, Loss: 0.11477088928222656
step: 9000, Loss: 0.11464641988277435
step: 9100, Loss: 0.19271863996982574
step: 9200, Loss: 0.11347399652004242
step: 9300, Loss: 0.11377459764480591
step: 9400, Loss: 0.11342475563287735
step: 9500, Loss: 0.11403223872184753
step: 9600, Loss: 0.1134469211101532
step: 9700, Loss: 0.11279784142971039
step: 9800, Loss: 0.11485517024993896
step: 9900, Loss: 0.11370062828063965
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9210526315789473
recall: 1.0
F_score: 0.958904109589041
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.11641930043697357
step: 100, Loss: 0.12205333262681961
step: 200, Loss: 0.11722055077552795
step: 300, Loss: 0.11509287357330322
step: 400, Loss: 0.11483220756053925
step: 500, Loss: 0.11634030193090439
step: 600, Loss: 0.1148504689335823
step: 700, Loss: 0.11415527015924454
step: 800, Loss: 0.11433834582567215
step: 900, Loss: 0.1147889494895935
step: 1000, Loss: 0.11356355249881744
step: 1100, Loss: 0.1131434366106987
step: 1200, Loss: 0.11243144422769547
step: 1300, Loss: 0.1143319234251976
step: 1400, Loss: 0.11393903195858002
step: 1500, Loss: 0.19224153459072113
step: 1600, Loss: 0.11344072222709656
step: 1700, Loss: 0.11461453884840012
step: 1800, Loss: 0.1146344244480133
step: 1900, Loss: 0.1149521991610527
step: 2000, Loss: 0.11374060064554214
step: 2100, Loss: 0.11455035209655762
step: 2200, Loss: 0.1143864169716835
step: 2300, Loss: 0.11370849609375
step: 2400, Loss: 0.11419369280338287
step: 2500, Loss: 0.11286511272192001
step: 2600, Loss: 0.11452075093984604
step: 2700, Loss: 0.11359038203954697
step: 2800, Loss: 0.11389937996864319
step: 2900, Loss: 0.11598299443721771
step: 3000, Loss: 0.11412446945905685
step: 3100, Loss: 0.11422067880630493
step: 3200, Loss: 0.11469243466854095
step: 3300, Loss: 0.11518614739179611
step: 3400, Loss: 0.19644448161125183
step: 3500, Loss: 0.1162554994225502
step: 3600, Loss: 0.11364869773387909
step: 3700, Loss: 0.11442738771438599
step: 3800, Loss: 0.11437274515628815
step: 3900, Loss: 0.1139337494969368
step: 4000, Loss: 0.11341620981693268
step: 4100, Loss: 0.11466838419437408
step: 4200, Loss: 0.11372044682502747
step: 4300, Loss: 0.11463379114866257
step: 4400, Loss: 0.11508015543222427
step: 4500, Loss: 0.11475323140621185
step: 4600, Loss: 0.11907468736171722
step: 4700, Loss: 0.11586668342351913
step: 4800, Loss: 0.11413886398077011
step: 4900, Loss: 0.11563672870397568
step: 5000, Loss: 0.11394680291414261
step: 5100, Loss: 0.11288285255432129
step: 5200, Loss: 0.11644896119832993
step: 5300, Loss: 0.21958941221237183
step: 5400, Loss: 0.15147502720355988
step: 5500, Loss: 0.1363886147737503
step: 5600, Loss: 0.12445603311061859
step: 5700, Loss: 0.1184137836098671
step: 5800, Loss: 0.12653841078281403
step: 5900, Loss: 0.12130250036716461
step: 6000, Loss: 0.1269432008266449
step: 6100, Loss: 0.11832180619239807
step: 6200, Loss: 0.12136362493038177
step: 6300, Loss: 0.12130513042211533
step: 6400, Loss: 0.11952638626098633
step: 6500, Loss: 0.1180974543094635
step: 6600, Loss: 0.11815626919269562
step: 6700, Loss: 0.12066534161567688
step: 6800, Loss: 0.11882976442575455
step: 6900, Loss: 0.11921937018632889
step: 7000, Loss: 0.11743565648794174
step: 7100, Loss: 0.11437788605690002
step: 7200, Loss: 0.1941724717617035
step: 7300, Loss: 0.1169719323515892
step: 7400, Loss: 0.11502158641815186
step: 7500, Loss: 0.1151682510972023
step: 7600, Loss: 0.11422640830278397
step: 7700, Loss: 0.11571557074785233
step: 7800, Loss: 0.11558453738689423
step: 7900, Loss: 0.11569203436374664
step: 8000, Loss: 0.11561219394207001
step: 8100, Loss: 0.11749938130378723
step: 8200, Loss: 0.1154116615653038
step: 8300, Loss: 0.11426541954278946
step: 8400, Loss: 0.11437729746103287
step: 8500, Loss: 0.11527673155069351
step: 8600, Loss: 0.11493846774101257
step: 8700, Loss: 0.11533503234386444
step: 8800, Loss: 0.11522582918405533
step: 8900, Loss: 0.11539754271507263
step: 9000, Loss: 0.11466050893068314
step: 9100, Loss: 0.1919003129005432
step: 9200, Loss: 0.115780770778656
step: 9300, Loss: 0.1154283732175827
step: 9400, Loss: 0.11353252083063126
step: 9500, Loss: 0.11415527760982513
step: 9600, Loss: 0.1140645295381546
step: 9700, Loss: 0.1147349625825882
step: 9800, Loss: 0.11478972434997559
step: 9900, Loss: 0.11316129565238953
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.9354838709677419
recall: 0.9354838709677419
F_score: 0.9354838709677419
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.11551813781261444
step: 100, Loss: 0.11824868619441986
step: 200, Loss: 0.11521416902542114
step: 300, Loss: 0.11562256515026093
step: 400, Loss: 0.11418065428733826
step: 500, Loss: 0.11518301069736481
step: 600, Loss: 0.1145496517419815
step: 700, Loss: 0.11449085175991058
step: 800, Loss: 0.11326304823160172
step: 900, Loss: 0.1138404831290245
step: 1000, Loss: 0.11517306417226791
step: 1100, Loss: 0.11424722522497177
step: 1200, Loss: 0.11312636733055115
step: 1300, Loss: 0.11503042280673981
step: 1400, Loss: 0.11303974688053131
step: 1500, Loss: 0.19341357052326202
step: 1600, Loss: 0.11322089284658432
step: 1700, Loss: 0.11451555788516998
step: 1800, Loss: 0.11363716423511505
step: 1900, Loss: 0.11391302198171616
step: 2000, Loss: 0.11401307582855225
step: 2100, Loss: 0.1131545752286911
step: 2200, Loss: 0.11413101106882095
step: 2300, Loss: 0.11370284110307693
step: 2400, Loss: 0.11387953907251358
step: 2500, Loss: 0.11456407606601715
step: 2600, Loss: 0.11448786407709122
step: 2700, Loss: 0.11514479666948318
step: 2800, Loss: 0.11894682049751282
step: 2900, Loss: 0.11762005090713501
step: 3000, Loss: 0.9420509934425354
step: 3100, Loss: 0.15805993974208832
step: 3200, Loss: 0.1446436047554016
step: 3300, Loss: 0.12843412160873413
step: 3400, Loss: 0.2231179177761078
step: 3500, Loss: 0.12279299646615982
step: 3600, Loss: 0.11967696994543076
step: 3700, Loss: 0.12130974233150482
step: 3800, Loss: 0.1187056303024292
step: 3900, Loss: 0.11892515420913696
step: 4000, Loss: 0.12178396433591843
step: 4100, Loss: 0.1229206770658493
step: 4200, Loss: 0.11574804782867432
step: 4300, Loss: 0.11553709208965302
step: 4400, Loss: 0.11769778281450272
step: 4500, Loss: 0.11589199304580688
step: 4600, Loss: 0.11666266620159149
step: 4700, Loss: 0.11686506867408752
step: 4800, Loss: 0.11685343086719513
step: 4900, Loss: 0.11876803636550903
step: 5000, Loss: 0.11680351942777634
step: 5100, Loss: 0.11715031415224075
step: 5200, Loss: 0.11600303649902344
step: 5300, Loss: 0.20136961340904236
step: 5400, Loss: 0.11507762223482132
step: 5500, Loss: 0.11440611630678177
step: 5600, Loss: 0.11650104075670242
step: 5700, Loss: 0.11309515684843063
step: 5800, Loss: 0.11549722403287888
step: 5900, Loss: 0.11522338539361954
step: 6000, Loss: 0.11436909437179565
step: 6100, Loss: 0.11558900028467178
step: 6200, Loss: 0.11352113634347916
step: 6300, Loss: 0.11451993137598038
step: 6400, Loss: 0.11381690949201584
step: 6500, Loss: 0.11462323367595673
step: 6600, Loss: 0.113362155854702
step: 6700, Loss: 0.1144045889377594
step: 6800, Loss: 0.11471953988075256
step: 6900, Loss: 0.11356920748949051
step: 7000, Loss: 0.11481326073408127
step: 7100, Loss: 0.1139102429151535
step: 7200, Loss: 0.19271861016750336
step: 7300, Loss: 0.11297406256198883
step: 7400, Loss: 0.11471499502658844
step: 7500, Loss: 0.11574067920446396
step: 7600, Loss: 0.11429708451032639
step: 7700, Loss: 0.11341937631368637
step: 7800, Loss: 0.11340038478374481
step: 7900, Loss: 0.11419951915740967
step: 8000, Loss: 0.11435067653656006
step: 8100, Loss: 0.11436359584331512
step: 8200, Loss: 0.11328151822090149
step: 8300, Loss: 0.1135999858379364
step: 8400, Loss: 0.11315912008285522
step: 8500, Loss: 0.11396404355764389
step: 8600, Loss: 0.11310084164142609
step: 8700, Loss: 0.11396872252225876
step: 8800, Loss: 0.11496633291244507
step: 8900, Loss: 0.11395841836929321
step: 9000, Loss: 0.11354587227106094
step: 9100, Loss: 0.19288967549800873
step: 9200, Loss: 0.11322437226772308
step: 9300, Loss: 0.11318439245223999
step: 9400, Loss: 0.1146366074681282
step: 9500, Loss: 0.11331614851951599
step: 9600, Loss: 0.11398843675851822
step: 9700, Loss: 0.11523047089576721
step: 9800, Loss: 0.11494217813014984
step: 9900, Loss: 0.11654920876026154
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9705882352941176
recall: 1.0
F_score: 0.9850746268656716
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.11581071466207504
step: 100, Loss: 0.11990593373775482
step: 200, Loss: 0.11533866822719574
step: 300, Loss: 0.11703532934188843
step: 400, Loss: 0.11440644413232803
step: 500, Loss: 0.11412746459245682
step: 600, Loss: 0.1138198971748352
step: 700, Loss: 0.11568896472454071
step: 800, Loss: 0.1136850118637085
step: 900, Loss: 0.11495424807071686
step: 1000, Loss: 0.11519584059715271
step: 1100, Loss: 0.11391375213861465
step: 1200, Loss: 0.11316195130348206
step: 1300, Loss: 0.11464618891477585
step: 1400, Loss: 0.11352232098579407
step: 1500, Loss: 0.19212153553962708
step: 1600, Loss: 0.11369716376066208
step: 1700, Loss: 0.11466468870639801
step: 1800, Loss: 0.11362946778535843
step: 1900, Loss: 0.11390330642461777
step: 2000, Loss: 0.11363323032855988
step: 2100, Loss: 0.11373583972454071
step: 2200, Loss: 0.11412691324949265
step: 2300, Loss: 0.11394771933555603
step: 2400, Loss: 0.11780550330877304
step: 2500, Loss: 0.11492183059453964
step: 2600, Loss: 0.11437467485666275
step: 2700, Loss: 0.11385581642389297
step: 2800, Loss: 0.11411130428314209
step: 2900, Loss: 0.11429940164089203
step: 3000, Loss: 0.11418966948986053
step: 3100, Loss: 0.11461431533098221
step: 3200, Loss: 0.11375245451927185
step: 3300, Loss: 0.1157313659787178
step: 3400, Loss: 0.19692470133304596
step: 3500, Loss: 2.4323599338531494
step: 3600, Loss: 0.14894652366638184
step: 3700, Loss: 0.1276247650384903
step: 3800, Loss: 0.12171584367752075
step: 3900, Loss: 0.12709127366542816
step: 4000, Loss: 0.12137563526630402
step: 4100, Loss: 0.11979624629020691
step: 4200, Loss: 0.12432847917079926
step: 4300, Loss: 0.11883342266082764
step: 4400, Loss: 0.11829335987567902
step: 4500, Loss: 0.11848999559879303
step: 4600, Loss: 0.12434432655572891
step: 4700, Loss: 0.12215574830770493
step: 4800, Loss: 0.1180933341383934
step: 4900, Loss: 0.117919921875
step: 5000, Loss: 0.1197955310344696
step: 5100, Loss: 0.11832302808761597
step: 5200, Loss: 0.11693179607391357
step: 5300, Loss: 0.2018134444952011
step: 5400, Loss: 0.11612799763679504
step: 5500, Loss: 0.11926506459712982
step: 5600, Loss: 0.11543119698762894
step: 5700, Loss: 0.11519013345241547
step: 5800, Loss: 0.1169084906578064
step: 5900, Loss: 0.11480963230133057
step: 6000, Loss: 0.11539243906736374
step: 6100, Loss: 0.11713572591543198
step: 6200, Loss: 0.11588379740715027
step: 6300, Loss: 0.11566254496574402
step: 6400, Loss: 0.11421822011470795
step: 6500, Loss: 0.11396133899688721
step: 6600, Loss: 0.11403041332960129
step: 6700, Loss: 0.11400646716356277
step: 6800, Loss: 0.11645308136940002
step: 6900, Loss: 0.11340143531560898
step: 7000, Loss: 0.11418653279542923
step: 7100, Loss: 0.11458687484264374
step: 7200, Loss: 0.19253237545490265
step: 7300, Loss: 0.11382898688316345
step: 7400, Loss: 0.11485493183135986
step: 7500, Loss: 0.1136934757232666
step: 7600, Loss: 0.11492366343736649
step: 7700, Loss: 0.115432970225811
step: 7800, Loss: 0.113752081990242
step: 7900, Loss: 0.11415500938892365
step: 8000, Loss: 0.11432218551635742
step: 8100, Loss: 0.11478932201862335
step: 8200, Loss: 0.11330631375312805
step: 8300, Loss: 0.11380171030759811
step: 8400, Loss: 0.11374083161354065
step: 8500, Loss: 0.11380220204591751
step: 8600, Loss: 0.11375613510608673
step: 8700, Loss: 0.11368241906166077
step: 8800, Loss: 0.11400546133518219
step: 8900, Loss: 0.11266453564167023
step: 9000, Loss: 0.11449829488992691
step: 9100, Loss: 0.19309288263320923
step: 9200, Loss: 0.1138177216053009
step: 9300, Loss: 0.11331445723772049
step: 9400, Loss: 0.11418655514717102
step: 9500, Loss: 0.1141156554222107
step: 9600, Loss: 0.11314655095338821
step: 9700, Loss: 0.11357913166284561
step: 9800, Loss: 0.11305563896894455
step: 9900, Loss: 0.11662332713603973
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9142857142857143
recall: 1.0
F_score: 0.955223880597015
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11618831753730774
step: 100, Loss: 0.11798429489135742
step: 200, Loss: 0.11444029211997986
step: 300, Loss: 0.1161118745803833
step: 400, Loss: 0.11548279225826263
step: 500, Loss: 0.11476154625415802
step: 600, Loss: 0.11513460427522659
step: 700, Loss: 0.11541061103343964
step: 800, Loss: 0.11449667811393738
step: 900, Loss: 0.11419283598661423
step: 1000, Loss: 0.113724485039711
step: 1100, Loss: 0.11382528394460678
step: 1200, Loss: 0.11412112414836884
step: 1300, Loss: 0.11513163149356842
step: 1400, Loss: 0.114284448325634
step: 1500, Loss: 0.19247660040855408
step: 1600, Loss: 0.11423606425523758
step: 1700, Loss: 0.11307588964700699
step: 1800, Loss: 0.11374027281999588
step: 1900, Loss: 0.11393312364816666
step: 2000, Loss: 0.11389082670211792
step: 2100, Loss: 0.11301441490650177
step: 2200, Loss: 0.11435307562351227
step: 2300, Loss: 0.11419211328029633
step: 2400, Loss: 0.11292023211717606
step: 2500, Loss: 0.11346756666898727
step: 2600, Loss: 0.11388453841209412
step: 2700, Loss: 0.11433564126491547
step: 2800, Loss: 0.11419632285833359
step: 2900, Loss: 0.11730440706014633
step: 3000, Loss: 0.11439213156700134
step: 3100, Loss: 0.11535312235355377
step: 3200, Loss: 0.11702155321836472
step: 3300, Loss: 0.11798183619976044
step: 3400, Loss: 0.19517739117145538
step: 3500, Loss: 0.11450284719467163
step: 3600, Loss: 0.1152709499001503
step: 3700, Loss: 0.11477523297071457
step: 3800, Loss: 0.11345706880092621
step: 3900, Loss: 0.11547994613647461
step: 4000, Loss: 2.7017858028411865
step: 4100, Loss: 0.15494704246520996
step: 4200, Loss: 0.14401300251483917
step: 4300, Loss: 0.13457533717155457
step: 4400, Loss: 0.12388890981674194
step: 4500, Loss: 0.1320568025112152
step: 4600, Loss: 0.12389039993286133
step: 4700, Loss: 0.12119310349225998
step: 4800, Loss: 0.12071433663368225
step: 4900, Loss: 0.11967260390520096
step: 5000, Loss: 0.12223507463932037
step: 5100, Loss: 0.11950932443141937
step: 5200, Loss: 0.11835114657878876
step: 5300, Loss: 0.2025505155324936
step: 5400, Loss: 0.11905611306428909
step: 5500, Loss: 0.11775505542755127
step: 5600, Loss: 0.11433084309101105
step: 5700, Loss: 0.1191692054271698
step: 5800, Loss: 0.1153450757265091
step: 5900, Loss: 0.1184699684381485
step: 6000, Loss: 0.11534930765628815
step: 6100, Loss: 0.1172618567943573
step: 6200, Loss: 0.11663151532411575
step: 6300, Loss: 0.11636272072792053
step: 6400, Loss: 0.11697500944137573
step: 6500, Loss: 0.11509255319833755
step: 6600, Loss: 0.11564262211322784
step: 6700, Loss: 0.11668606102466583
step: 6800, Loss: 0.11598320305347443
step: 6900, Loss: 0.11560530960559845
step: 7000, Loss: 0.1154896542429924
step: 7100, Loss: 0.1138257086277008
step: 7200, Loss: 0.1962316483259201
step: 7300, Loss: 0.11323241889476776
step: 7400, Loss: 0.11404004693031311
step: 7500, Loss: 0.11344204097986221
step: 7600, Loss: 0.11383996158838272
step: 7700, Loss: 0.114326611161232
step: 7800, Loss: 0.11373772472143173
step: 7900, Loss: 0.1135173812508583
step: 8000, Loss: 0.11335766315460205
step: 8100, Loss: 0.11396211385726929
step: 8200, Loss: 0.11305122077465057
step: 8300, Loss: 0.11282161623239517
step: 8400, Loss: 0.11363265663385391
step: 8500, Loss: 0.11401136219501495
step: 8600, Loss: 0.11485069990158081
step: 8700, Loss: 0.11355482041835785
step: 8800, Loss: 0.11389287561178207
step: 8900, Loss: 0.11420635879039764
step: 9000, Loss: 0.11558939516544342
step: 9100, Loss: 0.1917891502380371
step: 9200, Loss: 0.11403949558734894
step: 9300, Loss: 0.11395255476236343
step: 9400, Loss: 0.11367732286453247
step: 9500, Loss: 0.11494103074073792
step: 9600, Loss: 0.11447449773550034
step: 9700, Loss: 0.11426340043544769
step: 9800, Loss: 0.11353462934494019
step: 9900, Loss: 0.11330960690975189
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9473684210526315
recall: 1.0
F_score: 0.972972972972973
subject 29 Avgacc: 0.9472222222222222 Avgfscore: 0.9458984756426487 
 Max acc:0.9861111111111112, Max f score:0.9859154929577464
******** mix subject_30 ********

[475, 285]
******fold 1******

Training... train_data length:855
step: 0, Loss: 33.60641860961914
step: 100, Loss: 0.6072736382484436
step: 200, Loss: 0.35305118560791016
step: 300, Loss: 0.17009536921977997
step: 400, Loss: 0.14713281393051147
step: 500, Loss: 0.1428949236869812
step: 600, Loss: 0.14198023080825806
step: 700, Loss: 0.13186964392662048
step: 800, Loss: 0.14037065207958221
step: 900, Loss: 0.1405785232782364
step: 1000, Loss: 0.1320478320121765
step: 1100, Loss: 0.12382657080888748
step: 1200, Loss: 0.12721729278564453
step: 1300, Loss: 0.12255068123340607
step: 1400, Loss: 0.12510065734386444
step: 1500, Loss: 0.12621071934700012
step: 1600, Loss: 0.12211672216653824
step: 1700, Loss: 0.12247312068939209
step: 1800, Loss: 0.12280435860157013
step: 1900, Loss: 0.11970977485179901
step: 2000, Loss: 0.124547079205513
step: 2100, Loss: 0.11938931792974472
step: 2200, Loss: 0.12229631096124649
step: 2300, Loss: 0.13204708695411682
step: 2400, Loss: 0.12043110281229019
step: 2500, Loss: 0.11924690008163452
step: 2600, Loss: 0.1232159286737442
step: 2700, Loss: 0.13143311440944672
step: 2800, Loss: 0.11979363858699799
step: 2900, Loss: 0.11728335916996002
step: 3000, Loss: 0.11999232321977615
step: 3100, Loss: 0.11625072360038757
step: 3200, Loss: 0.12343323975801468
step: 3300, Loss: 0.12065990269184113
step: 3400, Loss: 0.11868110299110413
step: 3500, Loss: 0.1163366436958313
step: 3600, Loss: 1.1278753280639648
step: 3700, Loss: 0.16535413265228271
step: 3800, Loss: 0.14323973655700684
step: 3900, Loss: 0.13733065128326416
step: 4000, Loss: 0.12911546230316162
step: 4100, Loss: 0.12855444848537445
step: 4200, Loss: 0.12459486722946167
step: 4300, Loss: 0.1200791671872139
step: 4400, Loss: 0.13178546726703644
step: 4500, Loss: 0.12268844991922379
step: 4600, Loss: 0.1213684007525444
step: 4700, Loss: 0.12206147611141205
step: 4800, Loss: 0.12062384188175201
step: 4900, Loss: 0.11708767712116241
step: 5000, Loss: 0.12409292906522751
step: 5100, Loss: 0.12192748486995697
step: 5200, Loss: 0.11713317781686783
step: 5300, Loss: 0.11991897970438004
step: 5400, Loss: 0.11921629309654236
step: 5500, Loss: 0.11538635939359665
step: 5600, Loss: 0.11572025716304779
step: 5700, Loss: 0.11576471477746964
step: 5800, Loss: 0.11727747321128845
step: 5900, Loss: 0.11819379031658173
step: 6000, Loss: 0.11748534440994263
step: 6100, Loss: 0.11497167497873306
step: 6200, Loss: 0.11565686017274857
step: 6300, Loss: 0.11647982895374298
step: 6400, Loss: 0.11470399051904678
step: 6500, Loss: 0.11681528389453888
step: 6600, Loss: 0.11735817044973373
step: 6700, Loss: 0.11517554521560669
step: 6800, Loss: 0.11616864800453186
step: 6900, Loss: 0.11508134752511978
step: 7000, Loss: 0.11406838893890381
step: 7100, Loss: 0.11542624235153198
step: 7200, Loss: 0.1142793595790863
step: 7300, Loss: 0.11539681255817413
step: 7400, Loss: 0.11434858292341232
step: 7500, Loss: 0.11549574881792068
step: 7600, Loss: 0.11441345512866974
step: 7700, Loss: 0.11616584658622742
step: 7800, Loss: 0.11407876759767532
step: 7900, Loss: 0.1142871156334877
step: 8000, Loss: 0.11553821712732315
step: 8100, Loss: 0.11571183055639267
step: 8200, Loss: 0.11513683199882507
step: 8300, Loss: 0.11745775490999222
step: 8400, Loss: 0.11601009964942932
step: 8500, Loss: 0.11398652195930481
step: 8600, Loss: 0.11475952714681625
step: 8700, Loss: 0.11555524170398712
step: 8800, Loss: 0.11450790613889694
step: 8900, Loss: 0.11503207683563232
step: 9000, Loss: 0.11520753055810928
step: 9100, Loss: 0.11580116301774979
step: 9200, Loss: 0.11689707636833191
step: 9300, Loss: 0.11602196097373962
step: 9400, Loss: 0.1145426332950592
step: 9500, Loss: 0.11583632230758667
step: 9600, Loss: 0.1149122416973114
step: 9700, Loss: 0.11556360125541687
step: 9800, Loss: 0.11614011228084564
step: 9900, Loss: 0.11763669550418854
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 1.0
recall: 0.925
F_score: 0.961038961038961
******fold 2******

Training... train_data length:855
step: 0, Loss: 0.1245560348033905
step: 100, Loss: 0.11974543333053589
step: 200, Loss: 0.11829043924808502
step: 300, Loss: 0.11815579980611801
step: 400, Loss: 0.11523278057575226
step: 500, Loss: 0.11548498272895813
step: 600, Loss: 0.1150183230638504
step: 700, Loss: 0.11629330366849899
step: 800, Loss: 0.1144552007317543
step: 900, Loss: 0.11508212238550186
step: 1000, Loss: 0.11345084756612778
step: 1100, Loss: 0.11571522802114487
step: 1200, Loss: 0.114010289311409
step: 1300, Loss: 0.11577723175287247
step: 1400, Loss: 0.11444521695375443
step: 1500, Loss: 0.11681315302848816
step: 1600, Loss: 0.11431044340133667
step: 1700, Loss: 0.11376338452100754
step: 1800, Loss: 0.11476022750139236
step: 1900, Loss: 0.11745604127645493
step: 2000, Loss: 0.11437550187110901
step: 2100, Loss: 0.1151423454284668
step: 2200, Loss: 0.11573773622512817
step: 2300, Loss: 0.11380532383918762
step: 2400, Loss: 0.11519531905651093
step: 2500, Loss: 0.11403127014636993
step: 2600, Loss: 0.11581239104270935
step: 2700, Loss: 0.11472856998443604
step: 2800, Loss: 0.11402169615030289
step: 2900, Loss: 0.11417051404714584
step: 3000, Loss: 0.11442004144191742
step: 3100, Loss: 0.1135927364230156
step: 3200, Loss: 0.1132296547293663
step: 3300, Loss: 0.11468998342752457
step: 3400, Loss: 0.11383755505084991
step: 3500, Loss: 0.11323978006839752
step: 3600, Loss: 0.11489458382129669
step: 3700, Loss: 0.11485607922077179
step: 3800, Loss: 0.11415289342403412
step: 3900, Loss: 0.1158754825592041
step: 4000, Loss: 0.11479350924491882
step: 4100, Loss: 0.11424580216407776
step: 4200, Loss: 0.11592961102724075
step: 4300, Loss: 0.11421319097280502
step: 4400, Loss: 0.11407455056905746
step: 4500, Loss: 0.11365267634391785
step: 4600, Loss: 0.11626894772052765
step: 4700, Loss: 0.11526870727539062
step: 4800, Loss: 0.11386223137378693
step: 4900, Loss: 0.11830801516771317
step: 5000, Loss: 0.11463082581758499
step: 5100, Loss: 0.1142808273434639
step: 5200, Loss: 0.1140226200222969
step: 5300, Loss: 0.11398957669734955
step: 5400, Loss: 0.11535017192363739
step: 5500, Loss: 0.11532517522573471
step: 5600, Loss: 0.11390680819749832
step: 5700, Loss: 0.1139344796538353
step: 5800, Loss: 0.11636210232973099
step: 5900, Loss: 1.3986332416534424
step: 6000, Loss: 0.2264293134212494
step: 6100, Loss: 0.12897171080112457
step: 6200, Loss: 0.1268041431903839
step: 6300, Loss: 0.13144925236701965
step: 6400, Loss: 0.13191305100917816
step: 6500, Loss: 0.118110790848732
step: 6600, Loss: 0.12311851233243942
step: 6700, Loss: 0.11933435499668121
step: 6800, Loss: 0.11787514388561249
step: 6900, Loss: 0.11805075407028198
step: 7000, Loss: 0.12109357118606567
step: 7100, Loss: 0.11639244109392166
step: 7200, Loss: 0.1203467845916748
step: 7300, Loss: 0.11681725084781647
step: 7400, Loss: 0.1171949952840805
step: 7500, Loss: 0.1156219094991684
step: 7600, Loss: 0.11793482303619385
step: 7700, Loss: 0.11485333740711212
step: 7800, Loss: 0.11617463827133179
step: 7900, Loss: 0.11534853279590607
step: 8000, Loss: 0.11930380016565323
step: 8100, Loss: 0.11680079251527786
step: 8200, Loss: 0.12022217363119125
step: 8300, Loss: 0.11489330232143402
step: 8400, Loss: 0.11700809001922607
step: 8500, Loss: 0.11406309902667999
step: 8600, Loss: 0.11620024591684341
step: 8700, Loss: 0.1151101365685463
step: 8800, Loss: 0.11600111424922943
step: 8900, Loss: 0.11363536864519119
step: 9000, Loss: 0.11651311814785004
step: 9100, Loss: 0.11341549456119537
step: 9200, Loss: 0.115069679915905
step: 9300, Loss: 0.11348018795251846
step: 9400, Loss: 0.11359215527772903
step: 9500, Loss: 0.1133793368935585
step: 9600, Loss: 0.11347610503435135
step: 9700, Loss: 0.11415072530508041
step: 9800, Loss: 0.11377041041851044
step: 9900, Loss: 0.11575306951999664
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******

Training... train_data length:855
step: 0, Loss: 0.14529168605804443
step: 100, Loss: 0.11504626274108887
step: 200, Loss: 0.11485854536294937
step: 300, Loss: 0.1155175268650055
step: 400, Loss: 0.11703640967607498
step: 500, Loss: 0.11496967077255249
step: 600, Loss: 0.11411391198635101
step: 700, Loss: 0.11416949331760406
step: 800, Loss: 0.114874467253685
step: 900, Loss: 0.11432114988565445
step: 1000, Loss: 0.11555115133523941
step: 1100, Loss: 0.1139935553073883
step: 1200, Loss: 0.1140468642115593
step: 1300, Loss: 0.11305122822523117
step: 1400, Loss: 0.1126878559589386
step: 1500, Loss: 0.11377254873514175
step: 1600, Loss: 0.11532013863325119
step: 1700, Loss: 0.11320120096206665
step: 1800, Loss: 0.11440018564462662
step: 1900, Loss: 0.11500031501054764
step: 2000, Loss: 0.11478619277477264
step: 2100, Loss: 0.11468842625617981
step: 2200, Loss: 0.11438816785812378
step: 2300, Loss: 0.11526252329349518
step: 2400, Loss: 0.11556471139192581
step: 2500, Loss: 0.11731944978237152
step: 2600, Loss: 0.1160050556063652
step: 2700, Loss: 0.11307499557733536
step: 2800, Loss: 0.11465194076299667
step: 2900, Loss: 0.11444760113954544
step: 3000, Loss: 0.11445928364992142
step: 3100, Loss: 0.11400144547224045
step: 3200, Loss: 0.11409582197666168
step: 3300, Loss: 0.11549165844917297
step: 3400, Loss: 0.11358988285064697
step: 3500, Loss: 0.11414623260498047
step: 3600, Loss: 0.11409401148557663
step: 3700, Loss: 0.6166132092475891
step: 3800, Loss: 0.13412220776081085
step: 3900, Loss: 0.12588554620742798
step: 4000, Loss: 0.121167853474617
step: 4100, Loss: 0.11949653923511505
step: 4200, Loss: 0.12210782617330551
step: 4300, Loss: 0.12087546288967133
step: 4400, Loss: 0.11954033374786377
step: 4500, Loss: 0.12299256026744843
step: 4600, Loss: 0.1170298382639885
step: 4700, Loss: 0.11850377172231674
step: 4800, Loss: 0.11810509860515594
step: 4900, Loss: 0.11636091023683548
step: 5000, Loss: 0.11407022923231125
step: 5100, Loss: 0.11973422765731812
step: 5200, Loss: 0.11505982279777527
step: 5300, Loss: 0.1154433935880661
step: 5400, Loss: 0.11528918147087097
step: 5500, Loss: 0.11611773073673248
step: 5600, Loss: 0.1155308187007904
step: 5700, Loss: 0.11567678302526474
step: 5800, Loss: 0.11468062549829483
step: 5900, Loss: 0.1143413856625557
step: 6000, Loss: 0.11455925554037094
step: 6100, Loss: 0.11435364186763763
step: 6200, Loss: 0.11392949521541595
step: 6300, Loss: 0.11570427566766739
step: 6400, Loss: 0.11535273492336273
step: 6500, Loss: 0.11504944413900375
step: 6600, Loss: 0.11467661708593369
step: 6700, Loss: 0.11528919637203217
step: 6800, Loss: 0.11338098347187042
step: 6900, Loss: 0.11452539265155792
step: 7000, Loss: 0.11500688642263412
step: 7100, Loss: 0.11436519026756287
step: 7200, Loss: 0.11436709761619568
step: 7300, Loss: 0.11327677220106125
step: 7400, Loss: 0.11361569166183472
step: 7500, Loss: 0.11544694006443024
step: 7600, Loss: 0.1127372533082962
step: 7700, Loss: 0.11391080170869827
step: 7800, Loss: 0.1131972074508667
step: 7900, Loss: 0.11386077105998993
step: 8000, Loss: 0.11400901526212692
step: 8100, Loss: 0.11457762122154236
step: 8200, Loss: 0.11342216283082962
step: 8300, Loss: 0.11512480676174164
step: 8400, Loss: 0.1136188879609108
step: 8500, Loss: 0.11318490654230118
step: 8600, Loss: 0.11354339867830276
step: 8700, Loss: 0.11472378671169281
step: 8800, Loss: 0.11391515284776688
step: 8900, Loss: 0.11349330097436905
step: 9000, Loss: 0.11330008506774902
step: 9100, Loss: 0.11365674436092377
step: 9200, Loss: 0.11516374349594116
step: 9300, Loss: 0.1135658472776413
step: 9400, Loss: 0.11347396671772003
step: 9500, Loss: 0.11596278846263885
step: 9600, Loss: 0.11389639973640442
step: 9700, Loss: 0.11335881054401398
step: 9800, Loss: 0.11335674673318863
step: 9900, Loss: 0.1163281798362732
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

Training... train_data length:855
step: 0, Loss: 0.13546797633171082
step: 100, Loss: 0.11658693104982376
step: 200, Loss: 0.11629615724086761
step: 300, Loss: 0.11386936902999878
step: 400, Loss: 0.11809439957141876
step: 500, Loss: 0.11440744251012802
step: 600, Loss: 0.11489874124526978
step: 700, Loss: 0.11501666903495789
step: 800, Loss: 0.11478254199028015
step: 900, Loss: 0.11450076103210449
step: 1000, Loss: 0.11424241214990616
step: 1100, Loss: 0.11545512825250626
step: 1200, Loss: 0.11415210366249084
step: 1300, Loss: 0.11362895369529724
step: 1400, Loss: 0.11379558593034744
step: 1500, Loss: 0.11524748057126999
step: 1600, Loss: 0.1151069849729538
step: 1700, Loss: 0.11495852470397949
step: 1800, Loss: 0.112638458609581
step: 1900, Loss: 0.11423106491565704
step: 2000, Loss: 0.11394457519054413
step: 2100, Loss: 0.11544626206159592
step: 2200, Loss: 0.114598847925663
step: 2300, Loss: 0.1127934381365776
step: 2400, Loss: 0.11435481905937195
step: 2500, Loss: 0.11476839333772659
step: 2600, Loss: 0.11482521891593933
step: 2700, Loss: 0.11597175151109695
step: 2800, Loss: 0.11371759325265884
step: 2900, Loss: 0.11653336137533188
step: 3000, Loss: 0.11305344104766846
step: 3100, Loss: 1.5831594467163086
step: 3200, Loss: 0.15230658650398254
step: 3300, Loss: 0.13958564400672913
step: 3400, Loss: 0.12867075204849243
step: 3500, Loss: 0.1252927929162979
step: 3600, Loss: 0.12480298429727554
step: 3700, Loss: 0.12737387418746948
step: 3800, Loss: 0.12196002900600433
step: 3900, Loss: 0.11859531700611115
step: 4000, Loss: 0.12082618474960327
step: 4100, Loss: 0.12213526666164398
step: 4200, Loss: 0.12034730613231659
step: 4300, Loss: 0.1208433285355568
step: 4400, Loss: 0.11610937118530273
step: 4500, Loss: 0.11720548570156097
step: 4600, Loss: 0.11579757928848267
step: 4700, Loss: 0.1182827427983284
step: 4800, Loss: 0.1178542822599411
step: 4900, Loss: 0.11947942525148392
step: 5000, Loss: 0.11571536958217621
step: 5100, Loss: 0.11563888937234879
step: 5200, Loss: 0.11590138077735901
step: 5300, Loss: 0.11734291911125183
step: 5400, Loss: 0.11491266638040543
step: 5500, Loss: 0.11508703231811523
step: 5600, Loss: 0.11438357830047607
step: 5700, Loss: 0.11489914357662201
step: 5800, Loss: 0.11515521258115768
step: 5900, Loss: 0.11490344256162643
step: 6000, Loss: 0.1136544942855835
step: 6100, Loss: 0.1148286908864975
step: 6200, Loss: 0.11423064768314362
step: 6300, Loss: 0.11496300995349884
step: 6400, Loss: 0.1137559786438942
step: 6500, Loss: 0.11550689488649368
step: 6600, Loss: 0.11470834165811539
step: 6700, Loss: 0.11519349366426468
step: 6800, Loss: 0.11483661830425262
step: 6900, Loss: 0.1145615205168724
step: 7000, Loss: 0.11372476071119308
step: 7100, Loss: 0.11484335362911224
step: 7200, Loss: 0.11456925421953201
step: 7300, Loss: 0.1154681146144867
step: 7400, Loss: 0.11500407755374908
step: 7500, Loss: 0.11463763564825058
step: 7600, Loss: 0.11403076350688934
step: 7700, Loss: 0.1134541928768158
step: 7800, Loss: 0.11458683013916016
step: 7900, Loss: 0.11485658586025238
step: 8000, Loss: 0.11325222253799438
step: 8100, Loss: 0.11457205563783646
step: 8200, Loss: 0.11425447463989258
step: 8300, Loss: 0.11355356872081757
step: 8400, Loss: 0.11458608508110046
step: 8500, Loss: 0.11522936820983887
step: 8600, Loss: 0.11388115584850311
step: 8700, Loss: 0.11367987096309662
step: 8800, Loss: 0.11368468403816223
step: 8900, Loss: 0.11575020104646683
step: 9000, Loss: 0.11427532881498337
step: 9100, Loss: 0.11601522564888
step: 9200, Loss: 0.11309558153152466
step: 9300, Loss: 0.11420273780822754
step: 9400, Loss: 0.11343545466661453
step: 9500, Loss: 0.11384929716587067
step: 9600, Loss: 0.11364640295505524
step: 9700, Loss: 0.113413006067276
step: 9800, Loss: 0.11408193409442902
step: 9900, Loss: 0.11334968358278275
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

Training... train_data length:855
step: 0, Loss: 0.1487044095993042
step: 100, Loss: 0.11705714464187622
step: 200, Loss: 0.1138271614909172
step: 300, Loss: 0.11547163873910904
step: 400, Loss: 0.11581195890903473
step: 500, Loss: 0.1153021901845932
step: 600, Loss: 0.11352545022964478
step: 700, Loss: 0.11397167295217514
step: 800, Loss: 0.11573415994644165
step: 900, Loss: 0.11343476176261902
step: 1000, Loss: 0.11541914939880371
step: 1100, Loss: 0.11377730220556259
step: 1200, Loss: 0.11374014616012573
step: 1300, Loss: 0.11351510137319565
step: 1400, Loss: 0.11496329307556152
step: 1500, Loss: 0.1145852729678154
step: 1600, Loss: 0.11389466375112534
step: 1700, Loss: 0.11406400799751282
step: 1800, Loss: 0.11398196965456009
step: 1900, Loss: 0.11367518454790115
step: 2000, Loss: 0.11326776444911957
step: 2100, Loss: 0.11587320268154144
step: 2200, Loss: 0.11327183246612549
step: 2300, Loss: 0.11481457948684692
step: 2400, Loss: 0.11265237629413605
step: 2500, Loss: 0.11447832733392715
step: 2600, Loss: 0.11644379049539566
step: 2700, Loss: 0.11573056876659393
step: 2800, Loss: 0.11606550216674805
step: 2900, Loss: 0.11436818540096283
step: 3000, Loss: 0.11535458266735077
step: 3100, Loss: 0.11384773254394531
step: 3200, Loss: 0.11463434249162674
step: 3300, Loss: 0.11435844004154205
step: 3400, Loss: 0.11531151086091995
step: 3500, Loss: 0.11491752415895462
step: 3600, Loss: 0.1154600977897644
step: 3700, Loss: 0.11461635679006577
step: 3800, Loss: 0.11274340748786926
step: 3900, Loss: 0.11401741206645966
step: 4000, Loss: 0.11321065574884415
step: 4100, Loss: 0.11344185471534729
step: 4200, Loss: 0.11402659863233566
step: 4300, Loss: 0.11564496159553528
step: 4400, Loss: 0.11497589945793152
step: 4500, Loss: 0.7367249131202698
step: 4600, Loss: 0.14532043039798737
step: 4700, Loss: 0.13590224087238312
step: 4800, Loss: 0.12024429440498352
step: 4900, Loss: 0.13257275521755219
step: 5000, Loss: 0.12118856608867645
step: 5100, Loss: 0.12164421379566193
step: 5200, Loss: 0.12225523591041565
step: 5300, Loss: 0.12335734814405441
step: 5400, Loss: 0.11842454224824905
step: 5500, Loss: 0.12093714624643326
step: 5600, Loss: 0.11905345320701599
step: 5700, Loss: 0.12378302961587906
step: 5800, Loss: 0.12066042423248291
step: 5900, Loss: 0.12028565257787704
step: 6000, Loss: 0.1171082854270935
step: 6100, Loss: 0.12108129262924194
step: 6200, Loss: 0.11614078283309937
step: 6300, Loss: 0.1180104985833168
step: 6400, Loss: 0.11645067483186722
step: 6500, Loss: 0.11513467133045197
step: 6600, Loss: 0.11482176184654236
step: 6700, Loss: 0.11706171929836273
step: 6800, Loss: 0.11540646106004715
step: 6900, Loss: 0.11499563604593277
step: 7000, Loss: 0.11493886262178421
step: 7100, Loss: 0.1176045686006546
step: 7200, Loss: 0.1143687292933464
step: 7300, Loss: 0.11642267554998398
step: 7400, Loss: 0.11391796916723251
step: 7500, Loss: 0.11335384845733643
step: 7600, Loss: 0.11426235735416412
step: 7700, Loss: 0.11400693655014038
step: 7800, Loss: 0.11386872082948685
step: 7900, Loss: 0.11752797663211823
step: 8000, Loss: 0.11470555514097214
step: 8100, Loss: 0.11453479528427124
step: 8200, Loss: 0.11385616660118103
step: 8300, Loss: 0.11372760683298111
step: 8400, Loss: 0.11473669111728668
step: 8500, Loss: 0.11593908816576004
step: 8600, Loss: 0.1134817823767662
step: 8700, Loss: 0.11429672688245773
step: 8800, Loss: 0.11361829191446304
step: 8900, Loss: 0.11550936847925186
step: 9000, Loss: 0.11335781216621399
step: 9100, Loss: 0.1146821528673172
step: 9200, Loss: 0.11385664343833923
step: 9300, Loss: 0.11491818726062775
step: 9400, Loss: 0.11305423825979233
step: 9500, Loss: 0.1129906177520752
step: 9600, Loss: 0.11330143362283707
step: 9700, Loss: 0.11612294614315033
step: 9800, Loss: 0.11386123299598694
step: 9900, Loss: 0.11393007636070251
training successfully ended.
validating...
validate data length:95
acc: 0.9659090909090909
precision: 0.9459459459459459
recall: 0.9722222222222222
F_score: 0.9589041095890412
******fold 6******

Training... train_data length:855
step: 0, Loss: 0.16699518263339996
step: 100, Loss: 0.11516175419092178
step: 200, Loss: 0.12313611805438995
step: 300, Loss: 0.11482842266559601
step: 400, Loss: 0.11467831581830978
step: 500, Loss: 0.11467863619327545
step: 600, Loss: 0.11442721635103226
step: 700, Loss: 0.114999920129776
step: 800, Loss: 0.11791270226240158
step: 900, Loss: 0.11414948105812073
step: 1000, Loss: 0.11492416262626648
step: 1100, Loss: 0.11560417711734772
step: 1200, Loss: 0.11350902915000916
step: 1300, Loss: 0.11507771909236908
step: 1400, Loss: 0.11552263051271439
step: 1500, Loss: 0.11369115859270096
step: 1600, Loss: 0.11423912644386292
step: 1700, Loss: 0.11367017775774002
step: 1800, Loss: 0.1138545572757721
step: 1900, Loss: 0.1139703020453453
step: 2000, Loss: 0.11578640341758728
step: 2100, Loss: 0.11442466825246811
step: 2200, Loss: 0.11392608284950256
step: 2300, Loss: 0.11354857683181763
step: 2400, Loss: 0.11516858637332916
step: 2500, Loss: 0.11408386379480362
step: 2600, Loss: 0.11505726724863052
step: 2700, Loss: 0.1139892190694809
step: 2800, Loss: 0.11426720768213272
step: 2900, Loss: 0.11585740745067596
step: 3000, Loss: 0.11346734315156937
step: 3100, Loss: 0.11396446824073792
step: 3200, Loss: 0.11400711536407471
step: 3300, Loss: 0.11550037562847137
step: 3400, Loss: 0.114063560962677
step: 3500, Loss: 0.11404719948768616
step: 3600, Loss: 0.11331495642662048
step: 3700, Loss: 0.11442556232213974
step: 3800, Loss: 0.11558834463357925
step: 3900, Loss: 0.11660120636224747
step: 4000, Loss: 0.11648645251989365
step: 4100, Loss: 0.1148703321814537
step: 4200, Loss: 0.11305789649486542
step: 4300, Loss: 0.22732672095298767
step: 4400, Loss: 0.13377372920513153
step: 4500, Loss: 0.14097018539905548
step: 4600, Loss: 0.131470188498497
step: 4700, Loss: 0.12605713307857513
step: 4800, Loss: 0.12840791046619415
step: 4900, Loss: 0.12113399058580399
step: 5000, Loss: 0.11937044560909271
step: 5100, Loss: 0.12117677927017212
step: 5200, Loss: 0.11940789222717285
step: 5300, Loss: 0.12159591913223267
step: 5400, Loss: 0.11468401551246643
step: 5500, Loss: 0.11900375783443451
step: 5600, Loss: 0.11937754601240158
step: 5700, Loss: 0.11839468032121658
step: 5800, Loss: 0.11685973405838013
step: 5900, Loss: 0.11606289446353912
step: 6000, Loss: 0.11601373553276062
step: 6100, Loss: 0.11539659649133682
step: 6200, Loss: 0.12231693416833878
step: 6300, Loss: 0.1159478947520256
step: 6400, Loss: 0.11650167405605316
step: 6500, Loss: 0.1159771978855133
step: 6600, Loss: 0.11519958823919296
step: 6700, Loss: 0.11497589200735092
step: 6800, Loss: 0.1167064905166626
step: 6900, Loss: 0.11367932707071304
step: 7000, Loss: 0.11385675519704819
step: 7100, Loss: 0.11603067070245743
step: 7200, Loss: 0.11405476182699203
step: 7300, Loss: 0.11436143517494202
step: 7400, Loss: 0.11361461132764816
step: 7500, Loss: 0.1141531690955162
step: 7600, Loss: 0.1134505569934845
step: 7700, Loss: 0.11411263793706894
step: 7800, Loss: 0.1156051754951477
step: 7900, Loss: 0.1143888458609581
step: 8000, Loss: 0.1138119250535965
step: 8100, Loss: 0.1140897274017334
step: 8200, Loss: 0.11300094425678253
step: 8300, Loss: 0.11368929594755173
step: 8400, Loss: 0.1136002466082573
step: 8500, Loss: 0.11373038589954376
step: 8600, Loss: 0.11519531905651093
step: 8700, Loss: 0.1137847900390625
step: 8800, Loss: 0.11446329951286316
step: 8900, Loss: 0.11342786997556686
step: 9000, Loss: 0.11430007219314575
step: 9100, Loss: 0.11364727467298508
step: 9200, Loss: 0.11425769329071045
step: 9300, Loss: 0.11405190080404282
step: 9400, Loss: 0.11352885514497757
step: 9500, Loss: 0.11304600536823273
step: 9600, Loss: 0.11330584436655045
step: 9700, Loss: 0.11448600143194199
step: 9800, Loss: 0.11323481798171997
step: 9900, Loss: 0.11339510232210159
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:855
step: 0, Loss: 0.14265434443950653
step: 100, Loss: 0.11623495072126389
step: 200, Loss: 0.1252041608095169
step: 300, Loss: 0.12056945264339447
step: 400, Loss: 0.11761002987623215
step: 500, Loss: 0.1179453507065773
step: 600, Loss: 0.11471239477396011
step: 700, Loss: 0.11582181602716446
step: 800, Loss: 0.11598131060600281
step: 900, Loss: 0.11514859646558762
step: 1000, Loss: 0.11372356116771698
step: 1100, Loss: 0.11564023047685623
step: 1200, Loss: 0.11445708572864532
step: 1300, Loss: 0.1133153885602951
step: 1400, Loss: 0.11472207307815552
step: 1500, Loss: 0.11471949517726898
step: 1600, Loss: 0.1140180304646492
step: 1700, Loss: 0.11347509920597076
step: 1800, Loss: 0.11453606188297272
step: 1900, Loss: 0.11339045315980911
step: 2000, Loss: 0.11439430713653564
step: 2100, Loss: 0.11454969644546509
step: 2200, Loss: 0.11498764902353287
step: 2300, Loss: 0.11419480293989182
step: 2400, Loss: 0.11482322216033936
step: 2500, Loss: 0.1123262494802475
step: 2600, Loss: 0.1135602593421936
step: 2700, Loss: 0.11394467949867249
step: 2800, Loss: 0.11543947458267212
step: 2900, Loss: 0.11431486159563065
step: 3000, Loss: 0.11334849894046783
step: 3100, Loss: 0.11413230001926422
step: 3200, Loss: 0.11311959475278854
step: 3300, Loss: 0.11425434052944183
step: 3400, Loss: 0.11399070918560028
step: 3500, Loss: 0.11437776684761047
step: 3600, Loss: 0.11286450922489166
step: 3700, Loss: 0.11358512938022614
step: 3800, Loss: 0.11505913734436035
step: 3900, Loss: 0.11360500752925873
step: 4000, Loss: 0.1136893630027771
step: 4100, Loss: 0.11482401192188263
step: 4200, Loss: 0.11347530782222748
step: 4300, Loss: 0.1139165535569191
step: 4400, Loss: 0.113880455493927
step: 4500, Loss: 0.11313816159963608
step: 4600, Loss: 0.11258269846439362
step: 4700, Loss: 0.11461113393306732
step: 4800, Loss: 0.11427890509366989
step: 4900, Loss: 0.11498832702636719
step: 5000, Loss: 0.11342162638902664
step: 5100, Loss: 0.11616072058677673
step: 5200, Loss: 0.278608500957489
step: 5300, Loss: 0.13422498106956482
step: 5400, Loss: 0.1254015564918518
step: 5500, Loss: 0.1288364678621292
step: 5600, Loss: 0.1281244456768036
step: 5700, Loss: 0.1282871961593628
step: 5800, Loss: 0.1213451474905014
step: 5900, Loss: 0.11629922688007355
step: 6000, Loss: 0.11568241566419601
step: 6100, Loss: 0.11761952936649323
step: 6200, Loss: 0.12435832619667053
step: 6300, Loss: 0.11820844560861588
step: 6400, Loss: 0.11597659438848495
step: 6500, Loss: 0.11470596492290497
step: 6600, Loss: 0.11512275040149689
step: 6700, Loss: 0.11666470766067505
step: 6800, Loss: 0.1167183667421341
step: 6900, Loss: 0.1159423291683197
step: 7000, Loss: 0.11600182950496674
step: 7100, Loss: 0.11459708213806152
step: 7200, Loss: 0.11440538614988327
step: 7300, Loss: 0.11481697857379913
step: 7400, Loss: 0.11493688076734543
step: 7500, Loss: 0.11646535247564316
step: 7600, Loss: 0.11462867259979248
step: 7700, Loss: 0.11578567326068878
step: 7800, Loss: 0.11531482636928558
step: 7900, Loss: 0.11488216370344162
step: 8000, Loss: 0.11415528506040573
step: 8100, Loss: 0.116728276014328
step: 8200, Loss: 0.114390067756176
step: 8300, Loss: 0.11435624957084656
step: 8400, Loss: 0.11375056207180023
step: 8500, Loss: 0.11463430523872375
step: 8600, Loss: 0.11425327509641647
step: 8700, Loss: 0.11396253108978271
step: 8800, Loss: 0.1142798364162445
step: 8900, Loss: 0.11254475265741348
step: 9000, Loss: 0.11353547871112823
step: 9100, Loss: 0.1150093674659729
step: 9200, Loss: 0.11509712785482407
step: 9300, Loss: 0.11411638557910919
step: 9400, Loss: 0.11460762470960617
step: 9500, Loss: 0.1152481883764267
step: 9600, Loss: 0.11364571005105972
step: 9700, Loss: 0.1149420216679573
step: 9800, Loss: 0.11373261362314224
step: 9900, Loss: 0.11332625895738602
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

Training... train_data length:855
step: 0, Loss: 0.13272513449192047
step: 100, Loss: 0.11512070894241333
step: 200, Loss: 0.11913639307022095
step: 300, Loss: 0.11560584604740143
step: 400, Loss: 0.11509972810745239
step: 500, Loss: 0.11448792368173599
step: 600, Loss: 0.11483950912952423
step: 700, Loss: 0.11579512059688568
step: 800, Loss: 0.11533825844526291
step: 900, Loss: 0.11389730870723724
step: 1000, Loss: 0.11349982023239136
step: 1100, Loss: 0.1137578934431076
step: 1200, Loss: 0.11351567506790161
step: 1300, Loss: 0.11413244158029556
step: 1400, Loss: 0.11603465676307678
step: 1500, Loss: 0.11440920829772949
step: 1600, Loss: 0.11519281566143036
step: 1700, Loss: 0.11368682235479355
step: 1800, Loss: 0.11412401497364044
step: 1900, Loss: 0.11372306197881699
step: 2000, Loss: 0.11346564441919327
step: 2100, Loss: 0.11520756781101227
step: 2200, Loss: 0.11413314938545227
step: 2300, Loss: 0.11491397768259048
step: 2400, Loss: 0.11754854768514633
step: 2500, Loss: 0.11342112720012665
step: 2600, Loss: 0.11403437703847885
step: 2700, Loss: 0.11502465605735779
step: 2800, Loss: 0.11450186371803284
step: 2900, Loss: 0.11314688622951508
step: 3000, Loss: 0.11391380429267883
step: 3100, Loss: 0.11355043947696686
step: 3200, Loss: 0.11673082411289215
step: 3300, Loss: 0.11622478067874908
step: 3400, Loss: 0.11400014162063599
step: 3500, Loss: 0.11472441256046295
step: 3600, Loss: 0.11293057352304459
step: 3700, Loss: 0.11595366895198822
step: 3800, Loss: 0.11459609866142273
step: 3900, Loss: 0.11399546265602112
step: 4000, Loss: 0.11600247025489807
step: 4100, Loss: 0.1145821139216423
step: 4200, Loss: 0.11490137875080109
step: 4300, Loss: 0.11319103091955185
step: 4400, Loss: 0.11606204509735107
step: 4500, Loss: 0.20570366084575653
step: 4600, Loss: 0.13599656522274017
step: 4700, Loss: 0.14091768860816956
step: 4800, Loss: 0.12353037297725677
step: 4900, Loss: 0.11963149905204773
step: 5000, Loss: 0.12194785475730896
step: 5100, Loss: 0.12001585215330124
step: 5200, Loss: 0.11813168227672577
step: 5300, Loss: 0.12074543535709381
step: 5400, Loss: 0.11834687739610672
step: 5500, Loss: 0.11719494313001633
step: 5600, Loss: 0.1178060919046402
step: 5700, Loss: 0.12175662815570831
step: 5800, Loss: 0.11743473261594772
step: 5900, Loss: 0.11951936781406403
step: 6000, Loss: 0.11473973095417023
step: 6100, Loss: 0.11492237448692322
step: 6200, Loss: 0.1162300631403923
step: 6300, Loss: 0.11702412366867065
step: 6400, Loss: 0.1184781864285469
step: 6500, Loss: 0.11780655384063721
step: 6600, Loss: 0.11520377546548843
step: 6700, Loss: 0.11407259106636047
step: 6800, Loss: 0.11609799414873123
step: 6900, Loss: 0.11467191576957703
step: 7000, Loss: 0.11550796777009964
step: 7100, Loss: 0.11576461791992188
step: 7200, Loss: 0.11425170302391052
step: 7300, Loss: 0.11484750360250473
step: 7400, Loss: 0.11634641885757446
step: 7500, Loss: 0.11452962458133698
step: 7600, Loss: 0.11470350623130798
step: 7700, Loss: 0.11629679799079895
step: 7800, Loss: 0.1138681173324585
step: 7900, Loss: 0.11489555984735489
step: 8000, Loss: 0.11379565298557281
step: 8100, Loss: 0.11594349145889282
step: 8200, Loss: 0.11478742957115173
step: 8300, Loss: 0.11447495967149734
step: 8400, Loss: 0.11274458467960358
step: 8500, Loss: 0.11440226435661316
step: 8600, Loss: 0.11475680768489838
step: 8700, Loss: 0.11386026442050934
step: 8800, Loss: 0.1134156584739685
step: 8900, Loss: 0.11426316946744919
step: 9000, Loss: 0.11359164863824844
step: 9100, Loss: 0.11466167867183685
step: 9200, Loss: 0.11340701580047607
step: 9300, Loss: 0.11425691843032837
step: 9400, Loss: 0.11343991756439209
step: 9500, Loss: 0.11358598619699478
step: 9600, Loss: 0.11361328512430191
step: 9700, Loss: 0.11426609754562378
step: 9800, Loss: 0.11327644437551498
step: 9900, Loss: 0.11293890327215195
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

Training... train_data length:855
step: 0, Loss: 0.13886111974716187
step: 100, Loss: 0.11513032019138336
step: 200, Loss: 0.11398286372423172
step: 300, Loss: 0.11366397142410278
step: 400, Loss: 0.11416273564100266
step: 500, Loss: 0.11476864665746689
step: 600, Loss: 0.11315584927797318
step: 700, Loss: 0.1134214997291565
step: 800, Loss: 0.1159343346953392
step: 900, Loss: 0.11368878185749054
step: 1000, Loss: 0.11311424523591995
step: 1100, Loss: 0.11317133903503418
step: 1200, Loss: 0.11448460817337036
step: 1300, Loss: 0.11504140496253967
step: 1400, Loss: 0.11475344002246857
step: 1500, Loss: 0.11487271636724472
step: 1600, Loss: 0.11491382122039795
step: 1700, Loss: 0.11570267379283905
step: 1800, Loss: 0.11350434273481369
step: 1900, Loss: 0.11385497450828552
step: 2000, Loss: 0.11349271237850189
step: 2100, Loss: 0.11435507237911224
step: 2200, Loss: 0.11569938063621521
step: 2300, Loss: 0.11461374163627625
step: 2400, Loss: 0.11408410966396332
step: 2500, Loss: 0.11368042230606079
step: 2600, Loss: 0.1135098934173584
step: 2700, Loss: 0.11487247049808502
step: 2800, Loss: 0.1136835366487503
step: 2900, Loss: 0.11324983835220337
step: 3000, Loss: 0.11349659413099289
step: 3100, Loss: 0.11430083215236664
step: 3200, Loss: 0.11402919143438339
step: 3300, Loss: 0.1148858517408371
step: 3400, Loss: 0.11386586725711823
step: 3500, Loss: 0.11466086655855179
step: 3600, Loss: 0.11403358727693558
step: 3700, Loss: 0.11394107341766357
step: 3800, Loss: 0.11578348278999329
step: 3900, Loss: 0.11524315923452377
step: 4000, Loss: 0.11347611248493195
step: 4100, Loss: 0.11509624123573303
step: 4200, Loss: 0.11458803713321686
step: 4300, Loss: 0.1149115189909935
step: 4400, Loss: 0.11518707126379013
step: 4500, Loss: 0.11455383151769638
step: 4600, Loss: 0.11437030881643295
step: 4700, Loss: 0.11616279184818268
step: 4800, Loss: 0.11326155811548233
step: 4900, Loss: 2.9236083030700684
step: 5000, Loss: 0.13363663852214813
step: 5100, Loss: 0.12046036124229431
step: 5200, Loss: 0.12332834303379059
step: 5300, Loss: 0.1253843605518341
step: 5400, Loss: 0.12506389617919922
step: 5500, Loss: 0.1210845559835434
step: 5600, Loss: 0.11759020388126373
step: 5700, Loss: 0.11878445744514465
step: 5800, Loss: 0.11534122377634048
step: 5900, Loss: 0.12047171592712402
step: 6000, Loss: 0.11814159154891968
step: 6100, Loss: 0.11593931168317795
step: 6200, Loss: 0.11614799499511719
step: 6300, Loss: 0.11622444540262222
step: 6400, Loss: 0.11433658748865128
step: 6500, Loss: 0.11816137284040451
step: 6600, Loss: 0.1149706095457077
step: 6700, Loss: 0.11489421129226685
step: 6800, Loss: 0.11399663239717484
step: 6900, Loss: 0.11537064611911774
step: 7000, Loss: 0.11444374918937683
step: 7100, Loss: 0.11454852670431137
step: 7200, Loss: 0.11637051403522491
step: 7300, Loss: 0.11608554422855377
step: 7400, Loss: 0.11450725793838501
step: 7500, Loss: 0.11631188541650772
step: 7600, Loss: 0.11712778359651566
step: 7700, Loss: 0.11425638943910599
step: 7800, Loss: 0.11535543203353882
step: 7900, Loss: 0.11421142518520355
step: 8000, Loss: 0.11398129165172577
step: 8100, Loss: 0.11433076113462448
step: 8200, Loss: 0.11397942900657654
step: 8300, Loss: 0.11586299538612366
step: 8400, Loss: 0.11385861039161682
step: 8500, Loss: 0.11378303170204163
step: 8600, Loss: 0.11468690633773804
step: 8700, Loss: 0.11482316255569458
step: 8800, Loss: 0.1128745973110199
step: 8900, Loss: 0.1135992482304573
step: 9000, Loss: 0.11283895373344421
step: 9100, Loss: 0.1138947606086731
step: 9200, Loss: 0.11471890658140182
step: 9300, Loss: 0.11332203447818756
step: 9400, Loss: 0.11540889739990234
step: 9500, Loss: 0.11397291719913483
step: 9600, Loss: 0.11351130157709122
step: 9700, Loss: 0.11316768825054169
step: 9800, Loss: 0.1130085438489914
step: 9900, Loss: 0.11390070617198944
training successfully ended.
validating...
validate data length:95
acc: 0.9886363636363636
precision: 1.0
recall: 0.975
F_score: 0.9873417721518987
******fold 10******

Training... train_data length:855
step: 0, Loss: 0.14164969325065613
step: 100, Loss: 0.1203254982829094
step: 200, Loss: 0.11505885422229767
step: 300, Loss: 0.11467030644416809
step: 400, Loss: 0.1145876869559288
step: 500, Loss: 0.11346931010484695
step: 600, Loss: 0.11400273442268372
step: 700, Loss: 0.1147848442196846
step: 800, Loss: 0.11464975774288177
step: 900, Loss: 0.11396394670009613
step: 1000, Loss: 0.11446758359670639
step: 1100, Loss: 0.11416493356227875
step: 1200, Loss: 0.11362320184707642
step: 1300, Loss: 0.11644083261489868
step: 1400, Loss: 0.11747968941926956
step: 1500, Loss: 0.11294381320476532
step: 1600, Loss: 0.11522293835878372
step: 1700, Loss: 0.11388823390007019
step: 1800, Loss: 0.1136939749121666
step: 1900, Loss: 0.11394166946411133
step: 2000, Loss: 0.11369664967060089
step: 2100, Loss: 0.11589417606592178
step: 2200, Loss: 0.11350971460342407
step: 2300, Loss: 0.11409661918878555
step: 2400, Loss: 0.11347249150276184
step: 2500, Loss: 0.11410529166460037
step: 2600, Loss: 0.11425185948610306
step: 2700, Loss: 0.11584512889385223
step: 2800, Loss: 0.1128939688205719
step: 2900, Loss: 0.11364302039146423
step: 3000, Loss: 0.11348920315504074
step: 3100, Loss: 0.11389225721359253
step: 3200, Loss: 0.1130082979798317
step: 3300, Loss: 0.11367636173963547
step: 3400, Loss: 0.11369318515062332
step: 3500, Loss: 0.11330284178256989
step: 3600, Loss: 0.11373671144247055
step: 3700, Loss: 0.11419971287250519
step: 3800, Loss: 0.1139325201511383
step: 3900, Loss: 0.1157890111207962
step: 4000, Loss: 0.12010206282138824
step: 4100, Loss: 0.16290101408958435
step: 4200, Loss: 0.12706145644187927
step: 4300, Loss: 0.1303522139787674
step: 4400, Loss: 0.1238136738538742
step: 4500, Loss: 0.1311672329902649
step: 4600, Loss: 0.12066681683063507
step: 4700, Loss: 0.12321542203426361
step: 4800, Loss: 0.11935371160507202
step: 4900, Loss: 0.12222224473953247
step: 5000, Loss: 0.11962126940488815
step: 5100, Loss: 0.12102492153644562
step: 5200, Loss: 0.11988741159439087
step: 5300, Loss: 0.11931571364402771
step: 5400, Loss: 0.11636357754468918
step: 5500, Loss: 0.11883717030286789
step: 5600, Loss: 0.11555047333240509
step: 5700, Loss: 0.11811775714159012
step: 5800, Loss: 0.11719604581594467
step: 5900, Loss: 0.11600667238235474
step: 6000, Loss: 0.11449307203292847
step: 6100, Loss: 0.11677844822406769
step: 6200, Loss: 0.11690583825111389
step: 6300, Loss: 0.1180199682712555
step: 6400, Loss: 0.11494628340005875
step: 6500, Loss: 0.1173463687300682
step: 6600, Loss: 0.11389769613742828
step: 6700, Loss: 0.11470012366771698
step: 6800, Loss: 0.11323264241218567
step: 6900, Loss: 0.11657387018203735
step: 7000, Loss: 0.11437603831291199
step: 7100, Loss: 0.11491883546113968
step: 7200, Loss: 0.11393844336271286
step: 7300, Loss: 0.11515773832798004
step: 7400, Loss: 0.11306840181350708
step: 7500, Loss: 0.11416690051555634
step: 7600, Loss: 0.11494812369346619
step: 7700, Loss: 0.11489750444889069
step: 7800, Loss: 0.11257311701774597
step: 7900, Loss: 0.11397849023342133
step: 8000, Loss: 0.11517828702926636
step: 8100, Loss: 0.11483355611562729
step: 8200, Loss: 0.11290295422077179
step: 8300, Loss: 0.11335867643356323
step: 8400, Loss: 0.11446434259414673
step: 8500, Loss: 0.11460794508457184
step: 8600, Loss: 0.11304812878370285
step: 8700, Loss: 0.11576643586158752
step: 8800, Loss: 0.11537232249975204
step: 8900, Loss: 0.11433803290128708
step: 9000, Loss: 0.11378839612007141
step: 9100, Loss: 0.11341363191604614
step: 9200, Loss: 0.11306706070899963
step: 9300, Loss: 0.11412081867456436
step: 9400, Loss: 0.11287511885166168
step: 9500, Loss: 0.11284802854061127
step: 9600, Loss: 0.11352547258138657
step: 9700, Loss: 0.1137104481458664
step: 9800, Loss: 0.11328833550214767
step: 9900, Loss: 0.11460450291633606
training successfully ended.
validating...
validate data length:95
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 30 Avgacc: 0.9920454545454545 Avgfscore: 0.9907284842779902 
 Max acc:1.0, Max f score:1.0
******** mix subject_31 ********

[342, 418]
******fold 1******

Training... train_data length:684
step: 0, Loss: 43.80244064331055
step: 100, Loss: 8.404608726501465
step: 200, Loss: 8.049571990966797
step: 300, Loss: 4.340176582336426
step: 400, Loss: 1.594648838043213
step: 500, Loss: 0.5512738227844238
step: 600, Loss: 0.8838672637939453
step: 700, Loss: 2.0745255947113037
step: 800, Loss: 0.23218926787376404
step: 900, Loss: 0.6555297374725342
step: 1000, Loss: 0.18567070364952087
step: 1100, Loss: 0.16908633708953857
step: 1200, Loss: 0.1683485060930252
step: 1300, Loss: 0.16232775151729584
step: 1400, Loss: 0.16641691327095032
step: 1500, Loss: 0.2223432958126068
step: 1600, Loss: 0.1390862762928009
step: 1700, Loss: 0.1692999005317688
step: 1800, Loss: 0.1424565613269806
step: 1900, Loss: 0.13957248628139496
step: 2000, Loss: 0.14187294244766235
step: 2100, Loss: 0.13915249705314636
step: 2200, Loss: 0.13387323915958405
step: 2300, Loss: 0.1396835595369339
step: 2400, Loss: 0.14612573385238647
step: 2500, Loss: 0.13988709449768066
step: 2600, Loss: 0.13235816359519958
step: 2700, Loss: 0.12809841334819794
step: 2800, Loss: 0.1252245008945465
step: 2900, Loss: 0.1211307942867279
step: 3000, Loss: 0.1273634135723114
step: 3100, Loss: 0.13041512668132782
step: 3200, Loss: 0.12861795723438263
step: 3300, Loss: 0.12700720131397247
step: 3400, Loss: 0.20488297939300537
step: 3500, Loss: 0.11947250366210938
step: 3600, Loss: 0.12116863578557968
step: 3700, Loss: 0.1224215179681778
step: 3800, Loss: 0.12631049752235413
step: 3900, Loss: 0.12340918183326721
step: 4000, Loss: 0.12296560406684875
step: 4100, Loss: 0.12137260288000107
step: 4200, Loss: 0.1257711946964264
step: 4300, Loss: 0.12697145342826843
step: 4400, Loss: 0.11783234030008316
step: 4500, Loss: 0.11956934630870819
step: 4600, Loss: 0.12413995712995529
step: 4700, Loss: 0.11780709028244019
step: 4800, Loss: 0.1149098202586174
step: 4900, Loss: 0.11522731930017471
step: 5000, Loss: 0.11585047096014023
step: 5100, Loss: 0.11884254962205887
step: 5200, Loss: 0.11563912034034729
step: 5300, Loss: 0.1952964961528778
step: 5400, Loss: 0.11611752957105637
step: 5500, Loss: 0.11663065105676651
step: 5600, Loss: 0.11667318642139435
step: 5700, Loss: 0.11922357976436615
step: 5800, Loss: 0.11909523606300354
step: 5900, Loss: 0.11792309582233429
step: 6000, Loss: 0.11427827179431915
step: 6100, Loss: 0.1178760975599289
step: 6200, Loss: 0.11677951365709305
step: 6300, Loss: 0.11855687946081161
step: 6400, Loss: 0.11849649250507355
step: 6500, Loss: 9.103241920471191
step: 6600, Loss: 0.40145522356033325
step: 6700, Loss: 0.6359560489654541
step: 6800, Loss: 0.15915754437446594
step: 6900, Loss: 0.16090574860572815
step: 7000, Loss: 0.15497498214244843
step: 7100, Loss: 0.15590032935142517
step: 7200, Loss: 0.21844586730003357
step: 7300, Loss: 0.13347330689430237
step: 7400, Loss: 0.13891249895095825
step: 7500, Loss: 0.13298365473747253
step: 7600, Loss: 0.13078926503658295
step: 7700, Loss: 0.12978219985961914
step: 7800, Loss: 0.13070522248744965
step: 7900, Loss: 0.13651299476623535
step: 8000, Loss: 0.12492063641548157
step: 8100, Loss: 0.13481812179088593
step: 8200, Loss: 0.12605586647987366
step: 8300, Loss: 0.12694640457630157
step: 8400, Loss: 0.12245649099349976
step: 8500, Loss: 0.12039797008037567
step: 8600, Loss: 0.11996370553970337
step: 8700, Loss: 0.11934670805931091
step: 8800, Loss: 0.11876623332500458
step: 8900, Loss: 0.1251710206270218
step: 9000, Loss: 0.12060441821813583
step: 9100, Loss: 0.19893747568130493
step: 9200, Loss: 0.11664697527885437
step: 9300, Loss: 0.12180525064468384
step: 9400, Loss: 0.11810024082660675
step: 9500, Loss: 0.11810782551765442
step: 9600, Loss: 0.11591797322034836
step: 9700, Loss: 0.12132863700389862
step: 9800, Loss: 0.11792874336242676
step: 9900, Loss: 0.11402840167284012
training successfully ended.
validating...
validate data length:76
acc: 0.5833333333333334
precision: 0.5238095238095238
recall: 0.6875
F_score: 0.5945945945945946
******fold 2******

Training... train_data length:684
step: 0, Loss: 2.220513105392456
step: 100, Loss: 0.1613781601190567
step: 200, Loss: 0.1511075496673584
step: 300, Loss: 0.13073371350765228
step: 400, Loss: 0.13386741280555725
step: 500, Loss: 0.1395496428012848
step: 600, Loss: 0.14093032479286194
step: 700, Loss: 0.12399539351463318
step: 800, Loss: 0.11911104619503021
step: 900, Loss: 0.1195792704820633
step: 1000, Loss: 0.12481629103422165
step: 1100, Loss: 0.11702325940132141
step: 1200, Loss: 0.1174371987581253
step: 1300, Loss: 0.11856423318386078
step: 1400, Loss: 0.11812807619571686
step: 1500, Loss: 0.1976090371608734
step: 1600, Loss: 0.11539788544178009
step: 1700, Loss: 0.11925939470529556
step: 1800, Loss: 0.11718247085809708
step: 1900, Loss: 0.11449157446622849
step: 2000, Loss: 0.11845309287309647
step: 2100, Loss: 0.1157122477889061
step: 2200, Loss: 0.11480922996997833
step: 2300, Loss: 0.11713895946741104
step: 2400, Loss: 0.11778731644153595
step: 2500, Loss: 0.11595020443201065
step: 2600, Loss: 0.11385481804609299
step: 2700, Loss: 0.11432654410600662
step: 2800, Loss: 0.11870676279067993
step: 2900, Loss: 0.11773155629634857
step: 3000, Loss: 0.11823665350675583
step: 3100, Loss: 0.11979883164167404
step: 3200, Loss: 0.11796069890260696
step: 3300, Loss: 0.12604016065597534
step: 3400, Loss: 2.0428287982940674
step: 3500, Loss: 0.7165466547012329
step: 3600, Loss: 1.8676707744598389
step: 3700, Loss: 0.2509284019470215
step: 3800, Loss: 0.15377779304981232
step: 3900, Loss: 0.16548535227775574
step: 4000, Loss: 0.13622808456420898
step: 4100, Loss: 0.13628539443016052
step: 4200, Loss: 0.13512511551380157
step: 4300, Loss: 0.1342545747756958
step: 4400, Loss: 0.13976018130779266
step: 4500, Loss: 0.13158874213695526
step: 4600, Loss: 0.1280369907617569
step: 4700, Loss: 0.12361209094524384
step: 4800, Loss: 0.12810172140598297
step: 4900, Loss: 0.12325169146060944
step: 5000, Loss: 0.12164658308029175
step: 5100, Loss: 0.1203451082110405
step: 5200, Loss: 0.12471161782741547
step: 5300, Loss: 0.19942788779735565
step: 5400, Loss: 0.11679672449827194
step: 5500, Loss: 0.12362920492887497
step: 5600, Loss: 0.12253715097904205
step: 5700, Loss: 0.12003164738416672
step: 5800, Loss: 0.12121962755918503
step: 5900, Loss: 0.12068342417478561
step: 6000, Loss: 0.11834733933210373
step: 6100, Loss: 0.12198248505592346
step: 6200, Loss: 0.11814942955970764
step: 6300, Loss: 0.11956321448087692
step: 6400, Loss: 0.1215735673904419
step: 6500, Loss: 0.12044142186641693
step: 6600, Loss: 0.11623139679431915
step: 6700, Loss: 0.11677295714616776
step: 6800, Loss: 0.11504184454679489
step: 6900, Loss: 0.11376596987247467
step: 7000, Loss: 0.11616762727499008
step: 7100, Loss: 0.11892043054103851
step: 7200, Loss: 0.19593065977096558
step: 7300, Loss: 0.11508089303970337
step: 7400, Loss: 0.11728104948997498
step: 7500, Loss: 0.1134331002831459
step: 7600, Loss: 0.1162521243095398
step: 7700, Loss: 0.11658360064029694
step: 7800, Loss: 0.11284954100847244
step: 7900, Loss: 0.11545325815677643
step: 8000, Loss: 0.11646378040313721
step: 8100, Loss: 0.113482266664505
step: 8200, Loss: 0.11417872458696365
step: 8300, Loss: 0.11450991034507751
step: 8400, Loss: 0.11414875090122223
step: 8500, Loss: 0.11434133350849152
step: 8600, Loss: 0.114516980946064
step: 8700, Loss: 0.11382827162742615
step: 8800, Loss: 0.11396320164203644
step: 8900, Loss: 0.11426033079624176
step: 9000, Loss: 0.1147102564573288
step: 9100, Loss: 0.19351431727409363
step: 9200, Loss: 0.11440521478652954
step: 9300, Loss: 0.11833600699901581
step: 9400, Loss: 4.418701648712158
step: 9500, Loss: 0.4528912901878357
step: 9600, Loss: 0.2518925070762634
step: 9700, Loss: 0.14073631167411804
step: 9800, Loss: 0.1475580632686615
step: 9900, Loss: 0.14243502914905548
training successfully ended.
validating...
validate data length:76
acc: 0.8611111111111112
precision: 0.8108108108108109
recall: 0.9090909090909091
F_score: 0.8571428571428571
******fold 3******

Training... train_data length:684
step: 0, Loss: 2.1277263164520264
step: 100, Loss: 0.1492939293384552
step: 200, Loss: 0.13198046386241913
step: 300, Loss: 0.1414508819580078
step: 400, Loss: 0.1330791562795639
step: 500, Loss: 0.12288346141576767
step: 600, Loss: 0.12655162811279297
step: 700, Loss: 0.12298610806465149
step: 800, Loss: 0.11951617151498795
step: 900, Loss: 0.1237783133983612
step: 1000, Loss: 0.1175343319773674
step: 1100, Loss: 0.12004830688238144
step: 1200, Loss: 0.1148068904876709
step: 1300, Loss: 0.11803195625543594
step: 1400, Loss: 0.11557270586490631
step: 1500, Loss: 0.19975963234901428
step: 1600, Loss: 0.11756904423236847
step: 1700, Loss: 0.11552338302135468
step: 1800, Loss: 0.11500012874603271
step: 1900, Loss: 0.11479201912879944
step: 2000, Loss: 0.11507129669189453
step: 2100, Loss: 0.11702706664800644
step: 2200, Loss: 0.11582222580909729
step: 2300, Loss: 0.11453469097614288
step: 2400, Loss: 0.11559975147247314
step: 2500, Loss: 0.12747833132743835
step: 2600, Loss: 0.11607567220926285
step: 2700, Loss: 0.11662578582763672
step: 2800, Loss: 0.11538601666688919
step: 2900, Loss: 0.11499106884002686
step: 3000, Loss: 0.11446674168109894
step: 3100, Loss: 0.12152481824159622
step: 3200, Loss: 0.11623020470142365
step: 3300, Loss: 0.11857448518276215
step: 3400, Loss: 0.19055810570716858
step: 3500, Loss: 0.11471353471279144
step: 3600, Loss: 0.119998499751091
step: 3700, Loss: 0.1178303137421608
step: 3800, Loss: 0.12539154291152954
step: 3900, Loss: 2.5847504138946533
step: 4000, Loss: 1.2127240896224976
step: 4100, Loss: 0.14788049459457397
step: 4200, Loss: 0.1341426968574524
step: 4300, Loss: 0.13847163319587708
step: 4400, Loss: 0.1301499605178833
step: 4500, Loss: 0.1378462016582489
step: 4600, Loss: 0.12628987431526184
step: 4700, Loss: 0.12848129868507385
step: 4800, Loss: 0.12817661464214325
step: 4900, Loss: 0.13010385632514954
step: 5000, Loss: 0.12715275585651398
step: 5100, Loss: 0.12131178379058838
step: 5200, Loss: 0.12456206977367401
step: 5300, Loss: 0.20436254143714905
step: 5400, Loss: 0.12221155315637589
step: 5500, Loss: 0.11983644962310791
step: 5600, Loss: 0.11873804032802582
step: 5700, Loss: 0.11827139556407928
step: 5800, Loss: 0.11925114691257477
step: 5900, Loss: 0.11629947274923325
step: 6000, Loss: 0.11700838059186935
step: 6100, Loss: 0.12142278254032135
step: 6200, Loss: 0.12037338316440582
step: 6300, Loss: 0.11495689302682877
step: 6400, Loss: 0.11714106798171997
step: 6500, Loss: 0.11837057769298553
step: 6600, Loss: 0.11561650782823563
step: 6700, Loss: 0.11678022146224976
step: 6800, Loss: 0.11797989159822464
step: 6900, Loss: 0.11695491522550583
step: 7000, Loss: 0.11512158811092377
step: 7100, Loss: 0.11653369665145874
step: 7200, Loss: 0.19583097100257874
step: 7300, Loss: 0.11524572968482971
step: 7400, Loss: 0.11347581446170807
step: 7500, Loss: 0.11345833539962769
step: 7600, Loss: 0.1156345009803772
step: 7700, Loss: 0.11626658588647842
step: 7800, Loss: 0.11504432559013367
step: 7900, Loss: 0.1144215315580368
step: 8000, Loss: 0.11399362236261368
step: 8100, Loss: 0.11428139358758926
step: 8200, Loss: 0.1153569445014
step: 8300, Loss: 0.1132909432053566
step: 8400, Loss: 0.11392907798290253
step: 8500, Loss: 0.11374819278717041
step: 8600, Loss: 0.11623145639896393
step: 8700, Loss: 0.114152692258358
step: 8800, Loss: 0.11448860913515091
step: 8900, Loss: 0.11423855274915695
step: 9000, Loss: 0.11314401030540466
step: 9100, Loss: 0.19540424644947052
step: 9200, Loss: 0.1146339401602745
step: 9300, Loss: 0.1137666255235672
step: 9400, Loss: 0.11404585838317871
step: 9500, Loss: 0.11502639204263687
step: 9600, Loss: 0.11452259868383408
step: 9700, Loss: 0.11294647306203842
step: 9800, Loss: 0.11522216349840164
step: 9900, Loss: 0.11578361690044403
training successfully ended.
validating...
validate data length:76
acc: 0.9444444444444444
precision: 0.90625
recall: 0.9666666666666667
F_score: 0.9354838709677419
******fold 4******

Training... train_data length:684
step: 0, Loss: 0.31113100051879883
step: 100, Loss: 0.12909814715385437
step: 200, Loss: 0.11710594594478607
step: 300, Loss: 0.11891475319862366
step: 400, Loss: 0.12213081866502762
step: 500, Loss: 0.11915810406208038
step: 600, Loss: 0.11731801927089691
step: 700, Loss: 0.11635508388280869
step: 800, Loss: 0.11636815220117569
step: 900, Loss: 0.11503854393959045
step: 1000, Loss: 0.11652888357639313
step: 1100, Loss: 0.11383089423179626
step: 1200, Loss: 0.11429022252559662
step: 1300, Loss: 0.11452978104352951
step: 1400, Loss: 0.114061638712883
step: 1500, Loss: 0.19137156009674072
step: 1600, Loss: 0.11485807597637177
step: 1700, Loss: 0.11451610922813416
step: 1800, Loss: 0.11376884579658508
step: 1900, Loss: 0.1146046444773674
step: 2000, Loss: 0.11490022391080856
step: 2100, Loss: 0.11387127637863159
step: 2200, Loss: 0.1141965389251709
step: 2300, Loss: 0.11419951915740967
step: 2400, Loss: 0.11555737257003784
step: 2500, Loss: 0.12321150302886963
step: 2600, Loss: 0.11838877201080322
step: 2700, Loss: 0.11997859925031662
step: 2800, Loss: 0.1204078420996666
step: 2900, Loss: 0.11540587991476059
step: 3000, Loss: 0.1195506751537323
step: 3100, Loss: 0.11703625321388245
step: 3200, Loss: 0.11817231774330139
step: 3300, Loss: 2.9245188236236572
step: 3400, Loss: 0.23531761765480042
step: 3500, Loss: 0.14807626605033875
step: 3600, Loss: 0.14169177412986755
step: 3700, Loss: 0.13192780315876007
step: 3800, Loss: 0.12887755036354065
step: 3900, Loss: 0.12339755892753601
step: 4000, Loss: 0.12925775349140167
step: 4100, Loss: 0.13384947180747986
step: 4200, Loss: 0.12084577977657318
step: 4300, Loss: 0.11903442442417145
step: 4400, Loss: 0.12078019976615906
step: 4500, Loss: 0.1259315311908722
step: 4600, Loss: 0.12327872961759567
step: 4700, Loss: 0.12299378961324692
step: 4800, Loss: 0.1199847161769867
step: 4900, Loss: 0.12192615866661072
step: 5000, Loss: 0.11878348886966705
step: 5100, Loss: 0.11918149888515472
step: 5200, Loss: 0.12004275619983673
step: 5300, Loss: 0.19886550307273865
step: 5400, Loss: 0.12036670744419098
step: 5500, Loss: 0.11719899624586105
step: 5600, Loss: 0.11569792032241821
step: 5700, Loss: 0.11573200672864914
step: 5800, Loss: 0.11457355320453644
step: 5900, Loss: 0.11904621124267578
step: 6000, Loss: 0.11689388751983643
step: 6100, Loss: 0.1156085804104805
step: 6200, Loss: 0.11486495286226273
step: 6300, Loss: 0.11448279768228531
step: 6400, Loss: 0.11367929726839066
step: 6500, Loss: 0.11405232548713684
step: 6600, Loss: 0.11428423225879669
step: 6700, Loss: 0.11400669813156128
step: 6800, Loss: 0.1149536520242691
step: 6900, Loss: 0.11410126090049744
step: 7000, Loss: 0.1135222315788269
step: 7100, Loss: 0.11488880217075348
step: 7200, Loss: 0.19341649115085602
step: 7300, Loss: 0.11523792147636414
step: 7400, Loss: 0.11487799882888794
step: 7500, Loss: 0.11511296033859253
step: 7600, Loss: 0.11455624550580978
step: 7700, Loss: 0.11367175728082657
step: 7800, Loss: 0.11372551321983337
step: 7900, Loss: 0.11435499787330627
step: 8000, Loss: 0.11373769491910934
step: 8100, Loss: 0.11321590095758438
step: 8200, Loss: 0.11422811448574066
step: 8300, Loss: 0.11447178572416306
step: 8400, Loss: 0.11516667902469635
step: 8500, Loss: 0.11383949220180511
step: 8600, Loss: 0.11736096441745758
step: 8700, Loss: 0.11510201543569565
step: 8800, Loss: 0.1167241781949997
step: 8900, Loss: 0.11430467665195465
step: 9000, Loss: 0.11501024663448334
step: 9100, Loss: 0.19114702939987183
step: 9200, Loss: 0.11498120427131653
step: 9300, Loss: 0.1162433847784996
step: 9400, Loss: 0.11621475219726562
step: 9500, Loss: 0.11507469415664673
step: 9600, Loss: 0.11590591818094254
step: 9700, Loss: 0.11693356931209564
step: 9800, Loss: 0.11759251356124878
step: 9900, Loss: 0.11691582202911377
training successfully ended.
validating...
validate data length:76
acc: 0.9722222222222222
precision: 0.9444444444444444
recall: 1.0
F_score: 0.9714285714285714
******fold 5******

Training... train_data length:684
step: 0, Loss: 0.1721155047416687
step: 100, Loss: 0.11920562386512756
step: 200, Loss: 0.11727496981620789
step: 300, Loss: 0.11688266694545746
step: 400, Loss: 0.118196040391922
step: 500, Loss: 0.11466619372367859
step: 600, Loss: 0.11448867619037628
step: 700, Loss: 0.11357039958238602
step: 800, Loss: 0.11525199562311172
step: 900, Loss: 0.11654089391231537
step: 1000, Loss: 0.11368600279092789
step: 1100, Loss: 0.11376405507326126
step: 1200, Loss: 0.11478430032730103
step: 1300, Loss: 0.11469967663288116
step: 1400, Loss: 0.11382371932268143
step: 1500, Loss: 0.192668616771698
step: 1600, Loss: 0.11766259372234344
step: 1700, Loss: 0.11623936146497726
step: 1800, Loss: 8.111510276794434
step: 1900, Loss: 0.6416847705841064
step: 2000, Loss: 0.14960667490959167
step: 2100, Loss: 0.14654915034770966
step: 2200, Loss: 0.139351487159729
step: 2300, Loss: 0.1272049844264984
step: 2400, Loss: 0.12339562177658081
step: 2500, Loss: 0.12277430295944214
step: 2600, Loss: 0.12465327978134155
step: 2700, Loss: 0.1253262162208557
step: 2800, Loss: 0.12536321580410004
step: 2900, Loss: 0.12378917634487152
step: 3000, Loss: 0.12305866926908493
step: 3100, Loss: 0.12323363125324249
step: 3200, Loss: 0.11967821419239044
step: 3300, Loss: 0.12272528558969498
step: 3400, Loss: 0.20271390676498413
step: 3500, Loss: 0.11872759461402893
step: 3600, Loss: 0.12026961147785187
step: 3700, Loss: 0.11865000426769257
step: 3800, Loss: 0.11779728531837463
step: 3900, Loss: 0.1187654361128807
step: 4000, Loss: 0.11862470209598541
step: 4100, Loss: 0.11641576141119003
step: 4200, Loss: 0.11650493741035461
step: 4300, Loss: 0.11571051180362701
step: 4400, Loss: 0.11666857451200485
step: 4500, Loss: 0.11584986746311188
step: 4600, Loss: 0.11587659269571304
step: 4700, Loss: 0.11595165729522705
step: 4800, Loss: 0.11517562717199326
step: 4900, Loss: 0.11368101090192795
step: 5000, Loss: 0.11488695442676544
step: 5100, Loss: 0.11513637006282806
step: 5200, Loss: 0.1153392642736435
step: 5300, Loss: 0.19356216490268707
step: 5400, Loss: 0.11492441594600677
step: 5500, Loss: 0.11456958204507828
step: 5600, Loss: 0.11399701237678528
step: 5700, Loss: 0.11411066353321075
step: 5800, Loss: 0.11498954147100449
step: 5900, Loss: 0.11439308524131775
step: 6000, Loss: 0.11335932463407516
step: 6100, Loss: 0.11484471708536148
step: 6200, Loss: 0.1133972778916359
step: 6300, Loss: 0.11319635808467865
step: 6400, Loss: 0.11434520781040192
step: 6500, Loss: 0.11439484357833862
step: 6600, Loss: 0.11574700474739075
step: 6700, Loss: 0.1143387034535408
step: 6800, Loss: 0.11446280032396317
step: 6900, Loss: 0.11394166946411133
step: 7000, Loss: 0.11472417414188385
step: 7100, Loss: 0.11320050060749054
step: 7200, Loss: 0.1938319206237793
step: 7300, Loss: 0.1140582412481308
step: 7400, Loss: 0.1143665760755539
step: 7500, Loss: 0.11363072693347931
step: 7600, Loss: 0.11472238600254059
step: 7700, Loss: 0.11341282725334167
step: 7800, Loss: 0.11404647678136826
step: 7900, Loss: 0.11402932554483414
step: 8000, Loss: 0.11431881040334702
step: 8100, Loss: 0.11484085023403168
step: 8200, Loss: 0.11402283608913422
step: 8300, Loss: 0.1140918880701065
step: 8400, Loss: 0.11928241699934006
step: 8500, Loss: 0.11915045976638794
step: 8600, Loss: 0.11731754243373871
step: 8700, Loss: 0.11532896757125854
step: 8800, Loss: 0.11361975222826004
step: 8900, Loss: 0.11462768167257309
step: 9000, Loss: 0.11355683952569962
step: 9100, Loss: 0.19893234968185425
step: 9200, Loss: 0.12137340754270554
step: 9300, Loss: 0.11645938456058502
step: 9400, Loss: 0.1204889565706253
step: 9500, Loss: 0.12170654535293579
step: 9600, Loss: 0.1155446469783783
step: 9700, Loss: 0.11495109647512436
step: 9800, Loss: 0.1198158785700798
step: 9900, Loss: 1.4706289768218994
training successfully ended.
validating...
validate data length:76
acc: 0.9861111111111112
precision: 0.9705882352941176
recall: 1.0
F_score: 0.9850746268656716
******fold 6******

Training... train_data length:684
step: 0, Loss: 0.16085708141326904
step: 100, Loss: 0.14076140522956848
step: 200, Loss: 0.13584302365779877
step: 300, Loss: 0.13033199310302734
step: 400, Loss: 0.12725631892681122
step: 500, Loss: 0.12422683835029602
step: 600, Loss: 0.12469737976789474
step: 700, Loss: 0.12010865658521652
step: 800, Loss: 0.11784397810697556
step: 900, Loss: 0.11608333140611649
step: 1000, Loss: 0.1147535964846611
step: 1100, Loss: 0.11583676934242249
step: 1200, Loss: 0.11610393971204758
step: 1300, Loss: 0.11754569411277771
step: 1400, Loss: 0.11395471543073654
step: 1500, Loss: 0.19461789727210999
step: 1600, Loss: 0.11430884897708893
step: 1700, Loss: 0.1151248887181282
step: 1800, Loss: 0.11488044261932373
step: 1900, Loss: 0.11513198912143707
step: 2000, Loss: 0.11433824151754379
step: 2100, Loss: 0.11700683832168579
step: 2200, Loss: 0.11616965383291245
step: 2300, Loss: 0.11675063520669937
step: 2400, Loss: 0.11675670742988586
step: 2500, Loss: 0.11459378898143768
step: 2600, Loss: 0.11331799626350403
step: 2700, Loss: 0.11518537998199463
step: 2800, Loss: 0.11542373150587082
step: 2900, Loss: 0.11424524337053299
step: 3000, Loss: 0.11513233184814453
step: 3100, Loss: 0.11521251499652863
step: 3200, Loss: 0.1145198792219162
step: 3300, Loss: 0.11960113048553467
step: 3400, Loss: 0.1940612643957138
step: 3500, Loss: 0.11491523683071136
step: 3600, Loss: 0.11691978573799133
step: 3700, Loss: 0.11527875810861588
step: 3800, Loss: 0.11542574316263199
step: 3900, Loss: 0.11589077115058899
step: 4000, Loss: 0.11385168135166168
step: 4100, Loss: 0.1146344244480133
step: 4200, Loss: 0.11753242462873459
step: 4300, Loss: 0.11484670639038086
step: 4400, Loss: 0.12109173834323883
step: 4500, Loss: 0.1148446798324585
step: 4600, Loss: 0.12946143746376038
step: 4700, Loss: 1.773561716079712
step: 4800, Loss: 0.14435020089149475
step: 4900, Loss: 0.15400005877017975
step: 5000, Loss: 0.13190743327140808
step: 5100, Loss: 0.1321904957294464
step: 5200, Loss: 0.1336822807788849
step: 5300, Loss: 0.1990220844745636
step: 5400, Loss: 0.12762737274169922
step: 5500, Loss: 0.12929219007492065
step: 5600, Loss: 0.11808806657791138
step: 5700, Loss: 0.12151874601840973
step: 5800, Loss: 0.1254742443561554
step: 5900, Loss: 0.12188124656677246
step: 6000, Loss: 0.11940309405326843
step: 6100, Loss: 0.12048420310020447
step: 6200, Loss: 0.11834821105003357
step: 6300, Loss: 0.11896364390850067
step: 6400, Loss: 0.12032416462898254
step: 6500, Loss: 0.12035900354385376
step: 6600, Loss: 0.11738653481006622
step: 6700, Loss: 0.11623431742191315
step: 6800, Loss: 0.1154172420501709
step: 6900, Loss: 0.11695609241724014
step: 7000, Loss: 0.11464784294366837
step: 7100, Loss: 0.11525078117847443
step: 7200, Loss: 0.1924620121717453
step: 7300, Loss: 0.11432509124279022
step: 7400, Loss: 0.12048304826021194
step: 7500, Loss: 0.11459237337112427
step: 7600, Loss: 0.11412185430526733
step: 7700, Loss: 0.11580915749073029
step: 7800, Loss: 0.12115968018770218
step: 7900, Loss: 0.11410020291805267
step: 8000, Loss: 0.11445111781358719
step: 8100, Loss: 0.1143777072429657
step: 8200, Loss: 0.11515650153160095
step: 8300, Loss: 0.11429791152477264
step: 8400, Loss: 0.11535396426916122
step: 8500, Loss: 0.11436421424150467
step: 8600, Loss: 0.11360179632902145
step: 8700, Loss: 0.114188052713871
step: 8800, Loss: 0.11393795907497406
step: 8900, Loss: 0.11422489583492279
step: 9000, Loss: 0.11331970244646072
step: 9100, Loss: 0.19215062260627747
step: 9200, Loss: 0.11377067863941193
step: 9300, Loss: 0.11360730975866318
step: 9400, Loss: 0.11453504860401154
step: 9500, Loss: 0.11399029940366745
step: 9600, Loss: 0.11354625225067139
step: 9700, Loss: 0.11494966596364975
step: 9800, Loss: 0.11310183256864548
step: 9900, Loss: 0.11329779773950577
training successfully ended.
validating...
validate data length:76
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

Training... train_data length:684
step: 0, Loss: 0.12243235111236572
step: 100, Loss: 0.11897671222686768
step: 200, Loss: 0.11806559562683105
step: 300, Loss: 0.1153736412525177
step: 400, Loss: 0.11568465828895569
step: 500, Loss: 0.11493704468011856
step: 600, Loss: 0.11343611031770706
step: 700, Loss: 0.11491501331329346
step: 800, Loss: 0.11475532501935959
step: 900, Loss: 0.11391199380159378
step: 1000, Loss: 0.1149454191327095
step: 1100, Loss: 0.1129988357424736
step: 1200, Loss: 0.11500866711139679
step: 1300, Loss: 0.11606699228286743
step: 1400, Loss: 0.11610260605812073
step: 1500, Loss: 0.19633372128009796
step: 1600, Loss: 0.11699458956718445
step: 1700, Loss: 0.11535198986530304
step: 1800, Loss: 0.11642369627952576
step: 1900, Loss: 0.11380119621753693
step: 2000, Loss: 4.706521987915039
step: 2100, Loss: 2.1299238204956055
step: 2200, Loss: 0.1598779857158661
step: 2300, Loss: 0.1299615353345871
step: 2400, Loss: 0.1334763765335083
step: 2500, Loss: 0.1247682198882103
step: 2600, Loss: 0.1248776763677597
step: 2700, Loss: 0.1237373873591423
step: 2800, Loss: 0.12070810049772263
step: 2900, Loss: 0.12024029344320297
step: 3000, Loss: 0.12323830276727676
step: 3100, Loss: 0.12325409054756165
step: 3200, Loss: 0.12045541405677795
step: 3300, Loss: 0.11933530122041702
step: 3400, Loss: 0.19949406385421753
step: 3500, Loss: 0.1183769628405571
step: 3600, Loss: 0.12020818889141083
step: 3700, Loss: 0.12156365066766739
step: 3800, Loss: 0.11493010073900223
step: 3900, Loss: 0.1181904599070549
step: 4000, Loss: 0.11655056476593018
step: 4100, Loss: 0.11680767685174942
step: 4200, Loss: 0.11827567219734192
step: 4300, Loss: 0.11748099327087402
step: 4400, Loss: 0.11734412610530853
step: 4500, Loss: 0.11482485383749008
step: 4600, Loss: 0.1144302636384964
step: 4700, Loss: 0.11469133198261261
step: 4800, Loss: 0.11560653150081635
step: 4900, Loss: 0.1141686886548996
step: 5000, Loss: 0.11476965248584747
step: 5100, Loss: 0.11525533348321915
step: 5200, Loss: 0.11348449438810349
step: 5300, Loss: 0.1948307305574417
step: 5400, Loss: 0.11433716863393784
step: 5500, Loss: 0.11436446011066437
step: 5600, Loss: 0.11400032788515091
step: 5700, Loss: 0.11405724287033081
step: 5800, Loss: 0.11422305554151535
step: 5900, Loss: 0.11482153832912445
step: 6000, Loss: 0.11537343263626099
step: 6100, Loss: 0.11476054787635803
step: 6200, Loss: 0.11470037698745728
step: 6300, Loss: 0.1138928160071373
step: 6400, Loss: 0.11325401067733765
step: 6500, Loss: 0.11545956879854202
step: 6600, Loss: 0.11466311663389206
step: 6700, Loss: 0.1134912520647049
step: 6800, Loss: 0.11421126127243042
step: 6900, Loss: 0.11396882683038712
step: 7000, Loss: 0.11392340064048767
step: 7100, Loss: 0.1148621216416359
step: 7200, Loss: 0.1924142837524414
step: 7300, Loss: 0.1138448566198349
step: 7400, Loss: 0.11405681073665619
step: 7500, Loss: 0.11356934159994125
step: 7600, Loss: 0.11557064950466156
step: 7700, Loss: 0.11496955156326294
step: 7800, Loss: 0.11480163037776947
step: 7900, Loss: 0.11798875033855438
step: 8000, Loss: 0.11261681467294693
step: 8100, Loss: 0.11411048471927643
step: 8200, Loss: 0.11662229895591736
step: 8300, Loss: 0.11533821374177933
step: 8400, Loss: 0.11441103368997574
step: 8500, Loss: 0.11680857837200165
step: 8600, Loss: 0.11796024441719055
step: 8700, Loss: 0.11797230690717697
step: 8800, Loss: 0.1145448386669159
step: 8900, Loss: 0.11538656055927277
step: 9000, Loss: 0.11694642901420593
step: 9100, Loss: 0.20101623237133026
step: 9200, Loss: 0.11870764940977097
step: 9300, Loss: 0.11833535879850388
step: 9400, Loss: 0.11686389148235321
step: 9500, Loss: 0.11576791107654572
step: 9600, Loss: 0.11748277395963669
step: 9700, Loss: 0.11548083275556564
step: 9800, Loss: 0.11669600009918213
step: 9900, Loss: 1.934046745300293
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.9142857142857143
recall: 0.9142857142857143
F_score: 0.9142857142857143
******fold 8******

Training... train_data length:684
step: 0, Loss: 0.12030258774757385
step: 100, Loss: 0.12273916602134705
step: 200, Loss: 0.11803674697875977
step: 300, Loss: 0.11821010708808899
step: 400, Loss: 0.11372853815555573
step: 500, Loss: 0.11438322812318802
step: 600, Loss: 0.11812835931777954
step: 700, Loss: 0.11359206587076187
step: 800, Loss: 0.1137714833021164
step: 900, Loss: 0.1133449524641037
step: 1000, Loss: 0.11437706649303436
step: 1100, Loss: 0.11327152699232101
step: 1200, Loss: 0.11357315629720688
step: 1300, Loss: 0.11529630422592163
step: 1400, Loss: 0.11711067706346512
step: 1500, Loss: 0.19307038187980652
step: 1600, Loss: 0.11431116610765457
step: 1700, Loss: 0.11476175487041473
step: 1800, Loss: 0.11427334696054459
step: 1900, Loss: 0.11824989318847656
step: 2000, Loss: 0.12215590476989746
step: 2100, Loss: 1.2342219352722168
step: 2200, Loss: 0.18166115880012512
step: 2300, Loss: 0.14117389917373657
step: 2400, Loss: 0.12779571115970612
step: 2500, Loss: 0.12875394523143768
step: 2600, Loss: 0.128697007894516
step: 2700, Loss: 0.1290651261806488
step: 2800, Loss: 0.12162269651889801
step: 2900, Loss: 0.12266603112220764
step: 3000, Loss: 0.12101093679666519
step: 3100, Loss: 0.1223062127828598
step: 3200, Loss: 0.11923713237047195
step: 3300, Loss: 0.1189073771238327
step: 3400, Loss: 0.19489708542823792
step: 3500, Loss: 0.117496058344841
step: 3600, Loss: 0.11801765859127045
step: 3700, Loss: 0.11932505667209625
step: 3800, Loss: 0.11492040008306503
step: 3900, Loss: 0.11577475816011429
step: 4000, Loss: 0.11751635372638702
step: 4100, Loss: 0.1177653968334198
step: 4200, Loss: 0.11751971393823624
step: 4300, Loss: 0.11376041918992996
step: 4400, Loss: 0.11468219757080078
step: 4500, Loss: 0.11696936935186386
step: 4600, Loss: 0.11629115045070648
step: 4700, Loss: 0.113703653216362
step: 4800, Loss: 0.11611717939376831
step: 4900, Loss: 0.11508650332689285
step: 5000, Loss: 0.11484073847532272
step: 5100, Loss: 0.11435706913471222
step: 5200, Loss: 0.11421270668506622
step: 5300, Loss: 0.1921752691268921
step: 5400, Loss: 0.11516807228326797
step: 5500, Loss: 0.11311739683151245
step: 5600, Loss: 0.11360751837491989
step: 5700, Loss: 0.11462688446044922
step: 5800, Loss: 0.11423785984516144
step: 5900, Loss: 0.11386828124523163
step: 6000, Loss: 0.11283193528652191
step: 6100, Loss: 0.1143333911895752
step: 6200, Loss: 0.11480182409286499
step: 6300, Loss: 0.1128561943769455
step: 6400, Loss: 0.11573530733585358
step: 6500, Loss: 0.1134188249707222
step: 6600, Loss: 0.11337769776582718
step: 6700, Loss: 0.11351458728313446
step: 6800, Loss: 0.11359007656574249
step: 6900, Loss: 0.11408306658267975
step: 7000, Loss: 0.11382259428501129
step: 7100, Loss: 0.1147518903017044
step: 7200, Loss: 0.19283756613731384
step: 7300, Loss: 0.11543607711791992
step: 7400, Loss: 0.1150682270526886
step: 7500, Loss: 0.11491487175226212
step: 7600, Loss: 0.11476342380046844
step: 7700, Loss: 0.1150425374507904
step: 7800, Loss: 0.1166156679391861
step: 7900, Loss: 0.11339253187179565
step: 8000, Loss: 0.11342541873455048
step: 8100, Loss: 0.11419779807329178
step: 8200, Loss: 0.11304286122322083
step: 8300, Loss: 0.1198297068476677
step: 8400, Loss: 0.12022356688976288
step: 8500, Loss: 0.1146911010146141
step: 8600, Loss: 0.11368480324745178
step: 8700, Loss: 0.11347103118896484
step: 8800, Loss: 0.11339958012104034
step: 8900, Loss: 0.11546087265014648
step: 9000, Loss: 0.11725738644599915
step: 9100, Loss: 0.20301863551139832
step: 9200, Loss: 13.678450584411621
step: 9300, Loss: 0.43983709812164307
step: 9400, Loss: 0.20846228301525116
step: 9500, Loss: 0.13676047325134277
step: 9600, Loss: 0.12923574447631836
step: 9700, Loss: 0.13184720277786255
step: 9800, Loss: 0.1188054233789444
step: 9900, Loss: 0.128976508975029
training successfully ended.
validating...
validate data length:76
acc: 0.9583333333333334
precision: 0.9090909090909091
recall: 1.0
F_score: 0.9523809523809523
******fold 9******

Training... train_data length:684
step: 0, Loss: 0.12369993329048157
step: 100, Loss: 0.12087462842464447
step: 200, Loss: 0.11871754378080368
step: 300, Loss: 0.11750098317861557
step: 400, Loss: 0.11417224258184433
step: 500, Loss: 0.11355789005756378
step: 600, Loss: 0.11324949562549591
step: 700, Loss: 0.11375564336776733
step: 800, Loss: 0.11322913318872452
step: 900, Loss: 0.11481297761201859
step: 1000, Loss: 0.11426303535699844
step: 1100, Loss: 0.11615359783172607
step: 1200, Loss: 0.11584316194057465
step: 1300, Loss: 0.11601410806179047
step: 1400, Loss: 0.1170790046453476
step: 1500, Loss: 0.1975046545267105
step: 1600, Loss: 0.11521166563034058
step: 1700, Loss: 0.11416773498058319
step: 1800, Loss: 0.11447299271821976
step: 1900, Loss: 0.11554385721683502
step: 2000, Loss: 0.11527605354785919
step: 2100, Loss: 0.11508655548095703
step: 2200, Loss: 0.11701905727386475
step: 2300, Loss: 0.11638388782739639
step: 2400, Loss: 0.1162833645939827
step: 2500, Loss: 0.11848804354667664
step: 2600, Loss: 0.11599203944206238
step: 2700, Loss: 0.11555224657058716
step: 2800, Loss: 6.625294208526611
step: 2900, Loss: 0.21714648604393005
step: 3000, Loss: 0.19258159399032593
step: 3100, Loss: 0.14094644784927368
step: 3200, Loss: 0.1310855746269226
step: 3300, Loss: 0.12400270998477936
step: 3400, Loss: 0.21094569563865662
step: 3500, Loss: 0.12791158258914948
step: 3600, Loss: 0.12777617573738098
step: 3700, Loss: 0.12095889449119568
step: 3800, Loss: 0.11822766065597534
step: 3900, Loss: 0.1176004633307457
step: 4000, Loss: 0.11709821224212646
step: 4100, Loss: 0.12404540926218033
step: 4200, Loss: 0.11795181781053543
step: 4300, Loss: 0.11614704132080078
step: 4400, Loss: 0.11897478997707367
step: 4500, Loss: 0.11799049377441406
step: 4600, Loss: 0.11750782281160355
step: 4700, Loss: 0.11740577220916748
step: 4800, Loss: 0.1162610799074173
step: 4900, Loss: 0.11979468911886215
step: 5000, Loss: 0.11735091358423233
step: 5100, Loss: 0.11521223187446594
step: 5200, Loss: 0.11638569086790085
step: 5300, Loss: 0.19849708676338196
step: 5400, Loss: 0.11557843536138535
step: 5500, Loss: 0.11662720888853073
step: 5600, Loss: 0.11393088847398758
step: 5700, Loss: 0.11529101431369781
step: 5800, Loss: 0.11905261874198914
step: 5900, Loss: 0.11612556874752045
step: 6000, Loss: 0.11412547528743744
step: 6100, Loss: 0.11520043015480042
step: 6200, Loss: 0.11418052762746811
step: 6300, Loss: 0.11398733407258987
step: 6400, Loss: 0.11399029195308685
step: 6500, Loss: 0.11471560597419739
step: 6600, Loss: 0.11506415158510208
step: 6700, Loss: 0.11787602305412292
step: 6800, Loss: 0.11458297818899155
step: 6900, Loss: 0.1133372113108635
step: 7000, Loss: 0.11388149857521057
step: 7100, Loss: 0.1140640377998352
step: 7200, Loss: 0.19047369062900543
step: 7300, Loss: 0.11422853916883469
step: 7400, Loss: 0.11345966905355453
step: 7500, Loss: 0.11509813368320465
step: 7600, Loss: 0.11281481385231018
step: 7700, Loss: 0.11440269649028778
step: 7800, Loss: 0.1132403090596199
step: 7900, Loss: 0.11497490108013153
step: 8000, Loss: 0.11372342705726624
step: 8100, Loss: 0.11351364850997925
step: 8200, Loss: 0.11612741649150848
step: 8300, Loss: 0.11382642388343811
step: 8400, Loss: 0.1131557896733284
step: 8500, Loss: 0.11368576437234879
step: 8600, Loss: 0.11523796617984772
step: 8700, Loss: 0.11444778740406036
step: 8800, Loss: 0.1145927831530571
step: 8900, Loss: 0.11498099565505981
step: 9000, Loss: 0.11452376842498779
step: 9100, Loss: 0.19124440848827362
step: 9200, Loss: 0.11578628420829773
step: 9300, Loss: 0.11288145184516907
step: 9400, Loss: 0.11465868353843689
step: 9500, Loss: 0.11255589127540588
step: 9600, Loss: 0.11333650350570679
step: 9700, Loss: 0.11483071744441986
step: 9800, Loss: 0.11998651921749115
step: 9900, Loss: 7.822964668273926
training successfully ended.
validating...
validate data length:76
acc: 0.8472222222222222
precision: 0.7941176470588235
recall: 0.8709677419354839
F_score: 0.8307692307692308
******fold 10******

Training... train_data length:684
step: 0, Loss: 0.11988683044910431
step: 100, Loss: 0.12298126518726349
step: 200, Loss: 0.11631900817155838
step: 300, Loss: 0.11699602752923965
step: 400, Loss: 0.1141296848654747
step: 500, Loss: 0.11409026384353638
step: 600, Loss: 0.11695774644613266
step: 700, Loss: 0.11469750106334686
step: 800, Loss: 0.11455975472927094
step: 900, Loss: 0.11522378772497177
step: 1000, Loss: 0.11366966366767883
step: 1100, Loss: 0.1132051944732666
step: 1200, Loss: 0.11400997638702393
step: 1300, Loss: 0.11497882753610611
step: 1400, Loss: 0.11391229182481766
step: 1500, Loss: 0.19603896141052246
step: 1600, Loss: 0.11445674300193787
step: 1700, Loss: 0.11441651731729507
step: 1800, Loss: 0.11714540421962738
step: 1900, Loss: 0.11495114117860794
step: 2000, Loss: 0.12004212290048599
step: 2100, Loss: 0.11680854111909866
step: 2200, Loss: 0.11757681518793106
step: 2300, Loss: 0.11735936999320984
step: 2400, Loss: 0.11755375564098358
step: 2500, Loss: 0.11979435384273529
step: 2600, Loss: 0.48628532886505127
step: 2700, Loss: 0.13899226486682892
step: 2800, Loss: 0.139471173286438
step: 2900, Loss: 0.1304282546043396
step: 3000, Loss: 0.1383269876241684
step: 3100, Loss: 0.12117096781730652
step: 3200, Loss: 0.12012944370508194
step: 3300, Loss: 0.12455964833498001
step: 3400, Loss: 0.20174971222877502
step: 3500, Loss: 0.12272849678993225
step: 3600, Loss: 0.12458410859107971
step: 3700, Loss: 0.11894312500953674
step: 3800, Loss: 0.11810746043920517
step: 3900, Loss: 0.11817135661840439
step: 4000, Loss: 0.12031795084476471
step: 4100, Loss: 0.11816594004631042
step: 4200, Loss: 0.11424276232719421
step: 4300, Loss: 0.11841559410095215
step: 4400, Loss: 0.11678222566843033
step: 4500, Loss: 0.11487853527069092
step: 4600, Loss: 0.11476612836122513
step: 4700, Loss: 0.1206500232219696
step: 4800, Loss: 0.11650490015745163
step: 4900, Loss: 0.1229567900300026
step: 5000, Loss: 0.11545739322900772
step: 5100, Loss: 0.11314992606639862
step: 5200, Loss: 0.11566811054944992
step: 5300, Loss: 0.19806130230426788
step: 5400, Loss: 0.11550189554691315
step: 5500, Loss: 0.11725780367851257
step: 5600, Loss: 0.11391671001911163
step: 5700, Loss: 0.11636288464069366
step: 5800, Loss: 0.11463388055562973
step: 5900, Loss: 0.11637024581432343
step: 6000, Loss: 0.11380315572023392
step: 6100, Loss: 0.11427641659975052
step: 6200, Loss: 0.11357475072145462
step: 6300, Loss: 0.11365799605846405
step: 6400, Loss: 0.11391472816467285
step: 6500, Loss: 0.1131783127784729
step: 6600, Loss: 0.11545658111572266
step: 6700, Loss: 0.11745479702949524
step: 6800, Loss: 0.1149827092885971
step: 6900, Loss: 0.11353864520788193
step: 7000, Loss: 0.11333057284355164
step: 7100, Loss: 0.1145356297492981
step: 7200, Loss: 0.1922144889831543
step: 7300, Loss: 0.11389842629432678
step: 7400, Loss: 0.11421174556016922
step: 7500, Loss: 0.11334561556577682
step: 7600, Loss: 0.11334048211574554
step: 7700, Loss: 0.11450209468603134
step: 7800, Loss: 0.11556768417358398
step: 7900, Loss: 0.11395657807588577
step: 8000, Loss: 0.11450771987438202
step: 8100, Loss: 0.1136287972331047
step: 8200, Loss: 0.11457046866416931
step: 8300, Loss: 0.11449384689331055
step: 8400, Loss: 0.11504719406366348
step: 8500, Loss: 0.11402685940265656
step: 8600, Loss: 0.11619921773672104
step: 8700, Loss: 0.11551899462938309
step: 8800, Loss: 0.11415748298168182
step: 8900, Loss: 0.11441554129123688
step: 9000, Loss: 0.11395332217216492
step: 9100, Loss: 0.19375327229499817
step: 9200, Loss: 0.1160876601934433
step: 9300, Loss: 2.201160430908203
step: 9400, Loss: 0.15293051302433014
step: 9500, Loss: 0.13737617433071136
step: 9600, Loss: 0.13311055302619934
step: 9700, Loss: 0.12212064862251282
step: 9800, Loss: 0.12643930315971375
step: 9900, Loss: 0.12045805156230927
training successfully ended.
validating...
validate data length:76
acc: 0.9166666666666666
precision: 0.8571428571428571
recall: 1.0
F_score: 0.923076923076923
subject 31 Avgacc: 0.898611111111111 Avgfscore: 0.8964237341512258 
 Max acc:1.0, Max f score:1.0
all subject avg max acc:0.9963304924242424
 all subject avg avg acc:0.957053877112471
 avg max fs:0.9967074219097847
 avg avg fs:0.9579466823013453
