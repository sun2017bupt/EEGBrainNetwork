/home/sjf/eegall/main.py:993: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,0],dtype=int)
/home/sjf/eegall/main.py:1000: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,1],dtype=int)
/home/sjf/eegall/main.py:998: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,0],dtype=int)
/home/sjf/eegall/main.py:995: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,1],dtype=int)
/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:
  File "/home/sjf/eegall/main.py", line 1007, in <module>
    train_models, max_acc, avg_acc, avg_f_score, max_f_score = cross_validation(args, i, harm_data, base_data, use_graph, use_label,seed=76, device=device)
  File "/home/sjf/eegall/main.py", line 538, in cross_validation
    trained_model, loss_record, acc_record = train(args.limit, model, device, base_train_data, harm_train_data, train_graph, train_labels, loss_fn, optimizer)
  File "/home/sjf/eegall/main.py", line 183, in train
    loss = loss_fn(output, label)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sjf/eegall/customloss.py", line 11, in forward
    log_probs = F.log_softmax(input, dim=-1)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/nn/functional.py", line 1907, in log_softmax
    ret = input.log_softmax(dim)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
--------- FACED DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([123, 312, 32, 7]), harm_x: torch.Size([123, 312, 32, 7]) all_labels: torch.Size([123, 312, 4]) 
 base_graph: torch.Size([123, 312, 32, 32]) harm_graph: torch.Size([123, 312, 32, 32])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave8//home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[52, 260]
******fold 1******

*******Initializing new model*******
Training... train_data length:468
step: 0, Loss: nan Acc:0.45454545454545453
Traceback (most recent call last):
  File "/home/sjf/eegall/main.py", line 1007, in <module>
    train_models, max_acc, avg_acc, avg_f_score, max_f_score = cross_validation(args, i, harm_data, base_data, use_graph, use_label,seed=76, device=device)
  File "/home/sjf/eegall/main.py", line 538, in cross_validation
    trained_model, loss_record, acc_record = train(args.limit, model, device, base_train_data, harm_train_data, train_graph, train_labels, loss_fn, optimizer)
  File "/home/sjf/eegall/main.py", line 197, in train
    loss.backward()
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:
  File "/home/sjf/eegall/main.py", line 1007, in <module>
    train_models, max_acc, avg_acc, avg_f_score, max_f_score = cross_validation(args, i, harm_data, base_data, use_graph, use_label,seed=76, device=device)
  File "/home/sjf/eegall/main.py", line 538, in cross_validation
    trained_model, loss_record, acc_record = train(args.limit, model, device, base_train_data, harm_train_data, train_graph, train_labels, loss_fn, optimizer)
  File "/home/sjf/eegall/main.py", line 183, in train
    loss = loss_fn(output, label)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sjf/eegall/customloss.py", line 11, in forward
    log_probs = F.log_softmax(input, dim=-1)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/nn/functional.py", line 1907, in log_softmax
    ret = input.log_softmax(dim)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
--------- DEAP DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([32, 760, 40, 7]), harm_x: torch.Size([32, 760, 40, 7]) all_labels: torch.Size([32, 760, 4]) 
 base_graph: torch.Size([32, 760, 40, 40]) harm_graph: torch.Size([32, 760, 40, 40])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave8//home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[228, 532]
******fold 1******

*******Initializing new model*******
Training... train_data length:957
step: 0, Loss: nan Acc:0.5
Traceback (most recent call last):
  File "/home/sjf/eegall/main.py", line 1007, in <module>
    train_models, max_acc, avg_acc, avg_f_score, max_f_score = cross_validation(args, i, harm_data, base_data, use_graph, use_label,seed=76, device=device)
  File "/home/sjf/eegall/main.py", line 538, in cross_validation
    trained_model, loss_record, acc_record = train(args.limit, model, device, base_train_data, harm_train_data, train_graph, train_labels, loss_fn, optimizer)
  File "/home/sjf/eegall/main.py", line 197, in train
    loss.backward()
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sjf/anaconda3/envs/brain/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
/home/sjf/eegall/main.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
--------- FACED DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([123, 312, 32, 7]), harm_x: torch.Size([123, 312, 32, 7]) all_labels: torch.Size([123, 312, 4]) 
 base_graph: torch.Size([123, 312, 32, 32]) harm_graph: torch.Size([123, 312, 32, 32])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave8//home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[130, 182]
******fold 1******

*******Initializing new model*******
Training... train_data length:327
step: 0, Loss: 4.230801582336426 Acc:0.36363636363636365
step: 100, Loss: 2.7618701457977295 Acc:0.8181818181818182
step: 200, Loss: 2.956570863723755 Acc:0.7272727272727273
step: 300, Loss: 0.5522599220275879 Acc:1.0
step: 400, Loss: 0.30236566066741943 Acc:1.0
step: 500, Loss: 0.37917855381965637 Acc:1.0
step: 600, Loss: 0.29664304852485657 Acc:1.0
step: 700, Loss: 0.4204961061477661 Acc:1.0
step: 800, Loss: 0.3686665892601013 Acc:1.0
step: 900, Loss: 0.31008630990982056 Acc:1.0
step: 1000, Loss: 0.2778695225715637 Acc:1.0
step: 1100, Loss: 0.35356444120407104 Acc:1.0
step: 1200, Loss: 0.28923487663269043 Acc:1.0
step: 1300, Loss: 0.27965500950813293 Acc:1.0
step: 1400, Loss: 0.27568918466567993 Acc:1.0
step: 1500, Loss: 0.3210989832878113 Acc:1.0
step: 1600, Loss: 0.2805631160736084 Acc:1.0
step: 1700, Loss: 0.33255648612976074 Acc:1.0
step: 1800, Loss: 0.29111671447753906 Acc:1.0
step: 1900, Loss: 0.27954256534576416 Acc:1.0
step: 2000, Loss: 0.2975226044654846 Acc:1.0
step: 2100, Loss: 0.2853047251701355 Acc:1.0
step: 2200, Loss: 0.27974647283554077 Acc:1.0
step: 2300, Loss: 0.2720758616924286 Acc:1.0
step: 2400, Loss: 0.27665042877197266 Acc:1.0
step: 2500, Loss: 0.27555859088897705 Acc:1.0
step: 2600, Loss: 0.31867414712905884 Acc:1.0
step: 2700, Loss: 0.2692773938179016 Acc:1.0
step: 2800, Loss: 0.2800532877445221 Acc:1.0
step: 2900, Loss: 0.2759694457054138 Acc:1.0
step: 3000, Loss: 0.2913706302642822 Acc:1.0
step: 3100, Loss: 0.2800401449203491 Acc:1.0
step: 3200, Loss: 0.4380079209804535 Acc:1.0
step: 3300, Loss: 0.26686403155326843 Acc:1.0
step: 3400, Loss: 0.27201154828071594 Acc:1.0
step: 3500, Loss: 0.2746415138244629 Acc:1.0
step: 3600, Loss: 0.2764192819595337 Acc:1.0
step: 3700, Loss: 0.2720491290092468 Acc:1.0
step: 3800, Loss: 0.2795805335044861 Acc:1.0
step: 3900, Loss: 0.2862927317619324 Acc:1.0
step: 4000, Loss: 0.28570398688316345 Acc:1.0
step: 4100, Loss: 0.3194696009159088 Acc:1.0
step: 4200, Loss: 0.28740382194519043 Acc:1.0
step: 4300, Loss: 0.26842501759529114 Acc:1.0
step: 4400, Loss: 0.2705257833003998 Acc:1.0
step: 4500, Loss: 0.2702029347419739 Acc:1.0
step: 4600, Loss: 0.2713402211666107 Acc:1.0
step: 4700, Loss: 0.2763065993785858 Acc:1.0
step: 4800, Loss: 0.2736341953277588 Acc:1.0
step: 4900, Loss: 0.27257686853408813 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 0.6388888888888888
precision: 0.55
recall: 0.7333333333333333
F_score: 0.6285714285714286
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:327
step: 0, Loss: 2.2424371242523193 Acc:0.8181818181818182
step: 100, Loss: 0.2665073275566101 Acc:1.0
step: 200, Loss: 0.2724642753601074 Acc:1.0
step: 300, Loss: 0.26549798250198364 Acc:1.0
step: 400, Loss: 0.2658722996711731 Acc:1.0
step: 500, Loss: 0.2713174819946289 Acc:1.0
step: 600, Loss: 0.26919588446617126 Acc:1.0
step: 700, Loss: 0.2740720808506012 Acc:1.0
step: 800, Loss: 0.2729421555995941 Acc:1.0
step: 900, Loss: 0.2693665623664856 Acc:1.0
step: 1000, Loss: 0.27000463008880615 Acc:1.0
step: 1100, Loss: 0.2698485553264618 Acc:1.0
step: 1200, Loss: 0.2657313942909241 Acc:1.0
step: 1300, Loss: 0.27089494466781616 Acc:1.0
step: 1400, Loss: 0.2762088477611542 Acc:1.0
step: 1500, Loss: 0.26479724049568176 Acc:1.0
step: 1600, Loss: 0.26995670795440674 Acc:1.0
step: 1700, Loss: 0.27426356077194214 Acc:1.0
step: 1800, Loss: 0.26508861780166626 Acc:1.0
step: 1900, Loss: 0.26800811290740967 Acc:1.0
step: 2000, Loss: 0.2705928087234497 Acc:1.0
step: 2100, Loss: 0.2668478786945343 Acc:1.0
step: 2200, Loss: 0.2657491862773895 Acc:1.0
step: 2300, Loss: 0.2667975127696991 Acc:1.0
step: 2400, Loss: 0.2658829689025879 Acc:1.0
step: 2500, Loss: 0.2666456699371338 Acc:1.0
step: 2600, Loss: 0.2666934132575989 Acc:1.0
step: 2700, Loss: 0.26886385679244995 Acc:1.0
step: 2800, Loss: 0.2668304443359375 Acc:1.0
step: 2900, Loss: 0.26607680320739746 Acc:1.0
step: 3000, Loss: 0.268475204706192 Acc:1.0
step: 3100, Loss: 0.2676166594028473 Acc:1.0
step: 3200, Loss: 0.265992671251297 Acc:1.0
step: 3300, Loss: 0.265897274017334 Acc:1.0
step: 3400, Loss: 0.26859354972839355 Acc:1.0
step: 3500, Loss: 0.2671259045600891 Acc:1.0
step: 3600, Loss: 0.2654884457588196 Acc:1.0
step: 3700, Loss: 0.2655656933784485 Acc:1.0
step: 3800, Loss: 0.26659032702445984 Acc:1.0
step: 3900, Loss: 0.265459805727005 Acc:1.0
step: 4000, Loss: 0.2668125033378601 Acc:1.0
step: 4100, Loss: 0.2672770619392395 Acc:1.0
step: 4200, Loss: 0.26517918705940247 Acc:1.0
step: 4300, Loss: 0.26828721165657043 Acc:1.0
step: 4400, Loss: 0.2659381926059723 Acc:1.0
step: 4500, Loss: 0.2644639313220978 Acc:1.0
step: 4600, Loss: 0.265238493680954 Acc:1.0
step: 4700, Loss: 0.2661539912223816 Acc:1.0
step: 4800, Loss: 0.26461416482925415 Acc:1.0
step: 4900, Loss: 0.2653029263019562 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 0.9722222222222222
precision: 0.95
recall: 1.0
F_score: 0.9743589743589743
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:327
step: 0, Loss: 0.2817775011062622 Acc:1.0
step: 100, Loss: 0.2651219964027405 Acc:1.0
step: 200, Loss: 0.26477712392807007 Acc:1.0
step: 300, Loss: 0.2653600871562958 Acc:1.0
step: 400, Loss: 0.2655814588069916 Acc:1.0
step: 500, Loss: 0.26606258749961853 Acc:1.0
step: 600, Loss: 0.2646321952342987 Acc:1.0
step: 700, Loss: 0.26524436473846436 Acc:1.0
step: 800, Loss: 0.2654682695865631 Acc:1.0
step: 900, Loss: 0.26445937156677246 Acc:1.0
step: 1000, Loss: 0.26563888788223267 Acc:1.0
step: 1100, Loss: 0.2654983103275299 Acc:1.0
step: 1200, Loss: 0.265460729598999 Acc:1.0
step: 1300, Loss: 0.26652416586875916 Acc:1.0
step: 1400, Loss: 0.2659441828727722 Acc:1.0
step: 1500, Loss: 0.2658347487449646 Acc:1.0
step: 1600, Loss: 0.264697402715683 Acc:1.0
step: 1700, Loss: 0.2653922140598297 Acc:1.0
step: 1800, Loss: 0.26443299651145935 Acc:1.0
step: 1900, Loss: 0.26458296179771423 Acc:1.0
step: 2000, Loss: 0.2644083499908447 Acc:1.0
step: 2100, Loss: 0.2643856406211853 Acc:1.0
step: 2200, Loss: 0.26494720578193665 Acc:1.0
step: 2300, Loss: 0.2659461498260498 Acc:1.0
step: 2400, Loss: 0.26530027389526367 Acc:1.0
step: 2500, Loss: 0.2655252516269684 Acc:1.0
step: 2600, Loss: 0.2657696008682251 Acc:1.0
step: 2700, Loss: 0.2650008201599121 Acc:1.0
step: 2800, Loss: 0.26518315076828003 Acc:1.0
step: 2900, Loss: 0.2658078968524933 Acc:1.0
step: 3000, Loss: 0.2649311125278473 Acc:1.0
step: 3100, Loss: 0.2646360397338867 Acc:1.0
step: 3200, Loss: 0.26601526141166687 Acc:1.0
step: 3300, Loss: 0.26436179876327515 Acc:1.0
step: 3400, Loss: 0.26470252871513367 Acc:1.0
step: 3500, Loss: 0.2649584114551544 Acc:1.0
step: 3600, Loss: 0.2645573318004608 Acc:1.0
step: 3700, Loss: 0.26460516452789307 Acc:1.0
step: 3800, Loss: 0.26487234234809875 Acc:1.0
step: 3900, Loss: 0.2644891142845154 Acc:1.0
step: 4000, Loss: 0.2644001841545105 Acc:1.0
step: 4100, Loss: 0.2647738456726074 Acc:1.0
step: 4200, Loss: 0.2651858627796173 Acc:1.0
step: 4300, Loss: 0.2655901610851288 Acc:1.0
step: 4400, Loss: 0.26696062088012695 Acc:1.0
step: 4500, Loss: 0.2648237347602844 Acc:1.0
step: 4600, Loss: 0.2643633782863617 Acc:1.0
step: 4700, Loss: 0.2647991478443146 Acc:1.0
step: 4800, Loss: 0.26471251249313354 Acc:1.0
step: 4800, Loss: 0.26427605748176575 Acc:1.0
step: 4900, Loss: 0.2643725275993347 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 0.6944444444444444
precision: 0.6818181818181818
recall: 0.7894736842105263
F_score: 0.7317073170731707
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:327
step: 0, Loss: 7.775956630706787 Acc:0.7272727272727273
step: 100, Loss: 1.0913374423980713 Acc:0.9090909090909091
step: 200, Loss: 0.3648069500923157 Acc:1.0
step: 300, Loss: 0.3512410521507263 Acc:1.0
step: 400, Loss: 0.2928849458694458 Acc:1.0
step: 500, Loss: 0.2815294861793518 Acc:1.0
step: 600, Loss: 0.28765037655830383 Acc:1.0
step: 700, Loss: 0.27238622307777405 Acc:1.0
step: 800, Loss: 0.2948392331600189 Acc:1.0
step: 900, Loss: 0.2868667244911194 Acc:1.0
step: 1000, Loss: 0.2664095163345337 Acc:1.0
step: 1100, Loss: 0.27763456106185913 Acc:1.0
step: 1200, Loss: 0.27185094356536865 Acc:1.0
step: 1300, Loss: 0.2726544737815857 Acc:1.0
step: 1400, Loss: 0.2863503396511078 Acc:1.0
step: 1500, Loss: 0.26643145084381104 Acc:1.0
step: 1600, Loss: 0.268822580575943 Acc:1.0
step: 1700, Loss: 0.2740101218223572 Acc:1.0
step: 1800, Loss: 0.2713930010795593 Acc:1.0
step: 1900, Loss: 0.2664205729961395 Acc:1.0
step: 2000, Loss: 0.3727785348892212 Acc:1.0
step: 2100, Loss: 0.616058886051178 Acc:1.0
step: 2200, Loss: 0.2769772708415985 Acc:1.0
step: 2300, Loss: 0.2825618088245392 Acc:1.0
step: 2400, Loss: 0.27006232738494873 Acc:1.0
step: 2500, Loss: 0.2726387083530426 Acc:1.0
step: 2600, Loss: 0.27990543842315674 Acc:1.0
step: 2700, Loss: 0.2655830383300781 Acc:1.0
step: 2800, Loss: 0.26581236720085144 Acc:1.0
step: 2900, Loss: 0.2734748125076294 Acc:1.0
step: 3000, Loss: 0.26531916856765747 Acc:1.0
step: 3100, Loss: 0.26833945512771606 Acc:1.0
step: 3200, Loss: 0.2721506357192993 Acc:1.0
step: 3300, Loss: 0.2675361633300781 Acc:1.0
step: 3400, Loss: 0.26519790291786194 Acc:1.0
step: 3500, Loss: 0.2701683044433594 Acc:1.0
step: 3600, Loss: 0.2654271721839905 Acc:1.0
step: 3700, Loss: 0.26602283120155334 Acc:1.0
step: 3800, Loss: 0.2681085169315338 Acc:1.0
step: 3900, Loss: 0.2648732662200928 Acc:1.0
step: 4000, Loss: 0.2649713158607483 Acc:1.0
step: 4100, Loss: 0.26640498638153076 Acc:1.0
step: 4200, Loss: 2.6103172302246094 Acc:0.9090909090909091
step: 4300, Loss: 0.3286327123641968 Acc:1.0
step: 4400, Loss: 0.2985759377479553 Acc:1.0
step: 4500, Loss: 0.27153298258781433 Acc:1.0
step: 4600, Loss: 0.26903364062309265 Acc:1.0
step: 4700, Loss: 0.2821546196937561 Acc:1.0
step: 4800, Loss: 0.26610249280929565 Acc:1.0
step: 4900, Loss: 0.2667997479438782 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 0.6944444444444444
precision: 0.6666666666666666
recall: 0.7058823529411765
F_score: 0.6857142857142857
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 2.0863375663757324 Acc:0.9090909090909091
step: 100, Loss: 0.42195388674736023 Acc:1.0
step: 200, Loss: 0.36780738830566406 Acc:1.0
step: 300, Loss: 0.29327714443206787 Acc:1.0
step: 400, Loss: 0.2684340476989746 Acc:1.0
step: 500, Loss: 0.3134438991546631 Acc:1.0
step: 600, Loss: 0.2757936716079712 Acc:1.0
step: 700, Loss: 0.2678337097167969 Acc:1.0
step: 800, Loss: 0.2711063623428345 Acc:1.0
step: 900, Loss: 0.2729756832122803 Acc:1.0
step: 1000, Loss: 0.267355740070343 Acc:1.0
step: 1100, Loss: 0.2706645429134369 Acc:1.0
step: 1200, Loss: 0.2662562131881714 Acc:1.0
step: 1300, Loss: 0.2671329975128174 Acc:1.0
step: 1400, Loss: 0.2666444480419159 Acc:1.0
step: 1500, Loss: 0.26529043912887573 Acc:1.0
step: 1600, Loss: 0.26518380641937256 Acc:1.0
step: 1700, Loss: 0.26497113704681396 Acc:1.0
step: 1800, Loss: 0.2657339572906494 Acc:1.0
step: 1900, Loss: 0.2648954391479492 Acc:1.0
step: 2000, Loss: 0.26555562019348145 Acc:1.0
step: 2100, Loss: 0.2649851143360138 Acc:1.0
step: 2200, Loss: 0.2655090391635895 Acc:1.0
step: 2300, Loss: 0.2643185555934906 Acc:1.0
step: 2400, Loss: 0.2645958662033081 Acc:1.0
step: 2500, Loss: 0.26433220505714417 Acc:1.0
step: 2600, Loss: 0.2644132077693939 Acc:1.0
step: 2700, Loss: 0.26425623893737793 Acc:1.0
step: 2800, Loss: 0.2645402252674103 Acc:1.0
step: 2900, Loss: 0.2643982768058777 Acc:1.0
step: 3000, Loss: 0.2642931044101715 Acc:1.0
step: 3100, Loss: 0.26465243101119995 Acc:1.0
step: 3200, Loss: 0.2642424702644348 Acc:1.0
step: 3300, Loss: 0.26424112915992737 Acc:1.0
step: 3400, Loss: 0.2644301950931549 Acc:1.0
step: 3500, Loss: 0.26419100165367126 Acc:1.0
step: 3600, Loss: 0.2641560435295105 Acc:1.0
step: 3700, Loss: 0.2645571231842041 Acc:1.0
step: 3800, Loss: 0.2641395330429077 Acc:1.0
step: 3900, Loss: 0.26420074701309204 Acc:1.0
step: 4000, Loss: 0.26434093713760376 Acc:1.0
step: 4100, Loss: 0.26408690214157104 Acc:1.0
step: 4200, Loss: 0.26424476504325867 Acc:1.0
step: 4300, Loss: 0.2642603814601898 Acc:1.0
step: 4400, Loss: 0.2640973627567291 Acc:1.0
step: 4500, Loss: 0.2641315460205078 Acc:1.0
step: 4600, Loss: 0.26450175046920776 Acc:1.0
step: 4700, Loss: 0.2641217112541199 Acc:1.0
step: 4800, Loss: 0.264081209897995 Acc:1.0
step: 4900, Loss: 0.26411014795303345 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.6388888888888888
precision: 0.65
recall: 0.6842105263157895
F_score: 0.6666666666666667
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.5727972984313965 Acc:1.0
step: 100, Loss: 2.008610963821411 Acc:0.9090909090909091
step: 200, Loss: 0.6932083368301392 Acc:1.0
step: 300, Loss: 0.305248886346817 Acc:1.0
step: 400, Loss: 0.5599001049995422 Acc:1.0
step: 500, Loss: 0.2738703191280365 Acc:1.0
step: 600, Loss: 0.2746330201625824 Acc:1.0
step: 700, Loss: 0.2787865400314331 Acc:1.0
step: 800, Loss: 0.2713472545146942 Acc:1.0
step: 900, Loss: 0.2671871781349182 Acc:1.0
step: 1000, Loss: 0.2687089741230011 Acc:1.0
step: 1100, Loss: 0.26562249660491943 Acc:1.0
step: 1200, Loss: 0.2651505768299103 Acc:1.0
step: 1300, Loss: 0.2662089169025421 Acc:1.0
step: 1400, Loss: 0.2662948668003082 Acc:1.0
step: 1500, Loss: 0.2644611597061157 Acc:1.0
step: 1600, Loss: 0.28303202986717224 Acc:1.0
step: 1700, Loss: 0.2761386036872864 Acc:1.0
step: 1800, Loss: 0.2715436816215515 Acc:1.0
step: 1900, Loss: 0.2669368386268616 Acc:1.0
step: 2000, Loss: 0.2662448287010193 Acc:1.0
step: 2100, Loss: 0.26585474610328674 Acc:1.0
step: 2200, Loss: 0.2649396061897278 Acc:1.0
step: 2300, Loss: 0.26488083600997925 Acc:1.0
step: 2400, Loss: 0.26453638076782227 Acc:1.0
step: 2500, Loss: 0.2643323242664337 Acc:1.0
step: 2600, Loss: 0.2667534053325653 Acc:1.0
step: 2700, Loss: 0.2646949589252472 Acc:1.0
step: 2800, Loss: 0.2643744945526123 Acc:1.0
step: 2900, Loss: 0.26512864232063293 Acc:1.0
step: 3000, Loss: 0.26442578434944153 Acc:1.0
step: 3100, Loss: 0.2642134130001068 Acc:1.0
step: 3200, Loss: 0.2643203139305115 Acc:1.0
step: 3300, Loss: 0.2641680836677551 Acc:1.0
step: 3400, Loss: 0.2641340494155884 Acc:1.0
step: 3500, Loss: 0.26445630192756653 Acc:1.0
step: 3600, Loss: 0.2642410099506378 Acc:1.0
step: 3700, Loss: 0.2642718553543091 Acc:1.0
step: 3800, Loss: 0.2642672657966614 Acc:1.0
step: 3900, Loss: 0.26419758796691895 Acc:1.0
step: 4000, Loss: 0.2640887200832367 Acc:1.0
step: 4100, Loss: 0.264534592628479 Acc:1.0
step: 4200, Loss: 0.26419469714164734 Acc:1.0
step: 4300, Loss: 0.26415330171585083 Acc:1.0
step: 4400, Loss: 0.26427435874938965 Acc:1.0
step: 4500, Loss: 0.26416105031967163 Acc:1.0
step: 4600, Loss: 0.2641324996948242 Acc:1.0
step: 4700, Loss: 0.2649891674518585 Acc:1.0
step: 4800, Loss: 0.26422595977783203 Acc:1.0
step: 4900, Loss: 0.2641325294971466 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.7222222222222222
precision: 0.7368421052631579
recall: 0.7368421052631579
F_score: 0.7368421052631579
step: 4800, Loss: 0.26648592948913574 Acc:1.0
step: 4900, Loss: 0.2692888677120209 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.8333333333333334
precision: 0.8636363636363636
recall: 0.7916666666666666
F_score: 0.8260869565217391
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 0.4066721200942993 Acc:1.0
step: 100, Loss: 0.2889542579650879 Acc:1.0
step: 200, Loss: 0.32339566946029663 Acc:1.0
step: 300, Loss: 0.40191102027893066 Acc:1.0
step: 400, Loss: 0.2704332768917084 Acc:1.0
step: 500, Loss: 0.2651281952857971 Acc:1.0
step: 600, Loss: 0.2690999507904053 Acc:1.0
step: 700, Loss: 0.29197177290916443 Acc:1.0
step: 800, Loss: 0.28981679677963257 Acc:1.0
step: 900, Loss: 0.28214532136917114 Acc:1.0
step: 1000, Loss: 0.2786928117275238 Acc:1.0
step: 1100, Loss: 0.26744699478149414 Acc:1.0
step: 1200, Loss: 0.2702149450778961 Acc:1.0
step: 1300, Loss: 0.32538244128227234 Acc:1.0
step: 1400, Loss: 0.3290397524833679 Acc:1.0
step: 1500, Loss: 0.2713923752307892 Acc:1.0
step: 1600, Loss: 0.27376729249954224 Acc:1.0
step: 1700, Loss: 0.2670029103755951 Acc:1.0
step: 1800, Loss: 0.26540452241897583 Acc:1.0
step: 1900, Loss: 0.27057769894599915 Acc:1.0
step: 2000, Loss: 0.27367207407951355 Acc:1.0
step: 2100, Loss: 0.2665005922317505 Acc:1.0
step: 2200, Loss: 0.26526057720184326 Acc:1.0
step: 2300, Loss: 0.266468346118927 Acc:1.0
step: 2400, Loss: 0.26590627431869507 Acc:1.0
step: 2500, Loss: 0.26477909088134766 Acc:1.0
step: 2600, Loss: 0.2651359438896179 Acc:1.0
step: 2700, Loss: 0.26516827940940857 Acc:1.0
step: 2800, Loss: 0.2655627727508545 Acc:1.0
step: 2900, Loss: 0.26471665501594543 Acc:1.0
step: 3000, Loss: 0.264643132686615 Acc:1.0
step: 3100, Loss: 0.2650315463542938 Acc:1.0
step: 3200, Loss: 0.2647542655467987 Acc:1.0
step: 3300, Loss: 0.26662787795066833 Acc:1.0
step: 3400, Loss: 0.26455917954444885 Acc:1.0
step: 3500, Loss: 0.26482459902763367 Acc:1.0
step: 3600, Loss: 0.26458829641342163 Acc:1.0
step: 3700, Loss: 0.2665638327598572 Acc:1.0
step: 3800, Loss: 0.265097439289093 Acc:1.0
step: 3900, Loss: 0.2652215361595154 Acc:1.0
step: 4000, Loss: 0.2677527368068695 Acc:1.0
step: 4100, Loss: 0.26426616311073303 Acc:1.0
step: 4200, Loss: 0.26424670219421387 Acc:1.0
step: 4300, Loss: 0.26469701528549194 Acc:1.0
step: 4400, Loss: 0.26419103145599365 Acc:1.0
step: 4500, Loss: 0.2664424180984497 Acc:1.0
step: 4600, Loss: 0.40856409072875977 Acc:1.0
step: 4700, Loss: 0.26475390791893005 Acc:1.0
step: 4800, Loss: 0.26414453983306885 Acc:1.0
step: 4900, Loss: 0.2646713852882385 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.875
precision: 0.875
recall: 0.875
F_score: 0.875
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 3.290984869003296 Acc:0.8181818181818182
step: 100, Loss: 0.2938910126686096 Acc:1.0
step: 200, Loss: 0.27060559391975403 Acc:1.0
step: 300, Loss: 0.41027361154556274 Acc:1.0
step: 400, Loss: 0.27528494596481323 Acc:1.0
step: 500, Loss: 0.27364227175712585 Acc:1.0
step: 600, Loss: 0.2730588912963867 Acc:1.0
step: 700, Loss: 0.2645018696784973 Acc:1.0
step: 800, Loss: 0.26742255687713623 Acc:1.0
step: 900, Loss: 0.27451229095458984 Acc:1.0
step: 1000, Loss: 0.2654179632663727 Acc:1.0
step: 1100, Loss: 0.2650763988494873 Acc:1.0
step: 1200, Loss: 0.26498836278915405 Acc:1.0
step: 1300, Loss: 0.26643237471580505 Acc:1.0
step: 1400, Loss: 0.2646595537662506 Acc:1.0
step: 1500, Loss: 0.26517951488494873 Acc:1.0
step: 1600, Loss: 0.2652605473995209 Acc:1.0
step: 1700, Loss: 0.26445499062538147 Acc:1.0
step: 1800, Loss: 0.26927679777145386 Acc:1.0
step: 1900, Loss: 0.26465660333633423 Acc:1.0
step: 2000, Loss: 0.26485881209373474 Acc:1.0
step: 2100, Loss: 0.2643856406211853 Acc:1.0
step: 2200, Loss: 0.27141073346138 Acc:1.0
step: 2300, Loss: 0.3675496280193329 Acc:1.0
step: 2400, Loss: 0.28718724846839905 Acc:1.0
step: 2500, Loss: 0.27158868312835693 Acc:1.0
step: 2600, Loss: 0.2685787081718445 Acc:1.0
step: 2700, Loss: 0.26895609498023987 Acc:1.0
step: 2800, Loss: 0.26649394631385803 Acc:1.0
step: 2900, Loss: 0.2648438513278961 Acc:1.0
step: 3000, Loss: 0.26687875390052795 Acc:1.0
step: 3100, Loss: 0.27103665471076965 Acc:1.0
step: 3200, Loss: 0.265470415353775 Acc:1.0
step: 3300, Loss: 0.2644850015640259 Acc:1.0
step: 3400, Loss: 0.2654893100261688 Acc:1.0
step: 3500, Loss: 0.26498734951019287 Acc:1.0
step: 3600, Loss: 0.2643046975135803 Acc:1.0
step: 3700, Loss: 0.2691992223262787 Acc:1.0
step: 3800, Loss: 0.26571905612945557 Acc:1.0
step: 3900, Loss: 0.2648461163043976 Acc:1.0
step: 4000, Loss: 0.3383691906929016 Acc:1.0
step: 4100, Loss: 0.27995964884757996 Acc:1.0
step: 4200, Loss: 0.2684648633003235 Acc:1.0
step: 4300, Loss: 0.26701679825782776 Acc:1.0
step: 4400, Loss: 0.2663510739803314 Acc:1.0
step: 4500, Loss: 0.2662123739719391 Acc:1.0
step: 4600, Loss: 0.4525737166404724 Acc:1.0
step: 4700, Loss: 0.267412006855011 Acc:1.0
step: 4800, Loss: 0.29067519307136536 Acc:1.0
step: 4900, Loss: 0.2672284245491028 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.9166666666666666
precision: 0.9
recall: 0.9
F_score: 0.9
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 0.9904425740242004 Acc:0.9090909090909091
step: 100, Loss: 0.2868681848049164 Acc:1.0
step: 200, Loss: 0.2764061689376831 Acc:1.0
step: 300, Loss: 0.4073522984981537 Acc:1.0
step: 400, Loss: 0.26887238025665283 Acc:1.0
step: 500, Loss: 0.2711348533630371 Acc:1.0
step: 600, Loss: 0.2699955105781555 Acc:1.0
step: 700, Loss: 0.26606446504592896 Acc:1.0
step: 800, Loss: 0.2715393304824829 Acc:1.0
step: 900, Loss: 0.2855972349643707 Acc:1.0
step: 1000, Loss: 0.2801671326160431 Acc:1.0
step: 1100, Loss: 0.2712516784667969 Acc:1.0
step: 1200, Loss: 0.2864155173301697 Acc:1.0
step: 1300, Loss: 0.2742968201637268 Acc:1.0
step: 1400, Loss: 0.27079346776008606 Acc:1.0
step: 1500, Loss: 0.2672494649887085 Acc:1.0
step: 1600, Loss: 0.27606847882270813 Acc:1.0
step: 1700, Loss: 0.2691464424133301 Acc:1.0
step: 1800, Loss: 0.2720339894294739 Acc:1.0
step: 1900, Loss: 0.2661380171775818 Acc:1.0
step: 2000, Loss: 0.265695184469223 Acc:1.0
step: 2100, Loss: 0.26537370681762695 Acc:1.0
step: 2200, Loss: 0.2668948471546173 Acc:1.0
step: 2300, Loss: 0.2647704482078552 Acc:1.0
step: 2400, Loss: 0.2647099792957306 Acc:1.0
step: 2500, Loss: 0.2674514055252075 Acc:1.0
step: 2600, Loss: 0.26622024178504944 Acc:1.0
step: 2700, Loss: 0.2643577456474304 Acc:1.0
step: 2800, Loss: 0.26598626375198364 Acc:1.0
step: 2900, Loss: 0.2643511891365051 Acc:1.0
step: 3000, Loss: 0.2645989954471588 Acc:1.0
step: 3100, Loss: 0.26432541012763977 Acc:1.0
step: 3200, Loss: 0.2649686336517334 Acc:1.0
step: 3300, Loss: 0.264595091342926 Acc:1.0
step: 3400, Loss: 0.2647530436515808 Acc:1.0
step: 3500, Loss: 0.2647290825843811 Acc:1.0
step: 3600, Loss: 0.26429635286331177 Acc:1.0
step: 3700, Loss: 0.26499050855636597 Acc:1.0
step: 3800, Loss: 0.2643432021141052 Acc:1.0
step: 3900, Loss: 0.2643255293369293 Acc:1.0
step: 4000, Loss: 0.264579176902771 Acc:1.0
step: 4100, Loss: 0.2642076909542084 Acc:1.0
step: 4200, Loss: 0.2645280659198761 Acc:1.0
step: 4300, Loss: 0.27297401428222656 Acc:1.0
step: 4400, Loss: 0.3000372350215912 Acc:1.0
step: 4500, Loss: 0.2769889533519745 Acc:1.0
step: 4600, Loss: 0.42296266555786133 Acc:1.0
step: 4700, Loss: 0.2647083103656769 Acc:1.0
step: 4800, Loss: 0.2654058039188385 Acc:1.0
step: 4900, Loss: 0.2723425030708313 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.8958333333333334
precision: 0.875
recall: 0.9130434782608695
F_score: 0.8936170212765957
******fold 7******

******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 3.0887396335601807 Acc:0.8181818181818182
step: 100, Loss: 0.5396080017089844 Acc:1.0
step: 200, Loss: 0.5358328819274902 Acc:1.0
step: 300, Loss: 0.2989170253276825 Acc:1.0
step: 400, Loss: 0.28300395607948303 Acc:1.0
step: 500, Loss: 0.28868988156318665 Acc:1.0
step: 600, Loss: 0.285788893699646 Acc:1.0
step: 700, Loss: 0.26846492290496826 Acc:1.0
step: 800, Loss: 0.2769545912742615 Acc:1.0
step: 900, Loss: 0.27533888816833496 Acc:1.0
step: 1000, Loss: 0.27674010396003723 Acc:1.0
step: 1100, Loss: 0.27465078234672546 Acc:1.0
step: 1200, Loss: 0.27691081166267395 Acc:1.0
step: 1300, Loss: 0.28456881642341614 Acc:1.0
step: 1400, Loss: 0.29689785838127136 Acc:1.0
step: 1500, Loss: 0.27323824167251587 Acc:1.0
step: 1600, Loss: 0.2694081664085388 Acc:1.0
step: 1700, Loss: 0.27122625708580017 Acc:1.0
step: 1800, Loss: 0.2679905295372009 Acc:1.0
step: 1900, Loss: 0.267640620470047 Acc:1.0
step: 2000, Loss: 0.26713600754737854 Acc:1.0
step: 2100, Loss: 0.26647281646728516 Acc:1.0
step: 2200, Loss: 0.26624375581741333 Acc:1.0
step: 2300, Loss: 0.2650983929634094 Acc:1.0
step: 2400, Loss: 0.26579776406288147 Acc:1.0
step: 2500, Loss: 0.26581913232803345 Acc:1.0
step: 2600, Loss: 0.26521027088165283 Acc:1.0
step: 2700, Loss: 0.26524603366851807 Acc:1.0
step: 2800, Loss: 0.26478657126426697 Acc:1.0
step: 2900, Loss: 0.264528751373291 Acc:1.0
step: 3000, Loss: 0.2646951377391815 Acc:1.0
step: 3100, Loss: 0.2646375894546509 Acc:1.0
step: 3200, Loss: 0.2643136978149414 Acc:1.0
step: 3300, Loss: 0.26454639434814453 Acc:1.0
step: 3400, Loss: 0.2643187344074249 Acc:1.0
step: 3500, Loss: 0.26419582962989807 Acc:1.0
step: 3600, Loss: 0.2642374336719513 Acc:1.0
step: 3700, Loss: 0.2644352316856384 Acc:1.0
step: 3800, Loss: 0.26432672142982483 Acc:1.0
step: 3900, Loss: 0.2641536593437195 Acc:1.0
step: 4000, Loss: 0.2641763687133789 Acc:1.0
step: 4100, Loss: 0.26413241028785706 Acc:1.0
step: 4200, Loss: 0.26425284147262573 Acc:1.0
step: 4300, Loss: 0.2643706500530243 Acc:1.0
step: 4400, Loss: 0.26417338848114014 Acc:1.0
step: 4500, Loss: 0.2641134262084961 Acc:1.0
step: 4600, Loss: 0.26416245102882385 Acc:1.0
step: 4700, Loss: 0.2641770839691162 Acc:1.0
step: 4800, Loss: 0.2641257345676422 Acc:1.0
step: 4900, Loss: 0.2641436457633972 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.6944444444444444
precision: 0.9333333333333333
recall: 0.5833333333333334
F_score: 0.7179487179487181
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26413655281066895 Acc:1.0
step: 100, Loss: 0.44232290983200073 Acc:1.0
step: 200, Loss: 1.96048903465271 Acc:0.9090909090909091
step: 300, Loss: 0.3007628917694092 Acc:1.0
step: 400, Loss: 0.32593539357185364 Acc:1.0
step: 500, Loss: 0.4220188558101654 Acc:1.0
step: 600, Loss: 0.2784824073314667 Acc:1.0
step: 700, Loss: 0.27997925877571106 Acc:1.0
step: 800, Loss: 0.3009471893310547 Acc:1.0
step: 900, Loss: 0.2901613712310791 Acc:1.0
step: 1000, Loss: 0.27767473459243774 Acc:1.0
step: 1100, Loss: 0.2843077480792999 Acc:1.0
step: 1200, Loss: 0.2766039967536926 Acc:1.0
step: 1300, Loss: 0.2779373526573181 Acc:1.0
step: 1400, Loss: 0.3109137713909149 Acc:1.0
step: 1500, Loss: 0.27945399284362793 Acc:1.0
step: 1600, Loss: 0.271516352891922 Acc:1.0
step: 1700, Loss: 0.2779003381729126 Acc:1.0
step: 1800, Loss: 0.26903972029685974 Acc:1.0
step: 1900, Loss: 0.2667137384414673 Acc:1.0
step: 2000, Loss: 0.2725834250450134 Acc:1.0
step: 2100, Loss: 0.26700296998023987 Acc:1.0
step: 2200, Loss: 0.2649638056755066 Acc:1.0
step: 2300, Loss: 0.269565612077713 Acc:1.0
step: 2400, Loss: 0.2665257751941681 Acc:1.0
step: 2500, Loss: 0.26464104652404785 Acc:1.0
step: 2600, Loss: 0.26837170124053955 Acc:1.0
step: 2700, Loss: 0.26588019728660583 Acc:1.0
step: 2800, Loss: 0.2647514045238495 Acc:1.0
step: 2900, Loss: 0.26862841844558716 Acc:1.0
step: 3000, Loss: 0.2662152349948883 Acc:1.0
step: 3100, Loss: 0.2645580768585205 Acc:1.0
step: 3200, Loss: 0.27068185806274414 Acc:1.0
step: 3300, Loss: 0.26533088088035583 Acc:1.0
step: 3400, Loss: 0.26463088393211365 Acc:1.0
step: 3500, Loss: 0.26699501276016235 Acc:1.0
step: 3600, Loss: 0.26517319679260254 Acc:1.0
step: 3700, Loss: 0.26489120721817017 Acc:1.0
step: 3800, Loss: 0.26629966497421265 Acc:1.0
step: 3900, Loss: 0.2650027573108673 Acc:1.0
step: 4000, Loss: 0.2654854655265808 Acc:1.0
step: 4100, Loss: 0.26618942618370056 Acc:1.0
step: 4200, Loss: 0.26466596126556396 Acc:1.0
step: 4300, Loss: 0.2646533250808716 Acc:1.0
step: 4400, Loss: 0.2684369385242462 Acc:1.0
step: 4500, Loss: 0.2647525370121002 Acc:1.0
step: 4600, Loss: 0.26454538106918335 Acc:1.0
step: 4700, Loss: 0.26932770013809204 Acc:1.0
step: 4800, Loss: 0.26482099294662476 Acc:1.0
step: 4900, Loss: 0.26477330923080444 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.6666666666666666
precision: 0.625
recall: 0.625
F_score: 0.625
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 5.78574275970459 Acc:0.7272727272727273
step: 100, Loss: 0.3724679946899414 Acc:1.0
step: 200, Loss: 1.674017071723938 Acc:0.9090909090909091
step: 300, Loss: 0.2848786413669586 Acc:1.0
step: 400, Loss: 0.325867623090744 Acc:1.0
step: 500, Loss: 0.29481515288352966 Acc:1.0
step: 600, Loss: 0.28264063596725464 Acc:1.0
step: 700, Loss: 0.278616726398468 Acc:1.0
step: 800, Loss: 0.28834742307662964 Acc:1.0
step: 900, Loss: 0.270822674036026 Acc:1.0
step: 1000, Loss: 0.27551138401031494 Acc:1.0
step: 1100, Loss: 0.27397799491882324 Acc:1.0
step: 1200, Loss: 0.31745433807373047 Acc:1.0
step: 1300, Loss: 0.2765803635120392 Acc:1.0
step: 1400, Loss: 0.296653687953949 Acc:1.0
step: 1500, Loss: 0.33711010217666626 Acc:1.0
step: 1600, Loss: 0.279938280582428 Acc:1.0
step: 1700, Loss: 0.2699240446090698 Acc:1.0
step: 1800, Loss: 0.27186381816864014 Acc:1.0
step: 1900, Loss: 0.2698436677455902 Acc:1.0
step: 2000, Loss: 0.27043867111206055 Acc:1.0
step: 2100, Loss: 0.26730164885520935 Acc:1.0
step: 2200, Loss: 0.26672184467315674 Acc:1.0
step: 2300, Loss: 0.27276611328125 Acc:1.0
step: 2400, Loss: 0.2681979835033417 Acc:1.0
step: 2500, Loss: 0.2664969861507416 Acc:1.0
step: 2600, Loss: 0.2655933201313019 Acc:1.0
step: 2700, Loss: 0.26585423946380615 Acc:1.0
step: 2800, Loss: 0.2653380036354065 Acc:1.0
step: 2900, Loss: 0.2657659649848938 Acc:1.0
step: 3000, Loss: 0.2666436433792114 Acc:1.0
step: 3100, Loss: 0.2659352123737335 Acc:1.0
step: 3200, Loss: 0.26518988609313965 Acc:1.0
step: 3300, Loss: 0.2661099135875702 Acc:1.0
step: 3400, Loss: 0.2660215497016907 Acc:1.0
step: 3500, Loss: 0.26495179533958435 Acc:1.0
step: 3600, Loss: 0.26555657386779785 Acc:1.0
step: 3700, Loss: 0.26509562134742737 Acc:1.0
step: 3800, Loss: 0.26554909348487854 Acc:1.0
step: 3900, Loss: 0.2654055655002594 Acc:1.0
step: 4000, Loss: 0.26521074771881104 Acc:1.0
step: 4100, Loss: 0.2650725543498993 Acc:1.0
step: 4200, Loss: 0.26528313755989075 Acc:1.0
step: 4300, Loss: 0.2647645175457001 Acc:1.0
step: 4400, Loss: 0.2652129828929901 Acc:1.0
step: 4500, Loss: 0.2655324935913086 Acc:1.0
step: 4600, Loss: 0.26480022072792053 Acc:1.0
step: 4700, Loss: 0.26513588428497314 Acc:1.0
step: 4800, Loss: 0.2648601233959198 Acc:1.0
step: 4900, Loss: 0.26497340202331543 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.5833333333333334
precision: 0.5882352941176471
recall: 0.5555555555555556
F_score: 0.5714285714285715
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.2642861008644104 Acc:1.0
step: 100, Loss: 0.44362175464630127 Acc:1.0
in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 1.0505375862121582 Acc:0.9090909090909091
step: 100, Loss: 0.30968618392944336 Acc:1.0
step: 200, Loss: 0.300261527299881 Acc:1.0
step: 300, Loss: 0.43447089195251465 Acc:1.0
step: 400, Loss: 0.29379937052726746 Acc:1.0
step: 500, Loss: 0.2858125567436218 Acc:1.0
step: 600, Loss: 0.27689021825790405 Acc:1.0
step: 700, Loss: 0.2826889157295227 Acc:1.0
step: 800, Loss: 0.2752198278903961 Acc:1.0
step: 900, Loss: 0.26808086037635803 Acc:1.0
step: 1000, Loss: 0.2669496238231659 Acc:1.0
step: 1100, Loss: 0.27878338098526 Acc:1.0
step: 1200, Loss: 0.2661333978176117 Acc:1.0
step: 1300, Loss: 0.26513779163360596 Acc:1.0
step: 1400, Loss: 0.2674916982650757 Acc:1.0
step: 1500, Loss: 0.2652267813682556 Acc:1.0
step: 1600, Loss: 0.26494887471199036 Acc:1.0
step: 1700, Loss: 0.2650386393070221 Acc:1.0
step: 1800, Loss: 0.2749839127063751 Acc:1.0
step: 1900, Loss: 0.26525142788887024 Acc:1.0
step: 2000, Loss: 0.26417276263237 Acc:1.0
step: 2100, Loss: 1.0592159032821655 Acc:0.9090909090909091
step: 2200, Loss: 0.285462886095047 Acc:1.0
step: 2300, Loss: 0.2722291350364685 Acc:1.0
step: 2400, Loss: 0.26833680272102356 Acc:1.0
step: 2500, Loss: 0.26524752378463745 Acc:1.0
step: 2600, Loss: 0.27246370911598206 Acc:1.0
step: 2700, Loss: 0.2663794755935669 Acc:1.0
step: 2800, Loss: 0.2662525773048401 Acc:1.0
step: 2900, Loss: 0.26526039838790894 Acc:1.0
step: 3000, Loss: 0.26575157046318054 Acc:1.0
step: 3100, Loss: 0.2693812847137451 Acc:1.0
step: 3200, Loss: 0.31230008602142334 Acc:1.0
step: 3300, Loss: 0.26535922288894653 Acc:1.0
step: 3400, Loss: 0.26475921273231506 Acc:1.0
step: 3500, Loss: 0.2650185823440552 Acc:1.0
step: 3600, Loss: 0.2643866539001465 Acc:1.0
step: 3700, Loss: 0.2643096446990967 Acc:1.0
step: 3800, Loss: 0.26447540521621704 Acc:1.0
step: 3900, Loss: 0.264374703168869 Acc:1.0
step: 4000, Loss: 0.2642294764518738 Acc:1.0
step: 4100, Loss: 0.264219731092453 Acc:1.0
step: 4200, Loss: 0.26414233446121216 Acc:1.0
step: 4300, Loss: 0.2643130123615265 Acc:1.0
step: 4400, Loss: 0.2643303871154785 Acc:1.0
step: 4500, Loss: 0.2644526958465576 Acc:1.0
step: 4600, Loss: 0.4166364073753357 Acc:1.0
step: 4700, Loss: 0.26436251401901245 Acc:1.0
step: 4800, Loss: 0.26454055309295654 Acc:1.0
step: 4900, Loss: 0.26477527618408203 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.9375
precision: 0.9166666666666666
recall: 0.9565217391304348
F_score: 0.9361702127659574
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 3.302760124206543 Acc:0.8181818181818182
step: 100, Loss: 0.2770462930202484 Acc:1.0
step: 200, Loss: 0.2683083713054657 Acc:1.0
step: 300, Loss: 0.4146343469619751 Acc:1.0
step: 400, Loss: 0.2659521996974945 Acc:1.0
step: 500, Loss: 0.2679120898246765 Acc:1.0
step: 600, Loss: 0.27977147698402405 Acc:1.0
step: 700, Loss: 0.2660229206085205 Acc:1.0
step: 800, Loss: 0.2662287950515747 Acc:1.0
step: 900, Loss: 0.2702734172344208 Acc:1.0
step: 1000, Loss: 0.2643730044364929 Acc:1.0
step: 1100, Loss: 0.2699759304523468 Acc:1.0
step: 1200, Loss: 0.26453956961631775 Acc:1.0
step: 1300, Loss: 0.26516327261924744 Acc:1.0
step: 1400, Loss: 0.26434624195098877 Acc:1.0
step: 1500, Loss: 0.26476848125457764 Acc:1.0
step: 1600, Loss: 0.2645646333694458 Acc:1.0
step: 1700, Loss: 0.2643733620643616 Acc:1.0
step: 1800, Loss: 0.2643703520298004 Acc:1.0
step: 1900, Loss: 0.26568228006362915 Acc:1.0
step: 2000, Loss: 0.26454299688339233 Acc:1.0
step: 2100, Loss: 0.2651449739933014 Acc:1.0
step: 2200, Loss: 0.26451575756073 Acc:1.0
step: 2300, Loss: 0.26453378796577454 Acc:1.0
step: 2400, Loss: 0.2645476758480072 Acc:1.0
step: 2500, Loss: 0.26454854011535645 Acc:1.0
step: 2600, Loss: 0.26426735520362854 Acc:1.0
step: 2700, Loss: 0.26445621252059937 Acc:1.0
step: 2800, Loss: 0.26426437497138977 Acc:1.0
step: 2900, Loss: 0.26417532563209534 Acc:1.0
step: 3000, Loss: 0.26428595185279846 Acc:1.0
step: 3100, Loss: 0.2645341157913208 Acc:1.0
step: 3200, Loss: 0.26490041613578796 Acc:1.0
step: 3300, Loss: 0.2645300030708313 Acc:1.0
step: 3400, Loss: 0.2648785710334778 Acc:1.0
step: 3500, Loss: 0.2642289102077484 Acc:1.0
step: 3600, Loss: 0.26426956057548523 Acc:1.0
step: 3700, Loss: 0.26459890604019165 Acc:1.0
step: 3800, Loss: 0.3406205177307129 Acc:1.0
step: 3900, Loss: 0.3929442763328552 Acc:1.0
step: 4000, Loss: 0.2768450975418091 Acc:1.0
step: 4100, Loss: 0.27225375175476074 Acc:1.0
step: 4200, Loss: 0.2687525749206543 Acc:1.0
step: 4300, Loss: 0.26594051718711853 Acc:1.0
step: 4400, Loss: 0.2676086127758026 Acc:1.0
step: 4500, Loss: 0.2653680741786957 Acc:1.0
step: 4600, Loss: 0.4246717691421509 Acc:1.0
step: 4700, Loss: 0.2684613764286041 Acc:1.0
step: 4800, Loss: 0.2656281590461731 Acc:1.0
step: 4900, Loss: 0.28950268030166626 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.9375
precision: 0.9285714285714286
recall: 0.9629629629629629
F_score: 0.9454545454545454
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 0.2680232524871826 Acc:1.0
step: 100, Loss: 0.27014291286468506 Acc:1.0
step: 200, Loss: 0.2643797993659973 Acc:1.0
step: 300, Loss: 0.42457544803619385 Acc:1.0
step: 400, Loss: 0.2646760046482086 Acc:1.0
step: 500, Loss: 0.2642867863178253 Acc:1.0
step: 600, Loss: 0.2663670480251312 Acc:1.0
step: 700, Loss: 0.26422572135925293 Acc:1.0
step: 800, Loss: 0.26455292105674744 Acc:1.0
step: 900, Loss: 0.26504895091056824 Acc:1.0
step: 1000, Loss: 0.26493507623672485 Acc:1.0
step: 1100, Loss: 0.26471245288848877 Acc:1.0
step: 1200, Loss: 0.2642914950847626 Acc:1.0
step: 1300, Loss: 0.26433253288269043 Acc:1.0
step: 1400, Loss: 0.26412108540534973 Acc:1.0
step: 1500, Loss: 0.26421287655830383 Acc:1.0
step: 1600, Loss: 0.2645447850227356 Acc:1.0
step: 1700, Loss: 0.2641056478023529 Acc:1.0
step: 1800, Loss: 0.622429609298706 Acc:1.0
step: 1900, Loss: 0.3560512065887451 Acc:1.0
step: 2000, Loss: 0.2740660011768341 Acc:1.0
step: 2100, Loss: 0.2731628119945526 Acc:1.0
step: 2200, Loss: 0.2679731845855713 Acc:1.0
step: 2300, Loss: 0.26621121168136597 Acc:1.0
step: 2400, Loss: 0.26708388328552246 Acc:1.0
step: 2500, Loss: 0.2647308111190796 Acc:1.0
step: 2600, Loss: 0.26449868083000183 Acc:1.0
step: 2700, Loss: 0.26481112837791443 Acc:1.0
step: 2800, Loss: 0.2661961317062378 Acc:1.0
step: 2900, Loss: 0.26726627349853516 Acc:1.0
step: 3000, Loss: 0.26566174626350403 Acc:1.0
step: 3100, Loss: 0.2647915184497833 Acc:1.0
step: 3200, Loss: 0.2658483684062958 Acc:1.0
step: 3300, Loss: 0.26618528366088867 Acc:1.0
step: 3400, Loss: 0.2649795413017273 Acc:1.0
step: 3500, Loss: 0.2648802697658539 Acc:1.0
step: 3600, Loss: 0.26423412561416626 Acc:1.0
step: 3700, Loss: 0.2648382782936096 Acc:1.0
step: 3800, Loss: 0.26558566093444824 Acc:1.0
step: 3900, Loss: 0.26408976316452026 Acc:1.0
step: 4000, Loss: 0.2642545998096466 Acc:1.0
step: 4100, Loss: 0.26415368914604187 Acc:1.0
step: 4200, Loss: 0.26420632004737854 Acc:1.0
step: 4300, Loss: 0.26444920897483826 Acc:1.0
step: 4400, Loss: 0.26418083906173706 Acc:1.0
step: 4500, Loss: 0.26419344544410706 Acc:1.0
step: 4600, Loss: 0.42151954770088196 Acc:1.0
step: 4700, Loss: 0.2643626630306244 Acc:1.0
step: 4800, Loss: 0.26419514417648315 Acc:1.0
step: 4900, Loss: 0.2645239531993866 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.9375
precision: 1.0
recall: 0.88
F_score: 0.9361702127659575
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:468
step: 0, Loss: 0.26430779695510864 Acc:1.0
step: 100, Loss: 1.5264036655426025 Acc:0.9090909090909091
step: 200, Loss: 0.2690025568008423 Acc:1.0
step: 4900, Loss: 0.264382004737854 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:327
step: 0, Loss: 0.2692093849182129 Acc:1.0
step: 100, Loss: 0.2652117908000946 Acc:1.0
step: 200, Loss: 0.26497966051101685 Acc:1.0
step: 300, Loss: 0.26492413878440857 Acc:1.0
step: 400, Loss: 0.264525443315506 Acc:1.0
step: 500, Loss: 0.26444506645202637 Acc:1.0
step: 600, Loss: 0.26497510075569153 Acc:1.0
step: 700, Loss: 0.2644607424736023 Acc:1.0
step: 800, Loss: 0.2644300162792206 Acc:1.0
step: 900, Loss: 0.2649824619293213 Acc:1.0
step: 1000, Loss: 0.26433998346328735 Acc:1.0
step: 1100, Loss: 0.2666083574295044 Acc:1.0
step: 1200, Loss: 0.26486435532569885 Acc:1.0
step: 1300, Loss: 0.2652275264263153 Acc:1.0
step: 1400, Loss: 0.267232745885849 Acc:1.0
step: 1500, Loss: 0.2652112543582916 Acc:1.0
step: 1600, Loss: 0.26485034823417664 Acc:1.0
step: 1700, Loss: 0.26464951038360596 Acc:1.0
step: 1800, Loss: 0.2641521692276001 Acc:1.0
step: 1900, Loss: 0.26455065608024597 Acc:1.0
step: 2000, Loss: 0.26565811038017273 Acc:1.0
step: 2100, Loss: 0.26447540521621704 Acc:1.0
step: 2200, Loss: 0.2657672166824341 Acc:1.0
step: 2300, Loss: 0.26600074768066406 Acc:1.0
step: 2400, Loss: 0.26544538140296936 Acc:1.0
step: 2500, Loss: 0.2654905617237091 Acc:1.0
step: 2600, Loss: 0.26640984416007996 Acc:1.0
step: 2700, Loss: 0.26480957865715027 Acc:1.0
step: 2800, Loss: 0.26447680592536926 Acc:1.0
step: 2900, Loss: 0.2645982503890991 Acc:1.0
step: 3000, Loss: 0.2643449008464813 Acc:1.0
step: 3100, Loss: 0.26450690627098083 Acc:1.0
step: 3200, Loss: 0.2650180160999298 Acc:1.0
step: 3300, Loss: 0.265619158744812 Acc:1.0
step: 3400, Loss: 0.2656196057796478 Acc:1.0
step: 3500, Loss: 0.2649760842323303 Acc:1.0
step: 3600, Loss: 0.26554620265960693 Acc:1.0
step: 3700, Loss: 0.26634836196899414 Acc:1.0
step: 3800, Loss: 0.26534581184387207 Acc:1.0
step: 3900, Loss: 0.26470646262168884 Acc:1.0
step: 4000, Loss: 0.2643420696258545 Acc:1.0
step: 4100, Loss: 0.26463770866394043 Acc:1.0
step: 4200, Loss: 0.26472538709640503 Acc:1.0
step: 4300, Loss: 0.26444557309150696 Acc:1.0
step: 4400, Loss: 0.2643541693687439 Acc:1.0
step: 4500, Loss: 0.2664317488670349 Acc:1.0
step: 4600, Loss: 0.2652205526828766 Acc:1.0
step: 4700, Loss: 0.2655799984931946 Acc:1.0
step: 4800, Loss: 0.264415979385376 Acc:1.0
step: 4900, Loss: 0.2642098069190979 Acc:1.0
training successfully ended.
validating...
validate data length:37
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26768726110458374 Acc:1.0
step: 100, Loss: 0.26457861065864563 Acc:1.0
step: 200, Loss: 0.2646261751651764 Acc:1.0
step: 300, Loss: 0.2642938494682312 Acc:1.0
step: 400, Loss: 0.26487553119659424 Acc:1.0
step: 500, Loss: 0.2667236328125 Acc:1.0
step: 600, Loss: 0.2653936743736267 Acc:1.0
step: 700, Loss: 0.2650678753852844 Acc:1.0
step: 800, Loss: 0.26421141624450684 Acc:1.0
step: 900, Loss: 0.2644307017326355 Acc:1.0
step: 1000, Loss: 0.2641601264476776 Acc:1.0
step: 1100, Loss: 0.26427340507507324 Acc:1.0
step: 1200, Loss: 0.26457488536834717 Acc:1.0
step: 1300, Loss: 0.26503437757492065 Acc:1.0
step: 1400, Loss: 0.26512792706489563 Acc:1.0
step: 1500, Loss: 0.26530420780181885 Acc:1.0
step: 1600, Loss: 0.2656365633010864 Acc:1.0
step: 1700, Loss: 0.266573965549469 Acc:1.0
step: 1800, Loss: 0.2660290002822876 Acc:1.0
step: 1900, Loss: 0.2651180922985077 Acc:1.0
step: 2000, Loss: 0.2644813358783722 Acc:1.0
step: 2100, Loss: 0.26435524225234985 Acc:1.0
step: 2200, Loss: 0.2641110122203827 Acc:1.0
step: 2300, Loss: 0.26444435119628906 Acc:1.0
step: 2400, Loss: 0.26485171914100647 Acc:1.0
step: 2500, Loss: 0.26471784710884094 Acc:1.0
step: 2600, Loss: 0.2650206685066223 Acc:1.0
step: 2700, Loss: 0.26487454771995544 Acc:1.0
step: 2800, Loss: 0.26529404520988464 Acc:1.0
step: 2900, Loss: 0.26614201068878174 Acc:1.0
step: 3000, Loss: 0.2659067213535309 Acc:1.0
step: 3100, Loss: 0.2655419111251831 Acc:1.0
step: 3200, Loss: 0.26478949189186096 Acc:1.0
step: 3300, Loss: 0.26455798745155334 Acc:1.0
step: 3400, Loss: 0.2641063332557678 Acc:1.0
step: 3500, Loss: 0.26468098163604736 Acc:1.0
step: 3600, Loss: 0.2646220326423645 Acc:1.0
step: 3700, Loss: 0.2647651433944702 Acc:1.0
step: 3800, Loss: 0.26498550176620483 Acc:1.0
step: 3900, Loss: 0.2648547887802124 Acc:1.0
step: 4000, Loss: 0.264726459980011 Acc:1.0
step: 4100, Loss: 0.26606956124305725 Acc:1.0
step: 4200, Loss: 0.26437804102897644 Acc:1.0
step: 4300, Loss: 0.26441919803619385 Acc:1.0
step: 4400, Loss: 0.26415324211120605 Acc:1.0
step: 4500, Loss: 0.264631450176239 Acc:1.0
step: 4600, Loss: 0.26467156410217285 Acc:1.0
step: 4700, Loss: 0.26456382870674133 Acc:1.0
step: 4800, Loss: 0.26677122712135315 Acc:1.0
step: 4900, Loss: 0.2652446925640106 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26768749952316284 Acc:1.0
step: 100, Loss: 0.26497122645378113 Acc:1.0
step: 200, Loss: 0.2646026611328125 Acc:1.0
step: 300, Loss: 0.2644100785255432 Acc:1.0
step: 400, Loss: 0.26541292667388916 Acc:1.0
step: 500, Loss: 0.26513171195983887 Acc:1.0
step: 600, Loss: 0.2651258111000061 Acc:1.0
step: 700, Loss: 0.26462024450302124 Acc:1.0
step: 800, Loss: 0.26526620984077454 Acc:1.0
step: 900, Loss: 0.26503393054008484 Acc:1.0
step: 1000, Loss: 0.2647643983364105 Acc:1.0
step: 1100, Loss: 0.2647375762462616 Acc:1.0
step: 1200, Loss: 0.26494383811950684 Acc:1.0
step: 1300, Loss: 0.2658030390739441 Acc:1.0
step: 1400, Loss: 0.2654670178890228 Acc:1.0
step: 1500, Loss: 0.26438915729522705 Acc:1.0
step: 1600, Loss: 0.26451390981674194 Acc:1.0
step: 1700, Loss: 0.26469701528549194 Acc:1.0
step: 1800, Loss: 0.2645365297794342 Acc:1.0
step: 1900, Loss: 0.2653053402900696 Acc:1.0
step: 2000, Loss: 0.26575952768325806 Acc:1.0
step: 2100, Loss: 0.2648264169692993 Acc:1.0
step: 2200, Loss: 0.2663748860359192 Acc:1.0
step: 2300, Loss: 0.2655295133590698 Acc:1.0
step: 2400, Loss: 0.2644035220146179 Acc:1.0
step: 2500, Loss: 0.2646733820438385 Acc:1.0
step: 2600, Loss: 0.2655751705169678 Acc:1.0
step: 2700, Loss: 0.26499027013778687 Acc:1.0
step: 2800, Loss: 0.2658434510231018 Acc:1.0
step: 2900, Loss: 0.265072762966156 Acc:1.0
step: 3000, Loss: 0.2653859257698059 Acc:1.0
step: 3100, Loss: 0.26539117097854614 Acc:1.0
step: 3200, Loss: 0.26530271768569946 Acc:1.0
step: 3300, Loss: 0.26513707637786865 Acc:1.0
step: 3400, Loss: 0.2647065222263336 Acc:1.0
step: 3500, Loss: 0.26433444023132324 Acc:1.0
step: 3600, Loss: 0.2646784782409668 Acc:1.0
step: 3700, Loss: 0.2644568383693695 Acc:1.0
step: 3800, Loss: 0.2642236351966858 Acc:1.0
step: 3900, Loss: 0.26482269167900085 Acc:1.0
step: 4000, Loss: 0.2641340494155884 Acc:1.0
step: 4100, Loss: 0.2643383741378784 Acc:1.0
step: 4200, Loss: 0.2646932303905487 Acc:1.0
step: 4300, Loss: 0.2643825113773346 Acc:1.0
step: 4400, Loss: 0.26423293352127075 Acc:1.0
step: 4500, Loss: 0.26461437344551086 Acc:1.0
step: 4600, Loss: 0.26450443267822266 Acc:1.0
step: 4700, Loss: 0.2646064758300781 Acc:1.0
step: 4800, Loss: 0.2644467353820801 Acc:1.0
step: 4900, Loss: 0.2642136514186859 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26781150698661804 Acc:1.0
step: 100, Loss: 0.2642001509666443 Acc:1.0
step: 200, Loss: 0.26481184363365173 Acc:1.0
/home/sjf/eegall/main.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
--------- DEAP DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([32, 760, 40, 7]), harm_x: torch.Size([32, 760, 40, 7]) all_labels: torch.Size([32, 760, 4]) 
 base_graph: torch.Size([32, 760, 40, 40]) harm_graph: torch.Size([32, 760, 40, 40])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave8//home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[380, 380]
******fold 1******

*******Initializing new model*******
Training... train_data length:684
step: 0, Loss: 15.085470199584961 Acc:0.5
step: 100, Loss: 5.296762943267822 Acc:0.868421052631579
step: 200, Loss: 2.7797513008117676 Acc:0.9473684210526315
step: 300, Loss: 1.1491291522979736 Acc:0.9736842105263158
step: 400, Loss: 0.159701406955719 Acc:1.0
step: 500, Loss: 3.223156452178955 Acc:0.9210526315789473
step: 600, Loss: 0.22511008381843567 Acc:1.0
step: 700, Loss: 0.12506315112113953 Acc:1.0
step: 800, Loss: 2.701610565185547 Acc:0.9473684210526315
step: 900, Loss: 0.12703001499176025 Acc:1.0
step: 1000, Loss: 0.12119891494512558 Acc:1.0
step: 1100, Loss: 0.12132406234741211 Acc:1.0
step: 1200, Loss: 0.11717083305120468 Acc:1.0
step: 1300, Loss: 0.11837409436702728 Acc:1.0
step: 1400, Loss: 0.9205938577651978 Acc:0.9736842105263158
step: 1500, Loss: 0.12670154869556427 Acc:1.0
step: 1600, Loss: 0.13158342242240906 Acc:1.0
step: 1700, Loss: 0.12023469805717468 Acc:1.0
step: 1800, Loss: 0.12256571650505066 Acc:1.0
step: 1900, Loss: 0.12026602029800415 Acc:1.0
step: 2000, Loss: 0.11585212498903275 Acc:1.0
step: 2100, Loss: 0.12070322036743164 Acc:1.0
step: 2200, Loss: 0.11756552010774612 Acc:1.0
step: 2300, Loss: 11.099345207214355 Acc:0.9210526315789473
step: 2400, Loss: 1.0648910999298096 Acc:0.9736842105263158
step: 2500, Loss: 0.11910838633775711 Acc:1.0
step: 2600, Loss: 0.11844705790281296 Acc:1.0
step: 2700, Loss: 0.11536571383476257 Acc:1.0
step: 2800, Loss: 0.1160675585269928 Acc:1.0
step: 2900, Loss: 1.4246187210083008 Acc:0.9736842105263158
step: 3000, Loss: 0.11711839586496353 Acc:1.0
step: 3100, Loss: 0.12242807447910309 Acc:1.0
step: 3200, Loss: 0.11723420023918152 Acc:1.0
step: 3300, Loss: 0.12217651307582855 Acc:1.0
step: 3400, Loss: 0.11912249028682709 Acc:1.0
step: 3500, Loss: 2.5974040031433105 Acc:0.9736842105263158
step: 3600, Loss: 0.11745499074459076 Acc:1.0
step: 3700, Loss: 0.12020987272262573 Acc:1.0
step: 3800, Loss: 0.1185699999332428 Acc:1.0
step: 3900, Loss: 0.12396019697189331 Acc:1.0
step: 4000, Loss: 0.12038961052894592 Acc:1.0
step: 4100, Loss: 0.11570083349943161 Acc:1.0
step: 4200, Loss: 0.12006354331970215 Acc:1.0
step: 4300, Loss: 0.11786691099405289 Acc:1.0
step: 4400, Loss: 0.12103432416915894 Acc:1.0
step: 4500, Loss: 0.11706745624542236 Acc:1.0
step: 4600, Loss: 0.11610817909240723 Acc:1.0
step: 4700, Loss: 0.1174856424331665 Acc:1.0
step: 4800, Loss: 1.7607187032699585 Acc:0.9736842105263158
step: 4900, Loss: 2.209651231765747 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:76
acc: 0.5277777777777778
precision: 0.5416666666666666
recall: 0.3611111111111111
F_score: 0.43333333333333335
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 11.215352058410645 Acc:0.9210526315789473
step: 100, Loss: 3.342925786972046 Acc:0.9210526315789473
step: 200, Loss: 0.12719447910785675 Acc:1.0
step: 300, Loss: 0.11814025044441223 Acc:1.0
step: 400, Loss: 0.1148335188627243 Acc:1.0
step: 500, Loss: 0.11531399935483932 Acc:1.0
step: 600, Loss: 0.11733538657426834 Acc:1.0
step: 700, Loss: 0.1190057247877121 Acc:1.0
step: 800, Loss: 0.13139954209327698 Acc:1.0
step: 900, Loss: 0.1197633147239685 Acc:1.0
step: 1000, Loss: 0.11437036842107773 Acc:1.0
step: 1100, Loss: 0.11917400360107422 Acc:1.0
step: 1200, Loss: 0.12039355933666229 Acc:1.0
step: 1300, Loss: 0.12106747925281525 Acc:1.0
step: 1400, Loss: 0.11476951092481613 Acc:1.0
step: 1500, Loss: 0.1176118478178978 Acc:1.0
step: 1600, Loss: 0.11602011322975159 Acc:1.0
step: 1700, Loss: 0.11929631978273392 Acc:1.0
step: 1800, Loss: 0.1153058335185051 Acc:1.0
step: 1900, Loss: 0.11753802001476288 Acc:1.0
step: 2000, Loss: 0.11705373972654343 Acc:1.0
step: 2100, Loss: 0.1206602230668068 Acc:1.0
step: 2200, Loss: 0.11817462742328644 Acc:1.0
step: 2300, Loss: 0.11789517104625702 Acc:1.0
step: 2400, Loss: 0.1271815299987793 Acc:1.0
step: 2500, Loss: 0.11838783323764801 Acc:1.0
step: 2600, Loss: 0.11709963530302048 Acc:1.0
step: 2700, Loss: 0.11788001656532288 Acc:1.0
step: 2800, Loss: 0.11543998122215271 Acc:1.0
step: 2900, Loss: 0.12137044221162796 Acc:1.0
step: 3000, Loss: 0.11568480730056763 Acc:1.0
step: 3100, Loss: 0.1164214164018631 Acc:1.0
step: 3200, Loss: 0.11490841209888458 Acc:1.0
step: 3300, Loss: 0.1149526908993721 Acc:1.0
step: 3400, Loss: 0.12319808453321457 Acc:1.0
step: 3500, Loss: 0.11824619770050049 Acc:1.0
step: 3600, Loss: 0.12493015825748444 Acc:1.0
step: 3700, Loss: 0.11714155972003937 Acc:1.0
step: 3800, Loss: 0.1152806282043457 Acc:1.0
step: 3900, Loss: 0.11458204686641693 Acc:1.0
step: 4000, Loss: 0.13235700130462646 Acc:1.0
step: 4100, Loss: 0.12303954362869263 Acc:1.0
step: 4200, Loss: 0.11511342227458954 Acc:1.0
step: 4300, Loss: 0.1153360977768898 Acc:1.0
step: 4400, Loss: 0.12957921624183655 Acc:1.0
step: 4500, Loss: 0.11487910896539688 Acc:1.0
step: 4600, Loss: 0.1173294261097908 Acc:1.0
step: 4700, Loss: 0.1183854267001152 Acc:1.0
step: 4800, Loss: 0.11876499652862549 Acc:1.0
step: 4900, Loss: 0.11749589443206787 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.6944444444444444
precision: 0.6956521739130435
recall: 0.8
F_score: 0.7441860465116279
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 17.538183212280273 Acc:0.868421052631579
step: 100, Loss: 0.1696431040763855 Acc:1.0
step: 200, Loss: 0.11734183132648468 Acc:1.0
step: 300, Loss: 0.11964087188243866 Acc:1.0
step: 400, Loss: 0.12072132527828217 Acc:1.0
step: 500, Loss: 0.12165103107690811 Acc:1.0
step: 600, Loss: 0.12054233998060226 Acc:1.0
step: 700, Loss: 0.11797654628753662 Acc:1.0
step: 800, Loss: 0.11705450713634491 Acc:1.0
step: 900, Loss: 0.11703469604253769 Acc:1.0
step: 1000, Loss: 0.11809603124856949 Acc:1.0
step: 1100, Loss: 0.12055346369743347 Acc:1.0
step: 1200, Loss: 0.1202196255326271 Acc:1.0
step: 1300, Loss: 0.12068216502666473 Acc:1.0
step: 1400, Loss: 3.63935923576355 Acc:0.9736842105263158
step: 1500, Loss: 0.11429507285356522 Acc:1.0
step: 1600, Loss: 0.11803985387086868 Acc:1.0
step: 1700, Loss: 0.12713217735290527 Acc:1.0
step: 1800, Loss: 0.12049927562475204 Acc:1.0
step: 1900, Loss: 0.11766023933887482 Acc:1.0
step: 2000, Loss: 0.11699365079402924 Acc:1.0
step: 2100, Loss: 0.12131553888320923 Acc:1.0
step: 2200, Loss: 0.11705332249403 Acc:1.0
step: 2300, Loss: 0.12739618122577667 Acc:1.0
step: 2400, Loss: 3.234907627105713 Acc:0.9736842105263158
step: 2500, Loss: 0.11769477277994156 Acc:1.0
step: 2600, Loss: 0.1164337545633316 Acc:1.0
step: 2700, Loss: 0.12448587268590927 Acc:1.0
step: 2800, Loss: 0.11679978668689728 Acc:1.0
step: 2900, Loss: 0.11810538172721863 Acc:1.0
step: 3000, Loss: 0.11554981768131256 Acc:1.0
step: 3100, Loss: 0.11764344573020935 Acc:1.0
step: 3200, Loss: 0.11963677406311035 Acc:1.0
step: 3300, Loss: 0.11563494056463242 Acc:1.0
step: 3400, Loss: 0.11677510291337967 Acc:1.0
step: 3500, Loss: 0.11694129556417465 Acc:1.0
step: 3600, Loss: 0.11556128412485123 Acc:1.0
step: 3700, Loss: 0.12372034788131714 Acc:1.0
step: 3800, Loss: 0.11637628823518753 Acc:1.0
step: 3900, Loss: 0.12416468560695648 Acc:1.0
step: 4000, Loss: 0.11858855932950974 Acc:1.0
step: 4100, Loss: 0.11761573702096939 Acc:1.0
step: 4200, Loss: 0.16417601704597473 Acc:1.0
step: 4300, Loss: 0.12242388725280762 Acc:1.0
step: 200, Loss: 1.9348677396774292 Acc:0.8181818181818182
step: 300, Loss: 0.31405678391456604 Acc:1.0
step: 400, Loss: 0.27155500650405884 Acc:1.0
step: 500, Loss: 0.28136077523231506 Acc:1.0
step: 600, Loss: 0.26880842447280884 Acc:1.0
step: 700, Loss: 0.2664104700088501 Acc:1.0
step: 800, Loss: 0.2724456787109375 Acc:1.0
step: 900, Loss: 0.27769505977630615 Acc:1.0
step: 1000, Loss: 0.2707434892654419 Acc:1.0
step: 1100, Loss: 0.2877919673919678 Acc:1.0
step: 1200, Loss: 0.26812171936035156 Acc:1.0
step: 1300, Loss: 0.26628661155700684 Acc:1.0
step: 1400, Loss: 0.2668362557888031 Acc:1.0
step: 1500, Loss: 0.26520147919654846 Acc:1.0
step: 1600, Loss: 0.26513856649398804 Acc:1.0
step: 1700, Loss: 0.267351895570755 Acc:1.0
step: 1800, Loss: 0.26471632719039917 Acc:1.0
step: 1900, Loss: 0.4419287443161011 Acc:1.0
step: 2000, Loss: 0.9480285048484802 Acc:0.9090909090909091
step: 2100, Loss: 0.27781182527542114 Acc:1.0
step: 2200, Loss: 0.26646631956100464 Acc:1.0
step: 2300, Loss: 0.2677439749240875 Acc:1.0
step: 2400, Loss: 0.26482442021369934 Acc:1.0
step: 2500, Loss: 0.2648046910762787 Acc:1.0
step: 2600, Loss: 0.26542484760284424 Acc:1.0
step: 2700, Loss: 0.2646750509738922 Acc:1.0
step: 2800, Loss: 0.26436322927474976 Acc:1.0
step: 2900, Loss: 0.26474979519844055 Acc:1.0
step: 3000, Loss: 0.2643154263496399 Acc:1.0
step: 3100, Loss: 0.2645375430583954 Acc:1.0
step: 3200, Loss: 0.2644504904747009 Acc:1.0
step: 3300, Loss: 0.2642348110675812 Acc:1.0
step: 3400, Loss: 0.26439258456230164 Acc:1.0
step: 3500, Loss: 0.2644501328468323 Acc:1.0
step: 3600, Loss: 0.26412948966026306 Acc:1.0
step: 3700, Loss: 0.26425406336784363 Acc:1.0
step: 3800, Loss: 0.26429152488708496 Acc:1.0
step: 3900, Loss: 0.26414376497268677 Acc:1.0
step: 4000, Loss: 0.2642807066440582 Acc:1.0
step: 4100, Loss: 0.2643854320049286 Acc:1.0
step: 4200, Loss: 0.2642558515071869 Acc:1.0
step: 4300, Loss: 0.2641778588294983 Acc:1.0
step: 4400, Loss: 0.2641400098800659 Acc:1.0
step: 4500, Loss: 0.26406246423721313 Acc:1.0
step: 4600, Loss: 0.26412808895111084 Acc:1.0
step: 4700, Loss: 0.2651805877685547 Acc:1.0
step: 4800, Loss: 0.26406344771385193 Acc:1.0
step: 4900, Loss: 0.2641996443271637 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.6388888888888888
precision: 0.5625
recall: 0.6
F_score: 0.5806451612903225
subject 0 Avgacc: 0.6694444444444445 Avgfscore: 0.6666830018367349 
 Max acc:0.7222222222222222, Max f score:0.7368421052631579 Avg Recall:0.6697841417268661 Max Recall:0.7894736842105263 Avg Precision:0.6739717218625887 Max Precision:0.9333333333333333
******** mix subject_1 ********

[156, 156]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:280
step: 0, Loss: 11.582037925720215 Acc:0.5454545454545454
step: 100, Loss: 2.983677625656128 Acc:0.8181818181818182
step: 200, Loss: 0.5082904100418091 Acc:1.0
step: 300, Loss: 0.887750506401062 Acc:1.0
step: 400, Loss: 0.31875139474868774 Acc:1.0
step: 500, Loss: 0.33268457651138306 Acc:1.0
step: 600, Loss: 0.28551727533340454 Acc:1.0
step: 700, Loss: 0.29867398738861084 Acc:1.0
step: 800, Loss: 0.2831127941608429 Acc:1.0
step: 900, Loss: 0.27998092770576477 Acc:1.0
step: 1000, Loss: 0.2782894968986511 Acc:1.0
step: 1100, Loss: 0.2913036644458771 Acc:1.0
step: 1200, Loss: 0.2903139591217041 Acc:1.0
step: 1300, Loss: 0.28105494379997253 Acc:1.0
step: 1400, Loss: 0.2882443070411682 Acc:1.0
step: 1500, Loss: 0.2885138690471649 Acc:1.0
step: 1600, Loss: 0.28060993552207947 Acc:1.0
step: 1700, Loss: 0.2748304009437561 Acc:1.0
step: 1800, Loss: 0.27043992280960083 Acc:1.0
step: 1900, Loss: 0.27435028553009033 Acc:1.0
step: 2000, Loss: 0.26853010058403015 Acc:1.0
step: 2100, Loss: 0.26936277747154236 Acc:1.0
step: 2200, Loss: 0.26866450905799866 Acc:1.0
step: 2300, Loss: 0.2785540819168091 Acc:1.0
step: 2400, Loss: 0.2715558409690857 Acc:1.0
step: 2500, Loss: 0.26789188385009766 Acc:1.0
step: 2600, Loss: 0.26648083329200745 Acc:1.0
step: 2700, Loss: 0.2663383483886719 Acc:1.0
step: 2800, Loss: 0.2700103223323822 Acc:1.0
step: 2900, Loss: 0.2667856514453888 Acc:1.0
step: 3000, Loss: 0.2679043412208557 Acc:1.0
step: 3100, Loss: 0.26515868306159973 Acc:1.0
step: 3200, Loss: 0.27061280608177185 Acc:1.0
step: 3300, Loss: 0.2659631073474884 Acc:1.0
step: 3400, Loss: 0.26629745960235596 Acc:1.0
step: 3500, Loss: 0.2663367986679077 Acc:1.0
step: 3600, Loss: 0.26560044288635254 Acc:1.0
step: 3700, Loss: 0.26503366231918335 Acc:1.0
step: 3800, Loss: 0.26608365774154663 Acc:1.0
step: 3900, Loss: 0.26451343297958374 Acc:1.0
step: 4000, Loss: 0.2662006616592407 Acc:1.0
step: 4100, Loss: 0.26481324434280396 Acc:1.0
step: 4200, Loss: 0.2644188702106476 Acc:1.0
step: 4300, Loss: 0.26651740074157715 Acc:1.0
step: 4400, Loss: 0.26451483368873596 Acc:1.0
step: 4500, Loss: 0.26795679330825806 Acc:1.0
step: 4600, Loss: 0.2647327184677124 Acc:1.0
step: 4700, Loss: 0.26469630002975464 Acc:1.0
step: 4800, Loss: 0.31875523924827576 Acc:1.0
step: 4900, Loss: 0.28022632002830505 Acc:1.0
training successfully ended.
validating...
validate data length:32
acc: 0.7
precision: 0.7142857142857143
recall: 0.6666666666666666
F_score: 0.689655172413793
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:280
step: 0, Loss: 5.205766677856445 Acc:0.7272727272727273
step: 100, Loss: 2.4044456481933594 Acc:0.9090909090909091
step: 200, Loss: 0.40494000911712646 Acc:1.0
step: 300, Loss: 0.3499307632446289 Acc:1.0
step: 400, Loss: 0.3167809247970581 Acc:1.0
step: 500, Loss: 0.30760398507118225 Acc:1.0
step: 600, Loss: 0.27174264192581177 Acc:1.0
step: 700, Loss: 0.2790718972682953 Acc:1.0
step: 800, Loss: 0.2966018617153168 Acc:1.0
step: 900, Loss: 0.27810782194137573 Acc:1.0
step: 1000, Loss: 0.2749313712120056 Acc:1.0
step: 1100, Loss: 2.266482353210449 Acc:0.8181818181818182
step: 1200, Loss: 0.32101285457611084 Acc:1.0
step: 1300, Loss: 0.27229446172714233 Acc:1.0
step: 1400, Loss: 0.2808037996292114 Acc:1.0
step: 1500, Loss: 0.28453922271728516 Acc:1.0
step: 1600, Loss: 0.2732323408126831 Acc:1.0
step: 1700, Loss: 0.271668940782547 Acc:1.0
step: 1800, Loss: 0.26507121324539185 Acc:1.0
step: 1900, Loss: 0.2656250298023224 Acc:1.0
step: 2000, Loss: 0.2651561498641968 Acc:1.0
step: 2100, Loss: 0.27039363980293274 Acc:1.0
step: 2200, Loss: 0.26579341292381287 Acc:1.0
step: 2300, Loss: 0.26653343439102173 Acc:1.0
step: 2400, Loss: 0.2662259340286255 Acc:1.0
step: 2500, Loss: 0.2656277120113373 Acc:1.0
step: 2600, Loss: 0.26460379362106323 Acc:1.0
step: 2700, Loss: 0.26481980085372925 Acc:1.0
step: 2800, Loss: 0.26602160930633545 Acc:1.0
step: 2900, Loss: 0.26581743359565735 Acc:1.0
step: 3000, Loss: 0.26539328694343567 Acc:1.0
step: 3100, Loss: 0.26508867740631104 Acc:1.0
step: 3200, Loss: 0.2645069360733032 Acc:1.0
step: 3300, Loss: 0.2647121250629425 Acc:1.0
step: 3400, Loss: 0.2654978930950165 Acc:1.0
step: 3500, Loss: 0.2642821669578552 Acc:1.0
step: 3600, Loss: 0.2653586268424988 Acc:1.0
step: 3700, Loss: 0.26481959223747253 Acc:1.0
step: 3800, Loss: 0.26466095447540283 Acc:1.0
step: 3900, Loss: 0.26436367630958557 Acc:1.0
step: 4000, Loss: 0.2642344534397125 Acc:1.0
step: 4100, Loss: 0.2646944522857666 Acc:1.0
step: 4200, Loss: 0.264642596244812 Acc:1.0
step: 4300, Loss: 0.2642032206058502 Acc:1.0
step: 4400, Loss: 0.2641950845718384 Acc:1.0
step: 4500, Loss: 0.26408421993255615 Acc:1.0
step: 4600, Loss: 0.2641899585723877 Acc:1.0
step: 4700, Loss: 0.2642459273338318 Acc:1.0
step: 4800, Loss: 0.2642805576324463 Acc:1.0
step: 4900, Loss: 0.2641530930995941 Acc:1.0
training successfully ended.
validating...
validate data length:32
acc: 0.43333333333333335
precision: 0.5
recall: 0.5294117647058824
F_score: 0.5142857142857143
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.27666354179382324 Acc:1.0step: 300, Loss: 0.4709753394126892 Acc:1.0
step: 400, Loss: 0.2680889666080475 Acc:1.0
step: 500, Loss: 0.2663413882255554 Acc:1.0
step: 600, Loss: 0.2732016146183014 Acc:1.0
step: 700, Loss: 0.26425719261169434 Acc:1.0
step: 800, Loss: 0.2679376006126404 Acc:1.0
step: 900, Loss: 0.2673020362854004 Acc:1.0
step: 1000, Loss: 0.26599809527397156 Acc:1.0
step: 1100, Loss: 0.2666148841381073 Acc:1.0
step: 1200, Loss: 0.2661161422729492 Acc:1.0
step: 1300, Loss: 0.26472318172454834 Acc:1.0
step: 1400, Loss: 0.2650056481361389 Acc:1.0
step: 1500, Loss: 0.2653755843639374 Acc:1.0
step: 1600, Loss: 0.26473939418792725 Acc:1.0
step: 1700, Loss: 0.2655455470085144 Acc:1.0
step: 1800, Loss: 0.2645401060581207 Acc:1.0
step: 1900, Loss: 0.26557981967926025 Acc:1.0
step: 2000, Loss: 0.26505911350250244 Acc:1.0
step: 2100, Loss: 0.2690935730934143 Acc:1.0
step: 2200, Loss: 0.2653241753578186 Acc:1.0
step: 2300, Loss: 0.2672380805015564 Acc:1.0
step: 2400, Loss: 0.2643201947212219 Acc:1.0
step: 2500, Loss: 0.2658691108226776 Acc:1.0
step: 2600, Loss: 0.264640748500824 Acc:1.0
step: 2700, Loss: 0.26477330923080444 Acc:1.0
step: 2800, Loss: 0.2651853561401367 Acc:1.0
step: 2900, Loss: 0.2644311189651489 Acc:1.0
step: 3000, Loss: 0.2646237015724182 Acc:1.0
step: 3100, Loss: 0.26528245210647583 Acc:1.0
step: 3200, Loss: 0.2645185887813568 Acc:1.0
step: 3300, Loss: 0.2650562524795532 Acc:1.0
step: 3400, Loss: 0.2646346092224121 Acc:1.0
step: 3500, Loss: 0.26466381549835205 Acc:1.0
step: 3600, Loss: 0.2646242678165436 Acc:1.0
step: 3700, Loss: 0.2647545337677002 Acc:1.0
step: 3800, Loss: 0.2643859386444092 Acc:1.0
step: 3900, Loss: 0.264498770236969 Acc:1.0
step: 4000, Loss: 0.2648705542087555 Acc:1.0
step: 4100, Loss: 0.2645638883113861 Acc:1.0
step: 4200, Loss: 0.2643354535102844 Acc:1.0
step: 4300, Loss: 0.26487967371940613 Acc:1.0
step: 4400, Loss: 0.26523712277412415 Acc:1.0
step: 4500, Loss: 0.26475900411605835 Acc:1.0
step: 4600, Loss: 0.41098836064338684 Acc:1.0
step: 4700, Loss: 0.26440322399139404 Acc:1.0
step: 4800, Loss: 0.2643865942955017 Acc:1.0
step: 4900, Loss: 0.26540297269821167 Acc:1.0
training successfully ended.
validating...
validate data length:52
acc: 0.875
precision: 0.8181818181818182
recall: 0.9
F_score: 0.8571428571428572
subject 0 Avgacc: 0.8645833333333334 Avgfscore: 0.8614654593651437 
 Max acc:0.9375, Max f score:0.9454545454545454 Avg Recall:0.858499194847021 Max Recall:0.9629629629629629 Avg Precision:0.8666186711838886 Max Precision:1.0
******** mix subject_1 ********

[104, 208]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 4.415417671203613 Acc:0.8181818181818182
step: 100, Loss: 3.887113094329834 Acc:0.5454545454545454
step: 200, Loss: 2.1906073093414307 Acc:0.9090909090909091
step: 300, Loss: 1.0296517610549927 Acc:1.0
step: 400, Loss: 1.715523600578308 Acc:0.8181818181818182
step: 500, Loss: 0.4090953469276428 Acc:1.0
step: 600, Loss: 0.37073642015457153 Acc:1.0
step: 700, Loss: 0.36920642852783203 Acc:1.0
step: 800, Loss: 0.47478175163269043 Acc:1.0
step: 900, Loss: 0.39795127511024475 Acc:1.0
step: 1000, Loss: 0.34511154890060425 Acc:1.0
step: 1100, Loss: 0.39721861481666565 Acc:1.0
step: 1200, Loss: 0.3772701025009155 Acc:1.0
step: 1300, Loss: 0.3222862482070923 Acc:1.0
step: 1400, Loss: 0.35475626587867737 Acc:1.0
step: 1500, Loss: 0.37506479024887085 Acc:1.0
step: 1600, Loss: 0.3230794072151184 Acc:1.0
step: 1700, Loss: 0.3468075096607208 Acc:1.0
step: 1800, Loss: 0.31320953369140625 Acc:1.0
step: 1900, Loss: 0.43821513652801514 Acc:1.0
step: 2000, Loss: 0.3157234787940979 Acc:1.0
step: 2100, Loss: 0.3270210027694702 Acc:1.0
step: 2200, Loss: 0.3420054316520691 Acc:1.0
step: 2300, Loss: 0.31404200196266174 Acc:1.0
step: 2400, Loss: 0.27422136068344116 Acc:1.0
step: 2500, Loss: 0.290374219417572 Acc:1.0
step: 2600, Loss: 0.2944886088371277 Acc:1.0
step: 2700, Loss: 0.2884334325790405 Acc:1.0
step: 2800, Loss: 0.31810784339904785 Acc:1.0
step: 2900, Loss: 0.30508655309677124 Acc:1.0
step: 3000, Loss: 0.27367672324180603 Acc:1.0
step: 3100, Loss: 0.29439452290534973 Acc:1.0
step: 3200, Loss: 0.3179813623428345 Acc:1.0
step: 3300, Loss: 0.27873969078063965 Acc:1.0
step: 3400, Loss: 0.2893594801425934 Acc:1.0
step: 3500, Loss: 0.31473901867866516 Acc:1.0
step: 3600, Loss: 0.28744083642959595 Acc:1.0
step: 3700, Loss: 0.29859039187431335 Acc:1.0
step: 3800, Loss: 0.29152315855026245 Acc:1.0
step: 3900, Loss: 0.2899695634841919 Acc:1.0
step: 4000, Loss: 0.2949012815952301 Acc:1.0
step: 4100, Loss: 0.9442538022994995 Acc:0.9090909090909091
step: 4200, Loss: 0.2943289279937744 Acc:1.0
step: 4300, Loss: 0.29051199555397034 Acc:1.0
step: 4400, Loss: 0.2817237973213196 Acc:1.0
step: 4500, Loss: 0.3045133650302887 Acc:1.0
step: 4600, Loss: 0.28254181146621704 Acc:1.0
step: 4700, Loss: 0.26779529452323914 Acc:1.0
step: 4800, Loss: 0.269536554813385 Acc:1.0
step: 4900, Loss: 0.28677839040756226 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 0.5
precision: 0.5294117647058824
recall: 0.4090909090909091
F_score: 0.46153846153846156
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 4.192807197570801 Acc:0.8181818181818182
step: 100, Loss: 1.8088871240615845 Acc:0.9090909090909091
step: 200, Loss: 1.8487132787704468 Acc:0.9090909090909091
step: 300, Loss: 0.3243444263935089 Acc:1.0
step: 400, Loss: 0.37855762243270874 Acc:1.0
step: 500, Loss: 0.29652339220046997 Acc:1.0
step: 600, Loss: 0.38573509454727173 Acc:1.0
step: 700, Loss: 0.2889541983604431 Acc:1.0
step: 800, Loss: 0.3023374080657959 Acc:1.0
step: 900, Loss: 0.2824666500091553 Acc:1.0
step: 1000, Loss: 0.28031468391418457 Acc:1.0
step: 1100, Loss: 0.2875986099243164 Acc:1.0
step: 1200, Loss: 0.28357094526290894 Acc:1.0
step: 1300, Loss: 0.27732834219932556 Acc:1.0
step: 1400, Loss: 0.2888443171977997 Acc:1.0
step: 1500, Loss: 0.27446821331977844 Acc:1.0
step: 1600, Loss: 0.2749776244163513 Acc:1.0
step: 1700, Loss: 0.27469539642333984 Acc:1.0
step: 1800, Loss: 0.2711583375930786 Acc:1.0
step: 1900, Loss: 0.2678069472312927 Acc:1.0
step: 2000, Loss: 0.27565881609916687 Acc:1.0
step: 2100, Loss: 0.2810043394565582 Acc:1.0
step: 2200, Loss: 0.274961918592453 Acc:1.0
step: 2300, Loss: 0.28770965337753296 Acc:1.0
step: 2400, Loss: 0.2677936553955078 Acc:1.0
step: 2500, Loss: 0.2692277431488037 Acc:1.0
step: 2600, Loss: 0.26652446389198303 Acc:1.0
step: 2700, Loss: 0.26624852418899536 Acc:1.0
step: 2800, Loss: 0.2668735980987549 Acc:1.0
step: 2900, Loss: 0.26566314697265625 Acc:1.0
step: 3000, Loss: 0.2669214606285095 Acc:1.0
step: 3100, Loss: 0.2711658179759979 Acc:1.0
step: 3200, Loss: 0.2743179202079773 Acc:1.0
step: 3300, Loss: 0.2711893916130066 Acc:1.0
step: 3400, Loss: 0.2686423659324646 Acc:1.0
step: 3500, Loss: 0.26892971992492676 Acc:1.0
step: 3600, Loss: 0.2692311108112335 Acc:1.0
step: 3700, Loss: 0.264879047870636 Acc:1.0
step: 3800, Loss: 0.27005255222320557 Acc:1.0
step: 3900, Loss: 1.3843920230865479 Acc:0.9090909090909091
step: 4000, Loss: 0.32841670513153076 Acc:1.0
step: 4100, Loss: 0.2890515625476837 Acc:1.0
step: 4200, Loss: 0.27786463499069214 Acc:1.0
step: 4300, Loss: 0.288624107837677 Acc:1.0
step: 4400, Loss: 0.2980954349040985 Acc:1.0
step: 4500, Loss: 0.2703952193260193 Acc:1.0
step: 4600, Loss: 0.2907616198062897 Acc:1.0
step: 4700, Loss: 0.27470171451568604 Acc:1.0
step: 4800, Loss: 0.2701621949672699 Acc:1.0
step: 4900, Loss: 0.2698797583580017 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 0.6904761904761905
precision: 0.6153846153846154
recall: 0.5
F_score: 0.5517241379310345
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 8.965500831604004 Acc:0.5454545454545454
step: 100, Loss: 0.5154775381088257 Acc:1.0

step: 100, Loss: 0.7416872978210449 Acc:0.9090909090909091
step: 200, Loss: 0.4041539430618286 Acc:1.0
step: 300, Loss: 0.38609302043914795 Acc:1.0
step: 400, Loss: 0.2829934358596802 Acc:1.0
step: 500, Loss: 0.2860572338104248 Acc:1.0
step: 600, Loss: 0.3078072965145111 Acc:1.0
step: 700, Loss: 0.2963010370731354 Acc:1.0
step: 800, Loss: 0.2734760642051697 Acc:1.0
step: 900, Loss: 0.27560675144195557 Acc:1.0
step: 1000, Loss: 0.27056312561035156 Acc:1.0
step: 1100, Loss: 0.2706807851791382 Acc:1.0
step: 1200, Loss: 0.2684005796909332 Acc:1.0
step: 1300, Loss: 0.26727771759033203 Acc:1.0
step: 1400, Loss: 0.2692805528640747 Acc:1.0
step: 1500, Loss: 0.2656771242618561 Acc:1.0
step: 1600, Loss: 0.2665363550186157 Acc:1.0
step: 1700, Loss: 0.27509063482284546 Acc:1.0
step: 1800, Loss: 0.2664755582809448 Acc:1.0
step: 1900, Loss: 1.1413025856018066 Acc:0.9090909090909091
step: 2000, Loss: 0.42064666748046875 Acc:1.0
step: 2100, Loss: 0.27340197563171387 Acc:1.0
step: 2200, Loss: 0.2718890309333801 Acc:1.0
step: 2300, Loss: 0.26908642053604126 Acc:1.0
step: 2400, Loss: 0.26746657490730286 Acc:1.0
step: 2500, Loss: 0.26839444041252136 Acc:1.0
step: 2600, Loss: 0.2660496234893799 Acc:1.0
step: 2700, Loss: 0.2670028507709503 Acc:1.0
step: 2800, Loss: 0.26582685112953186 Acc:1.0
step: 2900, Loss: 0.26559099555015564 Acc:1.0
step: 3000, Loss: 0.2648050785064697 Acc:1.0
step: 3100, Loss: 0.26436853408813477 Acc:1.0
step: 3200, Loss: 0.26601994037628174 Acc:1.0
step: 3300, Loss: 0.2646105885505676 Acc:1.0
step: 3400, Loss: 0.2644171416759491 Acc:1.0
step: 3500, Loss: 0.264844685792923 Acc:1.0
step: 3600, Loss: 0.26470834016799927 Acc:1.0
step: 3700, Loss: 0.264476478099823 Acc:1.0
step: 3800, Loss: 0.26442885398864746 Acc:1.0
step: 3900, Loss: 0.2646687924861908 Acc:1.0
step: 4000, Loss: 0.2644176185131073 Acc:1.0
step: 4100, Loss: 0.2650451958179474 Acc:1.0
step: 4200, Loss: 0.26428648829460144 Acc:1.0
step: 4300, Loss: 0.2642394006252289 Acc:1.0
step: 4400, Loss: 0.264847993850708 Acc:1.0
step: 4500, Loss: 0.26461172103881836 Acc:1.0
step: 4600, Loss: 0.2642376124858856 Acc:1.0
step: 4700, Loss: 0.26423442363739014 Acc:1.0
step: 4800, Loss: 0.26422345638275146 Acc:1.0
step: 4900, Loss: 0.26427018642425537 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.6
precision: 0.6666666666666666
recall: 0.4
F_score: 0.5
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 3.7144455909729004 Acc:0.8181818181818182
step: 100, Loss: 0.5957090854644775 Acc:1.0
step: 200, Loss: 0.3364405333995819 Acc:1.0
step: 300, Loss: 0.31232935190200806 Acc:1.0
step: 400, Loss: 0.3126796782016754 Acc:1.0
step: 500, Loss: 0.27781957387924194 Acc:1.0
step: 600, Loss: 0.3241826891899109 Acc:1.0
step: 700, Loss: 0.28373247385025024 Acc:1.0
step: 800, Loss: 0.3009694814682007 Acc:1.0
step: 900, Loss: 0.2780750095844269 Acc:1.0
step: 1000, Loss: 0.27668213844299316 Acc:1.0
step: 1100, Loss: 0.270072340965271 Acc:1.0
step: 1200, Loss: 0.27366942167282104 Acc:1.0
step: 1300, Loss: 0.26957112550735474 Acc:1.0
step: 1400, Loss: 0.2725306749343872 Acc:1.0
step: 1500, Loss: 0.28887131810188293 Acc:1.0
step: 1600, Loss: 0.27766847610473633 Acc:1.0
step: 1700, Loss: 0.2914483845233917 Acc:1.0
step: 1800, Loss: 0.2781859040260315 Acc:1.0
step: 1900, Loss: 0.2735099196434021 Acc:1.0
step: 2000, Loss: 0.27377185225486755 Acc:1.0
step: 2100, Loss: 0.2741551995277405 Acc:1.0
step: 2200, Loss: 0.2796880602836609 Acc:1.0
step: 2300, Loss: 0.26867568492889404 Acc:1.0
step: 2400, Loss: 0.2685586214065552 Acc:1.0
step: 2500, Loss: 0.2675093412399292 Acc:1.0
step: 2600, Loss: 0.26873159408569336 Acc:1.0
step: 2700, Loss: 0.2667655646800995 Acc:1.0
step: 2800, Loss: 0.26549509167671204 Acc:1.0
step: 2900, Loss: 0.26511049270629883 Acc:1.0
step: 3000, Loss: 0.2658224105834961 Acc:1.0
step: 3100, Loss: 0.2657161355018616 Acc:1.0
step: 3200, Loss: 0.2643020451068878 Acc:1.0
step: 3300, Loss: 0.2651069164276123 Acc:1.0
step: 3400, Loss: 0.26622065901756287 Acc:1.0
step: 3500, Loss: 0.26481547951698303 Acc:1.0
step: 3600, Loss: 0.26501986384391785 Acc:1.0
step: 3700, Loss: 0.26435309648513794 Acc:1.0
step: 3800, Loss: 0.26487427949905396 Acc:1.0
step: 3900, Loss: 0.2645382583141327 Acc:1.0
step: 4000, Loss: 0.26514768600463867 Acc:1.0
step: 4100, Loss: 0.26449111104011536 Acc:1.0
step: 4200, Loss: 0.2642812728881836 Acc:1.0
step: 4300, Loss: 0.26509279012680054 Acc:1.0
step: 4400, Loss: 0.26440542936325073 Acc:1.0
step: 4500, Loss: 0.2645334005355835 Acc:1.0
step: 4600, Loss: 0.26421600580215454 Acc:1.0
step: 4700, Loss: 0.2642807364463806 Acc:1.0
step: 4800, Loss: 0.26414015889167786 Acc:1.0
step: 4900, Loss: 0.26429733633995056 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.5666666666666667
precision: 0.42857142857142855
recall: 0.5454545454545454
F_score: 0.4799999999999999
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.2801373302936554 Acc:1.0
step: 100, Loss: 0.9696178436279297 Acc:0.9090909090909091
step: 200, Loss: 0.2940443754196167 Acc:1.0
step: 300, Loss: 0.32541346549987793 Acc:1.0
step: 400, Loss: 0.3144557476043701 Acc:1.0
step: 500, Loss: 0.2955513298511505 Acc:1.0
step: 600, Loss: 0.26897990703582764 Acc:1.0
step: 700, Loss: 0.29438263177871704 Acc:1.0
step: 800, Loss: 0.2887897193431854 Acc:1.0
step: 900, Loss: 0.2697066366672516 Acc:1.0
step: 1000, Loss: 0.32708606123924255 Acc:1.0
step: 1100, Loss: 0.2903376817703247 Acc:1.0
step: 1200, Loss: 0.2751837968826294 Acc:1.0
step: 1300, Loss: 0.26951441168785095 Acc:1.0
step: 1400, Loss: 0.2761584520339966 Acc:1.0
step: 1500, Loss: 0.2674735486507416 Acc:1.0
step: 1600, Loss: 0.26952362060546875 Acc:1.0
step: 1700, Loss: 0.2704290449619293 Acc:1.0
step: 1800, Loss: 0.2691997289657593 Acc:1.0
step: 1900, Loss: 0.2645952105522156 Acc:1.0
step: 2000, Loss: 0.26486676931381226 Acc:1.0
step: 2100, Loss: 0.2665209174156189 Acc:1.0
step: 2200, Loss: 0.2659887671470642 Acc:1.0
step: 2300, Loss: 0.26576748490333557 Acc:1.0
step: 2400, Loss: 0.2657370865345001 Acc:1.0
step: 2500, Loss: 0.26503270864486694 Acc:1.0
step: 2600, Loss: 0.26433828473091125 Acc:1.0
step: 2700, Loss: 0.2660984992980957 Acc:1.0
step: 2800, Loss: 0.26497820019721985 Acc:1.0
step: 2900, Loss: 0.2648646831512451 Acc:1.0
step: 3000, Loss: 0.26812949776649475 Acc:1.0
step: 3100, Loss: 0.26479145884513855 Acc:1.0
step: 3200, Loss: 0.2645927667617798 Acc:1.0
step: 3300, Loss: 0.2658122181892395 Acc:1.0
step: 3400, Loss: 0.2648997902870178 Acc:1.0
step: 3500, Loss: 0.2652081847190857 Acc:1.0
step: 3600, Loss: 0.26468971371650696 Acc:1.0
step: 3700, Loss: 0.2646029591560364 Acc:1.0
step: 3800, Loss: 0.2641964554786682 Acc:1.0
step: 3900, Loss: 0.2642934322357178 Acc:1.0
step: 4000, Loss: 0.26466014981269836 Acc:1.0
step: 4100, Loss: 1.8699781894683838 Acc:0.8181818181818182
step: 4200, Loss: 1.29741632938385 Acc:0.9090909090909091
step: 4300, Loss: 0.2971373200416565 Acc:1.0
step: 4400, Loss: 0.27733251452445984 Acc:1.0
step: 4500, Loss: 0.27017709612846375 Acc:1.0
step: 4600, Loss: 0.27039390802383423 Acc:1.0
step: 4700, Loss: 0.2783277630805969 Acc:1.0
step: 4800, Loss: 0.27001672983169556 Acc:1.0
step: 4900, Loss: 0.27637413144111633 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.6
precision: 0.5909090909090909
recall: 0.8125
F_score: 0.6842105263157896
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.42930692434310913 Acc:1.0
step: 100, Loss: 1.5650979280471802 Acc:0.9090909090909091
step: 200, Loss: 0.2886778712272644 Acc:1.0
step: 300, Loss: 0.3027711808681488 Acc:1.0
step: 400, Loss: 0.37539345026016235 Acc:1.0
step: 500, Loss: 0.2904067635536194 Acc:1.0
step: 600, Loss: 0.2879924774169922 Acc:1.0
step: 200, Loss: 0.28628042340278625 Acc:1.0
step: 300, Loss: 0.32077154517173767 Acc:1.0
step: 400, Loss: 0.3376067280769348 Acc:1.0
step: 500, Loss: 0.28663182258605957 Acc:1.0
step: 600, Loss: 0.3064565062522888 Acc:1.0
step: 700, Loss: 0.29009830951690674 Acc:1.0
step: 800, Loss: 0.2937985956668854 Acc:1.0
step: 900, Loss: 0.26687759160995483 Acc:1.0
step: 1000, Loss: 0.2794208526611328 Acc:1.0
step: 1100, Loss: 0.2702801525592804 Acc:1.0
step: 1200, Loss: 0.2716234624385834 Acc:1.0
step: 1300, Loss: 0.2943572402000427 Acc:1.0
step: 1400, Loss: 0.27778464555740356 Acc:1.0
step: 1500, Loss: 0.2705231308937073 Acc:1.0
step: 1600, Loss: 0.2680965065956116 Acc:1.0
step: 1700, Loss: 0.27178770303726196 Acc:1.0
step: 1800, Loss: 0.2830508351325989 Acc:1.0
step: 1900, Loss: 0.2655673921108246 Acc:1.0
step: 2000, Loss: 0.267668217420578 Acc:1.0
step: 2100, Loss: 0.2875750958919525 Acc:1.0
step: 2200, Loss: 0.2728424072265625 Acc:1.0
step: 2300, Loss: 0.3433450162410736 Acc:1.0
step: 2400, Loss: 0.2659692168235779 Acc:1.0
step: 2500, Loss: 0.27041029930114746 Acc:1.0
step: 2600, Loss: 0.269273966550827 Acc:1.0
step: 2700, Loss: 0.2697035074234009 Acc:1.0
step: 2800, Loss: 0.26538437604904175 Acc:1.0
step: 2900, Loss: 0.2909643352031708 Acc:1.0
step: 3000, Loss: 0.297649621963501 Acc:1.0
step: 3100, Loss: 0.291718453168869 Acc:1.0
step: 3200, Loss: 0.27276110649108887 Acc:1.0
step: 3300, Loss: 0.269148588180542 Acc:1.0
step: 3400, Loss: 0.2780860662460327 Acc:1.0
step: 3500, Loss: 0.27436211705207825 Acc:1.0
step: 3600, Loss: 0.26806408166885376 Acc:1.0
step: 3700, Loss: 0.2671840488910675 Acc:1.0
step: 3800, Loss: 0.2674202024936676 Acc:1.0
step: 3900, Loss: 0.2675577402114868 Acc:1.0
step: 4000, Loss: 0.2654440402984619 Acc:1.0
step: 4100, Loss: 0.265256404876709 Acc:1.0
step: 4200, Loss: 0.2715904712677002 Acc:1.0
step: 4300, Loss: 0.2662293612957001 Acc:1.0
step: 4400, Loss: 0.2671512961387634 Acc:1.0
step: 4500, Loss: 0.2658140957355499 Acc:1.0
step: 4600, Loss: 0.268031507730484 Acc:1.0
step: 4700, Loss: 0.27176356315612793 Acc:1.0
step: 4800, Loss: 0.26847296953201294 Acc:1.0
step: 4900, Loss: 0.265371173620224 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 0.7619047619047619
precision: 0.875
recall: 0.6363636363636364
F_score: 0.7368421052631579
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 4.356736183166504 Acc:0.7272727272727273
step: 100, Loss: 0.3047642707824707 Acc:1.0
step: 200, Loss: 0.34586232900619507 Acc:1.0
step: 300, Loss: 0.28180018067359924 Acc:1.0
step: 400, Loss: 0.28248241543769836 Acc:1.0
step: 500, Loss: 0.30671507120132446 Acc:1.0
step: 600, Loss: 0.27799317240715027 Acc:1.0
step: 700, Loss: 0.26803749799728394 Acc:1.0
step: 800, Loss: 0.27090388536453247 Acc:1.0
step: 900, Loss: 0.27112001180648804 Acc:1.0
step: 1000, Loss: 0.27053123712539673 Acc:1.0
step: 1100, Loss: 0.26971861720085144 Acc:1.0
step: 1200, Loss: 0.2861661911010742 Acc:1.0
step: 1300, Loss: 0.26493239402770996 Acc:1.0
step: 1400, Loss: 0.2675538957118988 Acc:1.0
step: 1500, Loss: 0.26503831148147583 Acc:1.0
step: 1600, Loss: 0.2687641382217407 Acc:1.0
step: 1700, Loss: 0.26909884810447693 Acc:1.0
step: 1800, Loss: 0.267250120639801 Acc:1.0
step: 1900, Loss: 0.2663278579711914 Acc:1.0
step: 2000, Loss: 0.2650708854198456 Acc:1.0
step: 2100, Loss: 0.2660270929336548 Acc:1.0
step: 2200, Loss: 0.2688637673854828 Acc:1.0
step: 2300, Loss: 0.2648140788078308 Acc:1.0
step: 2400, Loss: 0.26497435569763184 Acc:1.0
step: 2500, Loss: 0.26557213068008423 Acc:1.0
step: 2600, Loss: 0.2652997374534607 Acc:1.0
step: 2700, Loss: 0.27049925923347473 Acc:1.0
step: 2800, Loss: 0.268185555934906 Acc:1.0
step: 2900, Loss: 0.26686200499534607 Acc:1.0
step: 3000, Loss: 0.27021852135658264 Acc:1.0
step: 3100, Loss: 0.2716761827468872 Acc:1.0
step: 3200, Loss: 0.2701534628868103 Acc:1.0
step: 3300, Loss: 0.27028322219848633 Acc:1.0
step: 3400, Loss: 0.2715591490268707 Acc:1.0
step: 3500, Loss: 0.2659919559955597 Acc:1.0
step: 3600, Loss: 0.27206549048423767 Acc:1.0
step: 3700, Loss: 0.2694385051727295 Acc:1.0
step: 3800, Loss: 0.290075421333313 Acc:1.0
step: 3900, Loss: 0.2661874294281006 Acc:1.0
step: 4000, Loss: 0.26649218797683716 Acc:1.0
step: 4100, Loss: 0.26771092414855957 Acc:1.0
step: 4200, Loss: 0.26862573623657227 Acc:1.0
step: 4300, Loss: 0.27041709423065186 Acc:1.0
step: 4400, Loss: 0.5146803855895996 Acc:1.0
step: 4500, Loss: 0.28109318017959595 Acc:1.0
step: 4600, Loss: 0.2965729236602783 Acc:1.0
step: 4700, Loss: 0.2759276330471039 Acc:1.0
step: 4800, Loss: 0.26827752590179443 Acc:1.0
step: 4900, Loss: 0.26822587847709656 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 0.7857142857142857
precision: 0.8333333333333334
recall: 0.7142857142857143
F_score: 0.7692307692307692
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 0.32047533988952637 Acc:1.0
step: 100, Loss: 0.27951833605766296 Acc:1.0
step: 200, Loss: 0.3133302628993988 Acc:1.0
step: 300, Loss: 0.2716952860355377 Acc:1.0
step: 400, Loss: 0.29397451877593994 Acc:1.0
step: 500, Loss: 0.26801949739456177 Acc:1.0
step: 600, Loss: 0.26730984449386597 Acc:1.0
step: 700, Loss: 0.2663332223892212 Acc:1.0
step: 800, Loss: 0.27005070447921753 Acc:1.0
step: 900, Loss: 0.26623234152793884 Acc:1.0
step: 1000, Loss: 0.26565688848495483 Acc:1.0
step: 1100, Loss: 0.26613885164260864 Acc:1.0
step: 1200, Loss: 0.2761271595954895 Acc:1.0
step: 1300, Loss: 0.40254420042037964 Acc:1.0
step: 1400, Loss: 0.2881596088409424 Acc:1.0
step: 1500, Loss: 0.26516106724739075 Acc:1.0
step: 1600, Loss: 0.26731815934181213 Acc:1.0
step: 1700, Loss: 0.2768940329551697 Acc:1.0
step: 1800, Loss: 0.26631295680999756 Acc:1.0
step: 1900, Loss: 0.269029438495636 Acc:1.0
step: 2000, Loss: 0.26643186807632446 Acc:1.0
step: 2100, Loss: 0.26612013578414917 Acc:1.0
step: 2200, Loss: 0.2681126594543457 Acc:1.0
step: 2300, Loss: 0.2651211619377136 Acc:1.0
step: 2400, Loss: 0.2648693323135376 Acc:1.0
step: 2500, Loss: 0.26676318049430847 Acc:1.0
step: 2600, Loss: 0.26476195454597473 Acc:1.0
step: 2700, Loss: 0.26485249400138855 Acc:1.0
step: 2800, Loss: 0.2646063268184662 Acc:1.0
step: 2900, Loss: 0.2648443877696991 Acc:1.0
step: 3000, Loss: 0.26519784331321716 Acc:1.0
step: 3100, Loss: 0.2651057243347168 Acc:1.0
step: 3200, Loss: 0.26651638746261597 Acc:1.0
step: 3300, Loss: 0.26805058121681213 Acc:1.0
step: 3400, Loss: 0.2666804790496826 Acc:1.0
step: 3500, Loss: 0.2671452462673187 Acc:1.0
step: 3600, Loss: 0.26653018593788147 Acc:1.0
step: 3700, Loss: 0.2686344385147095 Acc:1.0
step: 3800, Loss: 0.2708365321159363 Acc:1.0
step: 3900, Loss: 0.2681117653846741 Acc:1.0
step: 4000, Loss: 0.26728177070617676 Acc:1.0
step: 4100, Loss: 0.26668810844421387 Acc:1.0
step: 4200, Loss: 0.2691841125488281 Acc:1.0
step: 4300, Loss: 0.26795682311058044 Acc:1.0
step: 4400, Loss: 0.27192994952201843 Acc:1.0
step: 4500, Loss: 0.2667609453201294 Acc:1.0
step: 4600, Loss: 0.35075220465660095 Acc:1.0
step: 4700, Loss: 0.30925363302230835 Acc:1.0
step: 4800, Loss: 0.27106040716171265 Acc:1.0
step: 4900, Loss: 0.2715751528739929 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 0.9285714285714286
precision: 0.9047619047619048
recall: 0.95
F_score: 0.9268292682926829
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:374
step: 0, Loss: 0.2700859606266022 Acc:1.0
step: 100, Loss: 0.2732171416282654 Acc:1.0
step: 200, Loss: 0.2738068699836731 Acc:1.0
step: 300, Loss: 0.26698240637779236 Acc:1.0
step: 400, Loss: 0.27604904770851135 Acc:1.0
step: 500, Loss: 0.2687820792198181 Acc:1.0
step: 600, Loss: 0.2655501663684845 Acc:1.0
step: 700, Loss: 0.26946955919265747 Acc:1.0
step: 800, Loss: 0.2682674527168274 Acc:1.0
step: 300, Loss: 0.264292448759079 Acc:1.0
step: 400, Loss: 0.2653288245201111 Acc:1.0
step: 500, Loss: 0.2648047208786011 Acc:1.0
step: 600, Loss: 0.26446276903152466 Acc:1.0
step: 700, Loss: 0.2653602063655853 Acc:1.0
step: 800, Loss: 0.26517653465270996 Acc:1.0
step: 900, Loss: 0.26493287086486816 Acc:1.0
step: 1000, Loss: 0.2646978497505188 Acc:1.0
step: 1100, Loss: 0.26453664898872375 Acc:1.0
step: 1200, Loss: 0.2653948664665222 Acc:1.0
step: 1300, Loss: 0.26533228158950806 Acc:1.0
step: 1400, Loss: 0.26542946696281433 Acc:1.0
step: 1500, Loss: 0.26515161991119385 Acc:1.0
step: 1600, Loss: 0.26475945115089417 Acc:1.0
step: 1700, Loss: 0.2645830810070038 Acc:1.0
step: 1800, Loss: 0.2645460367202759 Acc:1.0
step: 1900, Loss: 0.2646293044090271 Acc:1.0
step: 2000, Loss: 0.26435524225234985 Acc:1.0
step: 2100, Loss: 0.26530787348747253 Acc:1.0
step: 2200, Loss: 0.26437485218048096 Acc:1.0
step: 2300, Loss: 0.26598265767097473 Acc:1.0
step: 2400, Loss: 0.2654041349887848 Acc:1.0
step: 2500, Loss: 0.26449280977249146 Acc:1.0
step: 2600, Loss: 0.26477694511413574 Acc:1.0
step: 2700, Loss: 0.2646189033985138 Acc:1.0
step: 2800, Loss: 0.2643952965736389 Acc:1.0
step: 2900, Loss: 0.26557087898254395 Acc:1.0
step: 3000, Loss: 0.26428818702697754 Acc:1.0
step: 3100, Loss: 0.2645602822303772 Acc:1.0
step: 3200, Loss: 0.26513415575027466 Acc:1.0
step: 3300, Loss: 0.26466357707977295 Acc:1.0
step: 3400, Loss: 0.2647796869277954 Acc:1.0
step: 3500, Loss: 0.2647385001182556 Acc:1.0
step: 3600, Loss: 0.26473695039749146 Acc:1.0
step: 3700, Loss: 0.26512986421585083 Acc:1.0
step: 3800, Loss: 0.2646995484828949 Acc:1.0
step: 3900, Loss: 0.2643599808216095 Acc:1.0
step: 4000, Loss: 0.26417720317840576 Acc:1.0
step: 4100, Loss: 0.26455995440483093 Acc:1.0
step: 4200, Loss: 0.26562216877937317 Acc:1.0
step: 4300, Loss: 0.2649611234664917 Acc:1.0
step: 4400, Loss: 0.2645470201969147 Acc:1.0
step: 4500, Loss: 0.26435258984565735 Acc:1.0
step: 4600, Loss: 0.26447969675064087 Acc:1.0
step: 4700, Loss: 0.2649514377117157 Acc:1.0
step: 4800, Loss: 0.2644580900669098 Acc:1.0
step: 4900, Loss: 0.2646222710609436 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26768437027931213 Acc:1.0
step: 100, Loss: 0.26444655656814575 Acc:1.0
step: 200, Loss: 0.26442015171051025 Acc:1.0
step: 300, Loss: 0.26560086011886597 Acc:1.0
step: 400, Loss: 0.2646477520465851 Acc:1.0
step: 500, Loss: 0.2661876678466797 Acc:1.0
step: 600, Loss: 0.2685994803905487 Acc:1.0
step: 700, Loss: 0.2658866047859192 Acc:1.0
step: 800, Loss: 0.2647503912448883 Acc:1.0
step: 900, Loss: 0.2643381953239441 Acc:1.0
step: 1000, Loss: 0.2642143964767456 Acc:1.0
step: 1100, Loss: 0.26426222920417786 Acc:1.0
step: 1200, Loss: 0.26499345898628235 Acc:1.0
step: 1300, Loss: 0.265190988779068 Acc:1.0
step: 1400, Loss: 0.26449504494667053 Acc:1.0
step: 1500, Loss: 0.26494064927101135 Acc:1.0
step: 1600, Loss: 0.2647492289543152 Acc:1.0
step: 1700, Loss: 0.2673272490501404 Acc:1.0
step: 1800, Loss: 0.2656274139881134 Acc:1.0
step: 1900, Loss: 0.26475849747657776 Acc:1.0
step: 2000, Loss: 0.26452168822288513 Acc:1.0
step: 2100, Loss: 0.2647838294506073 Acc:1.0
step: 2200, Loss: 0.2644437551498413 Acc:1.0
step: 2300, Loss: 0.26461684703826904 Acc:1.0
step: 2400, Loss: 0.2649327516555786 Acc:1.0
step: 2500, Loss: 0.2645561993122101 Acc:1.0
step: 2600, Loss: 0.2657102346420288 Acc:1.0
step: 2700, Loss: 0.26560094952583313 Acc:1.0
step: 2800, Loss: 0.26557230949401855 Acc:1.0
step: 2900, Loss: 0.2646574378013611 Acc:1.0
step: 3000, Loss: 0.2641541063785553 Acc:1.0
step: 3100, Loss: 0.26424819231033325 Acc:1.0
step: 3200, Loss: 0.26423725485801697 Acc:1.0
step: 3300, Loss: 0.26477158069610596 Acc:1.0
step: 3400, Loss: 0.2650033235549927 Acc:1.0
step: 3500, Loss: 0.26538801193237305 Acc:1.0
step: 3600, Loss: 0.2653656005859375 Acc:1.0
step: 3700, Loss: 0.26550889015197754 Acc:1.0
step: 3800, Loss: 0.2652398347854614 Acc:1.0
step: 3900, Loss: 0.2655137777328491 Acc:1.0
step: 4000, Loss: 0.26498910784721375 Acc:1.0
step: 4100, Loss: 0.26483577489852905 Acc:1.0
step: 4200, Loss: 0.26468050479888916 Acc:1.0
step: 4300, Loss: 0.2643222212791443 Acc:1.0
step: 4400, Loss: 0.2648266553878784 Acc:1.0
step: 4500, Loss: 0.2647245526313782 Acc:1.0
step: 4600, Loss: 0.264868825674057 Acc:1.0
step: 4700, Loss: 0.264811247587204 Acc:1.0
step: 4800, Loss: 0.26462942361831665 Acc:1.0
step: 4900, Loss: 0.2651861906051636 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 0.9722222222222222
precision: 1.0
recall: 0.9375
F_score: 0.967741935483871
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.26852846145629883 Acc:1.0
step: 100, Loss: 0.26416927576065063 Acc:1.0
step: 200, Loss: 0.2644854784011841 Acc:1.0
step: 300, Loss: 0.2644163966178894 Acc:1.0
step: 400, Loss: 0.2644835114479065 Acc:1.0
step: 500, Loss: 0.2649804353713989 Acc:1.0
step: 600, Loss: 0.2647193968296051 Acc:1.0
step: 700, Loss: 0.2647640109062195 Acc:1.0
step: 800, Loss: 0.26579755544662476 Acc:1.0
step: 900, Loss: 0.26475459337234497 Acc:1.0
step: 1000, Loss: 0.2647320032119751 Acc:1.0
step: 1100, Loss: 0.2650846838951111 Acc:1.0
step: 1200, Loss: 0.26483088731765747 Acc:1.0
step: 1300, Loss: 0.26527905464172363 Acc:1.0
step: 1400, Loss: 0.2649874687194824 Acc:1.0
step: 1500, Loss: 0.26448631286621094 Acc:1.0
step: 1600, Loss: 0.2645551860332489 Acc:1.0
step: 1700, Loss: 0.2644394338130951 Acc:1.0
step: 1800, Loss: 0.2646099627017975 Acc:1.0
step: 1900, Loss: 0.26540330052375793 Acc:1.0
step: 2000, Loss: 0.26451224088668823 Acc:1.0
step: 2100, Loss: 0.2644651234149933 Acc:1.0
step: 2200, Loss: 0.264596551656723 Acc:1.0
step: 2300, Loss: 0.2648060619831085 Acc:1.0
step: 2400, Loss: 0.2643619477748871 Acc:1.0
step: 2500, Loss: 0.2642785906791687 Acc:1.0
step: 2600, Loss: 0.26442721486091614 Acc:1.0
step: 2700, Loss: 0.264533132314682 Acc:1.0
step: 2800, Loss: 0.2642258405685425 Acc:1.0
step: 2900, Loss: 0.2647826373577118 Acc:1.0
step: 3000, Loss: 0.2649432420730591 Acc:1.0
step: 3100, Loss: 0.2657684087753296 Acc:1.0
step: 3200, Loss: 0.26660874485969543 Acc:1.0
step: 3300, Loss: 0.26480260491371155 Acc:1.0
step: 3400, Loss: 0.2644810676574707 Acc:1.0
step: 3500, Loss: 0.2641090750694275 Acc:1.0
step: 3600, Loss: 0.2640555500984192 Acc:1.0
step: 3700, Loss: 0.26408904790878296 Acc:1.0
step: 3800, Loss: 0.2642272114753723 Acc:1.0
step: 3900, Loss: 0.2648286521434784 Acc:1.0
step: 4000, Loss: 0.2660120129585266 Acc:1.0
step: 4100, Loss: 0.26518091559410095 Acc:1.0
step: 4200, Loss: 0.265028178691864 Acc:1.0
step: 4300, Loss: 0.2649034261703491 Acc:1.0
step: 4400, Loss: 0.26459747552871704 Acc:1.0
step: 4500, Loss: 0.2648489773273468 Acc:1.0
step: 4600, Loss: 0.26474514603614807 Acc:1.0
step: 4700, Loss: 0.2646072208881378 Acc:1.0
step: 4800, Loss: 0.26430362462997437 Acc:1.0
step: 4900, Loss: 0.26427510380744934 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:328
step: 0, Loss: 0.2676873505115509 Acc:1.0
step: 100, Loss: 0.2644237279891968 Acc:1.0
step: 200, Loss: 0.2651941776275635 Acc:1.0
step: 300, Loss: 0.2645973265171051 Acc:1.0
step: 400, Loss: 0.26490333676338196 Acc:1.0
step: 500, Loss: 0.2670726180076599 Acc:1.0
step: 600, Loss: 0.2650935649871826 Acc:1.0
step: 700, Loss: 0.26537105441093445 Acc:1.0
step: 800, Loss: 0.26510465145111084 Acc:1.0
step: 900, Loss: 0.26431047916412354 Acc:1.0
step: 1000, Loss: 0.2641584277153015 Acc:1.0
step: 1100, Loss: 0.26466819643974304 Acc:1.0
step: 1200, Loss: 0.2644655108451843 Acc:1.0
step: 700, Loss: 0.2950701117515564 Acc:1.0
step: 800, Loss: 0.40349656343460083 Acc:1.0
step: 900, Loss: 0.2952921390533447 Acc:1.0
step: 1000, Loss: 0.2863937020301819 Acc:1.0
step: 1100, Loss: 0.2753492593765259 Acc:1.0
step: 1200, Loss: 0.2813265025615692 Acc:1.0
step: 1300, Loss: 0.28164857625961304 Acc:1.0
step: 1400, Loss: 0.2843737304210663 Acc:1.0
step: 1500, Loss: 0.27241986989974976 Acc:1.0
step: 1600, Loss: 0.27686581015586853 Acc:1.0
step: 1700, Loss: 0.27405643463134766 Acc:1.0
step: 1800, Loss: 0.27961376309394836 Acc:1.0
step: 1900, Loss: 0.27336016297340393 Acc:1.0
step: 2000, Loss: 0.26799535751342773 Acc:1.0
step: 2100, Loss: 0.2954757511615753 Acc:1.0
step: 2200, Loss: 0.5091800093650818 Acc:1.0
step: 2300, Loss: 0.27953004837036133 Acc:1.0
step: 2400, Loss: 0.27646636962890625 Acc:1.0
step: 2500, Loss: 0.2738822102546692 Acc:1.0
step: 2600, Loss: 0.2829797565937042 Acc:1.0
step: 2700, Loss: 0.2751927971839905 Acc:1.0
step: 2800, Loss: 0.26889917254447937 Acc:1.0
step: 2900, Loss: 0.26878148317337036 Acc:1.0
step: 3000, Loss: 0.2705451250076294 Acc:1.0
step: 3100, Loss: 0.2668021321296692 Acc:1.0
step: 3200, Loss: 0.2672044336795807 Acc:1.0
step: 3300, Loss: 0.2658984661102295 Acc:1.0
step: 3400, Loss: 0.2668342590332031 Acc:1.0
step: 3500, Loss: 0.26727330684661865 Acc:1.0
step: 3600, Loss: 0.2654578983783722 Acc:1.0
step: 3700, Loss: 0.2650861144065857 Acc:1.0
step: 3800, Loss: 0.26593026518821716 Acc:1.0
step: 3900, Loss: 0.2664841115474701 Acc:1.0
step: 4000, Loss: 0.2653806209564209 Acc:1.0
step: 4100, Loss: 0.26459214091300964 Acc:1.0
step: 4200, Loss: 0.26628759503364563 Acc:1.0
step: 4300, Loss: 0.26473549008369446 Acc:1.0
step: 4400, Loss: 0.26662710309028625 Acc:1.0
step: 4500, Loss: 0.26456090807914734 Acc:1.0
step: 4600, Loss: 0.26431524753570557 Acc:1.0
step: 4700, Loss: 0.2650774121284485 Acc:1.0
step: 4800, Loss: 0.26446348428726196 Acc:1.0
step: 4900, Loss: 0.2647496461868286 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.3333333333333333
precision: 0.36363636363636365
recall: 0.23529411764705882
F_score: 0.2857142857142857
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 1.6694997549057007 Acc:0.9090909090909091
step: 100, Loss: 0.49057042598724365 Acc:1.0
step: 200, Loss: 0.36581364274024963 Acc:1.0
step: 300, Loss: 0.31951823830604553 Acc:1.0
step: 400, Loss: 0.29302355647087097 Acc:1.0
step: 500, Loss: 0.2995914816856384 Acc:1.0
step: 600, Loss: 0.28202128410339355 Acc:1.0
step: 700, Loss: 0.2780657708644867 Acc:1.0
step: 800, Loss: 0.27554503083229065 Acc:1.0
step: 900, Loss: 0.2747437655925751 Acc:1.0
step: 1000, Loss: 0.4422233998775482 Acc:1.0
step: 1100, Loss: 0.3199938237667084 Acc:1.0
step: 1200, Loss: 0.4365372657775879 Acc:1.0
step: 1300, Loss: 0.2777107357978821 Acc:1.0
step: 1400, Loss: 0.29591885209083557 Acc:1.0
step: 1500, Loss: 0.28073829412460327 Acc:1.0
step: 1600, Loss: 0.2700183391571045 Acc:1.0
step: 1700, Loss: 0.2656242847442627 Acc:1.0
step: 1800, Loss: 0.26954302191734314 Acc:1.0
step: 1900, Loss: 0.2743746340274811 Acc:1.0
step: 2000, Loss: 0.26700401306152344 Acc:1.0
step: 2100, Loss: 0.26738855242729187 Acc:1.0
step: 2200, Loss: 0.2649723291397095 Acc:1.0
step: 2300, Loss: 0.26517659425735474 Acc:1.0
step: 2400, Loss: 0.26478245854377747 Acc:1.0
step: 2500, Loss: 0.26571089029312134 Acc:1.0
step: 2600, Loss: 0.26502525806427 Acc:1.0
step: 2700, Loss: 0.2658340036869049 Acc:1.0
step: 2800, Loss: 0.26514190435409546 Acc:1.0
step: 2900, Loss: 0.2644866108894348 Acc:1.0
step: 3000, Loss: 0.2644524872303009 Acc:1.0
step: 3100, Loss: 0.26467040181159973 Acc:1.0
step: 3200, Loss: 0.2645097076892853 Acc:1.0
step: 3300, Loss: 0.264638215303421 Acc:1.0
step: 3400, Loss: 0.2645512819290161 Acc:1.0
step: 3500, Loss: 0.2654641270637512 Acc:1.0
step: 3600, Loss: 0.26436319947242737 Acc:1.0
step: 3700, Loss: 0.2643231451511383 Acc:1.0
step: 3800, Loss: 0.2641518712043762 Acc:1.0
step: 3900, Loss: 0.2645030915737152 Acc:1.0
step: 4000, Loss: 0.2643834352493286 Acc:1.0
step: 4100, Loss: 0.26412248611450195 Acc:1.0
step: 4200, Loss: 0.2644791603088379 Acc:1.0
step: 4300, Loss: 0.2649976909160614 Acc:1.0
step: 4400, Loss: 0.2642374634742737 Acc:1.0
step: 4500, Loss: 0.26406222581863403 Acc:1.0
step: 4600, Loss: 0.26416003704071045 Acc:1.0
step: 4700, Loss: 0.264080673456192 Acc:1.0
step: 4800, Loss: 0.26449012756347656 Acc:1.0
step: 4900, Loss: 0.26414167881011963 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.5
precision: 0.5
recall: 0.8666666666666667
F_score: 0.6341463414634146
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 1.9377450942993164 Acc:0.9090909090909091
step: 100, Loss: 0.5277941226959229 Acc:1.0
step: 200, Loss: 0.27960842847824097 Acc:1.0
step: 300, Loss: 0.3013682961463928 Acc:1.0
step: 400, Loss: 0.2879174053668976 Acc:1.0
step: 500, Loss: 0.2829201817512512 Acc:1.0
step: 600, Loss: 0.26861855387687683 Acc:1.0
step: 700, Loss: 0.2733769118785858 Acc:1.0
step: 800, Loss: 0.27119505405426025 Acc:1.0
step: 900, Loss: 0.272941529750824 Acc:1.0
step: 1000, Loss: 0.27042102813720703 Acc:1.0
step: 1100, Loss: 0.26466837525367737 Acc:1.0
step: 1200, Loss: 0.2658926248550415 Acc:1.0
step: 1300, Loss: 0.2659848630428314 Acc:1.0
step: 1400, Loss: 0.2685181200504303 Acc:1.0
step: 1500, Loss: 0.2652367353439331 Acc:1.0
step: 1600, Loss: 0.2656037211418152 Acc:1.0
step: 1700, Loss: 0.2672455310821533 Acc:1.0
step: 1800, Loss: 0.2661091387271881 Acc:1.0
step: 1900, Loss: 0.26510462164878845 Acc:1.0
step: 2000, Loss: 0.26502805948257446 Acc:1.0
step: 2100, Loss: 0.26475387811660767 Acc:1.0
step: 2200, Loss: 0.2645381987094879 Acc:1.0
step: 2300, Loss: 0.49661535024642944 Acc:1.0
step: 2400, Loss: 0.4631979763507843 Acc:1.0
step: 2500, Loss: 0.27886122465133667 Acc:1.0
step: 2600, Loss: 0.27056682109832764 Acc:1.0
step: 2700, Loss: 0.27432459592819214 Acc:1.0
step: 2800, Loss: 0.2683957815170288 Acc:1.0
step: 2900, Loss: 0.2698500454425812 Acc:1.0
step: 3000, Loss: 0.2665671110153198 Acc:1.0
step: 3100, Loss: 0.2679389715194702 Acc:1.0
step: 3200, Loss: 0.26623156666755676 Acc:1.0
step: 3300, Loss: 0.26634681224823 Acc:1.0
step: 3400, Loss: 0.2668840289115906 Acc:1.0
step: 3500, Loss: 0.26617687940597534 Acc:1.0
step: 3600, Loss: 0.2661769986152649 Acc:1.0
step: 3700, Loss: 0.2645638585090637 Acc:1.0
step: 3800, Loss: 0.26438453793525696 Acc:1.0
step: 3900, Loss: 0.2647784650325775 Acc:1.0
step: 4000, Loss: 0.2653001844882965 Acc:1.0
step: 4100, Loss: 0.26473569869995117 Acc:1.0
step: 4200, Loss: 0.2643567621707916 Acc:1.0
step: 4300, Loss: 0.2647547125816345 Acc:1.0
step: 4400, Loss: 0.2646454870700836 Acc:1.0
step: 4500, Loss: 0.26438456773757935 Acc:1.0
step: 4600, Loss: 0.26424142718315125 Acc:1.0
step: 4700, Loss: 0.2644340693950653 Acc:1.0
step: 4800, Loss: 0.2643497586250305 Acc:1.0
step: 4900, Loss: 0.2643201947212219 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.6
precision: 0.5294117647058824
recall: 0.6923076923076923
F_score: 0.5999999999999999
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.28463178873062134 Acc:1.0
step: 100, Loss: 0.9843994975090027 Acc:0.9090909090909091
step: 200, Loss: 0.2961837649345398 Acc:1.0
step: 300, Loss: 0.30446314811706543 Acc:1.0
step: 400, Loss: 0.28110048174858093 Acc:1.0
step: 500, Loss: 0.3158823847770691 Acc:1.0
step: 600, Loss: 0.27865272760391235 Acc:1.0
step: 700, Loss: 0.27973631024360657 Acc:1.0
step: 800, Loss: 0.27023717761039734 Acc:1.0
step: 900, Loss: 0.2743784785270691 Acc:1.0
step: 1000, Loss: 0.2725522518157959 Acc:1.0
step: 1100, Loss: 0.26990869641304016 Acc:1.0
step: 1200, Loss: 0.27930474281311035 Acc:1.0
step: 900, Loss: 0.2667393088340759 Acc:1.0
step: 1000, Loss: 0.26752570271492004 Acc:1.0
step: 1100, Loss: 0.2749008536338806 Acc:1.0
step: 1200, Loss: 0.2658161520957947 Acc:1.0
step: 1300, Loss: 0.28345614671707153 Acc:1.0
step: 1400, Loss: 0.26666611433029175 Acc:1.0
step: 1500, Loss: 0.28463470935821533 Acc:1.0
step: 1600, Loss: 0.26920944452285767 Acc:1.0
step: 1700, Loss: 0.2651187777519226 Acc:1.0
step: 1800, Loss: 0.26542237401008606 Acc:1.0
step: 1900, Loss: 0.2648875117301941 Acc:1.0
step: 2000, Loss: 0.2652395963668823 Acc:1.0
step: 2100, Loss: 0.2649918794631958 Acc:1.0
step: 2200, Loss: 0.265821635723114 Acc:1.0
step: 2300, Loss: 0.2676633298397064 Acc:1.0
step: 2400, Loss: 0.277435302734375 Acc:1.0
step: 2500, Loss: 0.2702639400959015 Acc:1.0
step: 2600, Loss: 0.2651427686214447 Acc:1.0
step: 2700, Loss: 0.264712393283844 Acc:1.0
step: 2800, Loss: 0.2661145329475403 Acc:1.0
step: 2900, Loss: 0.2670000195503235 Acc:1.0
step: 3000, Loss: 0.2691681981086731 Acc:1.0
step: 3100, Loss: 0.2681826055049896 Acc:1.0
step: 3200, Loss: 0.26646313071250916 Acc:1.0
step: 3300, Loss: 0.26601502299308777 Acc:1.0
step: 3400, Loss: 0.26482173800468445 Acc:1.0
step: 3500, Loss: 0.26742440462112427 Acc:1.0
step: 3600, Loss: 0.27037572860717773 Acc:1.0
step: 3700, Loss: 0.2688332498073578 Acc:1.0
step: 3800, Loss: 0.26956406235694885 Acc:1.0
step: 3900, Loss: 0.26968157291412354 Acc:1.0
step: 4000, Loss: 0.3016374111175537 Acc:1.0
step: 4100, Loss: 0.26511189341545105 Acc:1.0
step: 4200, Loss: 0.26673761010169983 Acc:1.0
step: 4300, Loss: 0.26454827189445496 Acc:1.0
step: 4400, Loss: 0.26478320360183716 Acc:1.0
step: 4500, Loss: 0.265994668006897 Acc:1.0
step: 4600, Loss: 0.26426929235458374 Acc:1.0
step: 4700, Loss: 0.28193986415863037 Acc:1.0
step: 4800, Loss: 0.2676903009414673 Acc:1.0
step: 4900, Loss: 0.265947550535202 Acc:1.0
training successfully ended.
validating...
validate data length:42
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:375
step: 0, Loss: 0.26545774936676025 Acc:1.0
step: 100, Loss: 0.32321763038635254 Acc:1.0
step: 200, Loss: 0.29142364859580994 Acc:1.0
step: 300, Loss: 0.2646539509296417 Acc:1.0
step: 400, Loss: 0.2650910019874573 Acc:1.0
step: 500, Loss: 0.2743898034095764 Acc:1.0
step: 600, Loss: 0.2645488381385803 Acc:1.0
step: 700, Loss: 0.2647067606449127 Acc:1.0
step: 800, Loss: 0.26620566844940186 Acc:1.0
step: 900, Loss: 0.2648051977157593 Acc:1.0
step: 1000, Loss: 0.26774078607559204 Acc:1.0
step: 1100, Loss: 0.26745498180389404 Acc:1.0
step: 1200, Loss: 0.2684798240661621 Acc:1.0
step: 1300, Loss: 0.2691293954849243 Acc:1.0
step: 1400, Loss: 0.2933807373046875 Acc:1.0
step: 1500, Loss: 0.26523837447166443 Acc:1.0
step: 1600, Loss: 0.2677634656429291 Acc:1.0
step: 1700, Loss: 0.2675670385360718 Acc:1.0
step: 1800, Loss: 0.26596909761428833 Acc:1.0
step: 1900, Loss: 0.2656773030757904 Acc:1.0
step: 2000, Loss: 0.2660927176475525 Acc:1.0
step: 2100, Loss: 0.2780800759792328 Acc:1.0
step: 2200, Loss: 0.26932990550994873 Acc:1.0
step: 2300, Loss: 0.2672835886478424 Acc:1.0
step: 2400, Loss: 0.26561543345451355 Acc:1.0
step: 2500, Loss: 0.26783043146133423 Acc:1.0
step: 2600, Loss: 0.2904641032218933 Acc:1.0
step: 2700, Loss: 0.2735779881477356 Acc:1.0
step: 2800, Loss: 0.2665053606033325 Acc:1.0
step: 2900, Loss: 0.26553604006767273 Acc:1.0
step: 3000, Loss: 0.2647697329521179 Acc:1.0
step: 3100, Loss: 0.2646225094795227 Acc:1.0
step: 3200, Loss: 0.2645687460899353 Acc:1.0
step: 3300, Loss: 0.26537901163101196 Acc:1.0
step: 3400, Loss: 0.2654814124107361 Acc:1.0
step: 3500, Loss: 0.26621493697166443 Acc:1.0
step: 3600, Loss: 0.27433499693870544 Acc:1.0
step: 3700, Loss: 0.2648817300796509 Acc:1.0
step: 3800, Loss: 0.27157747745513916 Acc:1.0
step: 3900, Loss: 0.2680835425853729 Acc:1.0
step: 4000, Loss: 0.2786450684070587 Acc:1.0
step: 4100, Loss: 0.2741991877555847 Acc:1.0
step: 4200, Loss: 0.2650667428970337 Acc:1.0
step: 4300, Loss: 0.26616767048835754 Acc:1.0
step: 4400, Loss: 0.26713767647743225 Acc:1.0
step: 4500, Loss: 0.26930415630340576 Acc:1.0
step: 4600, Loss: 0.2668578326702118 Acc:1.0
step: 4700, Loss: 0.26658064126968384 Acc:1.0
step: 4800, Loss: 0.26549848914146423 Acc:1.0
step: 4900, Loss: 0.2668280303478241 Acc:1.0
training successfully ended.
validating...
validate data length:41
acc: 0.9444444444444444
precision: 1.0
recall: 0.8947368421052632
F_score: 0.9444444444444444
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:375
step: 0, Loss: 0.26601386070251465 Acc:1.0
step: 100, Loss: 0.266495943069458 Acc:1.0
step: 200, Loss: 0.26554614305496216 Acc:1.0
step: 300, Loss: 0.2670709490776062 Acc:1.0
step: 400, Loss: 0.2739129960536957 Acc:1.0
step: 500, Loss: 0.26612579822540283 Acc:1.0
step: 600, Loss: 0.266451895236969 Acc:1.0
step: 700, Loss: 0.26523557305336 Acc:1.0
step: 800, Loss: 0.27323800325393677 Acc:1.0
step: 900, Loss: 0.2653437554836273 Acc:1.0
step: 1000, Loss: 0.2657162547111511 Acc:1.0
step: 1100, Loss: 0.2663387656211853 Acc:1.0
step: 1200, Loss: 0.2652604877948761 Acc:1.0
step: 1300, Loss: 0.27581506967544556 Acc:1.0
step: 1400, Loss: 0.2682461142539978 Acc:1.0
step: 1500, Loss: 0.265455424785614 Acc:1.0
step: 1600, Loss: 0.2657906413078308 Acc:1.0
step: 1700, Loss: 0.2647884488105774 Acc:1.0
step: 1800, Loss: 0.264656662940979 Acc:1.0
step: 1900, Loss: 0.2648862600326538 Acc:1.0
step: 2000, Loss: 0.2646918296813965 Acc:1.0
step: 2100, Loss: 0.2645469605922699 Acc:1.0
step: 2200, Loss: 0.27609729766845703 Acc:1.0
step: 2300, Loss: 0.2728654742240906 Acc:1.0
step: 2400, Loss: 0.2677636444568634 Acc:1.0
step: 2500, Loss: 0.26652538776397705 Acc:1.0
step: 2600, Loss: 0.2645012438297272 Acc:1.0
step: 2700, Loss: 0.26692458987236023 Acc:1.0
step: 2800, Loss: 0.26526448130607605 Acc:1.0
step: 2900, Loss: 0.2730262875556946 Acc:1.0
step: 3000, Loss: 0.27233341336250305 Acc:1.0
step: 3100, Loss: 0.309733122587204 Acc:1.0
step: 3200, Loss: 0.2951159179210663 Acc:1.0
step: 3300, Loss: 0.2651173174381256 Acc:1.0
step: 3400, Loss: 0.26705503463745117 Acc:1.0
step: 3500, Loss: 0.2645673155784607 Acc:1.0
step: 3600, Loss: 0.2656119465827942 Acc:1.0
step: 3700, Loss: 0.2643599212169647 Acc:1.0
step: 3800, Loss: 0.26833686232566833 Acc:1.0
step: 3900, Loss: 0.2651582360267639 Acc:1.0
step: 4000, Loss: 0.2648003101348877 Acc:1.0
step: 4100, Loss: 0.2660641670227051 Acc:1.0
step: 4200, Loss: 0.26519346237182617 Acc:1.0
step: 4300, Loss: 0.264493465423584 Acc:1.0
step: 4400, Loss: 0.26673370599746704 Acc:1.0
step: 4500, Loss: 0.2648831009864807 Acc:1.0
step: 4600, Loss: 0.26659199595451355 Acc:1.0
step: 4700, Loss: 0.26977115869522095 Acc:1.0
step: 4800, Loss: 0.27046191692352295 Acc:1.0
step: 4900, Loss: 0.26454290747642517 Acc:1.0
training successfully ended.
validating...
validate data length:41
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:375
step: 0, Loss: 0.26587626338005066 Acc:1.0
step: 100, Loss: 0.28603315353393555 Acc:1.0
step: 200, Loss: 0.26585495471954346 Acc:1.0
step: 300, Loss: 0.2709829807281494 Acc:1.0
step: 400, Loss: 0.27084216475486755 Acc:1.0
step: 500, Loss: 0.2653203010559082 Acc:1.0
step: 600, Loss: 0.2781727910041809 Acc:1.0
step: 700, Loss: 0.26890406012535095 Acc:1.0
step: 800, Loss: 0.26643994450569153 Acc:1.0
step: 900, Loss: 0.2643492519855499 Acc:1.0
step: 1000, Loss: 0.26520296931266785 Acc:1.0
step: 1100, Loss: 0.27906468510627747 Acc:1.0
step: 1200, Loss: 0.26576632261276245 Acc:1.0
step: 1300, Loss: 0.26897892355918884 Acc:1.0
step: 1400, Loss: 0.29046446084976196 Acc:1.0
step: 1500, Loss: 0.2678753435611725 Acc:1.0
step: 1600, Loss: 0.2652932107448578 Acc:1.0
step: 1700, Loss: 0.264957070350647 Acc:1.0
step: 1800, Loss: 0.2661793529987335 Acc:1.0/home/sjf/eegall/main.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
step: 4400, Loss: 0.12137384712696075 Acc:1.0
step: 4500, Loss: 0.12041018903255463 Acc:1.0
step: 4600, Loss: 0.1184961199760437 Acc:1.0
step: 4700, Loss: 0.11555628478527069 Acc:1.0
step: 4800, Loss: 0.11796018481254578 Acc:1.0
step: 4900, Loss: 0.12081003934144974 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.6944444444444444
precision: 0.725
recall: 0.725
F_score: 0.7250000000000001
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 7.43551778793335 Acc:0.9473684210526315
step: 100, Loss: 0.24181780219078064 Acc:1.0
step: 200, Loss: 0.12405402958393097 Acc:1.0
step: 300, Loss: 3.010148525238037 Acc:0.9736842105263158
step: 400, Loss: 0.11761023849248886 Acc:1.0
step: 500, Loss: 0.11969870328903198 Acc:1.0
step: 600, Loss: 0.12237970530986786 Acc:1.0
step: 700, Loss: 0.11805582046508789 Acc:1.0
step: 800, Loss: 0.11947956681251526 Acc:1.0
step: 900, Loss: 0.11829176545143127 Acc:1.0
step: 1000, Loss: 7.315675258636475 Acc:0.9473684210526315
step: 1100, Loss: 0.11900728940963745 Acc:1.0
step: 1200, Loss: 0.11785353720188141 Acc:1.0
step: 1300, Loss: 0.12057530134916306 Acc:1.0
step: 1400, Loss: 8.110684394836426 Acc:0.9473684210526315
step: 1500, Loss: 0.1173909530043602 Acc:1.0
step: 1600, Loss: 0.11688129603862762 Acc:1.0
step: 1700, Loss: 0.1211782693862915 Acc:1.0
step: 1800, Loss: 0.11823317408561707 Acc:1.0
step: 1900, Loss: 0.11922117322683334 Acc:1.0
step: 2000, Loss: 7.17133903503418 Acc:0.9473684210526315
step: 2100, Loss: 0.11495507508516312 Acc:1.0
step: 2200, Loss: 0.11616843938827515 Acc:1.0
step: 2300, Loss: 0.11829078197479248 Acc:1.0
step: 2400, Loss: 0.11530271172523499 Acc:1.0
step: 2500, Loss: 0.1197517067193985 Acc:1.0
step: 2600, Loss: 0.1208045557141304 Acc:1.0
step: 2700, Loss: 0.11980632692575455 Acc:1.0
step: 2800, Loss: 0.1161099374294281 Acc:1.0
step: 2900, Loss: 0.11787088960409164 Acc:1.0
step: 3000, Loss: 0.12128275632858276 Acc:1.0
step: 3100, Loss: 0.11634721606969833 Acc:1.0
step: 3200, Loss: 0.12011821568012238 Acc:1.0
step: 3300, Loss: 0.1207062304019928 Acc:1.0
step: 3400, Loss: 0.11542422324419022 Acc:1.0
step: 3500, Loss: 0.1144169345498085 Acc:1.0
step: 3600, Loss: 0.1183517798781395 Acc:1.0
step: 3700, Loss: 0.11850607395172119 Acc:1.0
step: 3800, Loss: 0.11799931526184082 Acc:1.0
step: 3900, Loss: 0.11600856482982635 Acc:1.0
step: 4000, Loss: 0.11532127112150192 Acc:1.0
step: 4100, Loss: 0.11679404973983765 Acc:1.0
step: 4200, Loss: 0.11520353704690933 Acc:1.0
step: 4300, Loss: 0.11676733940839767 Acc:1.0
step: 4400, Loss: 0.11590352654457092 Acc:1.0
step: 4500, Loss: 0.11838364601135254 Acc:1.0
step: 4600, Loss: 2.6148931980133057 Acc:0.9736842105263158
step: 4700, Loss: 0.11574244499206543 Acc:1.0
step: 4800, Loss: 0.11855317652225494 Acc:1.0
step: 4900, Loss: 0.11671993136405945 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.5972222222222222
precision: 0.5405405405405406
recall: 0.625
F_score: 0.5797101449275363
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 12.472000122070312 Acc:0.9210526315789473
step: 100, Loss: 2.7748496532440186 Acc:0.9736842105263158
step: 200, Loss: 0.1172356903553009 Acc:1.0
step: 300, Loss: 3.1179940700531006 Acc:0.9736842105263158
step: 400, Loss: 1.9017891883850098 Acc:0.9736842105263158
step: 500, Loss: 0.1153203696012497 Acc:1.0
step: 600, Loss: 0.1258259415626526 Acc:1.0
step: 700, Loss: 0.12046018242835999 Acc:1.0
step: 800, Loss: 0.11850716918706894 Acc:1.0
step: 900, Loss: 0.11887694150209427 Acc:1.0
step: 1000, Loss: 0.11676008254289627 Acc:1.0
step: 1100, Loss: 0.11449413746595383 Acc:1.0
step: 1200, Loss: 0.11664247512817383 Acc:1.0
step: 1300, Loss: 0.17360219359397888 Acc:1.0
step: 1400, Loss: 0.11715707927942276 Acc:1.0
step: 1500, Loss: 0.11863485723733902 Acc:1.0
step: 1600, Loss: 0.11569877713918686 Acc:1.0
step: 1700, Loss: 0.12113814800977707 Acc:1.0
step: 1800, Loss: 0.1148533821105957 Acc:1.0
step: 1900, Loss: 0.12447789311408997 Acc:1.0
step: 2000, Loss: 0.1179276704788208 Acc:1.0
step: 2100, Loss: 0.11862163245677948 Acc:1.0
step: 2200, Loss: 3.286893129348755 Acc:0.9736842105263158
step: 2300, Loss: 0.12739376723766327 Acc:1.0
step: 2400, Loss: 0.12133342772722244 Acc:1.0
step: 2500, Loss: 0.11895543336868286 Acc:1.0
step: 2600, Loss: 0.11459808051586151 Acc:1.0
step: 2700, Loss: 0.11570574343204498 Acc:1.0
step: 2800, Loss: 0.11881819367408752 Acc:1.0
step: 2900, Loss: 0.11818267405033112 Acc:1.0
step: 3000, Loss: 0.1208677664399147 Acc:1.0
step: 3100, Loss: 0.11777736991643906 Acc:1.0
step: 3200, Loss: 0.11498170346021652 Acc:1.0
step: 3300, Loss: 0.2730894088745117 Acc:1.0
step: 3400, Loss: 0.11695893108844757 Acc:1.0
step: 3500, Loss: 0.11668746173381805 Acc:1.0
step: 3600, Loss: 0.11938305199146271 Acc:1.0
step: 3700, Loss: 1.486085295677185 Acc:0.9736842105263158
step: 3800, Loss: 0.11517246812582016 Acc:1.0
step: 3900, Loss: 0.11892955005168915 Acc:1.0
step: 4000, Loss: 0.1232757493853569 Acc:1.0
step: 4100, Loss: 2.1151363849639893 Acc:0.9736842105263158
step: 4200, Loss: 0.11814810335636139 Acc:1.0
step: 4300, Loss: 0.12375816702842712 Acc:1.0
step: 4400, Loss: 0.11611707508563995 Acc:1.0
step: 4500, Loss: 0.1153629720211029 Acc:1.0
step: 4600, Loss: 0.12077226489782333 Acc:1.0
step: 4700, Loss: 0.11575064063072205 Acc:1.0
step: 4800, Loss: 0.12076462805271149 Acc:1.0
step: 4900, Loss: 0.12065887451171875 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.7361111111111112
precision: 0.6739130434782609
recall: 0.8857142857142857
F_score: 0.7654320987654321
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 7.621910095214844 Acc:0.9473684210526315
step: 100, Loss: 0.11761532723903656 Acc:1.0
step: 200, Loss: 0.11733198165893555 Acc:1.0
step: 300, Loss: 0.1172732561826706 Acc:1.0
step: 400, Loss: 0.11784994602203369 Acc:1.0
step: 500, Loss: 0.11603406071662903 Acc:1.0
step: 600, Loss: 0.11626766622066498 Acc:1.0
step: 700, Loss: 0.12044477462768555 Acc:1.0
step: 800, Loss: 0.1293664425611496 Acc:1.0
step: 900, Loss: 0.11486554145812988 Acc:1.0
step: 1000, Loss: 0.11660070717334747 Acc:1.0
step: 1100, Loss: 0.11743898689746857 Acc:1.0
step: 1200, Loss: 0.12631122767925262 Acc:1.0
step: 1300, Loss: 0.11748803406953812 Acc:1.0
step: 1400, Loss: 6.283889293670654 Acc:0.9473684210526315
step: 1500, Loss: 0.11766700446605682 Acc:1.0
step: 1600, Loss: 0.11824531853199005 Acc:1.0
step: 1700, Loss: 2.3764290809631348 Acc:0.9736842105263158
step: 1800, Loss: 0.11553113162517548 Acc:1.0
step: 1900, Loss: 0.11774526536464691 Acc:1.0
step: 2000, Loss: 0.11782634258270264 Acc:1.0
step: 2100, Loss: 0.11737751960754395 Acc:1.0
step: 2200, Loss: 0.11618614941835403 Acc:1.0
step: 2300, Loss: 0.11818008124828339 Acc:1.0
step: 2400, Loss: 0.11933868378400803 Acc:1.0
step: 2500, Loss: 0.11511655896902084 Acc:1.0
step: 2600, Loss: 0.1156901866197586 Acc:1.0
step: 2700, Loss: 0.1191159039735794 Acc:1.0
step: 2800, Loss: 0.13354654610157013 Acc:1.0
step: 2900, Loss: 0.14182668924331665 Acc:1.0
step: 3000, Loss: 0.11786923557519913 Acc:1.0
step: 3100, Loss: 0.11497291922569275 Acc:1.0
step: 3200, Loss: 0.12295247614383698 Acc:1.0
step: 3300, Loss: 0.12196443974971771 Acc:1.0
step: 3400, Loss: 0.11836503446102142 Acc:1.0
step: 3500, Loss: 0.11920500546693802 Acc:1.0
step: 3600, Loss: 0.11669082939624786 Acc:1.0
step: 3700, Loss: 0.12026619911193848 Acc:1.0
step: 3800, Loss: 0.11806939542293549 Acc:1.0
step: 3900, Loss: 0.11691121757030487 Acc:1.0
step: 4000, Loss: 0.12150955945253372 Acc:1.0
step: 4100, Loss: 0.12167605757713318 Acc:1.0
step: 4200, Loss: 0.11509956419467926 Acc:1.0
step: 4300, Loss: 0.12057747691869736 Acc:1.0
step: 4400, Loss: 0.12068910896778107 Acc:1.0
step: 4500, Loss: 0.12169007956981659 Acc:1.0
step: 1300, Loss: 0.2653939723968506 Acc:1.0
step: 1400, Loss: 0.2655484974384308 Acc:1.0
step: 1500, Loss: 0.2647184729576111 Acc:1.0
step: 1600, Loss: 0.26559391617774963 Acc:1.0
step: 1700, Loss: 0.266365110874176 Acc:1.0
step: 1800, Loss: 0.2652832865715027 Acc:1.0
step: 1900, Loss: 0.2649008631706238 Acc:1.0
step: 2000, Loss: 0.2648436427116394 Acc:1.0
step: 2100, Loss: 0.2644203007221222 Acc:1.0
step: 2200, Loss: 0.2654835879802704 Acc:1.0
step: 2300, Loss: 0.26541972160339355 Acc:1.0
step: 2400, Loss: 0.26488739252090454 Acc:1.0
step: 2500, Loss: 0.26458775997161865 Acc:1.0
step: 2600, Loss: 0.2643333077430725 Acc:1.0
step: 2700, Loss: 0.2641165852546692 Acc:1.0
step: 2800, Loss: 0.26403945684432983 Acc:1.0
step: 2900, Loss: 0.2652169466018677 Acc:1.0
step: 3000, Loss: 0.26426762342453003 Acc:1.0
step: 3100, Loss: 0.2644449472427368 Acc:1.0
step: 3200, Loss: 0.26455530524253845 Acc:1.0
step: 3300, Loss: 0.26462048292160034 Acc:1.0
step: 3400, Loss: 0.2650679349899292 Acc:1.0
step: 3500, Loss: 0.2661822438240051 Acc:1.0
step: 3600, Loss: 0.26474547386169434 Acc:1.0
step: 3700, Loss: 0.26474642753601074 Acc:1.0
step: 3800, Loss: 0.2644231915473938 Acc:1.0
step: 3900, Loss: 0.2645913362503052 Acc:1.0
step: 4000, Loss: 0.26477259397506714 Acc:1.0
step: 4100, Loss: 0.2647858262062073 Acc:1.0
step: 4200, Loss: 0.26474833488464355 Acc:1.0
step: 4300, Loss: 0.2648129463195801 Acc:1.0
step: 4400, Loss: 0.26503580808639526 Acc:1.0
step: 4500, Loss: 0.2643238306045532 Acc:1.0
step: 4600, Loss: 0.2645585536956787 Acc:1.0
step: 4700, Loss: 0.26425424218177795 Acc:1.0
step: 4800, Loss: 0.2650997042655945 Acc:1.0
step: 4900, Loss: 0.265121728181839 Acc:1.0
training successfully ended.
validating...
validate data length:36
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 0 Avgacc: 0.9583333333333334 Avgfscore: 0.9570672338414274 
 Max acc:1.0, Max f score:1.0 Avg Recall:0.9670833333333334 Max Recall:1.0 Avg Precision:0.95 Max Precision:1.0
******** mix subject_1 ********

[156, 156]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:280
step: 0, Loss: 3.711461305618286 Acc:0.45454545454545453
step: 100, Loss: 2.9562792778015137 Acc:0.7272727272727273
step: 200, Loss: 1.3985296487808228 Acc:0.9090909090909091
step: 300, Loss: 0.2703188359737396 Acc:1.0
step: 400, Loss: 0.27110713720321655 Acc:1.0
step: 500, Loss: 0.28336066007614136 Acc:1.0
step: 600, Loss: 0.26855361461639404 Acc:1.0
step: 700, Loss: 0.267086923122406 Acc:1.0
step: 800, Loss: 0.2715783715248108 Acc:1.0
step: 900, Loss: 0.26661673188209534 Acc:1.0
step: 1000, Loss: 0.2671422064304352 Acc:1.0
step: 1100, Loss: 0.26619952917099 Acc:1.0
step: 1200, Loss: 0.2698019742965698 Acc:1.0
step: 1300, Loss: 0.28305017948150635 Acc:1.0
step: 1400, Loss: 0.2707958221435547 Acc:1.0
step: 1500, Loss: 0.26900947093963623 Acc:1.0
step: 1600, Loss: 0.26553064584732056 Acc:1.0
step: 1700, Loss: 0.266475111246109 Acc:1.0
step: 1800, Loss: 0.26685553789138794 Acc:1.0
step: 1900, Loss: 0.26645463705062866 Acc:1.0
step: 2000, Loss: 0.2658080756664276 Acc:1.0
step: 2100, Loss: 0.26687341928482056 Acc:1.0
step: 2200, Loss: 0.26543954014778137 Acc:1.0
step: 2300, Loss: 0.26629433035850525 Acc:1.0
step: 2400, Loss: 0.26609718799591064 Acc:1.0
step: 2500, Loss: 0.2657904028892517 Acc:1.0
step: 2600, Loss: 0.2653202414512634 Acc:1.0
step: 2700, Loss: 0.2645787000656128 Acc:1.0
step: 2800, Loss: 0.265783429145813 Acc:1.0
step: 2900, Loss: 0.2647670805454254 Acc:1.0
step: 3000, Loss: 0.26469144225120544 Acc:1.0
step: 3100, Loss: 0.26609697937965393 Acc:1.0
step: 3200, Loss: 0.2657555043697357 Acc:1.0
step: 3300, Loss: 0.2651260495185852 Acc:1.0
step: 3400, Loss: 0.26513025164604187 Acc:1.0
step: 3500, Loss: 0.2650591731071472 Acc:1.0
step: 3600, Loss: 0.2644542455673218 Acc:1.0
step: 3700, Loss: 0.26448336243629456 Acc:1.0
step: 3800, Loss: 0.2645065188407898 Acc:1.0
step: 3900, Loss: 0.2655261754989624 Acc:1.0
step: 4000, Loss: 0.26618823409080505 Acc:1.0
step: 4100, Loss: 0.26514559984207153 Acc:1.0
step: 4200, Loss: 0.26476678252220154 Acc:1.0
step: 4300, Loss: 0.2652503252029419 Acc:1.0
step: 4400, Loss: 0.26465877890586853 Acc:1.0
step: 4500, Loss: 0.2648140788078308 Acc:1.0
step: 4600, Loss: 0.26416391134262085 Acc:1.0
step: 4700, Loss: 0.2643662691116333 Acc:1.0
step: 4800, Loss: 0.26430895924568176 Acc:1.0
step: 4900, Loss: 0.26542550325393677 Acc:1.0
training successfully ended.
validating...
validate data length:32
acc: 0.5666666666666667
precision: 0.5714285714285714
recall: 0.5333333333333333
F_score: 0.5517241379310344
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:280
step: 0, Loss: 0.28466033935546875 Acc:1.0
step: 100, Loss: 0.4111971855163574 Acc:1.0
step: 200, Loss: 0.26507291197776794 Acc:1.0
step: 300, Loss: 0.26439234614372253 Acc:1.0
step: 400, Loss: 0.2654852867126465 Acc:1.0
step: 500, Loss: 0.26533398032188416 Acc:1.0
step: 600, Loss: 0.26584169268608093 Acc:1.0
step: 700, Loss: 0.2644721269607544 Acc:1.0
step: 800, Loss: 0.2642180323600769 Acc:1.0
step: 900, Loss: 0.2643153667449951 Acc:1.0
step: 1000, Loss: 0.26552024483680725 Acc:1.0
step: 1100, Loss: 0.2644367516040802 Acc:1.0
step: 1200, Loss: 0.26500073075294495 Acc:1.0
step: 1300, Loss: 0.26487159729003906 Acc:1.0
step: 1400, Loss: 0.2651694715023041 Acc:1.0
step: 1500, Loss: 0.26456981897354126 Acc:1.0
step: 1600, Loss: 0.26461753249168396 Acc:1.0
step: 1700, Loss: 0.26441583037376404 Acc:1.0
step: 1800, Loss: 0.2644366919994354 Acc:1.0
step: 1900, Loss: 0.264708548784256 Acc:1.0
step: 2000, Loss: 0.2653740644454956 Acc:1.0
step: 2100, Loss: 0.2645006477832794 Acc:1.0
step: 2200, Loss: 0.26469898223876953 Acc:1.0
step: 2300, Loss: 0.2651558816432953 Acc:1.0
step: 2400, Loss: 0.26485854387283325 Acc:1.0
step: 2500, Loss: 0.2671166956424713 Acc:1.0
step: 2600, Loss: 0.2651064395904541 Acc:1.0
step: 2700, Loss: 0.2641930878162384 Acc:1.0
step: 2800, Loss: 0.2644284665584564 Acc:1.0
step: 2900, Loss: 0.26458099484443665 Acc:1.0
step: 3000, Loss: 0.26450520753860474 Acc:1.0
step: 3100, Loss: 0.2642837166786194 Acc:1.0
step: 3200, Loss: 0.26494455337524414 Acc:1.0
step: 3300, Loss: 0.26516851782798767 Acc:1.0
step: 3400, Loss: 0.2651244103908539 Acc:1.0
step: 3500, Loss: 0.2643042504787445 Acc:1.0
step: 3600, Loss: 0.26417306065559387 Acc:1.0
step: 3700, Loss: 0.26478514075279236 Acc:1.0
step: 3800, Loss: 0.2642861604690552 Acc:1.0
step: 3900, Loss: 0.2654012441635132 Acc:1.0
step: 4000, Loss: 0.2642596364021301 Acc:1.0
step: 4100, Loss: 0.26436731219291687 Acc:1.0
step: 4200, Loss: 0.26446208357810974 Acc:1.0
step: 4300, Loss: 0.2650585174560547 Acc:1.0
step: 4400, Loss: 0.2646353542804718 Acc:1.0
step: 4500, Loss: 0.264509379863739 Acc:1.0
step: 4600, Loss: 0.2646218538284302 Acc:1.0
step: 4700, Loss: 0.2651657462120056 Acc:1.0
step: 4800, Loss: 0.2667195498943329 Acc:1.0
step: 4900, Loss: 0.26557105779647827 Acc:1.0
training successfully ended.
validating...
validate data length:32
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.3283456265926361 Acc:1.0
step: 100, Loss: 0.2642460763454437 Acc:1.0
step: 200, Loss: 0.26410943269729614 Acc:1.0
step: 300, Loss: 0.2644648551940918 Acc:1.0
step: 400, Loss: 0.2649056315422058 Acc:1.0
step: 500, Loss: 0.2646999657154083 Acc:1.0
step: 600, Loss: 0.26506307721138 Acc:1.0
step: 700, Loss: 0.2658931016921997 Acc:1.0
step: 800, Loss: 0.2641960680484772 Acc:1.0
step: 900, Loss: 0.264042466878891 Acc:1.0
step: 1000, Loss: 0.2640247941017151 Acc:1.0
step: 1100, Loss: 0.2640726864337921 Acc:1.0
step: 1200, Loss: 0.26407575607299805 Acc:1.0
step: 1300, Loss: 0.26465854048728943 Acc:1.0
step: 1400, Loss: 0.2650621235370636 Acc:1.0
step: 1500, Loss: 0.2642611861228943 Acc:1.0
step: 1600, Loss: 0.2648026645183563 Acc:1.0
step: 4600, Loss: 4.410660743713379 Acc:0.9736842105263158
step: 4700, Loss: 0.11922228336334229 Acc:1.0
step: 4800, Loss: 0.11714580655097961 Acc:1.0
step: 4900, Loss: 0.11697711050510406 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.8194444444444444
precision: 0.8055555555555556
recall: 0.8285714285714286
F_score: 0.8169014084507044
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 6.163320541381836 Acc:0.9473684210526315
step: 100, Loss: 0.11768493056297302 Acc:1.0
step: 200, Loss: 0.1420840322971344 Acc:1.0
step: 300, Loss: 0.12431253492832184 Acc:1.0
step: 400, Loss: 0.12021946907043457 Acc:1.0
step: 500, Loss: 0.11821958422660828 Acc:1.0
step: 600, Loss: 0.12307432293891907 Acc:1.0
step: 700, Loss: 0.13058288395404816 Acc:1.0
step: 800, Loss: 0.12213338911533356 Acc:1.0
step: 900, Loss: 0.1173962652683258 Acc:1.0
step: 1000, Loss: 0.11945055425167084 Acc:1.0
step: 1100, Loss: 0.11713086813688278 Acc:1.0
step: 1200, Loss: 0.12031345069408417 Acc:1.0
step: 1300, Loss: 0.11425339430570602 Acc:1.0
step: 1400, Loss: 0.11882530897855759 Acc:1.0
step: 1500, Loss: 0.11833006143569946 Acc:1.0
step: 1600, Loss: 9.00767707824707 Acc:0.9210526315789473
step: 1700, Loss: 0.11802108585834503 Acc:1.0
step: 1800, Loss: 0.11929025501012802 Acc:1.0
step: 1900, Loss: 0.11846510320901871 Acc:1.0
step: 2000, Loss: 0.11772320419549942 Acc:1.0
step: 2100, Loss: 0.11788724362850189 Acc:1.0
step: 2200, Loss: 0.12033511698246002 Acc:1.0
step: 2300, Loss: 0.12020336091518402 Acc:1.0
step: 2400, Loss: 0.6080658435821533 Acc:1.0
step: 2500, Loss: 0.11536641418933868 Acc:1.0
step: 2600, Loss: 0.11860759556293488 Acc:1.0
step: 2700, Loss: 0.11904273182153702 Acc:1.0
step: 2800, Loss: 0.11715364456176758 Acc:1.0
step: 2900, Loss: 0.1163070872426033 Acc:1.0
step: 3000, Loss: 0.11971444636583328 Acc:1.0
step: 3100, Loss: 1.7840628623962402 Acc:0.9736842105263158
step: 3200, Loss: 0.11528870463371277 Acc:1.0
step: 3300, Loss: 0.12120610475540161 Acc:1.0
step: 3400, Loss: 0.1165652722120285 Acc:1.0
step: 3500, Loss: 0.1223847046494484 Acc:1.0
step: 3600, Loss: 0.11519961059093475 Acc:1.0
step: 3700, Loss: 0.11916696280241013 Acc:1.0
step: 3800, Loss: 0.11536159366369247 Acc:1.0
step: 3900, Loss: 0.11779286712408066 Acc:1.0
step: 4000, Loss: 0.11498311907052994 Acc:1.0
step: 4100, Loss: 0.1234375387430191 Acc:1.0
step: 4200, Loss: 0.12294241786003113 Acc:1.0
step: 4300, Loss: 0.12322921305894852 Acc:1.0
step: 4400, Loss: 0.12136238813400269 Acc:1.0
step: 4500, Loss: 0.11436392366886139 Acc:1.0
step: 4600, Loss: 0.11675772070884705 Acc:1.0
step: 4700, Loss: 0.11972671747207642 Acc:1.0
step: 4800, Loss: 0.11663466691970825 Acc:1.0
step: 4900, Loss: 0.1289236694574356 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.6944444444444444
precision: 0.6470588235294118
recall: 0.6875
F_score: 0.6666666666666667
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 0.11914601922035217 Acc:1.0
step: 100, Loss: 0.12089288234710693 Acc:1.0
step: 200, Loss: 0.12446296215057373 Acc:1.0
step: 300, Loss: 0.11413997411727905 Acc:1.0
step: 400, Loss: 0.2996169924736023 Acc:1.0
step: 500, Loss: 0.11597701907157898 Acc:1.0
step: 600, Loss: 0.11899964511394501 Acc:1.0
step: 700, Loss: 0.11691220104694366 Acc:1.0
step: 800, Loss: 0.11768195033073425 Acc:1.0
step: 900, Loss: 0.12586897611618042 Acc:1.0
step: 1000, Loss: 0.11772119998931885 Acc:1.0
step: 1100, Loss: 0.12462826818227768 Acc:1.0
step: 1200, Loss: 0.11883886158466339 Acc:1.0
step: 1300, Loss: 0.12261791527271271 Acc:1.0
step: 1400, Loss: 0.11663978546857834 Acc:1.0
step: 1500, Loss: 0.11758275330066681 Acc:1.0
step: 1600, Loss: 0.12006953358650208 Acc:1.0
step: 1700, Loss: 0.11765091121196747 Acc:1.0
step: 1800, Loss: 0.12313013523817062 Acc:1.0
step: 1900, Loss: 0.11560532450675964 Acc:1.0
step: 2000, Loss: 0.11975738406181335 Acc:1.0
step: 2100, Loss: 0.11743302643299103 Acc:1.0
step: 2200, Loss: 0.11755499243736267 Acc:1.0
step: 2300, Loss: 0.11766573786735535 Acc:1.0
step: 2400, Loss: 0.12372331321239471 Acc:1.0
step: 2500, Loss: 0.11806809157133102 Acc:1.0
step: 2600, Loss: 0.11607356369495392 Acc:1.0
step: 2700, Loss: 0.11565867811441422 Acc:1.0
step: 2800, Loss: 0.12404300272464752 Acc:1.0
step: 2900, Loss: 0.11935381591320038 Acc:1.0
step: 3000, Loss: 0.11324577778577805 Acc:1.0
step: 3100, Loss: 0.11923566460609436 Acc:1.0
step: 3200, Loss: 0.11730009317398071 Acc:1.0
step: 3300, Loss: 0.12743514776229858 Acc:1.0
step: 3400, Loss: 0.11473294347524643 Acc:1.0
step: 3500, Loss: 0.11752862483263016 Acc:1.0
step: 3600, Loss: 0.11644452065229416 Acc:1.0
step: 3700, Loss: 0.11830474436283112 Acc:1.0
step: 3800, Loss: 0.1170618087053299 Acc:1.0
step: 3900, Loss: 0.11736184358596802 Acc:1.0
step: 4000, Loss: 0.11866573989391327 Acc:1.0
step: 4100, Loss: 0.11879879236221313 Acc:1.0
step: 4200, Loss: 1.9823064804077148 Acc:0.9736842105263158
step: 4300, Loss: 0.11554364860057831 Acc:1.0
step: 4400, Loss: 0.1188170462846756 Acc:1.0
step: 4500, Loss: 0.11978733539581299 Acc:1.0
step: 4600, Loss: 0.11728288233280182 Acc:1.0
step: 4700, Loss: 0.12008524686098099 Acc:1.0
step: 4800, Loss: 0.11744417995214462 Acc:1.0
step: 4900, Loss: 0.11838100105524063 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.7222222222222222
precision: 0.7586206896551724
recall: 0.6285714285714286
F_score: 0.6875
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 5.549931526184082 Acc:0.9736842105263158
step: 100, Loss: 0.12273193895816803 Acc:1.0
step: 200, Loss: 0.11765025556087494 Acc:1.0
step: 300, Loss: 0.1167319118976593 Acc:1.0
step: 400, Loss: 0.11860817670822144 Acc:1.0
step: 500, Loss: 0.12556466460227966 Acc:1.0
step: 600, Loss: 0.11988005042076111 Acc:1.0
step: 700, Loss: 3.2126386165618896 Acc:0.9736842105263158
step: 800, Loss: 10.13206958770752 Acc:0.8947368421052632
step: 900, Loss: 0.12325622141361237 Acc:1.0
step: 1000, Loss: 0.11506502330303192 Acc:1.0
step: 1100, Loss: 0.11961193382740021 Acc:1.0
step: 1200, Loss: 0.11671775579452515 Acc:1.0
step: 1300, Loss: 0.11669689416885376 Acc:1.0
step: 1400, Loss: 0.11510893702507019 Acc:1.0
step: 1500, Loss: 4.568899631500244 Acc:0.9736842105263158
step: 1600, Loss: 0.13227608799934387 Acc:1.0
step: 1700, Loss: 0.25333496928215027 Acc:1.0
step: 1800, Loss: 0.1266838163137436 Acc:1.0
step: 1900, Loss: 0.11669370532035828 Acc:1.0
step: 2000, Loss: 0.11449333280324936 Acc:1.0
step: 2100, Loss: 0.11947448551654816 Acc:1.0
step: 2200, Loss: 0.12077596038579941 Acc:1.0
step: 2300, Loss: 0.11663694679737091 Acc:1.0
step: 2400, Loss: 1.22483491897583 Acc:0.9736842105263158
step: 2500, Loss: 0.11772371828556061 Acc:1.0
step: 2600, Loss: 0.11713385581970215 Acc:1.0
step: 2700, Loss: 0.11717063188552856 Acc:1.0
step: 2800, Loss: 0.11675827950239182 Acc:1.0
step: 2900, Loss: 0.11807923018932343 Acc:1.0
step: 3000, Loss: 0.11709285527467728 Acc:1.0
step: 3100, Loss: 0.1178499311208725 Acc:1.0
step: 3200, Loss: 0.11793167889118195 Acc:1.0
step: 3300, Loss: 0.12371894717216492 Acc:1.0
step: 3400, Loss: 0.16276541352272034 Acc:1.0
step: 3500, Loss: 0.11891137808561325 Acc:1.0
step: 3600, Loss: 1.1883124113082886 Acc:0.9736842105263158
step: 3700, Loss: 0.12070350348949432 Acc:1.0
step: 3800, Loss: 0.11568037420511246 Acc:1.0
step: 3900, Loss: 0.11736303567886353 Acc:1.0
step: 4000, Loss: 0.11899155378341675 Acc:1.0
step: 4100, Loss: 0.11892764270305634 Acc:1.0
step: 4200, Loss: 0.11787664145231247 Acc:1.0
step: 4300, Loss: 0.1954483687877655 Acc:1.0
step: 4400, Loss: 0.1200685203075409 Acc:1.0
step: 4500, Loss: 0.11888815462589264 Acc:1.0
step: 4600, Loss: 0.1156739592552185 Acc:1.0
step: 4700, Loss: 0.11950615793466568 Acc:1.0
step: 4800, Loss: 0.12004199624061584 Acc:1.0
step: 1700, Loss: 0.2643665671348572 Acc:1.0
step: 1800, Loss: 0.26546210050582886 Acc:1.0
step: 1900, Loss: 0.26472902297973633 Acc:1.0
step: 2000, Loss: 0.2647755742073059 Acc:1.0
step: 2100, Loss: 0.2642003893852234 Acc:1.0
step: 2200, Loss: 0.2640928328037262 Acc:1.0
step: 2300, Loss: 0.26428812742233276 Acc:1.0
step: 2400, Loss: 0.264612078666687 Acc:1.0
step: 2500, Loss: 0.26479917764663696 Acc:1.0
step: 2600, Loss: 0.2642265856266022 Acc:1.0
step: 2700, Loss: 0.2641292214393616 Acc:1.0
step: 2800, Loss: 0.2640939950942993 Acc:1.0
step: 2900, Loss: 0.26430338621139526 Acc:1.0
step: 3000, Loss: 0.2681615352630615 Acc:1.0
step: 3100, Loss: 0.2650674283504486 Acc:1.0
step: 3200, Loss: 0.26412636041641235 Acc:1.0
step: 3300, Loss: 0.2640208899974823 Acc:1.0
step: 3400, Loss: 0.2640455365180969 Acc:1.0
step: 3500, Loss: 0.2640818655490875 Acc:1.0
step: 3600, Loss: 0.26415133476257324 Acc:1.0
step: 3700, Loss: 0.2649456858634949 Acc:1.0
step: 3800, Loss: 0.26500657200813293 Acc:1.0
step: 3900, Loss: 0.2648415267467499 Acc:1.0
step: 4000, Loss: 0.264394074678421 Acc:1.0
step: 4100, Loss: 0.2652641832828522 Acc:1.0
step: 4200, Loss: 0.26577267050743103 Acc:1.0
step: 4300, Loss: 0.2657991051673889 Acc:1.0
step: 4400, Loss: 0.2640305161476135 Acc:1.0
step: 4500, Loss: 0.26402121782302856 Acc:1.0
step: 4600, Loss: 0.26400601863861084 Acc:1.0
step: 4700, Loss: 0.26400771737098694 Acc:1.0
step: 4800, Loss: 0.2640083432197571 Acc:1.0
step: 4900, Loss: 0.264053612947464 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.3199523985385895 Acc:1.0
step: 100, Loss: 0.2643338441848755 Acc:1.0
step: 200, Loss: 0.2641350030899048 Acc:1.0
step: 300, Loss: 0.26428094506263733 Acc:1.0
step: 400, Loss: 0.2645101845264435 Acc:1.0
step: 500, Loss: 0.26426658034324646 Acc:1.0
step: 600, Loss: 0.2643262445926666 Acc:1.0
step: 700, Loss: 0.2822703421115875 Acc:1.0
step: 800, Loss: 0.2646930515766144 Acc:1.0
step: 900, Loss: 0.26421383023262024 Acc:1.0
step: 1000, Loss: 0.2640273869037628 Acc:1.0
step: 1100, Loss: 0.2640461325645447 Acc:1.0
step: 1200, Loss: 0.2652931213378906 Acc:1.0
step: 1300, Loss: 0.26447319984436035 Acc:1.0
step: 1400, Loss: 0.26598578691482544 Acc:1.0
step: 1500, Loss: 0.2643830180168152 Acc:1.0
step: 1600, Loss: 0.2644660472869873 Acc:1.0
step: 1700, Loss: 0.26439768075942993 Acc:1.0
step: 1800, Loss: 0.2645522654056549 Acc:1.0
step: 1900, Loss: 0.264336496591568 Acc:1.0
step: 2000, Loss: 0.264506995677948 Acc:1.0
step: 2100, Loss: 0.26490098237991333 Acc:1.0
step: 2200, Loss: 0.26432016491889954 Acc:1.0
step: 2300, Loss: 0.26413530111312866 Acc:1.0
step: 2400, Loss: 0.26420387625694275 Acc:1.0
step: 2500, Loss: 0.26459285616874695 Acc:1.0
step: 2600, Loss: 0.2643159031867981 Acc:1.0
step: 2700, Loss: 0.26496008038520813 Acc:1.0
step: 2800, Loss: 0.26410558819770813 Acc:1.0
step: 2900, Loss: 0.26427069306373596 Acc:1.0
step: 3000, Loss: 0.26417431235313416 Acc:1.0
step: 3100, Loss: 0.26442351937294006 Acc:1.0
step: 3200, Loss: 0.2644703984260559 Acc:1.0
step: 3300, Loss: 0.26480522751808167 Acc:1.0
step: 3400, Loss: 0.267728328704834 Acc:1.0
step: 3500, Loss: 0.26619070768356323 Acc:1.0
step: 3600, Loss: 0.2647734582424164 Acc:1.0
step: 3700, Loss: 0.2640116214752197 Acc:1.0
step: 3800, Loss: 0.2640083134174347 Acc:1.0
step: 3900, Loss: 0.2640078663825989 Acc:1.0
step: 4000, Loss: 0.2640072703361511 Acc:1.0
step: 4100, Loss: 0.2640446424484253 Acc:1.0
step: 4200, Loss: 0.26440364122390747 Acc:1.0
step: 4300, Loss: 0.26484090089797974 Acc:1.0
step: 4400, Loss: 0.2643108665943146 Acc:1.0
step: 4500, Loss: 0.2648300230503082 Acc:1.0
step: 4600, Loss: 0.2646060585975647 Acc:1.0
step: 4700, Loss: 0.26407772302627563 Acc:1.0
step: 4800, Loss: 0.26432496309280396 Acc:1.0
step: 4900, Loss: 0.2647983729839325 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.3283437490463257 Acc:1.0
step: 100, Loss: 0.26427844166755676 Acc:1.0
step: 200, Loss: 0.2642950415611267 Acc:1.0
step: 300, Loss: 0.2641063928604126 Acc:1.0
step: 400, Loss: 0.26420408487319946 Acc:1.0
step: 500, Loss: 0.26450932025909424 Acc:1.0
step: 600, Loss: 0.26431983709335327 Acc:1.0
step: 700, Loss: 0.26462259888648987 Acc:1.0
step: 800, Loss: 0.265453577041626 Acc:1.0
step: 900, Loss: 0.26486513018608093 Acc:1.0
step: 1000, Loss: 0.26736876368522644 Acc:1.0
step: 1100, Loss: 0.26419806480407715 Acc:1.0
step: 1200, Loss: 0.26402145624160767 Acc:1.0
step: 1300, Loss: 0.26402100920677185 Acc:1.0
step: 1400, Loss: 0.2640646696090698 Acc:1.0
step: 1500, Loss: 0.26432040333747864 Acc:1.0
step: 1600, Loss: 0.2641606330871582 Acc:1.0
step: 1700, Loss: 0.2645534574985504 Acc:1.0
step: 1800, Loss: 0.2649425268173218 Acc:1.0
step: 1900, Loss: 0.264543741941452 Acc:1.0
step: 2000, Loss: 0.26464396715164185 Acc:1.0
step: 2100, Loss: 0.2646598815917969 Acc:1.0
step: 2200, Loss: 0.2641482353210449 Acc:1.0
step: 2300, Loss: 0.2640518546104431 Acc:1.0
step: 2400, Loss: 0.2640889286994934 Acc:1.0
step: 2500, Loss: 0.26423248648643494 Acc:1.0
step: 2600, Loss: 0.26426440477371216 Acc:1.0
step: 2700, Loss: 0.2656151354312897 Acc:1.0
step: 2800, Loss: 0.265705943107605 Acc:1.0
step: 2900, Loss: 0.2644495666027069 Acc:1.0
step: 3000, Loss: 0.2641179859638214 Acc:1.0
step: 3100, Loss: 0.264188289642334 Acc:1.0
step: 3200, Loss: 0.2650253474712372 Acc:1.0
step: 3300, Loss: 0.2644314467906952 Acc:1.0
step: 3400, Loss: 0.2647189199924469 Acc:1.0
step: 3500, Loss: 0.2645587623119354 Acc:1.0
step: 3600, Loss: 0.2644134759902954 Acc:1.0
step: 3700, Loss: 0.2643522024154663 Acc:1.0
step: 3800, Loss: 0.2642642855644226 Acc:1.0
step: 3900, Loss: 0.26582539081573486 Acc:1.0
step: 4000, Loss: 0.2648467421531677 Acc:1.0
step: 4100, Loss: 0.2647813558578491 Acc:1.0
step: 4200, Loss: 0.26486024260520935 Acc:1.0
step: 4300, Loss: 0.26555466651916504 Acc:1.0
step: 4400, Loss: 0.2645578384399414 Acc:1.0
step: 4500, Loss: 0.26423412561416626 Acc:1.0
step: 4600, Loss: 0.26403898000717163 Acc:1.0
step: 4700, Loss: 0.2640448212623596 Acc:1.0
step: 4800, Loss: 0.26410871744155884 Acc:1.0
step: 4900, Loss: 0.26518714427948 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 0.9411764705882353
recall: 1.0
F_score: 0.9696969696969697
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.32223978638648987 Acc:1.0
step: 100, Loss: 0.2642509937286377 Acc:1.0
step: 200, Loss: 0.2641221284866333 Acc:1.0
step: 300, Loss: 0.26414328813552856 Acc:1.0
step: 400, Loss: 0.26506203413009644 Acc:1.0
step: 500, Loss: 0.2651139795780182 Acc:1.0
step: 600, Loss: 0.2654150128364563 Acc:1.0
step: 700, Loss: 0.26531678438186646 Acc:1.0
step: 800, Loss: 0.26837900280952454 Acc:1.0
step: 900, Loss: 0.2642212510108948 Acc:1.0
step: 1000, Loss: 0.2640162706375122 Acc:1.0
step: 1100, Loss: 0.264010488986969 Acc:1.0
step: 1200, Loss: 0.2640224099159241 Acc:1.0
step: 1300, Loss: 0.26412710547447205 Acc:1.0
step: 1400, Loss: 0.2644132673740387 Acc:1.0
step: 1500, Loss: 0.2644329369068146 Acc:1.0
step: 1600, Loss: 0.26442235708236694 Acc:1.0
step: 1700, Loss: 0.2641433775424957 Acc:1.0
step: 1800, Loss: 0.26476147770881653 Acc:1.0
step: 1900, Loss: 0.26619571447372437 Acc:1.0
step: 2000, Loss: 0.2646547555923462 Acc:1.0
step: 2100, Loss: 0.26407527923583984 Acc:1.0
step: 2200, Loss: 0.26404446363449097 Acc:1.0
step: 2300, Loss: 0.264021635055542 Acc:1.0
step: 2400, Loss: 0.2642190456390381 Acc:1.0
step: 2500, Loss: 0.2644384801387787 Acc:1.0
step: 2600, Loss: 0.2657274901866913 Acc:1.0
step: 4900, Loss: 0.11892088502645493 Acc:1.0
training successfully ended.
validating...
validate data length:76
acc: 0.7638888888888888
precision: 0.7567567567567568
recall: 0.7777777777777778
F_score: 0.7671232876712328
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 0.1159970611333847 Acc:1.0
step: 100, Loss: 0.11719710379838943 Acc:1.0
step: 200, Loss: 0.11806277930736542 Acc:1.0
step: 300, Loss: 0.11976095288991928 Acc:1.0
step: 400, Loss: 0.11609556525945663 Acc:1.0
step: 500, Loss: 0.11839263141155243 Acc:1.0
step: 600, Loss: 0.12377779930830002 Acc:1.0
step: 700, Loss: 0.5860723853111267 Acc:0.9736842105263158
step: 800, Loss: 0.12783241271972656 Acc:1.0
step: 900, Loss: 0.12264442443847656 Acc:1.0
step: 1000, Loss: 0.11613178253173828 Acc:1.0
step: 1100, Loss: 0.12063712626695633 Acc:1.0
step: 1200, Loss: 0.11662480235099792 Acc:1.0
step: 1300, Loss: 3.4263241291046143 Acc:0.9736842105263158
step: 1400, Loss: 0.11913391947746277 Acc:1.0
step: 1500, Loss: 0.1167350560426712 Acc:1.0
step: 1600, Loss: 0.2155975103378296 Acc:1.0
step: 1700, Loss: 0.11640779674053192 Acc:1.0
step: 1800, Loss: 0.1149662435054779 Acc:1.0
step: 1900, Loss: 0.12149575352668762 Acc:1.0
step: 2000, Loss: 0.12768572568893433 Acc:1.0
step: 2100, Loss: 0.11964856088161469 Acc:1.0
step: 2200, Loss: 0.11737465113401413 Acc:1.0
step: 2300, Loss: 0.1158052608370781 Acc:1.0
step: 2400, Loss: 0.11418254673480988 Acc:1.0
step: 2500, Loss: 0.11517619341611862 Acc:1.0
step: 2600, Loss: 0.1160164400935173 Acc:1.0
step: 2700, Loss: 0.11910828948020935 Acc:1.0
step: 2800, Loss: 0.12144224345684052 Acc:1.0
step: 2900, Loss: 0.11562907695770264 Acc:1.0
step: 3000, Loss: 1.1604180335998535 Acc:0.9736842105263158
step: 3100, Loss: 0.11886513233184814 Acc:1.0
step: 3200, Loss: 2.2867493629455566 Acc:0.9736842105263158
step: 3300, Loss: 0.1140936017036438 Acc:1.0
step: 3400, Loss: 3.8959152698516846 Acc:0.9736842105263158
step: 3500, Loss: 0.11797935515642166 Acc:1.0
step: 3600, Loss: 0.11616243422031403 Acc:1.0
step: 3700, Loss: 0.11947977542877197 Acc:1.0
step: 3800, Loss: 0.12002302706241608 Acc:1.0
step: 3900, Loss: 0.1189991906285286 Acc:1.0
step: 4000, Loss: 0.11542139947414398 Acc:1.0
step: 4100, Loss: 0.12867596745491028 Acc:1.0
step: 4200, Loss: 0.1164674460887909 Acc:1.0
step: 4300, Loss: 0.11738602072000504 Acc:1.0
step: 4400, Loss: 0.12179116159677505 Acc:1.0
step: 4500, Loss: 0.11778190732002258 Acc:1.0
step: 4600, Loss: 0.11670829355716705 Acc:1.0
step: 4700, Loss: 0.11792689561843872 Acc:1.0
step: 4800, Loss: 0.12077997624874115 Acc:1.0
step: 4900, Loss: 2.839266300201416 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:76
acc: 0.6527777777777778
precision: 0.6
recall: 0.7272727272727273
F_score: 0.6575342465753425
subject 0 Avgacc: 0.6902777777777778 Avgfscore: 0.6843387232901876 
 Max acc:0.8194444444444444, Max f score:0.8169014084507044 Avg Recall:0.704651875901876 Max Recall:0.8857142857142857 Avg Precision:0.6744764250095409 Max Precision:0.8055555555555556
******** mix subject_1 ********

[247, 513]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 62.4085693359375 Acc:0.5263157894736842
step: 100, Loss: 9.68609619140625 Acc:0.7105263157894737
step: 200, Loss: 3.837042808532715 Acc:0.9473684210526315
step: 300, Loss: 1.8783307075500488 Acc:0.9736842105263158
step: 400, Loss: 1.16498863697052 Acc:0.9736842105263158
step: 500, Loss: 0.24172598123550415 Acc:1.0
step: 600, Loss: 0.13376766443252563 Acc:1.0
step: 700, Loss: 0.11956232041120529 Acc:1.0
step: 800, Loss: 0.12316211313009262 Acc:1.0
step: 900, Loss: 0.13313958048820496 Acc:1.0
step: 1000, Loss: 0.12349341809749603 Acc:1.0
step: 1100, Loss: 0.11977130174636841 Acc:1.0
step: 1200, Loss: 0.1244124323129654 Acc:1.0
step: 1300, Loss: 0.26937249302864075 Acc:1.0
step: 1400, Loss: 0.11804725974798203 Acc:1.0
step: 1500, Loss: 0.11852964758872986 Acc:1.0
step: 1600, Loss: 0.11824198067188263 Acc:1.0
step: 1700, Loss: 0.1143387109041214 Acc:1.0
step: 1800, Loss: 0.11650314182043076 Acc:1.0
step: 1900, Loss: 0.11907200515270233 Acc:1.0
step: 2000, Loss: 0.11627006530761719 Acc:1.0
step: 2100, Loss: 0.1210910975933075 Acc:1.0
step: 2200, Loss: 0.12190298736095428 Acc:1.0
step: 2300, Loss: 1.3344579935073853 Acc:0.9736842105263158
step: 2400, Loss: 0.11840670555830002 Acc:1.0
step: 2500, Loss: 0.11541301012039185 Acc:1.0
step: 2600, Loss: 0.4217401146888733 Acc:1.0
step: 2700, Loss: 0.12349791079759598 Acc:1.0
step: 2800, Loss: 0.11880728602409363 Acc:1.0
step: 2900, Loss: 0.11789627373218536 Acc:1.0
step: 3000, Loss: 0.12005683779716492 Acc:1.0
step: 3100, Loss: 0.11801610141992569 Acc:1.0
step: 3200, Loss: 0.45511752367019653 Acc:1.0
step: 3300, Loss: 0.11665045469999313 Acc:1.0
step: 3400, Loss: 0.12087240815162659 Acc:1.0
step: 3500, Loss: 0.12255030870437622 Acc:1.0
step: 3600, Loss: 0.11902546137571335 Acc:1.0
step: 3700, Loss: 0.12149456143379211 Acc:1.0
step: 3800, Loss: 0.11399990320205688 Acc:1.0
step: 3900, Loss: 0.11885528266429901 Acc:1.0
step: 4000, Loss: 0.11788688600063324 Acc:1.0
step: 4100, Loss: 0.1280285120010376 Acc:1.0
step: 4200, Loss: 0.11633309721946716 Acc:1.0
step: 4300, Loss: 0.1236758828163147 Acc:1.0
step: 4400, Loss: 0.11594051122665405 Acc:1.0
step: 4500, Loss: 0.11916056275367737 Acc:1.0
step: 4600, Loss: 0.12107003480195999 Acc:1.0
step: 4700, Loss: 0.11800818890333176 Acc:1.0
step: 4800, Loss: 0.11687026172876358 Acc:1.0
step: 4900, Loss: 0.11888505518436432 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.8235294117647058
precision: 0.825
recall: 0.75
F_score: 0.7857142857142856
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.3622141182422638 Acc:1.0
step: 100, Loss: 0.12672185897827148 Acc:1.0
step: 200, Loss: 0.1278105229139328 Acc:1.0
step: 300, Loss: 1.4780635833740234 Acc:0.9736842105263158
step: 400, Loss: 1.1319278478622437 Acc:0.9736842105263158
step: 500, Loss: 0.13051137328147888 Acc:1.0
step: 600, Loss: 0.12288154661655426 Acc:1.0
step: 700, Loss: 0.1255083978176117 Acc:1.0
step: 800, Loss: 0.12825480103492737 Acc:1.0
step: 900, Loss: 0.11761384457349777 Acc:1.0
step: 1000, Loss: 0.11930881440639496 Acc:1.0
step: 1100, Loss: 0.11901707947254181 Acc:1.0
step: 1200, Loss: 0.11981720477342606 Acc:1.0
step: 1300, Loss: 0.11832765489816666 Acc:1.0
step: 1400, Loss: 0.11602306365966797 Acc:1.0
step: 1500, Loss: 0.11610209941864014 Acc:1.0
step: 1600, Loss: 0.12607303261756897 Acc:1.0
step: 1700, Loss: 0.12052550911903381 Acc:1.0
step: 1800, Loss: 0.11917950212955475 Acc:1.0
step: 1900, Loss: 0.11947207152843475 Acc:1.0
step: 2000, Loss: 0.11663249880075455 Acc:1.0
step: 2100, Loss: 0.11779782176017761 Acc:1.0
step: 2200, Loss: 0.11966294050216675 Acc:1.0
step: 2300, Loss: 0.13212044537067413 Acc:1.0
step: 2400, Loss: 0.1169111430644989 Acc:1.0
step: 2500, Loss: 0.11773491650819778 Acc:1.0
step: 2600, Loss: 0.11923570930957794 Acc:1.0
step: 2700, Loss: 0.11515598744153976 Acc:1.0
step: 2800, Loss: 0.11585161089897156 Acc:1.0
step: 2900, Loss: 0.12125542759895325 Acc:1.0
step: 3000, Loss: 0.11767156422138214 Acc:1.0
step: 3100, Loss: 0.11564533412456512 Acc:1.0
step: 3200, Loss: 0.11842675507068634 Acc:1.0
step: 3300, Loss: 0.11709266155958176 Acc:1.0
step: 3400, Loss: 0.12169060111045837 Acc:1.0
step: 3500, Loss: 0.11926089227199554 Acc:1.0
step: 3600, Loss: 0.11620127409696579 Acc:1.0
step: 3700, Loss: 0.12038853764533997 Acc:1.0
step: 3800, Loss: 0.17197459936141968 Acc:1.0
step: 3900, Loss: 0.11800409853458405 Acc:1.0
step: 4000, Loss: 0.1169988289475441 Acc:1.0
step: 4100, Loss: 0.12193234264850616 Acc:1.0
step: 4200, Loss: 0.12089722603559494 Acc:1.0
step: 4300, Loss: 0.11915785074234009 Acc:1.0
step: 4400, Loss: 0.11972250044345856 Acc:1.0
step: 2700, Loss: 0.2657063603401184 Acc:1.0
step: 2800, Loss: 0.26408809423446655 Acc:1.0
step: 2900, Loss: 0.2641397714614868 Acc:1.0
step: 3000, Loss: 0.26413312554359436 Acc:1.0
step: 3100, Loss: 0.2640904486179352 Acc:1.0
step: 3200, Loss: 0.26427119970321655 Acc:1.0
step: 3300, Loss: 0.264451801776886 Acc:1.0
step: 3400, Loss: 0.26487216353416443 Acc:1.0
step: 3500, Loss: 0.2641792297363281 Acc:1.0
step: 3600, Loss: 0.264042466878891 Acc:1.0
step: 3700, Loss: 0.26414138078689575 Acc:1.0
step: 3800, Loss: 0.26435616612434387 Acc:1.0
step: 3900, Loss: 0.26589304208755493 Acc:1.0
step: 4000, Loss: 0.2661570906639099 Acc:1.0
step: 4100, Loss: 0.2640388011932373 Acc:1.0
step: 4200, Loss: 0.26401904225349426 Acc:1.0
step: 4300, Loss: 0.2640078067779541 Acc:1.0
step: 4400, Loss: 0.264006108045578 Acc:1.0
step: 4500, Loss: 0.26401013135910034 Acc:1.0
step: 4600, Loss: 0.2641531527042389 Acc:1.0
step: 4700, Loss: 0.26428449153900146 Acc:1.0
step: 4800, Loss: 0.26426076889038086 Acc:1.0
step: 4900, Loss: 0.2647867798805237 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 0.9666666666666667
precision: 1.0
recall: 0.9411764705882353
F_score: 0.9696969696969697
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.39263251423835754 Acc:1.0
step: 100, Loss: 0.2641429305076599 Acc:1.0
step: 200, Loss: 0.26411038637161255 Acc:1.0
step: 300, Loss: 0.2641792297363281 Acc:1.0
step: 400, Loss: 0.26456114649772644 Acc:1.0
step: 500, Loss: 0.26530829071998596 Acc:1.0
step: 600, Loss: 0.26453155279159546 Acc:1.0
step: 700, Loss: 0.264222115278244 Acc:1.0
step: 800, Loss: 0.26677098870277405 Acc:1.0
step: 900, Loss: 0.26430943608283997 Acc:1.0
step: 1000, Loss: 0.2640296220779419 Acc:1.0
step: 1100, Loss: 0.264024019241333 Acc:1.0
step: 1200, Loss: 0.26403915882110596 Acc:1.0
step: 1300, Loss: 0.2640586495399475 Acc:1.0
step: 1400, Loss: 0.26433107256889343 Acc:1.0
step: 1500, Loss: 0.26441842317581177 Acc:1.0
step: 1600, Loss: 0.2643354535102844 Acc:1.0
step: 1700, Loss: 0.2647413909435272 Acc:1.0
step: 1800, Loss: 0.2651388645172119 Acc:1.0
step: 1900, Loss: 0.2645650804042816 Acc:1.0
step: 2000, Loss: 0.26416200399398804 Acc:1.0
step: 2100, Loss: 0.2640652656555176 Acc:1.0
step: 2200, Loss: 0.264041543006897 Acc:1.0
step: 2300, Loss: 0.2640397846698761 Acc:1.0
step: 2400, Loss: 0.2642643451690674 Acc:1.0
step: 2500, Loss: 0.2650907039642334 Acc:1.0
step: 2600, Loss: 0.2657386064529419 Acc:1.0
step: 2700, Loss: 0.26434776186943054 Acc:1.0
step: 2800, Loss: 0.264273077249527 Acc:1.0
step: 2900, Loss: 0.2640599310398102 Acc:1.0
step: 3000, Loss: 0.26403260231018066 Acc:1.0
step: 3100, Loss: 0.2640846371650696 Acc:1.0
step: 3200, Loss: 0.2644430696964264 Acc:1.0
step: 3300, Loss: 0.2647359371185303 Acc:1.0
step: 3400, Loss: 0.26473480463027954 Acc:1.0
step: 3500, Loss: 0.264446496963501 Acc:1.0
step: 3600, Loss: 0.26405906677246094 Acc:1.0
step: 3700, Loss: 0.2641899883747101 Acc:1.0
step: 3800, Loss: 0.2653616964817047 Acc:1.0
step: 3900, Loss: 0.2649837136268616 Acc:1.0
step: 4000, Loss: 0.26527372002601624 Acc:1.0
step: 4100, Loss: 0.2640199363231659 Acc:1.0
step: 4200, Loss: 0.26401177048683167 Acc:1.0
step: 4300, Loss: 0.2640080451965332 Acc:1.0
step: 4400, Loss: 0.2640072703361511 Acc:1.0
step: 4500, Loss: 0.264044851064682 Acc:1.0
step: 4600, Loss: 0.2643404006958008 Acc:1.0
step: 4700, Loss: 0.2644307613372803 Acc:1.0
step: 4800, Loss: 0.26412180066108704 Acc:1.0
step: 4900, Loss: 0.2644924521446228 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.34788382053375244 Acc:1.0
step: 100, Loss: 0.2641901969909668 Acc:1.0
step: 200, Loss: 0.26416724920272827 Acc:1.0
step: 300, Loss: 0.2643871307373047 Acc:1.0
step: 400, Loss: 0.2643646001815796 Acc:1.0
step: 500, Loss: 0.2644570469856262 Acc:1.0
step: 600, Loss: 0.2652055025100708 Acc:1.0
step: 700, Loss: 0.2652709484100342 Acc:1.0
step: 800, Loss: 0.2650527358055115 Acc:1.0
step: 900, Loss: 0.26425793766975403 Acc:1.0
step: 1000, Loss: 0.26404622197151184 Acc:1.0
step: 1100, Loss: 0.2640600800514221 Acc:1.0
step: 1200, Loss: 0.2649807929992676 Acc:1.0
step: 1300, Loss: 0.26422837376594543 Acc:1.0
step: 1400, Loss: 0.2648729085922241 Acc:1.0
step: 1500, Loss: 0.26434704661369324 Acc:1.0
step: 1600, Loss: 0.26448145508766174 Acc:1.0
step: 1700, Loss: 0.2653065621852875 Acc:1.0
step: 1800, Loss: 0.26468557119369507 Acc:1.0
step: 1900, Loss: 0.26424267888069153 Acc:1.0
step: 2000, Loss: 0.2642550468444824 Acc:1.0
step: 2100, Loss: 0.2641865611076355 Acc:1.0
step: 2200, Loss: 0.26422178745269775 Acc:1.0
step: 2300, Loss: 0.2644263505935669 Acc:1.0
step: 2400, Loss: 0.26428940892219543 Acc:1.0
step: 2500, Loss: 0.26427021622657776 Acc:1.0
step: 2600, Loss: 0.2644556164741516 Acc:1.0
step: 2700, Loss: 0.26600325107574463 Acc:1.0
step: 2800, Loss: 0.2645556926727295 Acc:1.0
step: 2900, Loss: 0.26428160071372986 Acc:1.0
step: 3000, Loss: 0.2640504539012909 Acc:1.0
step: 3100, Loss: 0.2641492187976837 Acc:1.0
step: 3200, Loss: 0.26449522376060486 Acc:1.0
step: 3300, Loss: 0.2648887038230896 Acc:1.0
step: 3400, Loss: 0.26499509811401367 Acc:1.0
step: 3500, Loss: 0.26435327529907227 Acc:1.0
step: 3600, Loss: 0.264229953289032 Acc:1.0
step: 3700, Loss: 0.26430895924568176 Acc:1.0
step: 3800, Loss: 0.2642122805118561 Acc:1.0
step: 3900, Loss: 0.26564133167266846 Acc:1.0
step: 4000, Loss: 0.2662777602672577 Acc:1.0
step: 4100, Loss: 0.265868216753006 Acc:1.0
step: 4200, Loss: 0.265329509973526 Acc:1.0
step: 4300, Loss: 0.26451465487480164 Acc:1.0
step: 4400, Loss: 0.2640931308269501 Acc:1.0
step: 4500, Loss: 0.2640150189399719 Acc:1.0
step: 4600, Loss: 0.2640153169631958 Acc:1.0
step: 4700, Loss: 0.2640324532985687 Acc:1.0
step: 4800, Loss: 0.26423102617263794 Acc:1.0
step: 4900, Loss: 0.2644251883029938 Acc:1.0
training successfully ended.
validating...
validate data length:31
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Arousal-norm-lr5e-05FACED10_11_5000_scebaseseed74fold_best_model.pth
Training... train_data length:281
step: 0, Loss: 0.319991797208786 Acc:1.0
step: 100, Loss: 0.2644306421279907 Acc:1.0
step: 200, Loss: 0.26408106088638306 Acc:1.0
step: 300, Loss: 0.2644595503807068 Acc:1.0
step: 400, Loss: 0.26439762115478516 Acc:1.0
step: 500, Loss: 0.26430270075798035 Acc:1.0
step: 600, Loss: 0.2648474872112274 Acc:1.0
step: 700, Loss: 0.26501578092575073 Acc:1.0
step: 800, Loss: 0.26534152030944824 Acc:1.0
step: 900, Loss: 0.265503853559494 Acc:1.0
step: 1000, Loss: 0.26478299498558044 Acc:1.0
step: 1100, Loss: 0.26408296823501587 Acc:1.0
step: 1200, Loss: 0.26407453417778015 Acc:1.0
step: 1300, Loss: 0.2640314996242523 Acc:1.0
step: 1400, Loss: 0.2640944719314575 Acc:1.0
step: 1500, Loss: 0.26481086015701294 Acc:1.0
step: 1600, Loss: 0.26486465334892273 Acc:1.0
step: 1700, Loss: 0.26458537578582764 Acc:1.0
step: 1800, Loss: 0.26500755548477173 Acc:1.0
step: 1900, Loss: 0.26421719789505005 Acc:1.0
step: 2000, Loss: 0.2643451690673828 Acc:1.0
step: 2100, Loss: 0.2643362879753113 Acc:1.0
step: 2200, Loss: 0.2644597291946411 Acc:1.0
step: 2300, Loss: 0.26451724767684937 Acc:1.0
step: 2400, Loss: 0.26458197832107544 Acc:1.0
step: 2500, Loss: 0.2649788558483124 Acc:1.0
step: 2600, Loss: 0.26505452394485474 Acc:1.0
step: 2700, Loss: 0.26451125741004944 Acc:1.0
step: 2800, Loss: 0.2643287777900696 Acc:1.0
step: 2900, Loss: 0.26412028074264526 Acc:1.0
step: 3000, Loss: 0.264260470867157 Acc:1.0
step: 3100, Loss: 0.264484167098999 Acc:1.0
step: 3200, Loss: 0.26467904448509216 Acc:1.0
step: 3300, Loss: 0.2647131681442261 Acc:1.0
step: 3400, Loss: 0.26448121666908264 Acc:1.0
step: 3500, Loss: 0.26418203115463257 Acc:1.0
step: 3600, Loss: 0.26404038071632385 Acc:1.0
step: 4500, Loss: 0.11979445815086365 Acc:1.0
step: 4600, Loss: 0.11713049560785294 Acc:1.0
step: 4700, Loss: 7.706951141357422 Acc:0.9473684210526315
step: 4800, Loss: 0.11365503817796707 Acc:1.0
step: 4900, Loss: 0.12018267065286636 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9509803921568627
precision: 0.9230769230769231
recall: 0.9795918367346939
F_score: 0.9504950495049506
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.11886843293905258 Acc:1.0
step: 100, Loss: 0.11979836970567703 Acc:1.0
step: 200, Loss: 0.12263119220733643 Acc:1.0
step: 300, Loss: 0.1176445409655571 Acc:1.0
step: 400, Loss: 0.11807969957590103 Acc:1.0
step: 500, Loss: 0.11626751720905304 Acc:1.0
step: 600, Loss: 0.11726167052984238 Acc:1.0
step: 700, Loss: 0.11694943904876709 Acc:1.0
step: 800, Loss: 0.11816203594207764 Acc:1.0
step: 900, Loss: 0.11494489759206772 Acc:1.0
step: 1000, Loss: 0.11570698022842407 Acc:1.0
step: 1100, Loss: 1.1322224140167236 Acc:0.9736842105263158
step: 1200, Loss: 0.11504766345024109 Acc:1.0
step: 1300, Loss: 0.11966295540332794 Acc:1.0
step: 1400, Loss: 0.12363189458847046 Acc:1.0
step: 1500, Loss: 0.1193295419216156 Acc:1.0
step: 1600, Loss: 0.11797499656677246 Acc:1.0
step: 1700, Loss: 0.12173794209957123 Acc:1.0
step: 1800, Loss: 0.12085932493209839 Acc:1.0
step: 1900, Loss: 0.11967165768146515 Acc:1.0
step: 2000, Loss: 0.12006641179323196 Acc:1.0
step: 2100, Loss: 0.12049941718578339 Acc:1.0
step: 2200, Loss: 0.11999161541461945 Acc:1.0
step: 2300, Loss: 0.11814624816179276 Acc:1.0
step: 2400, Loss: 0.11539596319198608 Acc:1.0
step: 2500, Loss: 0.11866755783557892 Acc:1.0
step: 2600, Loss: 0.1184396892786026 Acc:1.0
step: 2700, Loss: 0.12846407294273376 Acc:1.0
step: 2800, Loss: 0.1233648955821991 Acc:1.0
step: 2900, Loss: 0.11934980005025864 Acc:1.0
step: 3000, Loss: 0.18255919218063354 Acc:1.0
step: 3100, Loss: 0.11399544775485992 Acc:1.0
step: 3200, Loss: 0.11676528304815292 Acc:1.0
step: 3300, Loss: 0.11799408495426178 Acc:1.0
step: 3400, Loss: 0.11796356737613678 Acc:1.0
step: 3500, Loss: 0.11990886926651001 Acc:1.0
step: 3600, Loss: 0.11575213074684143 Acc:1.0
step: 3700, Loss: 0.12613195180892944 Acc:1.0
step: 3800, Loss: 0.12527617812156677 Acc:1.0
step: 3900, Loss: 0.11341442912817001 Acc:1.0
step: 4000, Loss: 0.1205938309431076 Acc:1.0
step: 4100, Loss: 0.8647509813308716 Acc:0.9736842105263158
step: 4200, Loss: 0.11774659156799316 Acc:1.0
step: 4300, Loss: 0.11779901385307312 Acc:1.0
step: 4400, Loss: 0.11643211543560028 Acc:1.0
step: 4500, Loss: 0.11941727995872498 Acc:1.0
step: 4600, Loss: 0.12009778618812561 Acc:1.0
step: 4700, Loss: 0.11572727560997009 Acc:1.0
step: 4800, Loss: 0.12612730264663696 Acc:1.0
step: 4900, Loss: 0.11800695955753326 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9803921568627451
precision: 0.9661016949152542
recall: 1.0
F_score: 0.9827586206896551
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 1.7688826322555542 Acc:0.9736842105263158
step: 100, Loss: 0.12237643450498581 Acc:1.0
step: 200, Loss: 0.2503427267074585 Acc:1.0
step: 300, Loss: 0.12129423022270203 Acc:1.0
step: 400, Loss: 0.11854276806116104 Acc:1.0
step: 500, Loss: 0.40639498829841614 Acc:1.0
step: 600, Loss: 0.11716143786907196 Acc:1.0
step: 700, Loss: 0.12679941952228546 Acc:1.0
step: 800, Loss: 0.116639643907547 Acc:1.0
step: 900, Loss: 0.11751360446214676 Acc:1.0
step: 1000, Loss: 0.12246397882699966 Acc:1.0
step: 1100, Loss: 0.12105630338191986 Acc:1.0
step: 1200, Loss: 0.11453907936811447 Acc:1.0
step: 1300, Loss: 3.7942209243774414 Acc:0.9736842105263158
step: 1400, Loss: 0.11634669452905655 Acc:1.0
step: 1500, Loss: 0.11439576745033264 Acc:1.0
step: 1600, Loss: 0.11812124401330948 Acc:1.0
step: 1700, Loss: 0.11767352372407913 Acc:1.0
step: 1800, Loss: 0.1190776601433754 Acc:1.0
step: 1900, Loss: 0.11684169620275497 Acc:1.0
step: 2000, Loss: 0.11739305406808853 Acc:1.0
step: 2100, Loss: 0.12103428691625595 Acc:1.0
step: 2200, Loss: 0.11846087872982025 Acc:1.0
step: 2300, Loss: 0.11680150032043457 Acc:1.0
step: 2400, Loss: 0.11795101314783096 Acc:1.0
step: 2500, Loss: 0.11698412895202637 Acc:1.0
step: 2600, Loss: 0.1171422228217125 Acc:1.0
step: 2700, Loss: 0.11546549201011658 Acc:1.0
step: 2800, Loss: 0.11762899160385132 Acc:1.0
step: 2900, Loss: 0.11845126748085022 Acc:1.0
step: 3000, Loss: 0.11472825706005096 Acc:1.0
step: 3100, Loss: 0.11542616039514542 Acc:1.0
step: 3200, Loss: 0.11992069333791733 Acc:1.0
step: 3300, Loss: 0.12069486826658249 Acc:1.0
step: 3400, Loss: 0.12212399393320084 Acc:1.0
step: 3500, Loss: 0.11720876395702362 Acc:1.0
step: 3600, Loss: 0.11703328043222427 Acc:1.0
step: 3700, Loss: 0.11524005234241486 Acc:1.0
step: 3800, Loss: 1.3782856464385986 Acc:0.9736842105263158
step: 3900, Loss: 0.11888261139392853 Acc:1.0
step: 4000, Loss: 0.11976169794797897 Acc:1.0
step: 4100, Loss: 0.1847783625125885 Acc:1.0
step: 4200, Loss: 0.11578976362943649 Acc:1.0
step: 4300, Loss: 0.11870862543582916 Acc:1.0
step: 4400, Loss: 0.11748529225587845 Acc:1.0
step: 4500, Loss: 0.1159905269742012 Acc:1.0
step: 4600, Loss: 0.11794346570968628 Acc:1.0
step: 4700, Loss: 0.11815610527992249 Acc:1.0
step: 4800, Loss: 0.11555396765470505 Acc:1.0
step: 4900, Loss: 0.11386813223361969 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9411764705882353
precision: 0.9824561403508771
recall: 0.9180327868852459
F_score: 0.9491525423728813
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 2.759586811065674 Acc:0.9736842105263158
step: 100, Loss: 0.1161670833826065 Acc:1.0
step: 200, Loss: 0.1197245717048645 Acc:1.0
step: 300, Loss: 0.11512766033411026 Acc:1.0
step: 400, Loss: 0.12320499867200851 Acc:1.0
step: 500, Loss: 0.1177562028169632 Acc:1.0
step: 600, Loss: 0.12153874337673187 Acc:1.0
step: 700, Loss: 0.11760081350803375 Acc:1.0
step: 800, Loss: 0.12353675067424774 Acc:1.0
step: 900, Loss: 0.12294322997331619 Acc:1.0
step: 1000, Loss: 0.11760149151086807 Acc:1.0
step: 1100, Loss: 0.1188429519534111 Acc:1.0
step: 1200, Loss: 2.4686765670776367 Acc:0.9736842105263158
step: 1300, Loss: 0.11770473420619965 Acc:1.0
step: 1400, Loss: 0.1155305802822113 Acc:1.0
step: 1500, Loss: 0.11705618351697922 Acc:1.0
step: 1600, Loss: 0.11496428400278091 Acc:1.0
step: 1700, Loss: 0.1170215904712677 Acc:1.0
step: 1800, Loss: 1.0639898777008057 Acc:0.9736842105263158
step: 1900, Loss: 0.21396449208259583 Acc:1.0
step: 2000, Loss: 0.12523281574249268 Acc:1.0
step: 2100, Loss: 0.11741555482149124 Acc:1.0
step: 2200, Loss: 0.11586693674325943 Acc:1.0
step: 2300, Loss: 0.1232343539595604 Acc:1.0
step: 2400, Loss: 0.11661472916603088 Acc:1.0
step: 2500, Loss: 0.13225409388542175 Acc:1.0
step: 2600, Loss: 0.12061037123203278 Acc:1.0
step: 2700, Loss: 0.11533726006746292 Acc:1.0
step: 2800, Loss: 0.11920414119958878 Acc:1.0
step: 2900, Loss: 0.1408928483724594 Acc:1.0
step: 3000, Loss: 0.12090043723583221 Acc:1.0
step: 3100, Loss: 0.1202976256608963 Acc:1.0
step: 3200, Loss: 0.11515182256698608 Acc:1.0
step: 3300, Loss: 0.11837919056415558 Acc:1.0
step: 3400, Loss: 3.1258368492126465 Acc:0.9736842105263158
step: 3500, Loss: 0.11576640605926514 Acc:1.0
step: 3600, Loss: 0.11712362617254257 Acc:1.0
step: 3700, Loss: 0.115925133228302 Acc:1.0
step: 3800, Loss: 0.11757010221481323 Acc:1.0
step: 3900, Loss: 0.1183881014585495 Acc:1.0
step: 4000, Loss: 0.11577431857585907 Acc:1.0
step: 4100, Loss: 0.12180662155151367 Acc:1.0
step: 4200, Loss: 0.12372395396232605 Acc:1.0
step: 4300, Loss: 0.11972944438457489 Acc:1.0
step: 4400, Loss: 0.11510583758354187 Acc:1.0
step: 4500, Loss: 0.1190277487039566 Acc:1.0
step: 4600, Loss: 0.120820552110672 Acc:1.0
step: 4700, Loss: 0.12736622989177704 Acc:1.0
step: 4800, Loss: 0.11767418682575226 Acc:1.0
step: 4900, Loss: 0.11925005912780762 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9803921568627451
precision: 1.0
recall: 0.9622641509433962
F_score: 0.9807692307692307
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 4.648251533508301 Acc:0.9736842105263158
step: 100, Loss: 0.11528226733207703 Acc:1.0
step: 200, Loss: 0.18416118621826172 Acc:1.0
step: 300, Loss: 0.31851357221603394 Acc:1.0
step: 400, Loss: 0.11667080968618393 Acc:1.0
step: 500, Loss: 0.1203332394361496 Acc:1.0
step: 600, Loss: 0.11817716062068939 Acc:1.0
step: 700, Loss: 0.11735336482524872 Acc:1.0
step: 800, Loss: 0.12055675685405731 Acc:1.0
step: 900, Loss: 0.11649748682975769 Acc:1.0
step: 1000, Loss: 0.11508607864379883 Acc:1.0
step: 1100, Loss: 0.11639013886451721 Acc:1.0
step: 1200, Loss: 0.11846905946731567 Acc:1.0
step: 1300, Loss: 0.11926589906215668 Acc:1.0
step: 1400, Loss: 0.11874037235975266 Acc:1.0
step: 1500, Loss: 0.11822149157524109 Acc:1.0
step: 1600, Loss: 0.11419270187616348 Acc:1.0
step: 1700, Loss: 0.11985207349061966 Acc:1.0
step: 1800, Loss: 0.12447021901607513 Acc:1.0
step: 1900, Loss: 0.1204991489648819 Acc:1.0
step: 2000, Loss: 0.409811794757843 Acc:1.0
step: 2100, Loss: 0.11516880989074707 Acc:1.0
step: 2200, Loss: 0.11561740189790726 Acc:1.0
step: 2300, Loss: 0.11767100542783737 Acc:1.0
step: 2400, Loss: 0.11718037724494934 Acc:1.0
step: 2500, Loss: 0.11536303162574768 Acc:1.0
step: 2600, Loss: 0.119984470307827 Acc:1.0
step: 2700, Loss: 0.11367633938789368 Acc:1.0
step: 2800, Loss: 0.11667382717132568 Acc:1.0
step: 2900, Loss: 0.12335149943828583 Acc:1.0
step: 3000, Loss: 0.11788446456193924 Acc:1.0
step: 3100, Loss: 0.11568371206521988 Acc:1.0
step: 3200, Loss: 0.11803561449050903 Acc:1.0
step: 3300, Loss: 0.11754625290632248 Acc:1.0
step: 3400, Loss: 0.11884152889251709 Acc:1.0
step: 3500, Loss: 0.11720920354127884 Acc:1.0
step: 3600, Loss: 0.11623650789260864 Acc:1.0
step: 3700, Loss: 0.11656254529953003 Acc:1.0
step: 3800, Loss: 0.11691293120384216 Acc:1.0
step: 3900, Loss: 0.11992867290973663 Acc:1.0
step: 4000, Loss: 0.11694648861885071 Acc:1.0
step: 4100, Loss: 0.11487191915512085 Acc:1.0
step: 4200, Loss: 0.1270662546157837 Acc:1.0
step: 4300, Loss: 0.11697321385145187 Acc:1.0
step: 4400, Loss: 0.1218542829155922 Acc:1.0
step: 4500, Loss: 0.11469899117946625 Acc:1.0
step: 4600, Loss: 0.11758629977703094 Acc:1.0
step: 4700, Loss: 0.11744916439056396 Acc:1.0
step: 4800, Loss: 0.11537052690982819 Acc:1.0
step: 4900, Loss: 0.11861229687929153 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9803921568627451
precision: 1.0
recall: 0.9615384615384616
F_score: 0.9803921568627451
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11845903098583221 Acc:1.0
step: 100, Loss: 0.11994083970785141 Acc:1.0
step: 200, Loss: 0.1253163367509842 Acc:1.0
step: 300, Loss: 0.18521437048912048 Acc:1.0
step: 400, Loss: 0.11595034599304199 Acc:1.0
step: 500, Loss: 0.1225750520825386 Acc:1.0
step: 600, Loss: 3.2604713439941406 Acc:0.9736842105263158
step: 700, Loss: 0.11524146795272827 Acc:1.0
step: 800, Loss: 0.11572686582803726 Acc:1.0
step: 900, Loss: 0.12046466022729874 Acc:1.0
step: 1000, Loss: 0.12358579784631729 Acc:1.0
step: 1100, Loss: 0.11581850051879883 Acc:1.0
step: 1200, Loss: 0.11695566028356552 Acc:1.0
step: 1300, Loss: 0.12291514873504639 Acc:1.0
step: 1400, Loss: 0.120418019592762 Acc:1.0
step: 1500, Loss: 0.118658147752285 Acc:1.0
step: 1600, Loss: 0.11816231906414032 Acc:1.0
step: 1700, Loss: 0.11668697744607925 Acc:1.0
step: 1800, Loss: 0.11735014617443085 Acc:1.0
step: 1900, Loss: 0.11688608676195145 Acc:1.0
step: 2000, Loss: 0.12013266980648041 Acc:1.0
step: 2100, Loss: 0.12157028913497925 Acc:1.0
step: 2200, Loss: 0.15707135200500488 Acc:1.0
step: 2300, Loss: 0.11810871958732605 Acc:1.0
step: 2400, Loss: 0.11920854449272156 Acc:1.0
step: 2500, Loss: 0.12000244855880737 Acc:1.0
step: 2600, Loss: 0.11737433075904846 Acc:1.0
step: 2700, Loss: 0.11834590882062912 Acc:1.0
step: 2800, Loss: 0.1210920661687851 Acc:1.0
step: 2900, Loss: 0.11862150579690933 Acc:1.0
step: 3000, Loss: 0.12980635464191437 Acc:1.0
step: 3100, Loss: 0.11583085358142853 Acc:1.0
step: 3200, Loss: 0.11639529466629028 Acc:1.0
step: 3300, Loss: 0.12233360856771469 Acc:1.0
step: 3400, Loss: 0.11617022752761841 Acc:1.0
step: 3500, Loss: 0.11721080541610718 Acc:1.0
step: 3600, Loss: 0.11866343766450882 Acc:1.0
step: 3700, Loss: 0.11596457660198212 Acc:1.0
step: 3800, Loss: 0.11636173725128174 Acc:1.0
step: 3900, Loss: 0.1155390739440918 Acc:1.0
step: 4000, Loss: 1.4491875171661377 Acc:0.9736842105263158
step: 4100, Loss: 0.11749105155467987 Acc:1.0
step: 4200, Loss: 0.11862000823020935 Acc:1.0
step: 4300, Loss: 0.11960995942354202 Acc:1.0
step: 4400, Loss: 0.11839645355939865 Acc:1.0
step: 4500, Loss: 0.11900492012500763 Acc:1.0
step: 4600, Loss: 0.11339050531387329 Acc:1.0
step: 4700, Loss: 0.11373956501483917 Acc:1.0
step: 4800, Loss: 0.11944746971130371 Acc:1.0
step: 4900, Loss: 0.11585505306720734 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 0.9901960784313726
precision: 1.0
recall: 0.9772727272727273
F_score: 0.9885057471264368
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 2.146644115447998 Acc:0.9736842105263158
step: 100, Loss: 0.11953908205032349 Acc:1.0
step: 200, Loss: 0.11842895299196243 Acc:1.0
step: 300, Loss: 0.11665647476911545 Acc:1.0
step: 400, Loss: 0.11656663566827774 Acc:1.0
step: 500, Loss: 0.11872786283493042 Acc:1.0
step: 600, Loss: 0.12190854549407959 Acc:1.0
step: 700, Loss: 0.1166786178946495 Acc:1.0
step: 800, Loss: 0.11482402682304382 Acc:1.0
step: 900, Loss: 0.11872339993715286 Acc:1.0
step: 1000, Loss: 0.11776380985975266 Acc:1.0
step: 1100, Loss: 0.1168394461274147 Acc:1.0
step: 1200, Loss: 0.11825352162122726 Acc:1.0
step: 1300, Loss: 0.11885230243206024 Acc:1.0
step: 1400, Loss: 0.12063103169202805 Acc:1.0
step: 1500, Loss: 0.12219695746898651 Acc:1.0
step: 1600, Loss: 0.11610794067382812 Acc:1.0
step: 1700, Loss: 0.12211138010025024 Acc:1.0
step: 1800, Loss: 0.12070906162261963 Acc:1.0
step: 1900, Loss: 0.11618753522634506 Acc:1.0
step: 2000, Loss: 0.12423355132341385 Acc:1.0
step: 2100, Loss: 0.12236456573009491 Acc:1.0
step: 2200, Loss: 0.1862042099237442 Acc:1.0
step: 2300, Loss: 0.1164027750492096 Acc:1.0
step: 2400, Loss: 0.11857031285762787 Acc:1.0
step: 2500, Loss: 0.11419941484928131 Acc:1.0
step: 2600, Loss: 0.12766090035438538 Acc:1.0
step: 2700, Loss: 0.11835227906703949 Acc:1.0
step: 2800, Loss: 1.2019119262695312 Acc:0.9736842105263158
step: 2900, Loss: 0.115594282746315 Acc:1.0
step: 3000, Loss: 0.11768104135990143 Acc:1.0
step: 3100, Loss: 0.1127948984503746 Acc:1.0
step: 3200, Loss: 0.11460741609334946 Acc:1.0
step: 3300, Loss: 0.1154167503118515 Acc:1.0
step: 3400, Loss: 0.11833576112985611 Acc:1.0
step: 3500, Loss: 0.11702550202608109 Acc:1.0
step: 3600, Loss: 0.11761404573917389 Acc:1.0
step: 3700, Loss: 6.903771877288818 Acc:0.9473684210526315
step: 3800, Loss: 0.12032628804445267 Acc:1.0
step: 3900, Loss: 0.1223926693201065 Acc:1.0
step: 4000, Loss: 0.11672621220350266 Acc:1.0
step: 4100, Loss: 1.4620139598846436 Acc:0.9736842105263158
step: 4200, Loss: 0.120126873254776 Acc:1.0
step: 4300, Loss: 0.12027229368686676 Acc:1.0
step: 4400, Loss: 0.11711369454860687 Acc:1.0
step: 4500, Loss: 0.11693543195724487 Acc:1.0
step: 4600, Loss: 0.11940326541662216 Acc:1.0
step: 4700, Loss: 0.11758539080619812 Acc:1.0
step: 4800, Loss: 0.11544141173362732 Acc:1.0
step: 4900, Loss: 0.1204540878534317 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11783845722675323 Acc:1.0
step: 100, Loss: 0.11533698439598083 Acc:1.0
step: 200, Loss: 0.11568816751241684 Acc:1.0
step: 300, Loss: 0.11658743023872375 Acc:1.0
step: 400, Loss: 0.11652054637670517 Acc:1.0
step: 500, Loss: 0.11542271077632904 Acc:1.0
step: 600, Loss: 0.11671452969312668 Acc:1.0
step: 700, Loss: 0.12607307732105255 Acc:1.0
step: 800, Loss: 0.11977274715900421 Acc:1.0
step: 900, Loss: 0.11685329675674438 Acc:1.0
step: 1000, Loss: 0.11744160950183868 Acc:1.0
step: 1100, Loss: 0.1260964274406433 Acc:1.0
step: 1200, Loss: 0.11656919866800308 Acc:1.0
step: 1300, Loss: 0.12290909141302109 Acc:1.0
step: 1400, Loss: 0.11490035802125931 Acc:1.0
step: 1500, Loss: 0.11587658524513245 Acc:1.0
step: 1600, Loss: 0.11566393077373505 Acc:1.0
step: 1700, Loss: 0.11479195207357407 Acc:1.0
step: 1800, Loss: 0.11723579466342926 Acc:1.0
step: 1900, Loss: 0.11504481732845306 Acc:1.0
step: 2000, Loss: 0.11705943942070007 Acc:1.0
step: 2100, Loss: 0.11714772880077362 Acc:1.0
step: 2200, Loss: 0.11473237723112106 Acc:1.0
step: 2300, Loss: 0.116480752825737 Acc:1.0
step: 2400, Loss: 0.11644215881824493 Acc:1.0
step: 2500, Loss: 0.11750014871358871 Acc:1.0
step: 2600, Loss: 0.11743224412202835 Acc:1.0
step: 2700, Loss: 0.12274600565433502 Acc:1.0
step: 2800, Loss: 0.1185426339507103 Acc:1.0
step: 2900, Loss: 0.11918756365776062 Acc:1.0
step: 3000, Loss: 0.11929483711719513 Acc:1.0
step: 3100, Loss: 0.11759808659553528 Acc:1.0
step: 3200, Loss: 0.11268679797649384 Acc:1.0
step: 3300, Loss: 0.11453360319137573 Acc:1.0
step: 3400, Loss: 0.2823309302330017 Acc:1.0
step: 3500, Loss: 0.11589799076318741 Acc:1.0
step: 3600, Loss: 0.11693115532398224 Acc:1.0
step: 3700, Loss: 0.11848985403776169 Acc:1.0
step: 3800, Loss: 0.11866174638271332 Acc:1.0
step: 3900, Loss: 0.12454497814178467 Acc:1.0
step: 4000, Loss: 0.11784958094358444 Acc:1.0
step: 4100, Loss: 0.11917728930711746 Acc:1.0
step: 4200, Loss: 0.14728760719299316 Acc:1.0
step: 4300, Loss: 0.12116719037294388 Acc:1.0
step: 4400, Loss: 0.11723721027374268 Acc:1.0
step: 4500, Loss: 0.11651726067066193 Acc:1.0
step: 4600, Loss: 0.12123001366853714 Acc:1.0
step: 4700, Loss: 0.11728036403656006 Acc:1.0
step: 4800, Loss: 0.12144105136394501 Acc:1.0
step: 4900, Loss: 3.437056303024292 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11903156340122223 Acc:1.0
step: 100, Loss: 0.11597862839698792 Acc:1.0
step: 200, Loss: 0.6710086464881897 Acc:0.9736842105263158
step: 300, Loss: 0.12464718520641327 Acc:1.0
step: 400, Loss: 0.11890602856874466 Acc:1.0
step: 500, Loss: 0.11647795140743256 Acc:1.0
step: 600, Loss: 0.12010718137025833 Acc:1.0
step: 700, Loss: 0.11717478930950165 Acc:1.0
step: 800, Loss: 0.11855122447013855 Acc:1.0
step: 900, Loss: 0.9591740965843201 Acc:0.9736842105263158
step: 1000, Loss: 0.11531776189804077 Acc:1.0
step: 1100, Loss: 0.12339027971029282 Acc:1.0
step: 1200, Loss: 0.12574400007724762 Acc:1.0
step: 1300, Loss: 0.12530617415905 Acc:1.0
step: 1400, Loss: 3.4311227798461914 Acc:0.9736842105263158
step: 1500, Loss: 0.1170227974653244 Acc:1.0
step: 1600, Loss: 0.13511836528778076 Acc:1.0
step: 1700, Loss: 0.13780544698238373 Acc:1.0
step: 1800, Loss: 0.11794022470712662 Acc:1.0
step: 1900, Loss: 0.11920425295829773 Acc:1.0
step: 2000, Loss: 0.11933119595050812 Acc:1.0
step: 2100, Loss: 0.11619812250137329 Acc:1.0
step: 2200, Loss: 0.11933518201112747 Acc:1.0
step: 2300, Loss: 0.11843325197696686 Acc:1.0
step: 2400, Loss: 0.11656937748193741 Acc:1.0
step: 2500, Loss: 0.18009532988071442 Acc:1.0
step: 2600, Loss: 0.11957217752933502 Acc:1.0
step: 2700, Loss: 1.2649692296981812 Acc:0.9736842105263158
step: 2800, Loss: 0.12102851271629333 Acc:1.0
step: 2900, Loss: 0.11894610524177551 Acc:1.0
step: 3000, Loss: 0.12094350159168243 Acc:1.0
step: 3100, Loss: 0.13133886456489563 Acc:1.0
step: 3200, Loss: 0.11558680236339569 Acc:1.0
step: 3300, Loss: 0.39139649271965027 Acc:1.0
step: 3400, Loss: 0.12043352425098419 Acc:1.0
step: 3500, Loss: 0.11741447448730469 Acc:1.0
step: 3600, Loss: 0.11533764004707336 Acc:1.0
step: 3700, Loss: 0.1234903633594513 Acc:1.0
step: 3800, Loss: 0.1148758977651596 Acc:1.0
step: 3900, Loss: 0.12231137603521347 Acc:1.0
step: 4000, Loss: 0.11446225643157959 Acc:1.0
step: 4100, Loss: 0.11637058854103088 Acc:1.0
step: 4200, Loss: 0.11823421716690063 Acc:1.0
step: 4300, Loss: 0.11832788586616516 Acc:1.0
step: 4400, Loss: 0.11547978222370148 Acc:1.0
step: 4500, Loss: 0.12152525037527084 Acc:1.0
step: 4600, Loss: 0.11822917312383652 Acc:1.0
step: 4700, Loss: 0.12589131295681 Acc:1.0
step: 4800, Loss: 0.11878233402967453 Acc:1.0
step: 4900, Loss: 0.12095532566308975 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
subject 1 Avgacc: 0.9647058823529411 Avgfscore: 0.9617787633040183 
 Max acc:1.0, Max f score:1.0 Avg Recall:0.9548699963374524 Max Recall:1.0 Avg Precision:0.9696634758343056 Max Precision:1.0
******** mix subject_2 ********

[304, 456]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:820
step: 0, Loss: 77.221923828125 Acc:0.47368421052631576
step: 100, Loss: 15.752828598022461 Acc:0.5
step: 200, Loss: 12.335768699645996 Acc:0.6052631578947368
step: 300, Loss: 11.211856842041016 Acc:0.7105263157894737
step: 400, Loss: 8.435148239135742 Acc:0.8157894736842105
step: 500, Loss: 13.77601146697998 Acc:0.6052631578947368
step: 600, Loss: 9.838217735290527 Acc:0.7368421052631579
step: 700, Loss: 12.98379135131836 Acc:0.6578947368421053
step: 800, Loss: 10.317493438720703 Acc:0.7631578947368421
step: 900, Loss: 8.14737319946289 Acc:0.8157894736842105
step: 1000, Loss: 6.980767250061035 Acc:0.8421052631578947
step: 1100, Loss: 9.749241828918457 Acc:0.7894736842105263
step: 1200, Loss: 8.655487060546875 Acc:0.8157894736842105
step: 1300, Loss: 10.234442710876465 Acc:0.8157894736842105
step: 1400, Loss: 4.562725067138672 Acc:0.8947368421052632
step: 1500, Loss: 5.241045951843262 Acc:0.868421052631579
step: 1600, Loss: 5.152585506439209 Acc:0.868421052631579
step: 1700, Loss: 3.924544334411621 Acc:0.9473684210526315
step: 1800, Loss: 5.876345634460449 Acc:0.8947368421052632
step: 1900, Loss: 7.334835052490234 Acc:0.868421052631579
step: 2000, Loss: 4.475874423980713 Acc:0.9210526315789473
step: 2100, Loss: 5.7275614738464355 Acc:0.9210526315789473
step: 2200, Loss: 6.687478542327881 Acc:0.868421052631579
step: 2300, Loss: 7.367825508117676 Acc:0.8947368421052632
step: 2400, Loss: 4.611139297485352 Acc:0.8947368421052632
step: 2500, Loss: 5.739965438842773 Acc:0.9210526315789473
step: 2600, Loss: 5.202098369598389 Acc:0.8947368421052632
step: 2700, Loss: 3.0193936824798584 Acc:0.9473684210526315
step: 2800, Loss: 0.6513867974281311 Acc:1.0
step: 2900, Loss: 2.182360887527466 Acc:0.9736842105263158
step: 3000, Loss: 10.575459480285645 Acc:0.8421052631578947
step: 3100, Loss: 1.8432130813598633 Acc:0.9736842105263158
step: 3200, Loss: 3.5706701278686523 Acc:0.9210526315789473
step: 3300, Loss: 8.825881004333496 Acc:0.8947368421052632
step: 3400, Loss: 7.588934421539307 Acc:0.8947368421052632
step: 3500, Loss: 3.0807604789733887 Acc:0.9736842105263158
step: 3600, Loss: 3.902339458465576 Acc:0.9473684210526315
step: 3700, Loss: 4.641947269439697 Acc:0.8947368421052632
step: 3800, Loss: 2.6709437370300293 Acc:0.9473684210526315
step: 3900, Loss: 2.442694902420044 Acc:0.9736842105263158
step: 4000, Loss: 2.430833578109741 Acc:0.9736842105263158
step: 4100, Loss: 6.643165111541748 Acc:0.9210526315789473
step: 4200, Loss: 1.6995352506637573 Acc:0.9473684210526315
step: 4300, Loss: 2.509096622467041 Acc:0.9736842105263158
step: 4400, Loss: 4.190978527069092 Acc:0.9473684210526315
step: 4500, Loss: 6.315121650695801 Acc:0.9473684210526315
step: 4600, Loss: 0.26252448558807373 Acc:1.0
step: 4700, Loss: 3.3282358646392822 Acc:0.9736842105263158
step: 4800, Loss: 0.24375689029693604 Acc:1.0
step: 4900, Loss: 2.977674722671509 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:92
acc: 0.9666666666666667
precision: 0.9454545454545454
recall: 1.0
F_score: 0.9719626168224299
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:820
step: 0, Loss: 10.236384391784668 Acc:0.9210526315789473
step: 100, Loss: 6.08096170425415 Acc:0.9473684210526315
step: 200, Loss: 3.138270854949951 Acc:0.9736842105263158
step: 300, Loss: 3.2742340564727783 Acc:0.9736842105263158
step: 400, Loss: 0.13509300351142883 Acc:1.0
step: 500, Loss: 0.13868308067321777 Acc:1.0
step: 600, Loss: 0.1173352301120758 Acc:1.0
step: 700, Loss: 0.19757825136184692 Acc:1.0
step: 800, Loss: 7.554437160491943 Acc:0.9473684210526315
step: 900, Loss: 0.13210701942443848 Acc:1.0
step: 1000, Loss: 0.1209016665816307 Acc:1.0
step: 1100, Loss: 7.228244781494141 Acc:0.9473684210526315
step: 1200, Loss: 6.524608135223389 Acc:0.9473684210526315
step: 1300, Loss: 8.124847412109375 Acc:0.9210526315789473
step: 1400, Loss: 3.669916868209839 Acc:0.9736842105263158
step: 1500, Loss: 6.627724647521973 Acc:0.9210526315789473
step: 1600, Loss: 0.13976387679576874 Acc:1.0
step: 1700, Loss: 3.0163369178771973 Acc:0.9736842105263158
step: 1800, Loss: 0.12513524293899536 Acc:1.0
step: 1900, Loss: 3.4911720752716064 Acc:0.9736842105263158
step: 2000, Loss: 2.4136340618133545 Acc:0.9736842105263158
step: 2100, Loss: 0.13197138905525208 Acc:1.0
step: 2200, Loss: 4.514371395111084 Acc:0.9736842105263158
step: 2300, Loss: 7.444581985473633 Acc:0.9473684210526315
step: 2400, Loss: 1.775956630706787 Acc:0.9736842105263158
step: 2500, Loss: 2.8640053272247314 Acc:0.9736842105263158
step: 2600, Loss: 0.12806305289268494 Acc:1.0
step: 2700, Loss: 0.12271681427955627 Acc:1.0
step: 2800, Loss: 0.12045367062091827 Acc:1.0
step: 2900, Loss: 0.11948517709970474 Acc:1.0
step: 3000, Loss: 2.983494758605957 Acc:0.9736842105263158
step: 3100, Loss: 0.12037545442581177 Acc:1.0
step: 3200, Loss: 0.11414557695388794 Acc:1.0
step: 3300, Loss: 4.8233323097229 Acc:0.9473684210526315
step: 3400, Loss: 9.973363876342773 Acc:0.9210526315789473
step: 3500, Loss: 0.11657408624887466 Acc:1.0
step: 3600, Loss: 4.514809608459473 Acc:0.9473684210526315
step: 3700, Loss: 0.12423024326562881 Acc:1.0
step: 3800, Loss: 0.24580347537994385 Acc:1.0
step: 3900, Loss: 0.13096337020397186 Acc:1.0
step: 4000, Loss: 0.11524397879838943 Acc:1.0
step: 4100, Loss: 4.493184566497803 Acc:0.9736842105263158
step: 4200, Loss: 6.59970760345459 Acc:0.9473684210526315
step: 4300, Loss: 0.12793892621994019 Acc:1.0
step: 4400, Loss: 6.195832252502441 Acc:0.9473684210526315
step: 4500, Loss: 5.286731243133545 Acc:0.9473684210526315
step: 4600, Loss: 0.11761867254972458 Acc:1.0
step: 4700, Loss: 3.0934619903564453 Acc:0.9736842105263158
step: 4800, Loss: 1.7933616638183594 Acc:0.9736842105263158
step: 4900, Loss: 0.11276803910732269 Acc:1.0
training successfully ended.
validating...
validate data length:92
acc: 0.9444444444444444
precision: 0.9444444444444444
recall: 0.918918918918919
F_score: 0.9315068493150684
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 5.419020175933838 Acc:0.9473684210526315
step: 100, Loss: 8.19442367553711 Acc:0.9473684210526315
step: 200, Loss: 0.13568414747714996 Acc:1.0
step: 300, Loss: 3.788733959197998 Acc:0.9736842105263158
step: 400, Loss: 0.15401867032051086 Acc:1.0
step: 500, Loss: 0.11994463205337524 Acc:1.0
step: 600, Loss: 0.11598435789346695 Acc:1.0
step: 700, Loss: 5.878293991088867 Acc:0.9473684210526315
step: 800, Loss: 3.9466934204101562 Acc:0.9736842105263158
step: 900, Loss: 0.12958365678787231 Acc:1.0
step: 1000, Loss: 0.11981579661369324 Acc:1.0
step: 1100, Loss: 3.9729373455047607 Acc:0.9736842105263158
step: 1200, Loss: 10.842358589172363 Acc:0.9210526315789473
step: 1300, Loss: 0.162518709897995 Acc:1.0
step: 1400, Loss: 3.574864625930786 Acc:0.9736842105263158
step: 1500, Loss: 5.349045276641846 Acc:0.9473684210526315
step: 1600, Loss: 0.27859818935394287 Acc:1.0
step: 1700, Loss: 0.12502282857894897 Acc:1.0
step: 1800, Loss: 0.19664303958415985 Acc:1.0
step: 1900, Loss: 0.11472976207733154 Acc:1.0
step: 2000, Loss: 0.11861059814691544 Acc:1.0
step: 2100, Loss: 0.11661121249198914 Acc:1.0
step: 2200, Loss: 3.728588342666626 Acc:0.9736842105263158
step: 2300, Loss: 7.499380111694336 Acc:0.9473684210526315
step: 2400, Loss: 0.12056711316108704 Acc:1.0
step: 2500, Loss: 3.748352289199829 Acc:0.9736842105263158
step: 2600, Loss: 3.01759934425354 Acc:0.9736842105263158
step: 2700, Loss: 0.12281230837106705 Acc:1.0
step: 2800, Loss: 2.2588868141174316 Acc:0.9736842105263158
step: 2900, Loss: 0.2767643630504608 Acc:1.0
step: 3000, Loss: 0.1407998502254486 Acc:1.0
step: 3100, Loss: 3.9140982627868652 Acc:0.9736842105263158
step: 3200, Loss: 0.25593361258506775 Acc:1.0
step: 3300, Loss: 3.733534097671509 Acc:0.9736842105263158
step: 3400, Loss: 4.388874530792236 Acc:0.9473684210526315
step: 3500, Loss: 0.1174330934882164 Acc:1.0
step: 3600, Loss: 4.654995441436768 Acc:0.9736842105263158
step: 3700, Loss: 0.11730222404003143 Acc:1.0
step: 3800, Loss: 0.11905135214328766 Acc:1.0
step: 3900, Loss: 0.12390822917222977 Acc:1.0
step: 4000, Loss: 8.556614875793457 Acc:0.9210526315789473
step: 4100, Loss: 3.2044029235839844 Acc:0.9736842105263158
step: 4200, Loss: 4.290347576141357 Acc:0.9736842105263158
step: 4300, Loss: 0.12519700825214386 Acc:1.0
step: 4400, Loss: 12.589227676391602 Acc:0.9210526315789473
step: 4500, Loss: 0.9586612582206726 Acc:0.9736842105263158
step: 4600, Loss: 0.11900461465120316 Acc:1.0
step: 4700, Loss: 0.12036997079849243 Acc:1.0
step: 4800, Loss: 0.1260037124156952 Acc:1.0
step: 4900, Loss: 0.12201208621263504 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9555555555555556
precision: 1.0
recall: 0.9148936170212766
F_score: 0.9555555555555556
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 5.81270170211792 Acc:0.9473684210526315
step: 100, Loss: 2.8108980655670166 Acc:0.9736842105263158
step: 200, Loss: 2.303948163986206 Acc:0.9473684210526315
step: 300, Loss: 3.6946685314178467 Acc:0.9736842105263158
step: 400, Loss: 8.184259414672852 Acc:0.9210526315789473
step: 500, Loss: 0.3142767548561096 Acc:1.0
step: 600, Loss: 6.287858486175537 Acc:0.9473684210526315
step: 700, Loss: 0.12606897950172424 Acc:1.0
step: 800, Loss: 6.6957197189331055 Acc:0.9473684210526315
step: 900, Loss: 0.11747041344642639 Acc:1.0
step: 1000, Loss: 3.5474722385406494 Acc:0.9736842105263158
step: 1100, Loss: 14.411934852600098 Acc:0.868421052631579
step: 1200, Loss: 1.0136127471923828 Acc:0.9736842105263158
step: 1300, Loss: 3.0490217208862305 Acc:0.9736842105263158
step: 1400, Loss: 3.0082976818084717 Acc:0.9736842105263158
step: 1500, Loss: 0.1206291913986206 Acc:1.0
step: 1600, Loss: 0.12926311790943146 Acc:1.0
step: 1700, Loss: 0.11998812109231949 Acc:1.0
step: 1800, Loss: 0.11460481584072113 Acc:1.0
step: 1900, Loss: 4.33356237411499 Acc:0.9473684210526315
step: 2000, Loss: 0.11929967999458313 Acc:1.0
step: 2100, Loss: 4.419920921325684 Acc:0.9473684210526315
step: 2200, Loss: 7.883087158203125 Acc:0.9473684210526315
step: 2300, Loss: 9.082493782043457 Acc:0.9210526315789473
step: 2400, Loss: 0.12380558997392654 Acc:1.0
step: 2500, Loss: 3.6111230850219727 Acc:0.9736842105263158
step: 2600, Loss: 3.760105609893799 Acc:0.9736842105263158
step: 2700, Loss: 0.1193295270204544 Acc:1.0
step: 2800, Loss: 3.230970621109009 Acc:0.9736842105263158
step: 2900, Loss: 11.444334983825684 Acc:0.8947368421052632
step: 3000, Loss: 11.04396915435791 Acc:0.9210526315789473
step: 3100, Loss: 1.0238450765609741 Acc:0.9736842105263158
step: 3200, Loss: 0.6981738805770874 Acc:0.9736842105263158
step: 3300, Loss: 6.937741279602051 Acc:0.9473684210526315
step: 3400, Loss: 0.12640568614006042 Acc:1.0
step: 3500, Loss: 0.11712521314620972 Acc:1.0
step: 3600, Loss: 0.12318307161331177 Acc:1.0
step: 3700, Loss: 2.805048942565918 Acc:0.9473684210526315
step: 3800, Loss: 0.12628579139709473 Acc:1.0
step: 3900, Loss: 0.11835799366235733 Acc:1.0
step: 4000, Loss: 0.11932168900966644 Acc:1.0
step: 4100, Loss: 2.9059417247772217 Acc:0.9736842105263158
step: 4200, Loss: 0.7887493968009949 Acc:0.9736842105263158
step: 4300, Loss: 2.1314632892608643 Acc:0.9736842105263158
step: 4400, Loss: 24.309797286987305 Acc:0.7894736842105263
step: 4500, Loss: 2.7586963176727295 Acc:0.9473684210526315
step: 4600, Loss: 0.12888488173484802 Acc:1.0
step: 4700, Loss: 0.11464822292327881 Acc:1.0
step: 4800, Loss: 0.12961947917938232 Acc:1.0
step: 4900, Loss: 0.11733224242925644 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9444444444444444
precision: 1.0
recall: 0.9038461538461539
F_score: 0.9494949494949495
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 5.302182197570801 Acc:0.9210526315789473
step: 100, Loss: 7.387171268463135 Acc:0.9473684210526315
step: 200, Loss: 0.2077767252922058 Acc:1.0
step: 300, Loss: 3.3154659271240234 Acc:0.9736842105263158
step: 400, Loss: 0.12259313464164734 Acc:1.0
step: 500, Loss: 0.141331285238266 Acc:1.0
step: 600, Loss: 0.12466749548912048 Acc:1.0
step: 700, Loss: 4.240023612976074 Acc:0.9736842105263158
step: 800, Loss: 7.927799701690674 Acc:0.9210526315789473
step: 900, Loss: 0.14280965924263 Acc:1.0
step: 1000, Loss: 0.13693979382514954 Acc:1.0
step: 1100, Loss: 5.267847537994385 Acc:0.9473684210526315
step: 1200, Loss: 8.584848403930664 Acc:0.9210526315789473
step: 1300, Loss: 0.12535664439201355 Acc:1.0
step: 1400, Loss: 3.7861080169677734 Acc:0.9736842105263158
step: 1500, Loss: 0.1277761459350586 Acc:1.0
step: 1600, Loss: 0.12801724672317505 Acc:1.0
step: 1700, Loss: 0.11683036386966705 Acc:1.0
step: 1800, Loss: 0.8067674040794373 Acc:0.9736842105263158
step: 1900, Loss: 12.4658842086792 Acc:0.8947368421052632
step: 2000, Loss: 0.11694534868001938 Acc:1.0
step: 2100, Loss: 0.12142831832170486 Acc:1.0
step: 2200, Loss: 6.457854747772217 Acc:0.9473684210526315
step: 2300, Loss: 28.465471267700195 Acc:0.7894736842105263
step: 2400, Loss: 2.4306247234344482 Acc:0.9736842105263158
step: 2500, Loss: 3.308429479598999 Acc:0.9736842105263158
step: 2600, Loss: 4.440220355987549 Acc:0.9210526315789473
step: 2700, Loss: 0.1291588544845581 Acc:1.0
step: 2800, Loss: 2.0948846340179443 Acc:0.9736842105263158
step: 2900, Loss: 0.11601842194795609 Acc:1.0
step: 3000, Loss: 2.95729398727417 Acc:0.9736842105263158
step: 3100, Loss: 0.14481443166732788 Acc:1.0
step: 3200, Loss: 0.14545047283172607 Acc:1.0
step: 3300, Loss: 7.1574883460998535 Acc:0.9473684210526315
step: 3400, Loss: 2.628596067428589 Acc:0.9736842105263158
step: 3500, Loss: 0.1276322305202484 Acc:1.0
step: 3600, Loss: 0.1201629787683487 Acc:1.0
step: 3700, Loss: 0.25970402359962463 Acc:1.0
step: 3800, Loss: 0.11694598942995071 Acc:1.0
step: 3900, Loss: 0.11866417527198792 Acc:1.0
step: 4000, Loss: 2.0018198490142822 Acc:0.9736842105263158
step: 4100, Loss: 3.299865245819092 Acc:0.9736842105263158
step: 4200, Loss: 0.12519535422325134 Acc:1.0
step: 4300, Loss: 0.11585341393947601 Acc:1.0
step: 4400, Loss: 4.017050743103027 Acc:0.9736842105263158
step: 4500, Loss: 0.12116612493991852 Acc:1.0
step: 4600, Loss: 0.6782819032669067 Acc:0.9736842105263158
step: 4700, Loss: 0.11637146770954132 Acc:1.0
step: 4800, Loss: 0.25230926275253296 Acc:1.0
step: 4900, Loss: 0.2724687159061432 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.8888888888888888
precision: 1.0
recall: 0.7674418604651163
F_score: 0.868421052631579
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 3.8565070629119873 Acc:0.9736842105263158
step: 100, Loss: 4.836562633514404 Acc:0.9736842105263158
step: 200, Loss: 0.12931105494499207 Acc:1.0
step: 300, Loss: 4.712544918060303 Acc:0.9473684210526315
step: 400, Loss: 0.12411954998970032 Acc:1.0
step: 500, Loss: 7.0412278175354 Acc:0.9210526315789473
step: 600, Loss: 0.11794143170118332 Acc:1.0
step: 700, Loss: 0.12642821669578552 Acc:1.0
step: 800, Loss: 13.088501930236816 Acc:0.8947368421052632
step: 900, Loss: 3.6794321537017822 Acc:0.9210526315789473
step: 1000, Loss: 5.767861366271973 Acc:0.9473684210526315
step: 1100, Loss: 0.14260950684547424 Acc:1.0
step: 1200, Loss: 3.0813522338867188 Acc:0.9736842105263158
step: 1300, Loss: 8.800806045532227 Acc:0.9210526315789473
step: 1400, Loss: 3.021371364593506 Acc:0.9736842105263158
step: 1500, Loss: 2.079423189163208 Acc:0.9736842105263158
step: 1600, Loss: 0.11829066276550293 Acc:1.0
step: 1700, Loss: 0.12147863954305649 Acc:1.0
step: 1800, Loss: 7.095912456512451 Acc:0.9473684210526315
step: 1900, Loss: 3.49458909034729 Acc:0.9736842105263158
step: 2000, Loss: 0.11848679184913635 Acc:1.0
step: 2100, Loss: 1.611006498336792 Acc:0.9736842105263158
step: 2200, Loss: 0.11890313029289246 Acc:1.0
step: 2300, Loss: 8.84091854095459 Acc:0.9473684210526315
step: 2400, Loss: 0.1310281902551651 Acc:1.0
step: 2500, Loss: 4.465672492980957 Acc:0.9736842105263158
step: 2600, Loss: 2.9981906414031982 Acc:0.9736842105263158
step: 2700, Loss: 0.11810610443353653 Acc:1.0
step: 2800, Loss: 1.292582631111145 Acc:0.9736842105263158
step: 2900, Loss: 0.1175989881157875 Acc:1.0
step: 3000, Loss: 11.035944938659668 Acc:0.9210526315789473
step: 3100, Loss: 0.11547921597957611 Acc:1.0
step: 3200, Loss: 4.92703914642334 Acc:0.9736842105263158
step: 3300, Loss: 0.5103970170021057 Acc:0.9736842105263158
step: 3400, Loss: 0.11446043103933334 Acc:1.0
step: 3500, Loss: 2.379081964492798 Acc:0.9736842105263158
step: 3600, Loss: 1.0375027656555176 Acc:0.9736842105263158
step: 3700, Loss: 7.795968055725098 Acc:0.9473684210526315
step: 3800, Loss: 0.2332853078842163 Acc:1.0
step: 3900, Loss: 2.422217607498169 Acc:0.9736842105263158
step: 4000, Loss: 3.3101258277893066 Acc:0.9210526315789473
step: 4100, Loss: 4.802389144897461 Acc:0.9736842105263158
step: 4200, Loss: 0.11657623201608658 Acc:1.0
step: 4300, Loss: 3.6241118907928467 Acc:0.9736842105263158
step: 4400, Loss: 0.11804017424583435 Acc:1.0
step: 4500, Loss: 5.236832618713379 Acc:0.9473684210526315
step: 4600, Loss: 4.083269119262695 Acc:0.9736842105263158
step: 4700, Loss: 0.11808217316865921 Acc:1.0
step: 4800, Loss: 0.11715403199195862 Acc:1.0
step: 4900, Loss: 0.12778456509113312 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9444444444444444
precision: 0.9318181818181818
recall: 0.9534883720930233
F_score: 0.942528735632184
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 9.078344345092773 Acc:0.9210526315789473
step: 100, Loss: 3.474588632583618 Acc:0.9736842105263158
step: 200, Loss: 0.34180378913879395 Acc:1.0
step: 300, Loss: 3.7944791316986084 Acc:0.9736842105263158
step: 400, Loss: 0.128752201795578 Acc:1.0
step: 500, Loss: 1.0558985471725464 Acc:0.9736842105263158
step: 600, Loss: 0.12663012742996216 Acc:1.0
step: 700, Loss: 0.16386297345161438 Acc:1.0
step: 800, Loss: 7.58978796005249 Acc:0.9473684210526315
step: 900, Loss: 0.18069779872894287 Acc:1.0
step: 1000, Loss: 2.6459569931030273 Acc:0.9736842105263158
step: 1100, Loss: 9.800477981567383 Acc:0.9210526315789473
step: 1200, Loss: 1.8763377666473389 Acc:0.9473684210526315
step: 1300, Loss: 0.12199917435646057 Acc:1.0
step: 1400, Loss: 3.9637234210968018 Acc:0.9736842105263158
step: 1500, Loss: 10.249706268310547 Acc:0.9210526315789473
step: 1600, Loss: 0.1162644773721695 Acc:1.0
step: 1700, Loss: 0.12312687933444977 Acc:1.0
step: 1800, Loss: 3.30420184135437 Acc:0.9736842105263158
step: 1900, Loss: 4.3840227127075195 Acc:0.9736842105263158
step: 2000, Loss: 0.12179224193096161 Acc:1.0
step: 2100, Loss: 3.12117075920105 Acc:0.9736842105263158
step: 2200, Loss: 4.681773662567139 Acc:0.9736842105263158
step: 2300, Loss: 0.14627933502197266 Acc:1.0
step: 2400, Loss: 1.0172089338302612 Acc:0.9736842105263158
step: 2500, Loss: 2.641580104827881 Acc:0.9736842105263158
step: 2600, Loss: 17.491226196289062 Acc:0.868421052631579
step: 2700, Loss: 3.003251552581787 Acc:0.9736842105263158
step: 2800, Loss: 0.12459776550531387 Acc:1.0
step: 2900, Loss: 0.12702739238739014 Acc:1.0
step: 3000, Loss: 4.079094886779785 Acc:0.9736842105263158
step: 3100, Loss: 0.11936715245246887 Acc:1.0
step: 3200, Loss: 0.5229191780090332 Acc:0.9736842105263158
step: 3300, Loss: 6.9955668449401855 Acc:0.9473684210526315
step: 3400, Loss: 0.13449883460998535 Acc:1.0
step: 3500, Loss: 0.1179979145526886 Acc:1.0
step: 3600, Loss: 0.12292709201574326 Acc:1.0
step: 3700, Loss: 0.11644703149795532 Acc:1.0
step: 3800, Loss: 0.12842917442321777 Acc:1.0
step: 3900, Loss: 0.11918684840202332 Acc:1.0
step: 4000, Loss: 0.11558790504932404 Acc:1.0
step: 4100, Loss: 5.880833148956299 Acc:0.9473684210526315
step: 4200, Loss: 3.9003942012786865 Acc:0.9736842105263158
step: 4300, Loss: 0.1295318901538849 Acc:1.0
step: 4400, Loss: 4.753610134124756 Acc:0.9736842105263158
step: 4500, Loss: 3.989478588104248 Acc:0.9736842105263158
step: 4600, Loss: 0.11353817582130432 Acc:1.0
step: 4700, Loss: 0.1184350848197937 Acc:1.0
step: 4800, Loss: 0.11810699105262756 Acc:1.0
step: 4900, Loss: 0.1195419430732727 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.9444444444444444
recall: 0.8947368421052632
F_score: 0.918918918918919
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 5.012691020965576 Acc:0.9473684210526315
step: 100, Loss: 5.5677900314331055 Acc:0.9473684210526315
step: 200, Loss: 2.800443649291992 Acc:0.9736842105263158
step: 300, Loss: 0.13207197189331055 Acc:1.0
step: 400, Loss: 0.11810167133808136 Acc:1.0
step: 500, Loss: 6.669009208679199 Acc:0.9210526315789473
step: 600, Loss: 0.13125097751617432 Acc:1.0
step: 700, Loss: 0.14378556609153748 Acc:1.0
step: 800, Loss: 5.867120742797852 Acc:0.9473684210526315
step: 900, Loss: 0.11698305606842041 Acc:1.0
step: 1000, Loss: 4.763146877288818 Acc:0.9473684210526315
step: 1100, Loss: 2.8931190967559814 Acc:0.9736842105263158
step: 1200, Loss: 6.106688976287842 Acc:0.9473684210526315
step: 1300, Loss: 3.2703986167907715 Acc:0.9736842105263158
step: 1400, Loss: 0.11917629092931747 Acc:1.0
step: 1500, Loss: 0.16486907005310059 Acc:1.0
step: 1600, Loss: 0.15292105078697205 Acc:1.0
step: 1700, Loss: 3.912358283996582 Acc:0.9736842105263158
step: 1800, Loss: 0.11677879095077515 Acc:1.0
step: 1900, Loss: 8.734489440917969 Acc:0.9473684210526315
step: 2000, Loss: 3.5537521839141846 Acc:0.9736842105263158
step: 2100, Loss: 3.360941171646118 Acc:0.9736842105263158
step: 2200, Loss: 3.957866907119751 Acc:0.9736842105263158
step: 2300, Loss: 6.3385725021362305 Acc:0.9473684210526315
step: 2400, Loss: 0.12014777958393097 Acc:1.0
step: 2500, Loss: 0.12300039827823639 Acc:1.0
step: 2600, Loss: 0.12632538378238678 Acc:1.0
step: 2700, Loss: 0.11959080398082733 Acc:1.0
step: 2800, Loss: 0.13046619296073914 Acc:1.0
step: 2900, Loss: 0.4065713882446289 Acc:1.0
step: 3000, Loss: 9.623811721801758 Acc:0.9210526315789473
step: 3100, Loss: 0.1180584579706192 Acc:1.0
step: 3200, Loss: 0.11805294454097748 Acc:1.0
step: 3300, Loss: 5.031574726104736 Acc:0.9736842105263158
step: 3400, Loss: 4.052742958068848 Acc:0.9736842105263158
step: 3500, Loss: 0.11429022252559662 Acc:1.0
step: 3600, Loss: 0.11734358966350555 Acc:1.0
step: 3700, Loss: 0.20669955015182495 Acc:1.0
step: 3800, Loss: 0.12422296404838562 Acc:1.0
step: 3900, Loss: 0.11605268716812134 Acc:1.0
step: 4000, Loss: 0.1584937572479248 Acc:1.0
step: 4100, Loss: 13.839090347290039 Acc:0.8947368421052632
step: 4200, Loss: 0.11720012873411179 Acc:1.0
step: 4300, Loss: 1.8387641906738281 Acc:0.9736842105263158
step: 4400, Loss: 3.9723052978515625 Acc:0.9736842105263158
step: 4500, Loss: 13.151976585388184 Acc:0.868421052631579
step: 4600, Loss: 4.795496940612793 Acc:0.9473684210526315
step: 4700, Loss: 0.11708153039216995 Acc:1.0
step: 4800, Loss: 0.12320578098297119 Acc:1.0
step: 4900, Loss: 0.1189543753862381 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9444444444444444
precision: 0.9591836734693877
recall: 0.94
F_score: 0.9494949494949495
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 9.237401962280273 Acc:0.9210526315789473
step: 100, Loss: 10.65435791015625 Acc:0.9210526315789473
step: 200, Loss: 7.1896257400512695 Acc:0.9210526315789473
step: 300, Loss: 5.231236457824707 Acc:0.9736842105263158
step: 400, Loss: 1.5463404655456543 Acc:0.9736842105263158
step: 500, Loss: 0.33330515027046204 Acc:1.0
step: 600, Loss: 0.12470807135105133 Acc:1.0
step: 700, Loss: 0.1467733532190323 Acc:1.0
step: 800, Loss: 6.508035659790039 Acc:0.9473684210526315
step: 900, Loss: 0.12851735949516296 Acc:1.0
step: 1000, Loss: 3.8661863803863525 Acc:0.9736842105263158
step: 1100, Loss: 7.186443328857422 Acc:0.9210526315789473
step: 1200, Loss: 4.6530585289001465 Acc:0.9736842105263158
step: 1300, Loss: 0.41566625237464905 Acc:1.0
step: 1400, Loss: 3.5198452472686768 Acc:0.9736842105263158
step: 1500, Loss: 4.846114635467529 Acc:0.9473684210526315
step: 1600, Loss: 0.1260695606470108 Acc:1.0
step: 1700, Loss: 0.12109808623790741 Acc:1.0
step: 1800, Loss: 0.1241694763302803 Acc:1.0
step: 1900, Loss: 8.289152145385742 Acc:0.9473684210526315
step: 2000, Loss: 0.12334884703159332 Acc:1.0
step: 2100, Loss: 0.14036990702152252 Acc:1.0
step: 2200, Loss: 4.690075874328613 Acc:0.9736842105263158
step: 2300, Loss: 3.4584665298461914 Acc:0.9736842105263158
step: 2400, Loss: 1.5689432621002197 Acc:0.9736842105263158
step: 2500, Loss: 2.8775837421417236 Acc:0.9736842105263158
step: 2600, Loss: 0.1224307045340538 Acc:1.0
step: 2700, Loss: 1.845141887664795 Acc:0.9736842105263158
step: 2800, Loss: 0.12134401500225067 Acc:1.0
step: 2900, Loss: 0.12026537954807281 Acc:1.0
step: 3000, Loss: 6.810985565185547 Acc:0.9473684210526315
step: 3100, Loss: 0.1207737848162651 Acc:1.0
step: 3200, Loss: 0.13564527034759521 Acc:1.0
step: 3300, Loss: 5.53504753112793 Acc:0.9736842105263158
step: 3400, Loss: 8.08613395690918 Acc:0.9473684210526315
step: 3500, Loss: 0.13120701909065247 Acc:1.0
step: 3600, Loss: 0.12621288001537323 Acc:1.0
step: 3700, Loss: 0.13297083973884583 Acc:1.0
step: 3800, Loss: 0.11711975932121277 Acc:1.0
step: 3900, Loss: 0.11742548644542694 Acc:1.0
step: 4000, Loss: 0.11502137780189514 Acc:1.0
step: 4100, Loss: 3.1189725399017334 Acc:0.9736842105263158
step: 4200, Loss: 0.12074090540409088 Acc:1.0
step: 4300, Loss: 0.12599502503871918 Acc:1.0
step: 4400, Loss: 4.679049968719482 Acc:0.9736842105263158
step: 4500, Loss: 4.381911754608154 Acc:0.9473684210526315
step: 4600, Loss: 3.47369122505188 Acc:0.9736842105263158
step: 4700, Loss: 3.434814691543579 Acc:0.9736842105263158
step: 4800, Loss: 0.12805701792240143 Acc:1.0
step: 4900, Loss: 0.11582685261964798 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.9019607843137255
recall: 0.9787234042553191
F_score: 0.9387755102040817
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 11.015595436096191 Acc:0.8947368421052632
step: 100, Loss: 3.3450734615325928 Acc:0.9736842105263158
step: 200, Loss: 0.5511053204536438 Acc:0.9736842105263158
step: 300, Loss: 3.7984461784362793 Acc:0.9736842105263158
step: 400, Loss: 3.8120529651641846 Acc:0.9473684210526315
step: 500, Loss: 5.3706769943237305 Acc:0.9473684210526315
step: 600, Loss: 0.5844789743423462 Acc:0.9736842105263158
step: 700, Loss: 3.6090128421783447 Acc:0.9736842105263158
step: 800, Loss: 2.7226033210754395 Acc:0.9736842105263158
step: 900, Loss: 0.1270115077495575 Acc:1.0
step: 1000, Loss: 5.204196453094482 Acc:0.9473684210526315
step: 1100, Loss: 5.528278827667236 Acc:0.9473684210526315
step: 1200, Loss: 10.320114135742188 Acc:0.8947368421052632
step: 1300, Loss: 0.12459627538919449 Acc:1.0
step: 1400, Loss: 4.057527542114258 Acc:0.9736842105263158
step: 1500, Loss: 4.108431816101074 Acc:0.9736842105263158
step: 1600, Loss: 0.12619240581989288 Acc:1.0
step: 1700, Loss: 0.16433113813400269 Acc:1.0
step: 1800, Loss: 6.053297996520996 Acc:0.9473684210526315
step: 1900, Loss: 5.887085437774658 Acc:0.9473684210526315
step: 2000, Loss: 1.982391357421875 Acc:0.9736842105263158
step: 2100, Loss: 0.1148536205291748 Acc:1.0
step: 2200, Loss: 6.979267120361328 Acc:0.9473684210526315
step: 2300, Loss: 7.610383033752441 Acc:0.9473684210526315
step: 2400, Loss: 1.0312436819076538 Acc:0.9736842105263158
step: 2500, Loss: 0.1355573982000351 Acc:1.0
step: 2600, Loss: 0.1347329318523407 Acc:1.0
step: 2700, Loss: 0.11300075799226761 Acc:1.0
step: 2800, Loss: 0.11867658793926239 Acc:1.0
step: 2900, Loss: 0.11743797361850739 Acc:1.0
step: 3000, Loss: 2.734414577484131 Acc:0.9736842105263158
step: 3100, Loss: 0.11474527418613434 Acc:1.0
step: 3200, Loss: 0.12803083658218384 Acc:1.0
step: 3300, Loss: 9.170694351196289 Acc:0.9210526315789473
step: 3400, Loss: 3.667734384536743 Acc:0.9736842105263158
step: 3500, Loss: 0.12738549709320068 Acc:1.0
step: 3600, Loss: 0.26032567024230957 Acc:1.0
step: 3700, Loss: 0.12295382469892502 Acc:1.0
step: 3800, Loss: 0.11388503015041351 Acc:1.0
step: 3900, Loss: 0.12188437581062317 Acc:1.0
step: 4000, Loss: 0.11842077970504761 Acc:1.0
step: 4100, Loss: 3.042480945587158 Acc:0.9736842105263158
step: 4200, Loss: 0.14507515728473663 Acc:1.0
step: 4300, Loss: 0.11932076513767242 Acc:1.0
step: 4400, Loss: 4.773279666900635 Acc:0.9473684210526315
step: 4500, Loss: 3.923473834991455 Acc:0.9736842105263158
step: 4600, Loss: 0.12133339047431946 Acc:1.0
step: 4700, Loss: 0.5429941415786743 Acc:0.9736842105263158
step: 4800, Loss: 0.11632542312145233 Acc:1.0
step: 4900, Loss: 0.12222883105278015 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9555555555555556
precision: 0.9736842105263158
recall: 0.925
F_score: 0.9487179487179489
subject 2 Avgacc: 0.9411111111111111 Avgfscore: 0.9375377086787665 
 Max acc:0.9666666666666667, Max f score:0.9719626168224299 Avg Recall:0.9197049168705073 Max Recall:1.0 Avg Precision:0.9600990284471044 Max Precision:1.0
******** mix subject_3 ********

[456, 304]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:820
step: 0, Loss: 62.12041473388672 Acc:0.5
step: 100, Loss: 12.711896896362305 Acc:0.631578947368421
step: 200, Loss: 9.96518325805664 Acc:0.8157894736842105
step: 300, Loss: 11.867138862609863 Acc:0.7105263157894737
step: 400, Loss: 9.9000883102417 Acc:0.7105263157894737
step: 500, Loss: 7.880990982055664 Acc:0.8421052631578947
step: 600, Loss: 9.675766944885254 Acc:0.8157894736842105
step: 700, Loss: 9.273295402526855 Acc:0.7894736842105263
step: 800, Loss: 5.267292499542236 Acc:0.9210526315789473
step: 900, Loss: 9.066815376281738 Acc:0.7894736842105263
step: 1000, Loss: 8.537504196166992 Acc:0.8421052631578947
step: 1100, Loss: 8.970711708068848 Acc:0.7894736842105263
step: 1200, Loss: 3.8357386589050293 Acc:0.9473684210526315
step: 1300, Loss: 5.49603796005249 Acc:0.9210526315789473
step: 1400, Loss: 10.124818801879883 Acc:0.7894736842105263
step: 1500, Loss: 5.986971378326416 Acc:0.9210526315789473
step: 1600, Loss: 4.590587615966797 Acc:0.9210526315789473
step: 1700, Loss: 7.516608715057373 Acc:0.8157894736842105
step: 1800, Loss: 7.939661026000977 Acc:0.868421052631579
step: 1900, Loss: 5.304550647735596 Acc:0.9210526315789473
step: 2000, Loss: 7.391912937164307 Acc:0.8157894736842105
step: 2100, Loss: 7.935363292694092 Acc:0.868421052631579
step: 2200, Loss: 3.4974136352539062 Acc:0.9473684210526315
step: 2300, Loss: 3.898819923400879 Acc:0.9473684210526315
step: 2400, Loss: 5.115884780883789 Acc:0.9210526315789473
step: 2500, Loss: 7.439305305480957 Acc:0.8421052631578947
step: 2600, Loss: 4.3223419189453125 Acc:0.9473684210526315
step: 2700, Loss: 2.5639538764953613 Acc:0.9736842105263158
step: 2800, Loss: 12.31186294555664 Acc:0.7631578947368421
step: 2900, Loss: 7.0982537269592285 Acc:0.8947368421052632
step: 3000, Loss: 2.262112617492676 Acc:0.9473684210526315
step: 3100, Loss: 8.15040397644043 Acc:0.868421052631579
step: 3200, Loss: 6.585635662078857 Acc:0.9210526315789473
step: 3300, Loss: 4.04863977432251 Acc:0.9210526315789473
step: 3400, Loss: 4.825673580169678 Acc:0.9210526315789473
step: 3500, Loss: 3.625431776046753 Acc:0.9473684210526315
step: 3600, Loss: 6.7899088859558105 Acc:0.8157894736842105
step: 3700, Loss: 3.14430570602417 Acc:0.9736842105263158
step: 3800, Loss: 1.227272391319275 Acc:0.9736842105263158
step: 3900, Loss: 5.353027820587158 Acc:0.8947368421052632
step: 4000, Loss: 10.559126853942871 Acc:0.8421052631578947
step: 4100, Loss: 6.823610782623291 Acc:0.8947368421052632
step: 4200, Loss: 9.636771202087402 Acc:0.868421052631579
step: 4300, Loss: 6.252189636230469 Acc:0.8947368421052632
step: 4400, Loss: 5.367443561553955 Acc:0.868421052631579
step: 4500, Loss: 2.486213207244873 Acc:0.9736842105263158
step: 4600, Loss: 3.773757219314575 Acc:0.9473684210526315
step: 4700, Loss: 3.336588144302368 Acc:0.9473684210526315
step: 4800, Loss: 1.1921840906143188 Acc:1.0
step: 4900, Loss: 7.040245532989502 Acc:0.8947368421052632
training successfully ended.
validating...
validate data length:92
acc: 0.9
precision: 0.9393939393939394
recall: 0.8157894736842105
F_score: 0.8732394366197183
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:820
step: 0, Loss: 0.8192378282546997 Acc:1.0
step: 100, Loss: 4.160660743713379 Acc:0.9473684210526315
step: 200, Loss: 4.151529312133789 Acc:0.9473684210526315
step: 300, Loss: 2.3551132678985596 Acc:0.9736842105263158
step: 400, Loss: 0.9697862267494202 Acc:1.0
step: 500, Loss: 4.601279258728027 Acc:0.9473684210526315
step: 600, Loss: 9.001160621643066 Acc:0.868421052631579
step: 700, Loss: 3.9086625576019287 Acc:0.9473684210526315
step: 800, Loss: 10.61540699005127 Acc:0.868421052631579
step: 900, Loss: 8.383249282836914 Acc:0.868421052631579
step: 1000, Loss: 3.730278491973877 Acc:0.9473684210526315
step: 1100, Loss: 10.026484489440918 Acc:0.8421052631578947
step: 1200, Loss: 2.038935899734497 Acc:0.9736842105263158
step: 1300, Loss: 3.2252986431121826 Acc:0.9473684210526315
step: 1400, Loss: 5.096872329711914 Acc:0.9473684210526315
step: 1500, Loss: 2.268131971359253 Acc:0.9736842105263158
step: 1600, Loss: 2.6260530948638916 Acc:0.9736842105263158
step: 1700, Loss: 7.876623153686523 Acc:0.8947368421052632
step: 1800, Loss: 4.907015323638916 Acc:0.9210526315789473
step: 1900, Loss: 0.4807438850402832 Acc:1.0
step: 2000, Loss: 5.959680080413818 Acc:0.9210526315789473
step: 2100, Loss: 4.354918479919434 Acc:0.9210526315789473
step: 2200, Loss: 2.1260132789611816 Acc:0.9736842105263158
step: 2300, Loss: 5.474235534667969 Acc:0.9473684210526315
step: 2400, Loss: 3.8540220260620117 Acc:0.9473684210526315
step: 2500, Loss: 2.6578800678253174 Acc:0.9736842105263158
step: 2600, Loss: 2.2655203342437744 Acc:0.9736842105263158
step: 2700, Loss: 1.9912738800048828 Acc:0.9736842105263158
step: 2800, Loss: 5.728373050689697 Acc:0.9210526315789473
step: 2900, Loss: 5.467268943786621 Acc:0.9473684210526315
step: 3000, Loss: 0.5965876579284668 Acc:1.0
step: 3100, Loss: 8.986617088317871 Acc:0.8947368421052632
step: 3200, Loss: 5.375664710998535 Acc:0.9473684210526315
step: 3300, Loss: 0.26456406712532043 Acc:1.0
step: 3400, Loss: 4.074305057525635 Acc:0.9473684210526315
step: 3500, Loss: 3.0232431888580322 Acc:0.9736842105263158
step: 3600, Loss: 7.5804243087768555 Acc:0.9210526315789473
step: 3700, Loss: 0.21196958422660828 Acc:1.0
step: 3800, Loss: 0.14569243788719177 Acc:1.0
step: 3900, Loss: 10.772748947143555 Acc:0.8947368421052632
step: 4000, Loss: 10.271563529968262 Acc:0.8157894736842105
step: 4100, Loss: 1.4692485332489014 Acc:0.9736842105263158
step: 4200, Loss: 9.773195266723633 Acc:0.8947368421052632
step: 4300, Loss: 8.700922012329102 Acc:0.9210526315789473
step: 4400, Loss: 0.25357168912887573 Acc:1.0
step: 4500, Loss: 7.4048848152160645 Acc:0.9210526315789473
step: 4600, Loss: 2.672358989715576 Acc:0.9736842105263158
step: 4700, Loss: 0.17456769943237305 Acc:1.0
step: 4800, Loss: 1.4389041662216187 Acc:0.9736842105263158
step: 4900, Loss: 0.22513383626937866 Acc:1.0
training successfully ended.
validating...
validate data length:92
acc: 0.8777777777777778
precision: 1.0
recall: 0.7924528301886793
F_score: 0.8842105263157896
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.6151129007339478 Acc:1.0
step: 100, Loss: 2.269559144973755 Acc:0.9736842105263158
step: 200, Loss: 3.5779430866241455 Acc:0.9473684210526315
step: 300, Loss: 8.132437705993652 Acc:0.8947368421052632
step: 400, Loss: 3.8520684242248535 Acc:0.9736842105263158
step: 500, Loss: 4.352070331573486 Acc:0.9210526315789473
step: 600, Loss: 10.125051498413086 Acc:0.8157894736842105
step: 700, Loss: 0.7927631735801697 Acc:1.0
step: 800, Loss: 1.6757978200912476 Acc:0.9736842105263158
step: 900, Loss: 6.625328540802002 Acc:0.8947368421052632
step: 1000, Loss: 6.70109224319458 Acc:0.8947368421052632
step: 1100, Loss: 1.123302936553955 Acc:1.0
step: 1200, Loss: 0.7176538109779358 Acc:1.0
step: 1300, Loss: 4.502748966217041 Acc:0.9473684210526315
step: 1400, Loss: 5.23688268661499 Acc:0.8947368421052632
step: 1500, Loss: 12.852621078491211 Acc:0.8157894736842105
step: 1600, Loss: 4.859563827514648 Acc:0.9473684210526315
step: 1700, Loss: 3.80715274810791 Acc:0.9473684210526315
step: 1800, Loss: 3.6288247108459473 Acc:0.9473684210526315
step: 1900, Loss: 0.8734676241874695 Acc:1.0
step: 2000, Loss: 9.349867820739746 Acc:0.8947368421052632
step: 2100, Loss: 4.922972202301025 Acc:0.9210526315789473
step: 2200, Loss: 0.33487555384635925 Acc:1.0
step: 2300, Loss: 3.7757952213287354 Acc:0.9736842105263158
step: 2400, Loss: 3.9095377922058105 Acc:0.9473684210526315
step: 2500, Loss: 3.331455945968628 Acc:0.9473684210526315
step: 2600, Loss: 6.438317775726318 Acc:0.9210526315789473
step: 2700, Loss: 4.14360237121582 Acc:0.9473684210526315
step: 2800, Loss: 3.7048425674438477 Acc:0.9473684210526315
step: 2900, Loss: 0.8686401844024658 Acc:0.9736842105263158
step: 3000, Loss: 8.687396049499512 Acc:0.9210526315789473
step: 3100, Loss: 10.498580932617188 Acc:0.8947368421052632
step: 3200, Loss: 5.506394386291504 Acc:0.9473684210526315
step: 3300, Loss: 3.484917402267456 Acc:0.9473684210526315
step: 3400, Loss: 3.568692445755005 Acc:0.9736842105263158
step: 3500, Loss: 4.910733222961426 Acc:0.9473684210526315
step: 3600, Loss: 3.0476250648498535 Acc:0.9473684210526315
step: 3700, Loss: 2.9472196102142334 Acc:0.9473684210526315
step: 3800, Loss: 4.660177230834961 Acc:0.9210526315789473
step: 3900, Loss: 5.827500343322754 Acc:0.868421052631579
step: 4000, Loss: 4.341802597045898 Acc:0.9473684210526315
step: 4100, Loss: 0.15487559139728546 Acc:1.0
step: 4200, Loss: 11.36938190460205 Acc:0.868421052631579
step: 4300, Loss: 4.268706798553467 Acc:0.9210526315789473
step: 4400, Loss: 0.17803886532783508 Acc:1.0
step: 4500, Loss: 2.6549360752105713 Acc:0.9736842105263158
step: 4600, Loss: 5.478291034698486 Acc:0.9473684210526315
step: 4700, Loss: 0.9053120613098145 Acc:0.9736842105263158
step: 4800, Loss: 3.0886662006378174 Acc:0.9736842105263158
step: 4900, Loss: 4.537714958190918 Acc:0.9473684210526315
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.8936170212765957
recall: 0.9767441860465116
F_score: 0.9333333333333332
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.15604981780052185 Acc:1.0
step: 100, Loss: 2.6540613174438477 Acc:0.9736842105263158
step: 200, Loss: 2.4522061347961426 Acc:0.9736842105263158
step: 300, Loss: 2.570690155029297 Acc:0.9736842105263158
step: 400, Loss: 0.44697579741477966 Acc:1.0
step: 500, Loss: 2.471783399581909 Acc:0.9736842105263158
step: 600, Loss: 9.334942817687988 Acc:0.868421052631579
step: 700, Loss: 8.59488296508789 Acc:0.8421052631578947
step: 800, Loss: 0.444085955619812 Acc:1.0
step: 900, Loss: 4.570897579193115 Acc:0.9210526315789473
step: 1000, Loss: 2.184990644454956 Acc:0.9736842105263158
step: 1100, Loss: 0.14451299607753754 Acc:1.0
step: 1200, Loss: 3.0957982540130615 Acc:0.9736842105263158
step: 1300, Loss: 2.7790274620056152 Acc:0.9736842105263158
step: 1400, Loss: 0.2871500849723816 Acc:1.0
step: 1500, Loss: 0.1454273760318756 Acc:1.0
step: 1600, Loss: 0.13462889194488525 Acc:1.0
step: 1700, Loss: 10.165323257446289 Acc:0.9210526315789473
step: 1800, Loss: 16.07122039794922 Acc:0.8421052631578947
step: 1900, Loss: 0.3971947431564331 Acc:1.0
step: 2000, Loss: 6.062797546386719 Acc:0.9210526315789473
step: 2100, Loss: 8.28599739074707 Acc:0.9210526315789473
step: 2200, Loss: 3.008209228515625 Acc:0.9736842105263158
step: 2300, Loss: 3.8975071907043457 Acc:0.9473684210526315
step: 2400, Loss: 2.9363293647766113 Acc:0.9736842105263158
step: 2500, Loss: 6.140884876251221 Acc:0.9473684210526315
step: 2600, Loss: 0.1397186815738678 Acc:1.0
step: 2700, Loss: 0.14547011256217957 Acc:1.0
step: 2800, Loss: 2.498683452606201 Acc:0.9736842105263158
step: 2900, Loss: 8.676802635192871 Acc:0.9210526315789473
step: 3000, Loss: 0.18512146174907684 Acc:1.0
step: 3100, Loss: 17.826128005981445 Acc:0.8157894736842105
step: 3200, Loss: 8.989825248718262 Acc:0.8947368421052632
step: 3300, Loss: 0.1360032558441162 Acc:1.0
step: 3400, Loss: 5.756729602813721 Acc:0.9473684210526315
step: 3500, Loss: 4.156410217285156 Acc:0.9473684210526315
step: 3600, Loss: 0.3808334469795227 Acc:1.0
step: 3700, Loss: 0.13646864891052246 Acc:1.0
step: 3800, Loss: 1.4377832412719727 Acc:0.9736842105263158
step: 3900, Loss: 3.9436607360839844 Acc:0.9736842105263158
step: 4000, Loss: 3.509237051010132 Acc:0.9736842105263158
step: 4100, Loss: 0.7361177802085876 Acc:0.9736842105263158
step: 4200, Loss: 15.057242393493652 Acc:0.8421052631578947
step: 4300, Loss: 13.845693588256836 Acc:0.8421052631578947
step: 4400, Loss: 0.16414299607276917 Acc:1.0
step: 4500, Loss: 3.4181978702545166 Acc:0.9736842105263158
step: 4600, Loss: 2.8480422496795654 Acc:0.9736842105263158
step: 4700, Loss: 0.12798362970352173 Acc:1.0
step: 4800, Loss: 0.13701219856739044 Acc:1.0
step: 4900, Loss: 0.1394723355770111 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9777777777777777
precision: 0.9736842105263158
recall: 0.9736842105263158
F_score: 0.9736842105263158
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.12932556867599487 Acc:1.0
step: 100, Loss: 7.2081298828125 Acc:0.9473684210526315
step: 200, Loss: 3.469416618347168 Acc:0.9736842105263158
step: 300, Loss: 37.97744369506836 Acc:0.7105263157894737
step: 400, Loss: 0.1344948410987854 Acc:1.0
step: 500, Loss: 4.0145039558410645 Acc:0.9736842105263158
step: 600, Loss: 5.905527114868164 Acc:0.9473684210526315
step: 700, Loss: 6.557415962219238 Acc:0.9473684210526315
step: 800, Loss: 3.102849006652832 Acc:0.9736842105263158
step: 900, Loss: 2.900758981704712 Acc:0.9736842105263158
step: 1000, Loss: 0.1305321902036667 Acc:1.0
step: 1100, Loss: 7.284953594207764 Acc:0.8947368421052632
step: 1200, Loss: 3.9450430870056152 Acc:0.9736842105263158
step: 1300, Loss: 2.7293875217437744 Acc:0.9736842105263158
step: 1400, Loss: 0.1534913182258606 Acc:1.0
step: 1500, Loss: 6.560225963592529 Acc:0.9473684210526315
step: 1600, Loss: 3.1186821460723877 Acc:0.9736842105263158
step: 1700, Loss: 3.0248918533325195 Acc:0.9736842105263158
step: 1800, Loss: 3.9528274536132812 Acc:0.9736842105263158
step: 1900, Loss: 3.8894968032836914 Acc:0.9736842105263158
step: 2000, Loss: 0.12158655375242233 Acc:1.0
step: 2100, Loss: 0.13374929130077362 Acc:1.0
step: 2200, Loss: 0.1526365578174591 Acc:1.0
step: 2300, Loss: 6.099316596984863 Acc:0.9473684210526315
step: 2400, Loss: 3.6082966327667236 Acc:0.9473684210526315
step: 2500, Loss: 8.529444694519043 Acc:0.9210526315789473
step: 2600, Loss: 1.2515703439712524 Acc:0.9736842105263158
step: 2700, Loss: 5.803247451782227 Acc:0.9473684210526315
step: 2800, Loss: 3.000622034072876 Acc:0.9736842105263158
step: 2900, Loss: 0.5404903292655945 Acc:0.9736842105263158
step: 3000, Loss: 0.14172694087028503 Acc:1.0
step: 3100, Loss: 2.782477378845215 Acc:0.9473684210526315
step: 3200, Loss: 0.1190434992313385 Acc:1.0
step: 3300, Loss: 0.13419480621814728 Acc:1.0
step: 3400, Loss: 2.800459384918213 Acc:0.9736842105263158
step: 3500, Loss: 3.15159273147583 Acc:0.9736842105263158
step: 3600, Loss: 0.12563049793243408 Acc:1.0
step: 3700, Loss: 6.1698174476623535 Acc:0.9473684210526315
step: 3800, Loss: 5.8243279457092285 Acc:0.9473684210526315
step: 3900, Loss: 3.143848180770874 Acc:0.9736842105263158
step: 4000, Loss: 0.13722971081733704 Acc:1.0
step: 4100, Loss: 0.15221527218818665 Acc:1.0
step: 4200, Loss: 1.1397637128829956 Acc:0.9736842105263158
step: 4300, Loss: 0.1442595273256302 Acc:1.0
step: 4400, Loss: 0.12477491050958633 Acc:1.0
step: 4500, Loss: 3.989628314971924 Acc:0.9736842105263158
step: 4600, Loss: 2.6236841678619385 Acc:0.9736842105263158
step: 4700, Loss: 8.641895294189453 Acc:0.9473684210526315
step: 4800, Loss: 0.12298007309436798 Acc:1.0
step: 4900, Loss: 3.514453649520874 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:91
acc: 0.9777777777777777
precision: 0.9591836734693877
recall: 1.0
F_score: 0.9791666666666666
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.1265251636505127 Acc:1.0
step: 100, Loss: 3.3168797492980957 Acc:0.9736842105263158
step: 200, Loss: 3.2367749214172363 Acc:0.9736842105263158
step: 300, Loss: 0.1344653069972992 Acc:1.0
step: 400, Loss: 4.309897422790527 Acc:0.9736842105263158
step: 500, Loss: 10.130521774291992 Acc:0.9210526315789473
step: 600, Loss: 3.1004421710968018 Acc:0.9736842105263158
step: 700, Loss: 1.2080029249191284 Acc:0.9473684210526315
step: 800, Loss: 0.12422628700733185 Acc:1.0
step: 900, Loss: 8.10757064819336 Acc:0.8947368421052632
step: 1000, Loss: 3.0960311889648438 Acc:0.9736842105263158
step: 1100, Loss: 0.130290687084198 Acc:1.0
step: 1200, Loss: 3.566169500350952 Acc:0.9473684210526315
step: 1300, Loss: 3.333615779876709 Acc:0.9736842105263158
step: 1400, Loss: 8.716550827026367 Acc:0.9210526315789473
step: 1500, Loss: 0.13078437745571136 Acc:1.0
step: 1600, Loss: 0.12102483212947845 Acc:1.0
step: 1700, Loss: 3.9818193912506104 Acc:0.9736842105263158
step: 1800, Loss: 3.403965473175049 Acc:0.9473684210526315
step: 1900, Loss: 1.3936586380004883 Acc:0.9736842105263158
step: 2000, Loss: 16.124231338500977 Acc:0.868421052631579
step: 2100, Loss: 0.13854247331619263 Acc:1.0
step: 2200, Loss: 0.12823674082756042 Acc:1.0
step: 2300, Loss: 0.11784038692712784 Acc:1.0
step: 2400, Loss: 2.865168571472168 Acc:0.9736842105263158
step: 2500, Loss: 0.16213944554328918 Acc:1.0
step: 2600, Loss: 3.3139894008636475 Acc:0.9736842105263158
step: 2700, Loss: 6.286465644836426 Acc:0.9473684210526315
step: 2800, Loss: 5.717938423156738 Acc:0.9473684210526315
step: 2900, Loss: 0.13425707817077637 Acc:1.0
step: 3000, Loss: 0.12070027738809586 Acc:1.0
step: 3100, Loss: 4.368258476257324 Acc:0.9473684210526315
step: 3200, Loss: 0.12745454907417297 Acc:1.0
step: 3300, Loss: 3.4826102256774902 Acc:0.9736842105263158
step: 3400, Loss: 0.12269867956638336 Acc:1.0
step: 3500, Loss: 4.122051239013672 Acc:0.9736842105263158
step: 3600, Loss: 0.12343678623437881 Acc:1.0
step: 3700, Loss: 0.13416974246501923 Acc:1.0
step: 3800, Loss: 3.849536418914795 Acc:0.9473684210526315
step: 3900, Loss: 3.012524366378784 Acc:0.9736842105263158
step: 4000, Loss: 0.12196589261293411 Acc:1.0
step: 4100, Loss: 0.11905371397733688 Acc:1.0
step: 4200, Loss: 6.827366352081299 Acc:0.9473684210526315
step: 4300, Loss: 0.13258059322834015 Acc:1.0
step: 4400, Loss: 0.12340529263019562 Acc:1.0
step: 4500, Loss: 0.12998166680335999 Acc:1.0
step: 4600, Loss: 3.666457176208496 Acc:0.9736842105263158
step: 4700, Loss: 0.12755613029003143 Acc:1.0
step: 4800, Loss: 3.2555365562438965 Acc:0.9736842105263158
step: 4900, Loss: 0.1288684904575348 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.9361702127659575
recall: 0.9361702127659575
F_score: 0.9361702127659575
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.1347944140434265 Acc:1.0
step: 100, Loss: 5.78123664855957 Acc:0.9473684210526315
step: 200, Loss: 3.16489839553833 Acc:0.9736842105263158
step: 300, Loss: 5.8690595626831055 Acc:0.9210526315789473
step: 400, Loss: 0.12682867050170898 Acc:1.0
step: 500, Loss: 6.112371444702148 Acc:0.9473684210526315
step: 600, Loss: 5.050093650817871 Acc:0.9473684210526315
step: 700, Loss: 2.1516318321228027 Acc:0.9736842105263158
step: 800, Loss: 0.1268177181482315 Acc:1.0
step: 900, Loss: 9.22735595703125 Acc:0.9210526315789473
step: 1000, Loss: 0.14500361680984497 Acc:1.0
step: 1100, Loss: 0.12365420162677765 Acc:1.0
step: 1200, Loss: 2.885331630706787 Acc:0.9736842105263158
step: 1300, Loss: 5.365270614624023 Acc:0.9473684210526315
step: 1400, Loss: 0.13091406226158142 Acc:1.0
step: 1500, Loss: 0.19286897778511047 Acc:1.0
step: 1600, Loss: 3.5996205806732178 Acc:0.9736842105263158
step: 1700, Loss: 3.154975652694702 Acc:0.9736842105263158
step: 1800, Loss: 0.13330522179603577 Acc:1.0
step: 1900, Loss: 0.13291844725608826 Acc:1.0
step: 2000, Loss: 6.867266654968262 Acc:0.9210526315789473
step: 2100, Loss: 1.6099482774734497 Acc:0.9736842105263158
step: 2200, Loss: 9.26042366027832 Acc:0.9210526315789473
step: 2300, Loss: 4.107961654663086 Acc:0.9473684210526315
step: 2400, Loss: 3.402674674987793 Acc:0.9736842105263158
step: 2500, Loss: 0.3048383891582489 Acc:1.0
step: 2600, Loss: 0.13454239070415497 Acc:1.0
step: 2700, Loss: 0.1314474642276764 Acc:1.0
step: 2800, Loss: 3.206434726715088 Acc:0.9736842105263158
step: 2900, Loss: 6.408463001251221 Acc:0.9473684210526315
step: 3000, Loss: 0.12386178225278854 Acc:1.0
step: 3100, Loss: 24.404882431030273 Acc:0.7894736842105263
step: 3200, Loss: 0.13315413892269135 Acc:1.0
step: 3300, Loss: 0.13232991099357605 Acc:1.0
step: 3400, Loss: 3.567279100418091 Acc:0.9736842105263158
step: 3500, Loss: 6.060243606567383 Acc:0.9473684210526315
step: 3600, Loss: 2.9617557525634766 Acc:0.9736842105263158
step: 3700, Loss: 0.12047336250543594 Acc:1.0
step: 3800, Loss: 0.1338212788105011 Acc:1.0
step: 3900, Loss: 2.6669771671295166 Acc:0.9736842105263158
step: 4000, Loss: 0.13380563259124756 Acc:1.0
step: 4100, Loss: 0.13133487105369568 Acc:1.0
step: 4200, Loss: 0.1304132342338562 Acc:1.0
step: 4300, Loss: 4.245291709899902 Acc:0.9736842105263158
step: 4400, Loss: 2.540316581726074 Acc:0.9736842105263158
step: 4500, Loss: 3.671051263809204 Acc:0.9736842105263158
step: 4600, Loss: 3.2562732696533203 Acc:0.9736842105263158
step: 4700, Loss: 0.11936170607805252 Acc:1.0
step: 4800, Loss: 0.12222963571548462 Acc:1.0
step: 4900, Loss: 0.12219304591417313 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9444444444444444
precision: 0.9607843137254902
recall: 0.9423076923076923
F_score: 0.9514563106796117
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.12944136559963226 Acc:1.0
step: 100, Loss: 5.981101036071777 Acc:0.9210526315789473
step: 200, Loss: 0.12238618731498718 Acc:1.0
step: 300, Loss: 1.9409282207489014 Acc:0.9736842105263158
step: 400, Loss: 14.854348182678223 Acc:0.8421052631578947
step: 500, Loss: 6.450932502746582 Acc:0.9473684210526315
step: 600, Loss: 6.04050350189209 Acc:0.9473684210526315
step: 700, Loss: 0.13009585440158844 Acc:1.0
step: 800, Loss: 0.13670486211776733 Acc:1.0
step: 900, Loss: 13.817127227783203 Acc:0.868421052631579
step: 1000, Loss: 3.3690733909606934 Acc:0.9736842105263158
step: 1100, Loss: 0.12168878316879272 Acc:1.0
step: 1200, Loss: 3.050786256790161 Acc:0.9736842105263158
step: 1300, Loss: 0.14562499523162842 Acc:1.0
step: 1400, Loss: 14.246740341186523 Acc:0.868421052631579
step: 1500, Loss: 0.1204356849193573 Acc:1.0
step: 1600, Loss: 2.6238176822662354 Acc:0.9736842105263158
step: 1700, Loss: 19.67288589477539 Acc:0.8421052631578947
step: 1800, Loss: 8.561846733093262 Acc:0.9210526315789473
step: 1900, Loss: 0.12233161926269531 Acc:1.0
step: 2000, Loss: 3.7659411430358887 Acc:0.9736842105263158
step: 2100, Loss: 4.487616539001465 Acc:0.9473684210526315
step: 2200, Loss: 0.12300942838191986 Acc:1.0
step: 2300, Loss: 3.892791509628296 Acc:0.9473684210526315
step: 2400, Loss: 0.11968054622411728 Acc:1.0
step: 2500, Loss: 14.44145393371582 Acc:0.8947368421052632
step: 2600, Loss: 0.1328180879354477 Acc:1.0
step: 2700, Loss: 2.8865063190460205 Acc:0.9736842105263158
step: 2800, Loss: 3.7859277725219727 Acc:0.9736842105263158
step: 2900, Loss: 2.4302170276641846 Acc:0.9736842105263158
step: 3000, Loss: 0.12347909063100815 Acc:1.0
step: 3100, Loss: 6.220285892486572 Acc:0.9473684210526315
step: 3200, Loss: 0.13866031169891357 Acc:1.0
step: 3300, Loss: 0.1257668137550354 Acc:1.0
step: 3400, Loss: 8.465991020202637 Acc:0.9210526315789473
step: 3500, Loss: 0.12063203752040863 Acc:1.0
step: 3600, Loss: 12.24811840057373 Acc:0.9210526315789473
step: 3700, Loss: 0.14007429778575897 Acc:1.0
step: 3800, Loss: 3.8997621536254883 Acc:0.9736842105263158
step: 3900, Loss: 3.6755545139312744 Acc:0.9736842105263158
step: 4000, Loss: 0.11962728947401047 Acc:1.0
step: 4100, Loss: 3.699511766433716 Acc:0.9736842105263158
step: 4200, Loss: 36.34151077270508 Acc:0.7105263157894737
step: 4300, Loss: 3.191344738006592 Acc:0.9736842105263158
step: 4400, Loss: 0.1329766809940338 Acc:1.0
step: 4500, Loss: 3.9901227951049805 Acc:0.9736842105263158
step: 4600, Loss: 0.11962515115737915 Acc:1.0
step: 4700, Loss: 0.1326359659433365 Acc:1.0
step: 4800, Loss: 0.12522408366203308 Acc:1.0
step: 4900, Loss: 3.340343952178955 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.9473684210526315
recall: 0.9
F_score: 0.9230769230769231
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.1313626766204834 Acc:1.0
step: 100, Loss: 3.675845146179199 Acc:0.9736842105263158
step: 200, Loss: 26.713613510131836 Acc:0.7105263157894737
step: 300, Loss: 9.606313705444336 Acc:0.9210526315789473
step: 400, Loss: 0.12248104810714722 Acc:1.0
step: 500, Loss: 2.4009084701538086 Acc:0.9736842105263158
step: 600, Loss: 7.3683295249938965 Acc:0.9473684210526315
step: 700, Loss: 0.28995558619499207 Acc:1.0
step: 800, Loss: 0.12241542339324951 Acc:1.0
step: 900, Loss: 9.50653076171875 Acc:0.8947368421052632
step: 1000, Loss: 0.30971044301986694 Acc:1.0
step: 1100, Loss: 0.13506299257278442 Acc:1.0
step: 1200, Loss: 3.884157419204712 Acc:0.9736842105263158
step: 1300, Loss: 7.464297771453857 Acc:0.9473684210526315
step: 1400, Loss: 4.086723804473877 Acc:0.9736842105263158
step: 1500, Loss: 0.13194000720977783 Acc:1.0
step: 1600, Loss: 1.1463841199874878 Acc:0.9736842105263158
step: 1700, Loss: 8.987889289855957 Acc:0.8947368421052632
step: 1800, Loss: 0.5550138354301453 Acc:0.9736842105263158
step: 1900, Loss: 0.12600351870059967 Acc:1.0
step: 2000, Loss: 9.641100883483887 Acc:0.9210526315789473
step: 2100, Loss: 5.593967437744141 Acc:0.9473684210526315
step: 2200, Loss: 0.1259363740682602 Acc:1.0
step: 2300, Loss: 3.063011407852173 Acc:0.9736842105263158
step: 2400, Loss: 3.2146966457366943 Acc:0.9736842105263158
step: 2500, Loss: 0.13120165467262268 Acc:1.0
step: 2600, Loss: 0.13523343205451965 Acc:1.0
step: 2700, Loss: 6.691842555999756 Acc:0.9473684210526315
step: 2800, Loss: 0.12872295081615448 Acc:1.0
step: 2900, Loss: 0.16460251808166504 Acc:1.0
step: 3000, Loss: 0.13195551931858063 Acc:1.0
step: 3100, Loss: 11.468971252441406 Acc:0.8947368421052632
step: 3200, Loss: 3.715310573577881 Acc:0.9736842105263158
step: 3300, Loss: 0.13120755553245544 Acc:1.0
step: 3400, Loss: 4.358283042907715 Acc:0.9736842105263158
step: 3500, Loss: 3.3213467597961426 Acc:0.9736842105263158
step: 3600, Loss: 7.061920642852783 Acc:0.9210526315789473
step: 3700, Loss: 0.1196678876876831 Acc:1.0
step: 3800, Loss: 0.12034051865339279 Acc:1.0
step: 3900, Loss: 4.8163161277771 Acc:0.9473684210526315
step: 4000, Loss: 6.288114070892334 Acc:0.9473684210526315
step: 4100, Loss: 0.1305340975522995 Acc:1.0
step: 4200, Loss: 16.780536651611328 Acc:0.868421052631579
step: 4300, Loss: 1.3489669561386108 Acc:0.9736842105263158
step: 4400, Loss: 0.12515264749526978 Acc:1.0
step: 4500, Loss: 3.4174861907958984 Acc:0.9736842105263158
step: 4600, Loss: 2.9281678199768066 Acc:0.9736842105263158
step: 4700, Loss: 0.11662743985652924 Acc:1.0
step: 4800, Loss: 0.12231165915727615 Acc:1.0
step: 4900, Loss: 0.11669740080833435 Acc:1.0
training successfully ended.
validating...
validate data length:91
acc: 0.9555555555555556
precision: 0.9534883720930233
recall: 0.9534883720930233
F_score: 0.9534883720930233
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:821
step: 0, Loss: 0.17639362812042236 Acc:1.0
step: 100, Loss: 2.9038515090942383 Acc:0.9736842105263158
step: 200, Loss: 2.6628427505493164 Acc:0.9736842105263158
step: 300, Loss: 2.6697866916656494 Acc:0.9736842105263158
step: 400, Loss: 0.14658424258232117 Acc:1.0
step: 500, Loss: 5.967627048492432 Acc:0.9473684210526315
step: 600, Loss: 3.1072657108306885 Acc:0.9736842105263158
step: 700, Loss: 0.15215519070625305 Acc:1.0
step: 800, Loss: 0.14287063479423523 Acc:1.0
step: 900, Loss: 5.974602222442627 Acc:0.9210526315789473
step: 1000, Loss: 8.675901412963867 Acc:0.9210526315789473
step: 1100, Loss: 0.1763979196548462 Acc:1.0
step: 1200, Loss: 3.063843250274658 Acc:0.9736842105263158
step: 1300, Loss: 3.5967936515808105 Acc:0.9736842105263158
step: 1400, Loss: 0.13174733519554138 Acc:1.0
step: 1500, Loss: 0.16080251336097717 Acc:1.0
step: 1600, Loss: 6.334100246429443 Acc:0.9473684210526315
step: 1700, Loss: 6.704108715057373 Acc:0.9473684210526315
step: 1800, Loss: 4.2946696281433105 Acc:0.9736842105263158
step: 1900, Loss: 0.13207422196865082 Acc:1.0
step: 2000, Loss: 6.703261375427246 Acc:0.9210526315789473
step: 2100, Loss: 0.12997931241989136 Acc:1.0
step: 2200, Loss: 0.1251746118068695 Acc:1.0
step: 2300, Loss: 4.569509029388428 Acc:0.9473684210526315
step: 2400, Loss: 3.553809642791748 Acc:0.9736842105263158
step: 2500, Loss: 0.14212463796138763 Acc:1.0
step: 2600, Loss: 9.529662132263184 Acc:0.9210526315789473
step: 2700, Loss: 3.3742761611938477 Acc:0.9736842105263158
step: 2800, Loss: 3.136284112930298 Acc:0.9736842105263158
step: 2900, Loss: 0.3498941659927368 Acc:1.0
step: 3000, Loss: 0.15366284549236298 Acc:1.0
step: 3100, Loss: 0.6865939497947693 Acc:0.9736842105263158
step: 3200, Loss: 15.496417999267578 Acc:0.868421052631579
step: 3300, Loss: 0.13624544441699982 Acc:1.0
step: 3400, Loss: 3.0783443450927734 Acc:0.9736842105263158
step: 3500, Loss: 2.9709160327911377 Acc:0.9736842105263158
step: 3600, Loss: 5.065647602081299 Acc:0.9736842105263158
step: 3700, Loss: 11.874493598937988 Acc:0.868421052631579
step: 3800, Loss: 4.243072509765625 Acc:0.9736842105263158
step: 3900, Loss: 3.4377400875091553 Acc:0.9736842105263158
step: 4000, Loss: 8.963631629943848 Acc:0.9473684210526315
step: 4100, Loss: 0.11935265362262726 Acc:1.0
step: 4200, Loss: 0.13956350088119507 Acc:1.0
step: 4300, Loss: 6.2002339363098145 Acc:0.9210526315789473
step: 4400, Loss: 0.13637147843837738 Acc:1.0
step: 4500, Loss: 3.012671709060669 Acc:0.9736842105263158
step: 4600, Loss: 7.893331050872803 Acc:0.9473684210526315
step: 4700, Loss: 0.12132279574871063 Acc:1.0
step: 4800, Loss: 0.12154307961463928 Acc:1.0
step: 4900, Loss: 2.9474968910217285 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:91
acc: 0.9333333333333333
precision: 0.9782608695652174
recall: 0.9
F_score: 0.9375
subject 3 Avgacc: 0.9366666666666668 Avgfscore: 0.934532599207734 
 Max acc:0.9777777777777777, Max f score:0.9791666666666666 Avg Recall:0.9190636977612391 Max Recall:1.0 Avg Precision:0.9541951033868559 Max Precision:1.0
******** mix subject_4 ********

[247, 513]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 59.79719543457031 Acc:0.5
step: 100, Loss: 16.433347702026367 Acc:0.6052631578947368
step: 200, Loss: 11.884547233581543 Acc:0.6842105263157895
step: 300, Loss: 9.252446174621582 Acc:0.7368421052631579
step: 400, Loss: 9.333144187927246 Acc:0.7368421052631579
step: 500, Loss: 9.471297264099121 Acc:0.7368421052631579
step: 600, Loss: 9.277832984924316 Acc:0.7631578947368421
step: 700, Loss: 11.48558521270752 Acc:0.6842105263157895
step: 800, Loss: 7.687541961669922 Acc:0.8421052631578947
step: 900, Loss: 6.308075428009033 Acc:0.868421052631579
step: 1000, Loss: 9.600215911865234 Acc:0.7894736842105263
step: 1100, Loss: 6.711502552032471 Acc:0.868421052631579
step: 1200, Loss: 8.927278518676758 Acc:0.8157894736842105
step: 1300, Loss: 8.544507026672363 Acc:0.8157894736842105
step: 1400, Loss: 7.429320812225342 Acc:0.868421052631579
step: 1500, Loss: 6.0357561111450195 Acc:0.8947368421052632
step: 1600, Loss: 9.292659759521484 Acc:0.8157894736842105
step: 1700, Loss: 6.244204998016357 Acc:0.868421052631579
step: 1800, Loss: 7.750516414642334 Acc:0.8421052631578947
step: 1900, Loss: 3.966081142425537 Acc:0.9473684210526315
step: 2000, Loss: 8.238152503967285 Acc:0.8421052631578947
step: 2100, Loss: 5.605616092681885 Acc:0.9210526315789473
step: 2200, Loss: 6.413016319274902 Acc:0.9210526315789473
step: 2300, Loss: 6.345399856567383 Acc:0.9210526315789473
step: 2400, Loss: 7.79224157333374 Acc:0.8947368421052632
step: 2500, Loss: 3.943868637084961 Acc:0.9210526315789473
step: 2600, Loss: 4.656952857971191 Acc:0.9210526315789473
step: 2700, Loss: 6.261662483215332 Acc:0.9210526315789473
step: 2800, Loss: 9.112834930419922 Acc:0.8947368421052632
step: 2900, Loss: 2.611823558807373 Acc:0.9736842105263158
step: 3000, Loss: 2.58722186088562 Acc:0.9736842105263158
step: 3100, Loss: 3.2724015712738037 Acc:0.9736842105263158
step: 3200, Loss: 3.1431689262390137 Acc:0.9736842105263158
step: 3300, Loss: 2.455151081085205 Acc:0.9736842105263158
step: 3400, Loss: 4.165594577789307 Acc:0.9736842105263158
step: 3500, Loss: 2.840646743774414 Acc:0.9736842105263158
step: 3600, Loss: 2.8031580448150635 Acc:0.9736842105263158
step: 3700, Loss: 3.7840752601623535 Acc:0.9736842105263158
step: 3800, Loss: 6.0654802322387695 Acc:0.9473684210526315
step: 3900, Loss: 3.308704137802124 Acc:0.9736842105263158
step: 4000, Loss: 2.642223596572876 Acc:0.9736842105263158
step: 4100, Loss: 3.588883399963379 Acc:0.9736842105263158
step: 4200, Loss: 3.4203999042510986 Acc:0.9736842105263158
step: 4300, Loss: 7.601119041442871 Acc:0.9473684210526315
step: 4400, Loss: 3.091176748275757 Acc:0.9736842105263158
step: 4500, Loss: 3.113360643386841 Acc:0.9736842105263158
step: 4600, Loss: 0.12804141640663147 Acc:1.0
step: 4700, Loss: 0.5360881686210632 Acc:1.0
step: 4800, Loss: 0.11950216442346573 Acc:1.0
step: 4900, Loss: 0.11930130422115326 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.8921568627450981
precision: 0.8367346938775511
recall: 0.9318181818181818
F_score: 0.8817204301075268
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.11724784225225449 Acc:1.0
step: 100, Loss: 10.57273006439209 Acc:0.8947368421052632
step: 200, Loss: 0.1320435106754303 Acc:1.0
step: 300, Loss: 0.14450088143348694 Acc:1.0
step: 400, Loss: 4.3368635177612305 Acc:0.9736842105263158
step: 500, Loss: 0.136169895529747 Acc:1.0
step: 600, Loss: 0.13320468366146088 Acc:1.0
step: 700, Loss: 0.12347107380628586 Acc:1.0
step: 800, Loss: 0.4321524500846863 Acc:1.0
step: 900, Loss: 0.12814617156982422 Acc:1.0
step: 1000, Loss: 0.11846284568309784 Acc:1.0
step: 1100, Loss: 0.12193427979946136 Acc:1.0
step: 1200, Loss: 0.1283617615699768 Acc:1.0
step: 1300, Loss: 0.12919606268405914 Acc:1.0
step: 1400, Loss: 0.120786651968956 Acc:1.0
step: 1500, Loss: 1.3713839054107666 Acc:0.9736842105263158
step: 1600, Loss: 4.619266510009766 Acc:0.9736842105263158
step: 1700, Loss: 0.11817917227745056 Acc:1.0
step: 1800, Loss: 9.079235076904297 Acc:0.9210526315789473
step: 1900, Loss: 5.85450553894043 Acc:0.9473684210526315
step: 2000, Loss: 4.311652183532715 Acc:0.9736842105263158
step: 2100, Loss: 0.11506811529397964 Acc:1.0
step: 2200, Loss: 0.13454239070415497 Acc:1.0
step: 2300, Loss: 0.13385705649852753 Acc:1.0
step: 2400, Loss: 0.12359537184238434 Acc:1.0
step: 2500, Loss: 0.11501911282539368 Acc:1.0
step: 2600, Loss: 0.1179664134979248 Acc:1.0
step: 2700, Loss: 1.80379319190979 Acc:0.9736842105263158
step: 2800, Loss: 0.12764409184455872 Acc:1.0
step: 2900, Loss: 0.1332121640443802 Acc:1.0
step: 3000, Loss: 3.3415660858154297 Acc:0.9473684210526315
step: 3100, Loss: 0.29187268018722534 Acc:1.0
step: 3200, Loss: 0.11730422079563141 Acc:1.0
step: 3300, Loss: 0.1557747721672058 Acc:1.0
step: 3400, Loss: 0.12047059834003448 Acc:1.0
step: 3500, Loss: 0.26079291105270386 Acc:1.0
step: 3600, Loss: 0.12208867073059082 Acc:1.0
step: 3700, Loss: 0.11599992960691452 Acc:1.0
step: 3800, Loss: 0.12397987395524979 Acc:1.0
step: 3900, Loss: 0.11575080454349518 Acc:1.0
step: 4000, Loss: 0.12231358885765076 Acc:1.0
step: 4100, Loss: 2.9023494720458984 Acc:0.9736842105263158
step: 4200, Loss: 0.12319034337997437 Acc:1.0
step: 4300, Loss: 0.116084024310112 Acc:1.0
step: 4400, Loss: 0.3524375855922699 Acc:1.0
step: 4500, Loss: 7.798646450042725 Acc:0.9473684210526315
step: 4600, Loss: 5.466113090515137 Acc:0.9473684210526315
step: 4700, Loss: 0.11879992485046387 Acc:1.0
step: 4800, Loss: 0.12023670226335526 Acc:1.0
step: 4900, Loss: 0.11597330123186111 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9705882352941176
precision: 0.9423076923076923
recall: 1.0
F_score: 0.9702970297029703
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 7.851944923400879 Acc:0.9210526315789473
step: 100, Loss: 0.11874011904001236 Acc:1.0
step: 200, Loss: 0.12410271167755127 Acc:1.0
step: 300, Loss: 0.12985925376415253 Acc:1.0
step: 400, Loss: 0.12361058592796326 Acc:1.0
step: 500, Loss: 0.12537819147109985 Acc:1.0
step: 600, Loss: 4.157581329345703 Acc:0.9736842105263158
step: 700, Loss: 0.11873167008161545 Acc:1.0
step: 800, Loss: 0.1237170621752739 Acc:1.0
step: 900, Loss: 0.1195671558380127 Acc:1.0
step: 1000, Loss: 15.179858207702637 Acc:0.8947368421052632
step: 1100, Loss: 0.11536256223917007 Acc:1.0
step: 1200, Loss: 0.13640104234218597 Acc:1.0
step: 1300, Loss: 0.12050746381282806 Acc:1.0
step: 1400, Loss: 1.2568849325180054 Acc:0.9736842105263158
step: 1500, Loss: 0.11590129137039185 Acc:1.0
step: 1600, Loss: 0.1179097592830658 Acc:1.0
step: 1700, Loss: 0.11952510476112366 Acc:1.0
step: 1800, Loss: 0.11940588057041168 Acc:1.0
step: 1900, Loss: 3.5057339668273926 Acc:0.9736842105263158
step: 2000, Loss: 0.11676475405693054 Acc:1.0
step: 2100, Loss: 0.11897213757038116 Acc:1.0
step: 2200, Loss: 0.12261760234832764 Acc:1.0
step: 2300, Loss: 0.11816787719726562 Acc:1.0
step: 2400, Loss: 0.12277883291244507 Acc:1.0
step: 2500, Loss: 0.12107700109481812 Acc:1.0
step: 2600, Loss: 0.11800143122673035 Acc:1.0
step: 2700, Loss: 0.11618441343307495 Acc:1.0
step: 2800, Loss: 0.11353282630443573 Acc:1.0
step: 2900, Loss: 0.1194114089012146 Acc:1.0
step: 3000, Loss: 0.387791246175766 Acc:1.0
step: 3100, Loss: 0.35517629981040955 Acc:1.0
step: 3200, Loss: 0.1267419159412384 Acc:1.0
step: 3300, Loss: 0.12502536177635193 Acc:1.0
step: 3400, Loss: 2.6048054695129395 Acc:0.9736842105263158
step: 3500, Loss: 0.11925781518220901 Acc:1.0
step: 3600, Loss: 0.12515822052955627 Acc:1.0
step: 3700, Loss: 0.12070279568433762 Acc:1.0
step: 3800, Loss: 0.11689259111881256 Acc:1.0
step: 3900, Loss: 0.11982056498527527 Acc:1.0
step: 4000, Loss: 0.12012332677841187 Acc:1.0
step: 4100, Loss: 0.11769488453865051 Acc:1.0
step: 4200, Loss: 0.12004248797893524 Acc:1.0
step: 4300, Loss: 0.11997891962528229 Acc:1.0
step: 4400, Loss: 3.4547650814056396 Acc:0.9736842105263158
step: 4500, Loss: 21.96502685546875 Acc:0.8421052631578947
step: 4600, Loss: 0.11603619158267975 Acc:1.0
step: 4700, Loss: 0.11569517105817795 Acc:1.0
step: 4800, Loss: 0.11580029875040054 Acc:1.0
step: 4900, Loss: 0.11823257058858871 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.12048953771591187 Acc:1.0
step: 100, Loss: 0.12224389612674713 Acc:1.0
step: 200, Loss: 0.11733199656009674 Acc:1.0
step: 300, Loss: 0.11850276589393616 Acc:1.0
step: 400, Loss: 1.0897319316864014 Acc:0.9736842105263158
step: 500, Loss: 0.29261285066604614 Acc:1.0
step: 600, Loss: 0.11531941592693329 Acc:1.0
step: 700, Loss: 0.11992056667804718 Acc:1.0
step: 800, Loss: 0.12156976759433746 Acc:1.0
step: 900, Loss: 0.11984755843877792 Acc:1.0
step: 1000, Loss: 0.11742938309907913 Acc:1.0
step: 1100, Loss: 0.12463979423046112 Acc:1.0
step: 1200, Loss: 0.11883514374494553 Acc:1.0
step: 1300, Loss: 0.11790551245212555 Acc:1.0
step: 1400, Loss: 0.11635873466730118 Acc:1.0
step: 1500, Loss: 0.1333422064781189 Acc:1.0
step: 1600, Loss: 0.11608794331550598 Acc:1.0
step: 1700, Loss: 0.1222897544503212 Acc:1.0
step: 1800, Loss: 0.12062335014343262 Acc:1.0
step: 1900, Loss: 0.11986851692199707 Acc:1.0
step: 2000, Loss: 4.300070762634277 Acc:0.9473684210526315
step: 2100, Loss: 2.0342869758605957 Acc:0.9736842105263158
step: 2200, Loss: 0.11568693816661835 Acc:1.0
step: 2300, Loss: 2.876328945159912 Acc:0.9736842105263158
step: 2400, Loss: 0.12015452235937119 Acc:1.0
step: 2500, Loss: 0.11812732368707657 Acc:1.0
step: 2600, Loss: 0.11804209649562836 Acc:1.0
step: 2700, Loss: 0.11763733625411987 Acc:1.0
step: 2800, Loss: 0.11740098893642426 Acc:1.0
step: 2900, Loss: 0.11903740465641022 Acc:1.0
step: 3000, Loss: 0.11818468570709229 Acc:1.0
step: 3100, Loss: 0.1170894205570221 Acc:1.0
step: 3200, Loss: 0.11662736535072327 Acc:1.0
step: 3300, Loss: 0.12078583240509033 Acc:1.0
step: 3400, Loss: 0.11863875389099121 Acc:1.0
step: 3500, Loss: 0.11745674163103104 Acc:1.0
step: 3600, Loss: 0.11900047957897186 Acc:1.0
step: 3700, Loss: 0.12063707411289215 Acc:1.0
step: 3800, Loss: 0.27780312299728394 Acc:1.0
step: 3900, Loss: 2.586153507232666 Acc:0.9736842105263158
step: 4000, Loss: 0.11923131346702576 Acc:1.0
step: 4100, Loss: 0.12336654961109161 Acc:1.0
step: 4200, Loss: 0.11706604808568954 Acc:1.0
step: 4300, Loss: 0.12134033441543579 Acc:1.0
step: 4400, Loss: 0.11817139387130737 Acc:1.0
step: 4500, Loss: 3.4276068210601807 Acc:0.9736842105263158
step: 4600, Loss: 0.12416089326143265 Acc:1.0
step: 4700, Loss: 0.11682016402482986 Acc:1.0
step: 4800, Loss: 3.567060708999634 Acc:0.9736842105263158
step: 4900, Loss: 0.11719860881567001 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.11817575991153717 Acc:1.0
step: 100, Loss: 0.16861101984977722 Acc:1.0
step: 200, Loss: 0.578064501285553 Acc:0.9736842105263158
step: 300, Loss: 0.11779749393463135 Acc:1.0
step: 400, Loss: 0.12253572791814804 Acc:1.0
step: 500, Loss: 0.12008592486381531 Acc:1.0
step: 600, Loss: 0.3993895947933197 Acc:1.0
step: 700, Loss: 5.853542327880859 Acc:0.9473684210526315
step: 800, Loss: 0.12120682746171951 Acc:1.0
step: 900, Loss: 0.11797519773244858 Acc:1.0
step: 1000, Loss: 0.12144569307565689 Acc:1.0
step: 1100, Loss: 3.6137583255767822 Acc:0.9736842105263158
step: 1200, Loss: 0.12791688740253448 Acc:1.0
step: 1300, Loss: 0.11714847385883331 Acc:1.0
step: 1400, Loss: 0.12206050008535385 Acc:1.0
step: 1500, Loss: 0.12381747364997864 Acc:1.0
step: 1600, Loss: 4.062437057495117 Acc:0.9736842105263158
step: 1700, Loss: 0.11732268333435059 Acc:1.0
step: 1800, Loss: 0.11912377178668976 Acc:1.0
step: 1900, Loss: 4.624401092529297 Acc:0.9473684210526315
step: 2000, Loss: 0.12679260969161987 Acc:1.0
step: 2100, Loss: 0.11909352242946625 Acc:1.0
step: 2200, Loss: 0.11483040452003479 Acc:1.0
step: 2300, Loss: 0.11536017805337906 Acc:1.0
step: 2400, Loss: 0.11669514328241348 Acc:1.0
step: 2500, Loss: 2.8917815685272217 Acc:0.9736842105263158
step: 2600, Loss: 0.11844141781330109 Acc:1.0
step: 2700, Loss: 0.1206110417842865 Acc:1.0
step: 2800, Loss: 0.11788506805896759 Acc:1.0
step: 2900, Loss: 0.11786114424467087 Acc:1.0
step: 3000, Loss: 0.11754628270864487 Acc:1.0
step: 3100, Loss: 0.11596091091632843 Acc:1.0
step: 3200, Loss: 0.1207488626241684 Acc:1.0
step: 3300, Loss: 0.11860966682434082 Acc:1.0
step: 3400, Loss: 0.1161128506064415 Acc:1.0
step: 3500, Loss: 3.8174259662628174 Acc:0.9736842105263158
step: 3600, Loss: 0.11749151349067688 Acc:1.0
step: 3700, Loss: 0.11916639655828476 Acc:1.0
step: 3800, Loss: 0.2202955186367035 Acc:1.0
step: 3900, Loss: 0.13070856034755707 Acc:1.0
step: 4000, Loss: 0.11820517480373383 Acc:1.0
step: 4100, Loss: 0.12062077224254608 Acc:1.0
step: 4200, Loss: 1.6364452838897705 Acc:0.9736842105263158
step: 4300, Loss: 12.782461166381836 Acc:0.9210526315789473
step: 4400, Loss: 0.11419261991977692 Acc:1.0
step: 4500, Loss: 0.11768132448196411 Acc:1.0
step: 4600, Loss: 0.12083062529563904 Acc:1.0
step: 4700, Loss: 0.1174703910946846 Acc:1.0
step: 4800, Loss: 0.1205480545759201 Acc:1.0
step: 4900, Loss: 0.12110372632741928 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 0.9705882352941176
precision: 1.0
recall: 0.9433962264150944
F_score: 0.970873786407767
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:923
step: 0, Loss: 0.11757007986307144 Acc:1.0
step: 100, Loss: 3.3843963146209717 Acc:0.9736842105263158
step: 200, Loss: 0.11673089116811752 Acc:1.0
step: 300, Loss: 0.11380723118782043 Acc:1.0
step: 400, Loss: 0.11995968222618103 Acc:1.0
step: 500, Loss: 0.11652309447526932 Acc:1.0
step: 600, Loss: 0.12084629386663437 Acc:1.0
step: 700, Loss: 0.12128987908363342 Acc:1.0
step: 800, Loss: 0.17466622591018677 Acc:1.0
step: 900, Loss: 0.11344002932310104 Acc:1.0
step: 1000, Loss: 0.11715258657932281 Acc:1.0
step: 1100, Loss: 3.4525372982025146 Acc:0.9736842105263158
step: 1200, Loss: 0.11475540697574615 Acc:1.0
step: 1300, Loss: 0.11527866125106812 Acc:1.0
step: 1400, Loss: 0.1133427545428276 Acc:1.0
step: 1500, Loss: 0.11877097189426422 Acc:1.0
step: 1600, Loss: 0.11924498528242111 Acc:1.0
step: 1700, Loss: 0.11970213800668716 Acc:1.0
step: 1800, Loss: 0.11668337881565094 Acc:1.0
step: 1900, Loss: 0.11585941165685654 Acc:1.0
step: 2000, Loss: 3.76656174659729 Acc:0.9736842105263158
step: 2100, Loss: 0.1179707795381546 Acc:1.0
step: 2200, Loss: 0.11984185129404068 Acc:1.0
step: 2300, Loss: 0.12177550792694092 Acc:1.0
step: 2400, Loss: 0.11723678559064865 Acc:1.0
step: 2500, Loss: 0.11921625584363937 Acc:1.0
step: 2600, Loss: 0.11473877727985382 Acc:1.0
step: 2700, Loss: 0.11645855009555817 Acc:1.0
step: 2800, Loss: 0.12125921249389648 Acc:1.0
step: 2900, Loss: 0.11586669832468033 Acc:1.0
step: 3000, Loss: 0.11598436534404755 Acc:1.0
step: 3100, Loss: 0.11658887565135956 Acc:1.0
step: 3200, Loss: 0.11794207990169525 Acc:1.0
step: 3300, Loss: 7.5902180671691895 Acc:0.9473684210526315
step: 3400, Loss: 0.1172780692577362 Acc:1.0
step: 3500, Loss: 0.1184733510017395 Acc:1.0
step: 3600, Loss: 4.309621334075928 Acc:0.9736842105263158
step: 3700, Loss: 0.11766520142555237 Acc:1.0
step: 3800, Loss: 4.207828998565674 Acc:0.9736842105263158
step: 3900, Loss: 4.88992977142334 Acc:0.9473684210526315
step: 4000, Loss: 0.11452128738164902 Acc:1.0
step: 4100, Loss: 9.487933158874512 Acc:0.9210526315789473
step: 4200, Loss: 0.1210889220237732 Acc:1.0
step: 4300, Loss: 0.12346874177455902 Acc:1.0
step: 4400, Loss: 10.471000671386719 Acc:0.9210526315789473
step: 4500, Loss: 3.634891986846924 Acc:0.9736842105263158
step: 4600, Loss: 0.12016118317842484 Acc:1.0
step: 4700, Loss: 0.11732262372970581 Acc:1.0
step: 4800, Loss: 3.6830344200134277 Acc:0.9473684210526315
step: 4900, Loss: 0.11967572569847107 Acc:1.0
training successfully ended.
validating...
validate data length:103
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.1208096444606781 Acc:1.0
step: 100, Loss: 0.11745966970920563 Acc:1.0
step: 200, Loss: 0.1245281770825386 Acc:1.0
step: 300, Loss: 0.1221933662891388 Acc:1.0
step: 400, Loss: 0.11891740560531616 Acc:1.0
step: 500, Loss: 0.1222381442785263 Acc:1.0
step: 600, Loss: 0.1169913113117218 Acc:1.0
step: 700, Loss: 0.12128346413373947 Acc:1.0
step: 800, Loss: 0.11887545883655548 Acc:1.0
step: 900, Loss: 0.11730114370584488 Acc:1.0
step: 1000, Loss: 0.11707846820354462 Acc:1.0
step: 1100, Loss: 0.1162484809756279 Acc:1.0
step: 1200, Loss: 0.12113511562347412 Acc:1.0
step: 1300, Loss: 0.11514763534069061 Acc:1.0
step: 1400, Loss: 4.3089165687561035 Acc:0.9736842105263158
step: 1500, Loss: 0.1214437335729599 Acc:1.0
step: 1600, Loss: 0.11754445731639862 Acc:1.0
step: 1700, Loss: 0.1160186380147934 Acc:1.0
step: 1800, Loss: 0.11898285150527954 Acc:1.0
step: 1900, Loss: 0.11490451544523239 Acc:1.0
step: 2000, Loss: 0.11925357580184937 Acc:1.0
step: 2100, Loss: 4.255072593688965 Acc:0.9736842105263158
step: 2200, Loss: 0.121033675968647 Acc:1.0
step: 2300, Loss: 0.11732791364192963 Acc:1.0
step: 2400, Loss: 3.459231376647949 Acc:0.9736842105263158
step: 2500, Loss: 6.786081790924072 Acc:0.9473684210526315
step: 2600, Loss: 0.11941801011562347 Acc:1.0
step: 2700, Loss: 0.12073080241680145 Acc:1.0
step: 2800, Loss: 0.11543741077184677 Acc:1.0
step: 2900, Loss: 0.11883919686079025 Acc:1.0
step: 3000, Loss: 2.896327018737793 Acc:0.9736842105263158
step: 3100, Loss: 4.251086711883545 Acc:0.9736842105263158
step: 3200, Loss: 0.1180851086974144 Acc:1.0
step: 3300, Loss: 0.115925133228302 Acc:1.0
step: 3400, Loss: 0.12475286424160004 Acc:1.0
step: 3500, Loss: 0.12468144297599792 Acc:1.0
step: 3600, Loss: 0.11861686408519745 Acc:1.0
step: 3700, Loss: 0.11785966157913208 Acc:1.0
step: 3800, Loss: 0.11658231168985367 Acc:1.0
step: 3900, Loss: 0.11990652978420258 Acc:1.0
step: 4000, Loss: 0.11777067184448242 Acc:1.0
step: 4100, Loss: 0.11760228127241135 Acc:1.0
step: 4200, Loss: 0.11685550212860107 Acc:1.0
step: 4300, Loss: 0.12106546014547348 Acc:1.0
step: 4400, Loss: 0.11854199320077896 Acc:1.0
step: 4500, Loss: 6.937391757965088 Acc:0.9473684210526315
step: 4600, Loss: 2.886329174041748 Acc:0.9736842105263158
step: 4700, Loss: 0.1193000003695488 Acc:1.0
step: 4800, Loss: 0.11419472098350525 Acc:1.0
step: 4900, Loss: 0.11631707847118378 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 0.9901960784313726
precision: 1.0
recall: 0.9772727272727273
F_score: 0.9885057471264368
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11714358627796173 Acc:1.0
step: 100, Loss: 6.973317623138428 Acc:0.9210526315789473
step: 200, Loss: 0.12191921472549438 Acc:1.0
step: 300, Loss: 5.236912250518799 Acc:0.9473684210526315
step: 400, Loss: 0.1240205317735672 Acc:1.0
step: 500, Loss: 0.12033937871456146 Acc:1.0
step: 600, Loss: 0.1170816496014595 Acc:1.0
step: 700, Loss: 1.9983141422271729 Acc:0.9736842105263158
step: 800, Loss: 0.1179421991109848 Acc:1.0
step: 900, Loss: 3.583550214767456 Acc:0.9736842105263158
step: 1000, Loss: 0.22182390093803406 Acc:1.0
step: 1100, Loss: 0.1253063678741455 Acc:1.0
step: 1200, Loss: 0.12376338243484497 Acc:1.0
step: 1300, Loss: 0.12005140632390976 Acc:1.0
step: 1400, Loss: 0.11576925218105316 Acc:1.0
step: 1500, Loss: 0.11928194761276245 Acc:1.0
step: 1600, Loss: 0.11670920252799988 Acc:1.0
step: 1700, Loss: 0.11731872707605362 Acc:1.0
step: 1800, Loss: 0.11816013604402542 Acc:1.0
step: 1900, Loss: 0.11872206628322601 Acc:1.0
step: 2000, Loss: 4.451169967651367 Acc:0.9473684210526315
step: 2100, Loss: 0.11697661131620407 Acc:1.0
step: 2200, Loss: 0.1175650805234909 Acc:1.0
step: 2300, Loss: 0.11606592684984207 Acc:1.0
step: 2400, Loss: 0.11599119007587433 Acc:1.0
step: 2500, Loss: 0.13259881734848022 Acc:1.0
step: 2600, Loss: 6.93264627456665 Acc:0.9473684210526315
step: 2700, Loss: 0.12016895413398743 Acc:1.0
step: 2800, Loss: 0.12122706323862076 Acc:1.0
step: 2900, Loss: 2.0252766609191895 Acc:0.9736842105263158
step: 3000, Loss: 0.11801904439926147 Acc:1.0
step: 3100, Loss: 0.1287548542022705 Acc:1.0
step: 3200, Loss: 2.6315371990203857 Acc:0.9736842105263158
step: 3300, Loss: 0.11565682291984558 Acc:1.0
step: 3400, Loss: 0.12069027870893478 Acc:1.0
step: 3500, Loss: 0.11961430311203003 Acc:1.0
step: 3600, Loss: 0.12078804522752762 Acc:1.0
step: 3700, Loss: 0.12226621061563492 Acc:1.0
step: 3800, Loss: 0.13937370479106903 Acc:1.0
step: 3900, Loss: 0.12011829018592834 Acc:1.0
step: 4000, Loss: 0.12011478841304779 Acc:1.0
step: 4100, Loss: 0.11788210272789001 Acc:1.0
step: 4200, Loss: 0.11759629845619202 Acc:1.0
step: 4300, Loss: 0.11938457936048508 Acc:1.0
step: 4400, Loss: 0.11741285771131516 Acc:1.0
step: 4500, Loss: 0.11745551973581314 Acc:1.0
step: 4600, Loss: 0.11567410081624985 Acc:1.0
step: 4700, Loss: 0.1305302083492279 Acc:1.0
step: 4800, Loss: 0.11923655867576599 Acc:1.0
step: 4900, Loss: 0.11691019684076309 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11531311273574829 Acc:1.0
step: 100, Loss: 0.12049241364002228 Acc:1.0
step: 200, Loss: 0.1613326519727707 Acc:1.0
step: 300, Loss: 0.1158263236284256 Acc:1.0
step: 400, Loss: 19.37957000732422 Acc:0.8421052631578947
step: 500, Loss: 10.240862846374512 Acc:0.9210526315789473
step: 600, Loss: 0.1228497177362442 Acc:1.0
step: 700, Loss: 0.1177280992269516 Acc:1.0
step: 800, Loss: 0.9102340340614319 Acc:0.9736842105263158
step: 900, Loss: 0.11795766651630402 Acc:1.0
step: 1000, Loss: 0.1173301488161087 Acc:1.0
step: 1100, Loss: 0.12044411897659302 Acc:1.0
step: 1200, Loss: 7.359451770782471 Acc:0.9473684210526315
step: 1300, Loss: 0.1426946520805359 Acc:1.0
step: 1400, Loss: 0.11703702807426453 Acc:1.0
step: 1500, Loss: 0.12012256681919098 Acc:1.0
step: 1600, Loss: 0.1211143285036087 Acc:1.0
step: 1700, Loss: 0.12054191529750824 Acc:1.0
step: 1800, Loss: 0.11871486902236938 Acc:1.0
step: 1900, Loss: 4.186877727508545 Acc:0.9736842105263158
step: 2000, Loss: 4.736386775970459 Acc:0.9736842105263158
step: 2100, Loss: 0.1186838299036026 Acc:1.0
step: 2200, Loss: 0.677352249622345 Acc:0.9736842105263158
step: 2300, Loss: 5.054391860961914 Acc:0.9473684210526315
step: 2400, Loss: 2.666879415512085 Acc:0.9736842105263158
step: 2500, Loss: 0.12048390507698059 Acc:1.0
step: 2600, Loss: 0.1196943074464798 Acc:1.0
step: 2700, Loss: 0.1140400618314743 Acc:1.0
step: 2800, Loss: 0.11598607897758484 Acc:1.0
step: 2900, Loss: 0.1162877306342125 Acc:1.0
step: 3000, Loss: 0.12121670693159103 Acc:1.0
step: 3100, Loss: 0.1161818653345108 Acc:1.0
step: 3200, Loss: 0.12293712049722672 Acc:1.0
step: 3300, Loss: 0.11877076327800751 Acc:1.0
step: 3400, Loss: 3.436401844024658 Acc:0.9736842105263158
step: 3500, Loss: 11.362332344055176 Acc:0.868421052631579
step: 3600, Loss: 0.11994633823633194 Acc:1.0
step: 3700, Loss: 0.12235565483570099 Acc:1.0
step: 3800, Loss: 0.11872563511133194 Acc:1.0
step: 3900, Loss: 4.710032939910889 Acc:0.9736842105263158
step: 4000, Loss: 0.12056654691696167 Acc:1.0
step: 4100, Loss: 0.11541228741407394 Acc:1.0
step: 4200, Loss: 0.12130146473646164 Acc:1.0
step: 4300, Loss: 0.1166515201330185 Acc:1.0
step: 4400, Loss: 0.11784826219081879 Acc:1.0
step: 4500, Loss: 0.11710858345031738 Acc:1.0
step: 4600, Loss: 0.11758657544851303 Acc:1.0
step: 4700, Loss: 6.6187286376953125 Acc:0.9473684210526315
step: 4800, Loss: 3.3668160438537598 Acc:0.9736842105263158
step: 4900, Loss: 0.11878099292516708 Acc:1.0
training successfully ended.
validating...
validate data length:102
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:924
step: 0, Loss: 0.11941057443618774 Acc:1.0
step: 100, Loss: 0.11866285651922226 Acc:1.0
step: 200, Loss: 0.1166071966290474 Acc:1.0
step: 300, Loss: 0.28267890214920044 Acc:1.0
step: 400, Loss: 0.11715557426214218 Acc:1.0
step: 500, Loss: 0.12002816051244736 Acc:1.0
step: 600, Loss: 0.1182793527841568 Acc:1.0
step: 700, Loss: 0.12358153611421585 Acc:1.0
step: 800, Loss: 0.11996491998434067 Acc:1.0
step: 900, Loss: 0.11891153454780579 Acc:1.0
step: 1000, Loss: 0.12577395141124725 Acc:1.0
step: 1100, Loss: 0.11596254259347916 Acc:1.0
step: 1200, Loss: 0.3433813452720642 Acc:1.0
step: 1300, Loss: 0.11639422178268433 Acc:1.0
step: 1400, Loss: 3.319561243057251 Acc:0.9736842105263158
step: 1500, Loss: 0.11648856103420258 Acc:1.0
step: 1600, Loss: 0.11899557709693909 Acc:1.0
step: 1700, Loss: 0.1269722878932953 Acc:1.0
step: 1800, Loss: 0.11515867710113525 Acc:1.0
step: 1900, Loss: 0.11898939311504364 Acc:1.0
step: 2000, Loss: 0.11891844123601913 Acc:1.0
step: 2100, Loss: 0.11829304695129395 Acc:1.0
step: 2200, Loss: 0.11683710664510727 Acc:1.0
step: 2300, Loss: 0.11863415688276291 Acc:1.0
step: 2400, Loss: 0.11787107586860657 Acc:1.0
step: 2500, Loss: 0.11593437194824219 Acc:1.0
step: 2600, Loss: 0.11251191049814224 Acc:1.0
step: 2700, Loss: 0.12368209660053253 Acc:1.0
step: 2800, Loss: 0.1218148022890091 Acc:1.0
step: 2900, Loss: 0.11543391644954681 Acc:1.0
step: 3000, Loss: 0.11693981289863586 Acc:1.0
step: 3100, Loss: 7.7934746742248535 Acc:0.9473684210526315
step: 3200, Loss: 0.11548250913619995 Acc:1.0
step: 3300, Loss: 0.11629562824964523 Acc:1.0
step: 3400, Loss: 0.12096665054559708 Acc:1.0
step: 3500, Loss: 0.11505764722824097 Acc:1.0
step: 3600, Loss: 0.11770156025886536 Acc:1.0
step: 3700, Loss: 0.11472975462675095 Acc:1.0
step: 3800, Loss: 0.1145528256893158 Acc:1.0
step: 3900, Loss: 0.11973264813423157 Acc:1.0
step: 4000, Loss: 0.12351824343204498 Acc:1.0
step: 4100, Loss: 0.12072315067052841 Acc:1.0
step: 4200, Loss: 0.1181657612323761 Acc:1.0
step: 4300, Loss: 0.11732597649097443 Acc:1.0
step: 4400, Loss: 0.122623972594738 Acc:1.0
step: 4500, Loss: 0.11844877153635025 Acc:1.0
step: 4600, Loss: 0.11542744934558868 Acc:1.0
step: 4700, Loss: 0.11762209236621857 Acc:1.0
step: 4800, Loss: 0.11831464618444443 Acc:1.0
step: 4900, Loss: 4.425882816314697 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:102
acc: 0.9803921568627451
precision: 1.0
recall: 0.9565217391304348
F_score: 0.9777777777777777
subject 4 Avgacc: 0.9803921568627452 Avgfscore: 0.9789174771122479 
 Max acc:1.0, Max f score:1.0 Avg Recall:0.9809008874636438 Max Recall:1.0 Avg Precision:0.9779042386185243 Max Precision:1.0
******** mix subject_5 ********

[266, 494]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 76.88018035888672 Acc:0.5
step: 100, Loss: 25.104602813720703 Acc:0.39473684210526316
step: 200, Loss: 12.34090805053711 Acc:0.6052631578947368
step: 300, Loss: 12.536429405212402 Acc:0.631578947368421
step: 400, Loss: 10.228216171264648 Acc:0.7368421052631579
step: 500, Loss: 7.828481674194336 Acc:0.868421052631579
step: 600, Loss: 10.967534065246582 Acc:0.7368421052631579
step: 700, Loss: 6.325140953063965 Acc:0.8947368421052632
step: 800, Loss: 5.672515869140625 Acc:0.8947368421052632
step: 900, Loss: 6.307105541229248 Acc:0.868421052631579
step: 1000, Loss: 4.693173408508301 Acc:0.9210526315789473
step: 1100, Loss: 6.30302095413208 Acc:0.8947368421052632
step: 1200, Loss: 8.832602500915527 Acc:0.8421052631578947
step: 1300, Loss: 13.259739875793457 Acc:0.7894736842105263
step: 1400, Loss: 4.110159397125244 Acc:0.9473684210526315
step: 1500, Loss: 7.067706108093262 Acc:0.868421052631579
step: 1600, Loss: 4.9520063400268555 Acc:0.9210526315789473
step: 1700, Loss: 4.321629047393799 Acc:0.9210526315789473
step: 1800, Loss: 5.780683994293213 Acc:0.8947368421052632
step: 1900, Loss: 5.741984844207764 Acc:0.8947368421052632
step: 2000, Loss: 4.0958251953125 Acc:0.9473684210526315
step: 2100, Loss: 5.762476921081543 Acc:0.8947368421052632
step: 2200, Loss: 3.569446325302124 Acc:0.9473684210526315
step: 2300, Loss: 2.1841542720794678 Acc:0.9736842105263158
step: 2400, Loss: 1.9206516742706299 Acc:0.9736842105263158
step: 2500, Loss: 8.623540878295898 Acc:0.8421052631578947
step: 2600, Loss: 2.227247714996338 Acc:0.9736842105263158
step: 2700, Loss: 7.727243423461914 Acc:0.9210526315789473
step: 2800, Loss: 2.18597412109375 Acc:0.9736842105263158
step: 2900, Loss: 2.2330474853515625 Acc:0.9736842105263158
step: 3000, Loss: 2.8448333740234375 Acc:0.9736842105263158
step: 3100, Loss: 5.605081081390381 Acc:0.9210526315789473
step: 3200, Loss: 2.554176092147827 Acc:0.9736842105263158
step: 3300, Loss: 3.22415828704834 Acc:0.9473684210526315
step: 3400, Loss: 2.086995840072632 Acc:0.9736842105263158
step: 3500, Loss: 2.0752923488616943 Acc:0.9736842105263158
step: 3600, Loss: 5.063155174255371 Acc:0.9473684210526315
step: 3700, Loss: 3.7952682971954346 Acc:0.9473684210526315
step: 3800, Loss: 2.3584420680999756 Acc:0.9736842105263158
step: 3900, Loss: 2.157808303833008 Acc:0.9736842105263158
step: 4000, Loss: 3.782710075378418 Acc:0.9473684210526315
step: 4100, Loss: 1.9711737632751465 Acc:0.9736842105263158
step: 4200, Loss: 5.426679611206055 Acc:0.9473684210526315
step: 4300, Loss: 3.518076181411743 Acc:0.9473684210526315
step: 4400, Loss: 1.7386291027069092 Acc:0.9736842105263158
step: 4500, Loss: 2.2697343826293945 Acc:0.9736842105263158
step: 4600, Loss: 2.080585479736328 Acc:0.9736842105263158
step: 4700, Loss: 0.5715670585632324 Acc:1.0
step: 4800, Loss: 5.408984661102295 Acc:0.9473684210526315
step: 4900, Loss: 5.658344745635986 Acc:0.9210526315789473
training successfully ended.
validating...
validate data length:99
acc: 0.90625
precision: 0.9298245614035088
recall: 0.9137931034482759
F_score: 0.9217391304347825
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 2.9734396934509277 Acc:0.9473684210526315
step: 100, Loss: 0.49460962414741516 Acc:1.0
step: 200, Loss: 0.23261624574661255 Acc:1.0
step: 300, Loss: 5.240311145782471 Acc:0.9473684210526315
step: 400, Loss: 1.3076417446136475 Acc:0.9736842105263158
step: 500, Loss: 1.672950029373169 Acc:0.9736842105263158
step: 600, Loss: 0.20060577988624573 Acc:1.0
step: 700, Loss: 0.12693297863006592 Acc:1.0
step: 800, Loss: 4.3549275398254395 Acc:0.9736842105263158
step: 900, Loss: 4.92637300491333 Acc:0.9473684210526315
step: 1000, Loss: 3.559398889541626 Acc:0.9736842105263158
step: 1100, Loss: 7.524593830108643 Acc:0.9210526315789473
step: 1200, Loss: 0.15746867656707764 Acc:1.0
step: 1300, Loss: 0.8975902795791626 Acc:0.9736842105263158
step: 1400, Loss: 0.13897943496704102 Acc:1.0
step: 1500, Loss: 5.025421619415283 Acc:0.9473684210526315
step: 1600, Loss: 0.12486152350902557 Acc:1.0
step: 1700, Loss: 0.153026282787323 Acc:1.0
step: 1800, Loss: 0.1296122670173645 Acc:1.0
step: 1900, Loss: 0.13429012894630432 Acc:1.0
step: 2000, Loss: 2.6890769004821777 Acc:0.9736842105263158
step: 2100, Loss: 0.22058795392513275 Acc:1.0
step: 2200, Loss: 0.11858194321393967 Acc:1.0
step: 2300, Loss: 2.9659225940704346 Acc:0.9736842105263158
step: 2400, Loss: 6.826370716094971 Acc:0.9473684210526315
step: 2500, Loss: 2.976062059402466 Acc:0.9736842105263158
step: 2600, Loss: 0.11630754172801971 Acc:1.0
step: 2700, Loss: 3.2579402923583984 Acc:0.9736842105263158
step: 2800, Loss: 0.12576410174369812 Acc:1.0
step: 2900, Loss: 3.1424684524536133 Acc:0.9736842105263158
step: 3000, Loss: 0.12235283106565475 Acc:1.0
step: 3100, Loss: 0.11992032825946808 Acc:1.0
step: 3200, Loss: 0.6704724431037903 Acc:0.9736842105263158
step: 3300, Loss: 0.7954779863357544 Acc:0.9736842105263158
step: 3400, Loss: 3.317330837249756 Acc:0.9736842105263158
step: 3500, Loss: 4.335517883300781 Acc:0.9736842105263158
step: 3600, Loss: 0.12103857845067978 Acc:1.0
step: 3700, Loss: 0.12755325436592102 Acc:1.0
step: 3800, Loss: 0.14101597666740417 Acc:1.0
step: 3900, Loss: 0.1145944595336914 Acc:1.0
step: 4000, Loss: 0.12141682207584381 Acc:1.0
step: 4100, Loss: 2.0469698905944824 Acc:0.9736842105263158
step: 4200, Loss: 0.130692258477211 Acc:1.0
step: 4300, Loss: 1.7102335691452026 Acc:0.9736842105263158
step: 4400, Loss: 0.12201371043920517 Acc:1.0
step: 4500, Loss: 4.16049337387085 Acc:0.9473684210526315
step: 4600, Loss: 0.11894853413105011 Acc:1.0
step: 4700, Loss: 2.3817801475524902 Acc:0.9736842105263158
step: 4800, Loss: 0.11676514148712158 Acc:1.0
step: 4900, Loss: 0.11817184090614319 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 0.90625
precision: 0.8301886792452831
recall: 1.0
F_score: 0.9072164948453608
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 3.0244972705841064 Acc:0.9736842105263158
step: 100, Loss: 0.399203896522522 Acc:1.0
step: 200, Loss: 0.26802143454551697 Acc:1.0
step: 300, Loss: 2.74813175201416 Acc:0.9736842105263158
step: 400, Loss: 0.3357636332511902 Acc:1.0
step: 500, Loss: 3.266512870788574 Acc:0.9736842105263158
step: 600, Loss: 0.19318339228630066 Acc:1.0
step: 700, Loss: 0.2401401847600937 Acc:1.0
step: 800, Loss: 0.1373319923877716 Acc:1.0
step: 900, Loss: 3.5971853733062744 Acc:0.9473684210526315
step: 1000, Loss: 6.569553375244141 Acc:0.9473684210526315
step: 1100, Loss: 2.8874051570892334 Acc:0.9473684210526315
step: 1200, Loss: 0.16520343720912933 Acc:1.0
step: 1300, Loss: 0.13406819105148315 Acc:1.0
step: 1400, Loss: 3.449235439300537 Acc:0.9736842105263158
step: 1500, Loss: 0.14498069882392883 Acc:1.0
step: 1600, Loss: 0.15650534629821777 Acc:1.0
step: 1700, Loss: 0.12199874222278595 Acc:1.0
step: 1800, Loss: 0.11677825450897217 Acc:1.0
step: 1900, Loss: 0.12685000896453857 Acc:1.0
step: 2000, Loss: 4.565210342407227 Acc:0.9736842105263158
step: 2100, Loss: 3.514554262161255 Acc:0.9736842105263158
step: 2200, Loss: 0.1200522631406784 Acc:1.0
step: 2300, Loss: 0.5554043650627136 Acc:0.9736842105263158
step: 2400, Loss: 0.1198064386844635 Acc:1.0
step: 2500, Loss: 0.13656984269618988 Acc:1.0
step: 2600, Loss: 0.11690814793109894 Acc:1.0
step: 2700, Loss: 8.181070327758789 Acc:0.9473684210526315
step: 2800, Loss: 4.0463948249816895 Acc:0.9736842105263158
step: 2900, Loss: 0.9315057992935181 Acc:0.9736842105263158
step: 3000, Loss: 0.12079927325248718 Acc:1.0
step: 3100, Loss: 0.11671742051839828 Acc:1.0
step: 3200, Loss: 1.0972092151641846 Acc:0.9736842105263158
step: 3300, Loss: 5.300212860107422 Acc:0.9473684210526315
step: 3400, Loss: 0.11775203049182892 Acc:1.0
step: 3500, Loss: 0.12156468629837036 Acc:1.0
step: 3600, Loss: 1.8153785467147827 Acc:0.9736842105263158
step: 3700, Loss: 0.1183386966586113 Acc:1.0
step: 3800, Loss: 4.557222366333008 Acc:0.9736842105263158
step: 3900, Loss: 3.9515538215637207 Acc:0.9736842105263158
step: 4000, Loss: 0.11683115363121033 Acc:1.0
step: 4100, Loss: 0.8809627294540405 Acc:0.9473684210526315
step: 4200, Loss: 6.338862419128418 Acc:0.9473684210526315
step: 4300, Loss: 0.11716179549694061 Acc:1.0
step: 4400, Loss: 0.11970964074134827 Acc:1.0
step: 4500, Loss: 0.12256330251693726 Acc:1.0
step: 4600, Loss: 0.1174679547548294 Acc:1.0
step: 4700, Loss: 0.12367807328701019 Acc:1.0
step: 4800, Loss: 0.11865593492984772 Acc:1.0
step: 4900, Loss: 0.3779281973838806 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 0.9479166666666666
precision: 0.9090909090909091
recall: 1.0
F_score: 0.9523809523809523
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 4.4473557472229 Acc:0.9736842105263158
step: 100, Loss: 0.12039157748222351 Acc:1.0
step: 200, Loss: 0.11995168030261993 Acc:1.0
step: 300, Loss: 4.5160627365112305 Acc:0.9736842105263158
step: 400, Loss: 2.7328298091888428 Acc:0.9736842105263158
step: 500, Loss: 0.11716685444116592 Acc:1.0
step: 600, Loss: 0.14427562057971954 Acc:1.0
step: 700, Loss: 2.767885208129883 Acc:0.9736842105263158
step: 800, Loss: 0.5405645370483398 Acc:0.9736842105263158
step: 900, Loss: 4.0219550132751465 Acc:0.9736842105263158
step: 1000, Loss: 0.12697622179985046 Acc:1.0
step: 1100, Loss: 4.620354175567627 Acc:0.9736842105263158
step: 1200, Loss: 0.11628792434930801 Acc:1.0
step: 1300, Loss: 0.11746075004339218 Acc:1.0
step: 1400, Loss: 0.1194821149110794 Acc:1.0
step: 1500, Loss: 0.11942801624536514 Acc:1.0
step: 1600, Loss: 0.12334160506725311 Acc:1.0
step: 1700, Loss: 0.1260891556739807 Acc:1.0
step: 1800, Loss: 0.12912249565124512 Acc:1.0
step: 1900, Loss: 0.1207074299454689 Acc:1.0
step: 2000, Loss: 9.656387329101562 Acc:0.9210526315789473
step: 2100, Loss: 2.8163580894470215 Acc:0.9736842105263158
step: 2200, Loss: 0.11466261744499207 Acc:1.0
step: 2300, Loss: 0.12071272730827332 Acc:1.0
step: 2400, Loss: 3.298750400543213 Acc:0.9473684210526315
step: 2500, Loss: 0.11526399850845337 Acc:1.0
step: 2600, Loss: 0.3132401704788208 Acc:1.0
step: 2700, Loss: 0.12122972309589386 Acc:1.0
step: 2800, Loss: 0.11651022732257843 Acc:1.0
step: 2900, Loss: 0.12055401504039764 Acc:1.0
step: 3000, Loss: 0.26839354634284973 Acc:1.0
step: 3100, Loss: 0.12074178457260132 Acc:1.0
step: 3200, Loss: 0.1193174421787262 Acc:1.0
step: 3300, Loss: 0.11756352335214615 Acc:1.0
step: 3400, Loss: 0.12032665312290192 Acc:1.0
step: 3500, Loss: 0.11790814250707626 Acc:1.0
step: 3600, Loss: 0.11947865784168243 Acc:1.0
step: 3700, Loss: 0.11605111509561539 Acc:1.0
step: 3800, Loss: 0.11972921341657639 Acc:1.0
step: 3900, Loss: 2.827274799346924 Acc:0.9736842105263158
step: 4000, Loss: 0.5327516794204712 Acc:0.9736842105263158
step: 4100, Loss: 0.12798762321472168 Acc:1.0
step: 4200, Loss: 0.12002833187580109 Acc:1.0
step: 4300, Loss: 0.11582779884338379 Acc:1.0
step: 4400, Loss: 2.0199544429779053 Acc:0.9736842105263158
step: 4500, Loss: 0.11615190654993057 Acc:1.0
step: 4600, Loss: 0.11985114216804504 Acc:1.0
step: 4700, Loss: 0.11649753153324127 Acc:1.0
step: 4800, Loss: 0.11509181559085846 Acc:1.0
step: 4900, Loss: 2.6841678619384766 Acc:0.9736842105263158
training successfully ended.
validating...
validate data length:99
acc: 0.9583333333333334
precision: 0.9272727272727272
recall: 1.0
F_score: 0.9622641509433962
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 3.979238748550415 Acc:0.9736842105263158
step: 100, Loss: 0.11435617506504059 Acc:1.0
step: 200, Loss: 0.11789644509553909 Acc:1.0
step: 300, Loss: 0.11807513236999512 Acc:1.0
step: 400, Loss: 0.12109210342168808 Acc:1.0
step: 500, Loss: 0.11595513671636581 Acc:1.0
step: 600, Loss: 0.1209040954709053 Acc:1.0
step: 700, Loss: 0.11938536912202835 Acc:1.0
step: 800, Loss: 0.13021259009838104 Acc:1.0
step: 900, Loss: 1.9007478952407837 Acc:0.9736842105263158
step: 1000, Loss: 0.11774864792823792 Acc:1.0
step: 1100, Loss: 0.12171447277069092 Acc:1.0
step: 1200, Loss: 0.11815083771944046 Acc:1.0
step: 1300, Loss: 0.11877728253602982 Acc:1.0
step: 1400, Loss: 0.11842022836208344 Acc:1.0
step: 1500, Loss: 0.12188791483640671 Acc:1.0
step: 1600, Loss: 2.3016304969787598 Acc:0.9736842105263158
step: 1700, Loss: 0.12604312598705292 Acc:1.0
step: 1800, Loss: 0.11635839939117432 Acc:1.0
step: 1900, Loss: 0.11715467274188995 Acc:1.0
step: 2000, Loss: 0.14207684993743896 Acc:1.0
step: 2100, Loss: 1.3769237995147705 Acc:0.9736842105263158
step: 2200, Loss: 0.11903369426727295 Acc:1.0
step: 2300, Loss: 4.113871097564697 Acc:0.9736842105263158
step: 2400, Loss: 0.12017130851745605 Acc:1.0
step: 2500, Loss: 0.34294211864471436 Acc:1.0
step: 2600, Loss: 0.11827121675014496 Acc:1.0
step: 2700, Loss: 0.12063484638929367 Acc:1.0
step: 2800, Loss: 0.11842311173677444 Acc:1.0
step: 2900, Loss: 0.11828159540891647 Acc:1.0
step: 3000, Loss: 0.11613993346691132 Acc:1.0
step: 3100, Loss: 0.11987561732530594 Acc:1.0
step: 3200, Loss: 0.11903771013021469 Acc:1.0
step: 3300, Loss: 0.1186467632651329 Acc:1.0
step: 3400, Loss: 0.11873406916856766 Acc:1.0
step: 3500, Loss: 0.1180037334561348 Acc:1.0
step: 3600, Loss: 0.11568285524845123 Acc:1.0
step: 3700, Loss: 0.14405670762062073 Acc:1.0
step: 3800, Loss: 0.11787330359220505 Acc:1.0
step: 3900, Loss: 2.9827029705047607 Acc:0.9736842105263158
step: 4000, Loss: 0.11972661316394806 Acc:1.0
step: 4100, Loss: 0.11793406307697296 Acc:1.0
step: 4200, Loss: 3.330639123916626 Acc:0.9736842105263158
step: 4300, Loss: 0.12052682787179947 Acc:1.0
step: 4400, Loss: 0.11513661593198776 Acc:1.0
step: 4500, Loss: 0.11766351759433746 Acc:1.0
step: 4600, Loss: 0.11588220298290253 Acc:1.0
step: 4700, Loss: 0.1191137358546257 Acc:1.0
step: 4800, Loss: 1.5659732818603516 Acc:0.9736842105263158
step: 4900, Loss: 0.11450939625501633 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 6******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 0.1247321218252182 Acc:1.0
step: 100, Loss: 0.11848045885562897 Acc:1.0
step: 200, Loss: 0.11657793819904327 Acc:1.0
step: 300, Loss: 0.12639081478118896 Acc:1.0
step: 400, Loss: 0.11877837777137756 Acc:1.0
step: 500, Loss: 0.1160798892378807 Acc:1.0
step: 600, Loss: 0.11956536769866943 Acc:1.0
step: 700, Loss: 0.12063857167959213 Acc:1.0
step: 800, Loss: 0.1191810816526413 Acc:1.0
step: 900, Loss: 2.9457545280456543 Acc:0.9736842105263158
step: 1000, Loss: 0.12827453017234802 Acc:1.0
step: 1100, Loss: 0.12055268883705139 Acc:1.0
step: 1200, Loss: 0.1216035857796669 Acc:1.0
step: 1300, Loss: 1.1100765466690063 Acc:0.9736842105263158
step: 1400, Loss: 0.11433744430541992 Acc:1.0
step: 1500, Loss: 0.12178091704845428 Acc:1.0
step: 1600, Loss: 0.1192043200135231 Acc:1.0
step: 1700, Loss: 0.12331840395927429 Acc:1.0
step: 1800, Loss: 3.7389447689056396 Acc:0.9736842105263158
step: 1900, Loss: 0.11533357948064804 Acc:1.0
step: 2000, Loss: 0.11859455704689026 Acc:1.0
step: 2100, Loss: 0.1173495203256607 Acc:1.0
step: 2200, Loss: 0.11681223660707474 Acc:1.0
step: 2300, Loss: 0.11914531886577606 Acc:1.0
step: 2400, Loss: 0.11689382791519165 Acc:1.0
step: 2500, Loss: 0.12277466058731079 Acc:1.0
step: 2600, Loss: 0.12162231653928757 Acc:1.0
step: 2700, Loss: 0.12399075925350189 Acc:1.0
step: 2800, Loss: 5.403928279876709 Acc:0.9473684210526315
step: 2900, Loss: 0.12360629439353943 Acc:1.0
step: 3000, Loss: 2.8583614826202393 Acc:0.9736842105263158
step: 3100, Loss: 0.12038543820381165 Acc:1.0
step: 3200, Loss: 2.651853561401367 Acc:0.9736842105263158
step: 3300, Loss: 0.12200434505939484 Acc:1.0
step: 3400, Loss: 0.11616078019142151 Acc:1.0
step: 3500, Loss: 0.11932816356420517 Acc:1.0
step: 3600, Loss: 0.11965066194534302 Acc:1.0
step: 3700, Loss: 0.11857419461011887 Acc:1.0
step: 3800, Loss: 0.13450418412685394 Acc:1.0
step: 3900, Loss: 0.11922883987426758 Acc:1.0
step: 4000, Loss: 0.11448325961828232 Acc:1.0
step: 4100, Loss: 0.12241746485233307 Acc:1.0
step: 4200, Loss: 0.11991365253925323 Acc:1.0
step: 4300, Loss: 0.11795423924922943 Acc:1.0
step: 4400, Loss: 0.12276417762041092 Acc:1.0
step: 4500, Loss: 0.1197824478149414 Acc:1.0
step: 4600, Loss: 0.11881402134895325 Acc:1.0
step: 4700, Loss: 1.874316692352295 Acc:0.9736842105263158
step: 4800, Loss: 0.11562500894069672 Acc:1.0
step: 4900, Loss: 0.11671364307403564 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 0.96875
precision: 0.9387755102040817
recall: 1.0
F_score: 0.968421052631579
******fold 7******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 0.12110964208841324 Acc:1.0
step: 100, Loss: 0.11485935747623444 Acc:1.0
step: 200, Loss: 0.1172599270939827 Acc:1.0
step: 300, Loss: 13.554845809936523 Acc:0.8947368421052632
step: 400, Loss: 0.12154357135295868 Acc:1.0
step: 500, Loss: 0.12070229649543762 Acc:1.0
step: 600, Loss: 0.11891096830368042 Acc:1.0
step: 700, Loss: 0.12320723384618759 Acc:1.0
step: 800, Loss: 0.11660265177488327 Acc:1.0
step: 900, Loss: 0.117595374584198 Acc:1.0
step: 1000, Loss: 0.12036266922950745 Acc:1.0
step: 1100, Loss: 0.11589372903108597 Acc:1.0
step: 1200, Loss: 0.11455690860748291 Acc:1.0
step: 1300, Loss: 0.11679499596357346 Acc:1.0
step: 1400, Loss: 0.1164047047495842 Acc:1.0
step: 1500, Loss: 0.11679776757955551 Acc:1.0
step: 1600, Loss: 0.12173900008201599 Acc:1.0
step: 1700, Loss: 2.227813720703125 Acc:0.9736842105263158
step: 1800, Loss: 0.11886868625879288 Acc:1.0
step: 1900, Loss: 0.11953338235616684 Acc:1.0
step: 2000, Loss: 0.11729663610458374 Acc:1.0
step: 2100, Loss: 3.3488192558288574 Acc:0.9736842105263158
step: 2200, Loss: 0.11773082613945007 Acc:1.0
step: 2300, Loss: 0.11718808859586716 Acc:1.0
step: 2400, Loss: 0.11773791164159775 Acc:1.0
step: 2500, Loss: 0.13064634799957275 Acc:1.0
step: 2600, Loss: 0.11846053600311279 Acc:1.0
step: 2700, Loss: 0.11912201344966888 Acc:1.0
step: 2800, Loss: 0.11782991141080856 Acc:1.0
step: 2900, Loss: 0.1587570160627365 Acc:1.0
step: 3000, Loss: 0.11618109047412872 Acc:1.0
step: 3100, Loss: 0.11832351237535477 Acc:1.0
step: 3200, Loss: 0.11597154289484024 Acc:1.0
step: 3300, Loss: 0.1242089718580246 Acc:1.0
step: 3400, Loss: 0.12084968388080597 Acc:1.0
step: 3500, Loss: 0.13955749571323395 Acc:1.0
step: 3600, Loss: 0.11795222014188766 Acc:1.0
step: 3700, Loss: 0.11685686558485031 Acc:1.0
step: 3800, Loss: 0.12476473301649094 Acc:1.0
step: 3900, Loss: 0.12625156342983246 Acc:1.0
step: 4000, Loss: 0.11509187519550323 Acc:1.0
step: 4100, Loss: 0.11622321605682373 Acc:1.0
step: 4200, Loss: 0.11693204939365387 Acc:1.0
step: 4300, Loss: 0.11803621798753738 Acc:1.0
step: 4400, Loss: 0.11517924070358276 Acc:1.0
step: 4500, Loss: 0.1366768330335617 Acc:1.0
step: 4600, Loss: 0.11920911818742752 Acc:1.0
step: 4700, Loss: 0.12042231112718582 Acc:1.0
step: 4800, Loss: 0.12095189094543457 Acc:1.0
step: 4900, Loss: 0.11902505904436111 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 8******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:889
step: 0, Loss: 0.12390054762363434 Acc:1.0
step: 100, Loss: 0.12418091297149658 Acc:1.0
step: 200, Loss: 0.11864681541919708 Acc:1.0
step: 300, Loss: 0.1488877832889557 Acc:1.0
step: 400, Loss: 0.21010631322860718 Acc:1.0
step: 500, Loss: 0.11796563863754272 Acc:1.0
step: 600, Loss: 0.11749047785997391 Acc:1.0
step: 700, Loss: 0.12047402560710907 Acc:1.0
step: 800, Loss: 0.11868888139724731 Acc:1.0
step: 900, Loss: 0.11706097424030304 Acc:1.0
step: 1000, Loss: 0.12052251398563385 Acc:1.0
step: 1100, Loss: 4.053643703460693 Acc:0.9736842105263158
step: 1200, Loss: 0.11558528244495392 Acc:1.0
step: 1300, Loss: 0.1154097467660904 Acc:1.0
step: 1400, Loss: 0.1261332631111145 Acc:1.0
step: 1500, Loss: 0.11676193028688431 Acc:1.0
step: 1600, Loss: 8.15643310546875 Acc:0.9473684210526315
step: 1700, Loss: 0.1257196068763733 Acc:1.0
step: 1800, Loss: 0.1200234666466713 Acc:1.0
step: 1900, Loss: 0.11752534657716751 Acc:1.0
step: 2000, Loss: 0.1162935346364975 Acc:1.0
step: 2100, Loss: 0.11722574383020401 Acc:1.0
step: 2200, Loss: 0.11478251218795776 Acc:1.0
step: 2300, Loss: 0.11756015568971634 Acc:1.0
step: 2400, Loss: 0.11793132871389389 Acc:1.0
step: 2500, Loss: 0.1157182976603508 Acc:1.0
step: 2600, Loss: 0.11807478964328766 Acc:1.0
step: 2700, Loss: 0.11396698653697968 Acc:1.0
step: 2800, Loss: 0.11762918531894684 Acc:1.0
step: 2900, Loss: 0.11786913126707077 Acc:1.0
step: 3000, Loss: 0.12371388077735901 Acc:1.0
step: 3100, Loss: 0.12124153226613998 Acc:1.0
step: 3200, Loss: 0.11802105605602264 Acc:1.0
step: 3300, Loss: 0.1205802857875824 Acc:1.0
step: 3400, Loss: 0.11844372004270554 Acc:1.0
step: 3500, Loss: 0.11804648488759995 Acc:1.0
step: 3600, Loss: 0.11806245148181915 Acc:1.0
step: 3700, Loss: 0.12059134244918823 Acc:1.0
step: 3800, Loss: 0.11527031660079956 Acc:1.0
step: 3900, Loss: 0.11984740197658539 Acc:1.0
step: 4000, Loss: 0.1172310933470726 Acc:1.0
step: 4100, Loss: 0.11650799214839935 Acc:1.0
step: 4200, Loss: 0.11745592951774597 Acc:1.0
step: 4300, Loss: 0.1155066192150116 Acc:1.0
step: 4400, Loss: 0.11671406030654907 Acc:1.0
step: 4500, Loss: 0.11936865746974945 Acc:1.0
step: 4600, Loss: 0.21257953345775604 Acc:1.0
step: 4700, Loss: 0.11771920323371887 Acc:1.0
step: 4800, Loss: 4.185489654541016 Acc:0.9736842105263158
step: 4900, Loss: 0.11466199159622192 Acc:1.0
training successfully ended.
validating...
validate data length:99
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 9******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:890
step: 0, Loss: 0.11941802501678467 Acc:1.0
step: 100, Loss: 0.11567078530788422 Acc:1.0
step: 200, Loss: 3.690615177154541 Acc:0.9736842105263158
step: 300, Loss: 0.11922429502010345 Acc:1.0
step: 400, Loss: 0.11953339725732803 Acc:1.0
step: 500, Loss: 0.12354905903339386 Acc:1.0
step: 600, Loss: 2.8392422199249268 Acc:0.9736842105263158
step: 700, Loss: 0.11710213124752045 Acc:1.0
step: 800, Loss: 0.118283212184906 Acc:1.0
step: 900, Loss: 0.11868850886821747 Acc:1.0
step: 1000, Loss: 0.11954626441001892 Acc:1.0
step: 1100, Loss: 0.11528163403272629 Acc:1.0
step: 1200, Loss: 0.12255214899778366 Acc:1.0
step: 1300, Loss: 0.12145305424928665 Acc:1.0
step: 1400, Loss: 0.12397930026054382 Acc:1.0
step: 1500, Loss: 0.11309053748846054 Acc:1.0
step: 1600, Loss: 0.11728360503911972 Acc:1.0
step: 1700, Loss: 0.11568506062030792 Acc:1.0
step: 1800, Loss: 0.12194845080375671 Acc:1.0
step: 1900, Loss: 0.11885691434144974 Acc:1.0
step: 2000, Loss: 0.11938729882240295 Acc:1.0
step: 2100, Loss: 0.12288057059049606 Acc:1.0
step: 2200, Loss: 0.11679983884096146 Acc:1.0
step: 2300, Loss: 0.12032729387283325 Acc:1.0
step: 2400, Loss: 0.11650311946868896 Acc:1.0
step: 2500, Loss: 0.11458074301481247 Acc:1.0
step: 2600, Loss: 0.11883388459682465 Acc:1.0
step: 2700, Loss: 2.45147442817688 Acc:0.9736842105263158
step: 2800, Loss: 0.1195630431175232 Acc:1.0
step: 2900, Loss: 0.11770932376384735 Acc:1.0
step: 3000, Loss: 0.1208566278219223 Acc:1.0
step: 3100, Loss: 0.117948517203331 Acc:1.0
step: 3200, Loss: 2.560319423675537 Acc:0.9736842105263158
step: 3300, Loss: 0.12152986228466034 Acc:1.0
step: 3400, Loss: 0.11760371178388596 Acc:1.0
step: 3500, Loss: 0.11871783435344696 Acc:1.0
step: 3600, Loss: 0.11832182854413986 Acc:1.0
step: 3700, Loss: 0.11491959542036057 Acc:1.0
step: 3800, Loss: 0.11939917504787445 Acc:1.0
step: 3900, Loss: 0.11430509388446808 Acc:1.0
step: 4000, Loss: 0.11565185338258743 Acc:1.0
step: 4100, Loss: 0.11623203754425049 Acc:1.0
step: 4200, Loss: 0.11765201389789581 Acc:1.0
step: 4300, Loss: 0.11967360973358154 Acc:1.0
step: 4400, Loss: 0.11934250593185425 Acc:1.0
step: 4500, Loss: 0.11897660046815872 Acc:1.0
step: 4600, Loss: 0.11932171881198883 Acc:1.0
step: 4700, Loss: 2.744818925857544 Acc:0.9736842105263158
step: 4800, Loss: 0.12012103199958801 Acc:1.0
step: 4900, Loss: 0.11635073274374008 Acc:1.0
training successfully ended.
validating...
validate data length:98
acc: 0.9791666666666666
precision: 1.0
recall: 0.9607843137254902
F_score: 0.98
******fold 10******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:890
step: 0, Loss: 0.11894328892230988 Acc:1.0
step: 100, Loss: 0.12659764289855957 Acc:1.0
step: 200, Loss: 0.12021741271018982 Acc:1.0
step: 300, Loss: 0.1195836216211319 Acc:1.0
step: 400, Loss: 0.12028612196445465 Acc:1.0
step: 500, Loss: 0.11920120567083359 Acc:1.0
step: 600, Loss: 0.1178925633430481 Acc:1.0
step: 700, Loss: 0.12738682329654694 Acc:1.0
step: 800, Loss: 0.12542884051799774 Acc:1.0
step: 900, Loss: 0.12089387327432632 Acc:1.0
step: 1000, Loss: 0.11769872903823853 Acc:1.0
step: 1100, Loss: 0.12128022313117981 Acc:1.0
step: 1200, Loss: 1.323238730430603 Acc:0.9736842105263158
step: 1300, Loss: 0.12239837646484375 Acc:1.0
step: 1400, Loss: 0.11786288768053055 Acc:1.0
step: 1500, Loss: 0.2685754597187042 Acc:1.0
step: 1600, Loss: 0.11534449458122253 Acc:1.0
step: 1700, Loss: 0.2654482424259186 Acc:1.0
step: 1800, Loss: 0.1135757565498352 Acc:1.0
step: 1900, Loss: 1.046250820159912 Acc:0.9736842105263158
step: 2000, Loss: 0.11787672340869904 Acc:1.0
step: 2100, Loss: 4.039754867553711 Acc:0.9736842105263158
step: 2200, Loss: 0.11641767621040344 Acc:1.0
step: 2300, Loss: 0.630268931388855 Acc:0.9736842105263158
step: 2400, Loss: 0.17308498919010162 Acc:1.0
step: 2500, Loss: 0.11784811317920685 Acc:1.0
step: 2600, Loss: 0.12505154311656952 Acc:1.0
step: 2700, Loss: 2.4598724842071533 Acc:0.9736842105263158
step: 2800, Loss: 0.1187075823545456 Acc:1.0
step: 2900, Loss: 0.12633922696113586 Acc:1.0
step: 3000, Loss: 0.11640481650829315 Acc:1.0
step: 3100, Loss: 0.12322570383548737 Acc:1.0
step: 3200, Loss: 0.11789502948522568 Acc:1.0
step: 3300, Loss: 0.12443279474973679 Acc:1.0
step: 3400, Loss: 0.883326530456543 Acc:0.9736842105263158
step: 3500, Loss: 0.12056897580623627 Acc:1.0
step: 3600, Loss: 0.1601056009531021 Acc:1.0
step: 3700, Loss: 0.12037935107946396 Acc:1.0
step: 3800, Loss: 0.12578821182250977 Acc:1.0
step: 3900, Loss: 0.558503270149231 Acc:0.9736842105263158
step: 4000, Loss: 0.11786214262247086 Acc:1.0
step: 4100, Loss: 0.11847063899040222 Acc:1.0
step: 4200, Loss: 3.429567337036133 Acc:0.9736842105263158
step: 4300, Loss: 0.12240386009216309 Acc:1.0
step: 4400, Loss: 0.11472808569669724 Acc:1.0
step: 4500, Loss: 0.12495055794715881 Acc:1.0
step: 4600, Loss: 0.12229188531637192 Acc:1.0
step: 4700, Loss: 0.11919043958187103 Acc:1.0
step: 4800, Loss: 0.11680827289819717 Acc:1.0
step: 4900, Loss: 0.11932380497455597 Acc:1.0
training successfully ended.
validating...
validate data length:98
acc: 0.9895833333333334
precision: 0.9782608695652174
recall: 1.0
F_score: 0.989010989010989
subject 5 Avgacc: 0.965625 Avgfscore: 0.9681032770247061 
 Max acc:1.0, Max f score:1.0 Avg Recall:0.9874577417173767 Max Recall:1.0 Avg Precision:0.9513413256781726 Max Precision:1.0
******** mix subject_6 ********

[190, 570]
******fold 1******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:1026
step: 0, Loss: 66.20040130615234 Acc:0.5
step: 100, Loss: 20.747743606567383 Acc:0.47368421052631576
step: 200, Loss: 11.805349349975586 Acc:0.6578947368421053
step: 300, Loss: 13.388365745544434 Acc:0.6052631578947368
step: 400, Loss: 10.645590782165527 Acc:0.7368421052631579
step: 500, Loss: 10.19284725189209 Acc:0.7368421052631579
step: 600, Loss: 10.624788284301758 Acc:0.7368421052631579
step: 700, Loss: 9.7769775390625 Acc:0.7894736842105263
step: 800, Loss: 13.398670196533203 Acc:0.7368421052631579
step: 900, Loss: 6.005002498626709 Acc:0.8947368421052632
step: 1000, Loss: 9.662398338317871 Acc:0.8157894736842105
step: 1100, Loss: 7.813846111297607 Acc:0.8421052631578947
step: 1200, Loss: 6.1059041023254395 Acc:0.8947368421052632
step: 1300, Loss: 10.439685821533203 Acc:0.8157894736842105
step: 1400, Loss: 3.8456878662109375 Acc:0.9473684210526315
step: 1500, Loss: 4.969733238220215 Acc:0.9210526315789473
step: 1600, Loss: 7.421886920928955 Acc:0.868421052631579
step: 1700, Loss: 4.7764410972595215 Acc:0.9210526315789473
step: 1800, Loss: 4.4623799324035645 Acc:0.9210526315789473
step: 1900, Loss: 5.704380035400391 Acc:0.8947368421052632
step: 2000, Loss: 5.434455394744873 Acc:0.8947368421052632
step: 2100, Loss: 4.16463565826416 Acc:0.9473684210526315
step: 2200, Loss: 1.8810068368911743 Acc:0.9736842105263158
step: 2300, Loss: 8.477478981018066 Acc:0.8947368421052632
step: 2400, Loss: 2.5396437644958496 Acc:0.9736842105263158
step: 2500, Loss: 3.68798828125 Acc:0.9473684210526315
step: 2600, Loss: 4.491844177246094 Acc:0.9210526315789473
step: 2700, Loss: 11.438703536987305 Acc:0.8947368421052632
step: 2800, Loss: 20.540367126464844 Acc:0.8421052631578947
step: 2900, Loss: 6.847581386566162 Acc:0.9473684210526315
step: 3000, Loss: 6.7294921875 Acc:0.9473684210526315
step: 3100, Loss: 0.1325128972530365 Acc:1.0
step: 3200, Loss: 2.824385404586792 Acc:0.9736842105263158
step: 3300, Loss: 0.14611998200416565 Acc:1.0
step: 3400, Loss: 4.404659271240234 Acc:0.9736842105263158
step: 3500, Loss: 0.38741961121559143 Acc:1.0
step: 3600, Loss: 3.557124614715576 Acc:0.9736842105263158
step: 3700, Loss: 0.30805063247680664 Acc:1.0
step: 3800, Loss: 0.1321684569120407 Acc:1.0
step: 3900, Loss: 11.974393844604492 Acc:0.9210526315789473
step: 4000, Loss: 0.11654071509838104 Acc:1.0
step: 4100, Loss: 0.16669845581054688 Acc:1.0
step: 4200, Loss: 0.11926364153623581 Acc:1.0
step: 4300, Loss: 0.11854173243045807 Acc:1.0
step: 4400, Loss: 0.11663834750652313 Acc:1.0
step: 4500, Loss: 0.11968016624450684 Acc:1.0
step: 4600, Loss: 4.300362586975098 Acc:0.9736842105263158
step: 4700, Loss: 0.11789873242378235 Acc:1.0
step: 4800, Loss: 4.291072368621826 Acc:0.9473684210526315
step: 4900, Loss: 9.183158874511719 Acc:0.9210526315789473
training successfully ended.
validating...
validate data length:114
acc: 0.9035087719298246
precision: 0.8450704225352113
recall: 1.0
F_score: 0.916030534351145
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:1026
step: 0, Loss: 6.243868350982666 Acc:0.9473684210526315
step: 100, Loss: 0.8234273195266724 Acc:0.9736842105263158
step: 200, Loss: 2.9866280555725098 Acc:0.9736842105263158
step: 300, Loss: 0.11892461776733398 Acc:1.0
step: 400, Loss: 2.549170970916748 Acc:0.9736842105263158
step: 500, Loss: 0.12383558601140976 Acc:1.0
step: 600, Loss: 0.1207478940486908 Acc:1.0
step: 700, Loss: 0.11638019979000092 Acc:1.0
step: 800, Loss: 0.11395695060491562 Acc:1.0
step: 900, Loss: 0.1165878102183342 Acc:1.0
step: 1000, Loss: 9.076662063598633 Acc:0.9473684210526315
step: 1100, Loss: 1.9893081188201904 Acc:0.9736842105263158
step: 1200, Loss: 8.786420822143555 Acc:0.9473684210526315
step: 1300, Loss: 0.11833900213241577 Acc:1.0
step: 1400, Loss: 14.12398910522461 Acc:0.868421052631579
step: 1500, Loss: 3.004159688949585 Acc:0.9736842105263158
step: 1600, Loss: 0.11970407515764236 Acc:1.0
step: 1700, Loss: 0.11702367663383484 Acc:1.0
step: 1800, Loss: 0.11675086617469788 Acc:1.0
step: 1900, Loss: 10.211942672729492 Acc:0.9210526315789473
step: 2000, Loss: 0.11801838874816895 Acc:1.0
step: 2100, Loss: 3.802865743637085 Acc:0.9736842105263158
step: 2200, Loss: 2.5258734226226807 Acc:0.9736842105263158
step: 2300, Loss: 0.1218266487121582 Acc:1.0
step: 2400, Loss: 0.12120480090379715 Acc:1.0
step: 2500, Loss: 3.231776714324951 Acc:0.9736842105263158
step: 2600, Loss: 0.12107205390930176 Acc:1.0
step: 2700, Loss: 0.11951857060194016 Acc:1.0
step: 2800, Loss: 0.11990342289209366 Acc:1.0
step: 2900, Loss: 0.11965902894735336 Acc:1.0
step: 3000, Loss: 0.11322640627622604 Acc:1.0
step: 3100, Loss: 0.11982449889183044 Acc:1.0
step: 3200, Loss: 0.11850512027740479 Acc:1.0
step: 3300, Loss: 11.156378746032715 Acc:0.9210526315789473
step: 3400, Loss: 0.12261226028203964 Acc:1.0
step: 3500, Loss: 0.11937205493450165 Acc:1.0
step: 3600, Loss: 1.029834508895874 Acc:0.9736842105263158
step: 3700, Loss: 0.12588153779506683 Acc:1.0
step: 3800, Loss: 0.12132351845502853 Acc:1.0
step: 3900, Loss: 0.13213352859020233 Acc:1.0
step: 4000, Loss: 0.11458180844783783 Acc:1.0
step: 4100, Loss: 0.11748102307319641 Acc:1.0
step: 4200, Loss: 0.12154915183782578 Acc:1.0
step: 4300, Loss: 0.11590687930583954 Acc:1.0
step: 4400, Loss: 0.11480537801980972 Acc:1.0
step: 4500, Loss: 0.11561594903469086 Acc:1.0
step: 4600, Loss: 14.31659984588623 Acc:0.8947368421052632
step: 4700, Loss: 0.11645322293043137 Acc:1.0
step: 4800, Loss: 0.118467777967453 Acc:1.0
step: 4900, Loss: 9.883357048034668 Acc:0.9210526315789473
training successfully ended.
validating...
validate data length:114
acc: 0.9385964912280702
precision: 0.8852459016393442
recall: 1.0
F_score: 0.9391304347826086
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave8/Valence-norm-lr5e-05DEAP10_38_5000_scebaseseed74fold_best_model.pth
Training... train_data length:1026
step: 0, Loss: 0.11813859641551971 Acc:1.0
step: 100, Loss: 6.56749963760376 Acc:0.9473684210526315
step: 200, Loss: 6.989385604858398 Acc:0.9473684210526315
step: 300, Loss: 2.793097734451294 Acc:0.9736842105263158
step: 400, Loss: 0.11729378998279572 Acc:1.0
step: 500, Loss: 0.12215737998485565 Acc:1.0
step: 600, Loss: 0.11816650629043579 Acc:1.0
step: 700, Loss: 0.12072943150997162 Acc:1.0
step: 800, Loss: 0.12469062954187393 Acc:1.0
step: 900, Loss: 0.11680932343006134 Acc:1.0
step: 1000, Loss: 1.094297170639038 Acc:0.9736842105263158
step: 1100, Loss: 0.11617924273014069 Acc:1.0
step: 1200, Loss: 0.2903909981250763 Acc:1.0
step: 1300, Loss: 0.11988936364650726 Acc:1.0
step: 1400, Loss: 0.11856622993946075 Acc:1.0
step: 1500, Loss: 0.11718486249446869 Acc:1.0
step: 1600, Loss: 0.11800962686538696 Acc:1.0
step: 1700, Loss: 0.11756373941898346 Acc:1.0
step: 1800, Loss: 0.1202254444360733 Acc:1.0
step: 1900, Loss: 0.11607692390680313 Acc:1.0
step: 2000, Loss: 0.11689311265945435 Acc:1.0
step: 2100, Loss: 2.6623902320861816 Acc:0.9736842105263158
step: 2200, Loss: 0.12462416291236877 Acc:1.0
step: 2300, Loss: 7.972620964050293 Acc:0.9473684210526315
step: 2400, Loss: 0.13205842673778534 Acc:1.0
step: 2500, Loss: 3.8899612426757812 Acc:0.9736842105263158
step: 2600, Loss: 0.11563606560230255 Acc:1.0
step: 2700, Loss: 0.11562395095825195 Acc:1.0
