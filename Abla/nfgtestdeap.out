/home/sjf/eegall/main.py:997: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,0],dtype=int)
/home/sjf/eegall/main.py:999: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  use_label = torch.tensor(all_labels[i,:,1],dtype=int)
/home/sjf/eegall/main.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  base_val_data = torch.tensor(base_val_data)
/home/sjf/eegall/main.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  harm_val_data = torch.tensor(harm_val_data)
/home/sjf/eegall/main.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_graph = torch.tensor(val_graph)
/home/sjf/eegall/main.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_labels = torch.tensor(val_labels)
--------- DEAP DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([32, 760, 40, 7]), harm_x: torch.Size([32, 760, 40, 7]) all_labels: torch.Size([32, 760, 4]) 
 base_graph: torch.Size([32, 760, 40, 40]) harm_graph: torch.Size([32, 760, 40, 40])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave9//home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[380, 380]
******fold 1******

*******Initializing new model*******
Training... train_data length:684
step: 0, Loss: 3.3916566213284506e+34 Acc:0.5277777777777778
step: 100, Loss: 3.634060879065023e+34 Acc:0.5277777777777778
step: 200, Loss: 3.2944691674882575e+34 Acc:0.4166666666666667
step: 300, Loss: 2.844130844821006e+34 Acc:0.5277777777777778
step: 400, Loss: 3.5034757836689733e+34 Acc:0.4444444444444444
step: 500, Loss: 2.8952580160315003e+34 Acc:0.4722222222222222
step: 600, Loss: 3.345317059425889e+34 Acc:0.4444444444444444
step: 700, Loss: 2.6599154634550273e+34 Acc:0.4722222222222222
step: 800, Loss: 2.777948342028739e+34 Acc:0.5277777777777778
step: 900, Loss: 2.7501419804543187e+34 Acc:0.4166666666666667
step: 1000, Loss: 2.7378200204792877e+34 Acc:0.5
step: 1100, Loss: 2.5773032680493725e+34 Acc:0.5277777777777778
step: 1200, Loss: 3.239920082421171e+34 Acc:0.3611111111111111
step: 1300, Loss: 1.9634991721906202e+34 Acc:0.4444444444444444
step: 1400, Loss: 2.5461027224752395e+34 Acc:0.4722222222222222
step: 1500, Loss: 3.0677644653299023e+34 Acc:0.5555555555555556
step: 1600, Loss: 2.602106882264502e+34 Acc:0.5
step: 1700, Loss: 3.2602631510867475e+34 Acc:0.5555555555555556
step: 1800, Loss: 3.402670078681957e+34 Acc:0.4722222222222222
step: 1900, Loss: 3.3916566213284506e+34 Acc:0.5277777777777778
step: 2000, Loss: 3.634060879065023e+34 Acc:0.5277777777777778
step: 2100, Loss: 3.2944691674882575e+34 Acc:0.4166666666666667
step: 2200, Loss: 2.844130844821006e+34 Acc:0.5277777777777778
step: 2300, Loss: 3.5034757836689733e+34 Acc:0.4444444444444444
step: 2400, Loss: 2.8952580160315003e+34 Acc:0.4722222222222222
step: 2500, Loss: 3.345317059425889e+34 Acc:0.4444444444444444
step: 2600, Loss: 2.6599154634550273e+34 Acc:0.4722222222222222
step: 2700, Loss: 2.777948342028739e+34 Acc:0.5277777777777778
step: 2800, Loss: 2.7501419804543187e+34 Acc:0.4166666666666667
step: 2900, Loss: 2.7378200204792877e+34 Acc:0.5
step: 3000, Loss: 2.5773032680493725e+34 Acc:0.5277777777777778
step: 3100, Loss: 3.239920082421171e+34 Acc:0.3611111111111111
step: 3200, Loss: 1.9634991721906202e+34 Acc:0.4444444444444444
step: 3300, Loss: 2.5461027224752395e+34 Acc:0.4722222222222222
step: 3400, Loss: 3.0677644653299023e+34 Acc:0.5555555555555556
step: 3500, Loss: 2.602106882264502e+34 Acc:0.5
step: 3600, Loss: 3.2602631510867475e+34 Acc:0.5555555555555556
step: 3700, Loss: 3.402670078681957e+34 Acc:0.4722222222222222
step: 3800, Loss: 3.3916566213284506e+34 Acc:0.5277777777777778
step: 3900, Loss: 3.634060879065023e+34 Acc:0.5277777777777778
step: 4000, Loss: 3.2944691674882575e+34 Acc:0.4166666666666667
step: 4100, Loss: 2.844130844821006e+34 Acc:0.5277777777777778
step: 4200, Loss: 3.5034757836689733e+34 Acc:0.4444444444444444
step: 4300, Loss: 2.8952580160315003e+34 Acc:0.4722222222222222
step: 4400, Loss: 3.345317059425889e+34 Acc:0.4444444444444444
step: 4500, Loss: 2.6599154634550273e+34 Acc:0.4722222222222222
step: 4600, Loss: 2.777948342028739e+34 Acc:0.5277777777777778
step: 4700, Loss: 2.7501419804543187e+34 Acc:0.4166666666666667
step: 4800, Loss: 2.7378200204792877e+34 Acc:0.5
step: 4900, Loss: 2.5773032680493725e+34 Acc:0.5277777777777778
training successfully ended.
validating...
validate data length:76
acc: 0.5
precision: 0.5
recall: 0.9166666666666666
F_score: 0.6470588235294118
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 3.1808644004350696e+34 Acc:0.4722222222222222
step: 100, Loss: 3.634932636440688e+34 Acc:0.5
step: 200, Loss: 3.5488393536445625e+34 Acc:0.4444444444444444
step: 300, Loss: 2.6405238755036454e+34 Acc:0.5277777777777778
step: 400, Loss: 3.96533833774994e+34 Acc:0.3888888888888889
step: 500, Loss: 2.6797499863524655e+34 Acc:0.5
step: 600, Loss: 3.027235546383738e+34 Acc:0.5
step: 700, Loss: 3.1552506782582315e+34 Acc:0.5
step: 800, Loss: 3.084313495363077e+34 Acc:0.5
step: 900, Loss: 2.673635800498435e+34 Acc:0.4444444444444444
step: 1000, Loss: 2.911597586610028e+34 Acc:0.5
step: 1100, Loss: 2.825779126502624e+34 Acc:0.5
step: 1200, Loss: 2.5915081348241565e+34 Acc:0.4444444444444444
step: 1300, Loss: 2.259836268442723e+34 Acc:0.3611111111111111
step: 1400, Loss: 2.4995381030735356e+34 Acc:0.4722222222222222
step: 1500, Loss: 3.3897066181785683e+34 Acc:0.5277777777777778
step: 1600, Loss: 2.8144579172553513e+34 Acc:0.5277777777777778
step: 1700, Loss: 3.6149785285354546e+34 Acc:0.4722222222222222
step: 1800, Loss: 2.664700844470889e+34 Acc:0.5555555555555556
step: 1900, Loss: 3.1808644004350696e+34 Acc:0.4722222222222222
step: 2000, Loss: 3.634932636440688e+34 Acc:0.5
step: 2100, Loss: 3.5488393536445625e+34 Acc:0.4444444444444444
step: 2200, Loss: 2.6405238755036454e+34 Acc:0.5277777777777778
step: 2300, Loss: 3.96533833774994e+34 Acc:0.3888888888888889
step: 2400, Loss: 2.6797499863524655e+34 Acc:0.5
step: 2500, Loss: 3.027235546383738e+34 Acc:0.5
step: 2600, Loss: 3.1552506782582315e+34 Acc:0.5
step: 2700, Loss: 3.084313495363077e+34 Acc:0.5
step: 2800, Loss: 2.673635800498435e+34 Acc:0.4444444444444444
step: 2900, Loss: 2.911597586610028e+34 Acc:0.5
step: 3000, Loss: 2.825779126502624e+34 Acc:0.5
step: 3100, Loss: 2.5915081348241565e+34 Acc:0.4444444444444444
step: 3200, Loss: 2.259836268442723e+34 Acc:0.3611111111111111
step: 3300, Loss: 2.4995381030735356e+34 Acc:0.4722222222222222
step: 3400, Loss: 3.3897066181785683e+34 Acc:0.5277777777777778
step: 3500, Loss: 2.8144579172553513e+34 Acc:0.5277777777777778
step: 3600, Loss: 3.6149785285354546e+34 Acc:0.4722222222222222
step: 3700, Loss: 2.664700844470889e+34 Acc:0.5555555555555556
step: 3800, Loss: 3.1808644004350696e+34 Acc:0.4722222222222222
step: 3900, Loss: 3.634932636440688e+34 Acc:0.5
step: 4000, Loss: 3.5488393536445625e+34 Acc:0.4444444444444444
step: 4100, Loss: 2.6405238755036454e+34 Acc:0.5277777777777778
step: 4200, Loss: 3.96533833774994e+34 Acc:0.3888888888888889
step: 4300, Loss: 2.6797499863524655e+34 Acc:0.5
step: 4400, Loss: 3.027235546383738e+34 Acc:0.5
step: 4500, Loss: 3.1552506782582315e+34 Acc:0.5
step: 4600, Loss: 3.084313495363077e+34 Acc:0.5
step: 4700, Loss: 2.673635800498435e+34 Acc:0.4444444444444444
step: 4800, Loss: 2.911597586610028e+34 Acc:0.5
step: 4900, Loss: 2.825779126502624e+34 Acc:0.5
training successfully ended.
validating...
validate data length:76
acc: 0.5277777777777778
precision: 0.55
recall: 0.825
F_score: 0.66
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 3.3762046537580905e+34 Acc:0.4722222222222222
step: 100, Loss: 3.2163194983362186e+34 Acc:0.5833333333333334
step: 200, Loss: 3.609259493141964e+34 Acc:0.4444444444444444
step: 300, Loss: 2.4836508757853627e+34 Acc:0.5277777777777778
step: 400, Loss: 3.8795609772518404e+34 Acc:0.4166666666666667
step: 500, Loss: 2.7303730683789626e+34 Acc:0.4444444444444444
step: 600, Loss: 2.9095096769397693e+34 Acc:0.4444444444444444
step: 700, Loss: 3.182961470861619e+34 Acc:0.4166666666666667
step: 800, Loss: 3.0052034323285606e+34 Acc:0.5
step: 900, Loss: 2.895459800257904e+34 Acc:0.3611111111111111
step: 1000, Loss: 2.5176023717147957e+34 Acc:0.5555555555555556
step: 1100, Loss: 2.577936102997455e+34 Acc:0.5277777777777778
step: 1200, Loss: 2.845852819415652e+34 Acc:0.4444444444444444
step: 1300, Loss: 1.9531121124969923e+34 Acc:0.5
--------- DEAP DATA ---------

*********** ALL Loaded data ***************

base_x: torch.Size([32, 760, 40, 7]), harm_x: torch.Size([32, 760, 40, 7]) all_labels: torch.Size([32, 760, 4]) 
 base_graph: torch.Size([32, 760, 40, 40]) harm_graph: torch.Size([32, 760, 40, 40])

this is with limit version
Uing base graph and base feature for Time Graph part and harm feature for encoding!
before val model path:/home/sjf/eegall/intermodel/modelsave9//home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
******** mix subject_0 ********

[228, 532]
******fold 1******

*******Initializing new model*******
Training... train_data length:957
step: 0, Loss: 13.213092803955078 Acc:0.3888888888888889
step: 100, Loss: 5.71481466293335 Acc:0.8611111111111112
step: 200, Loss: 2.0940568447113037 Acc:0.9444444444444444
step: 300, Loss: 6.05350399017334 Acc:0.9166666666666666
step: 400, Loss: 0.20439371466636658 Acc:1.0
step: 500, Loss: 0.17357799410820007 Acc:1.0
step: 600, Loss: 2.0320048332214355 Acc:0.9722222222222222
step: 700, Loss: 0.154093399643898 Acc:1.0
step: 800, Loss: 0.15029974281787872 Acc:1.0
step: 900, Loss: 0.1355469524860382 Acc:1.0
step: 1000, Loss: 0.18074029684066772 Acc:1.0
step: 1100, Loss: 0.1361941397190094 Acc:1.0
step: 1200, Loss: 0.1321961134672165 Acc:1.0
step: 1300, Loss: 0.12877336144447327 Acc:1.0
step: 1400, Loss: 0.12793952226638794 Acc:1.0
step: 1500, Loss: 0.1294942945241928 Acc:1.0
step: 1600, Loss: 0.12757140398025513 Acc:1.0
step: 1700, Loss: 0.18822351098060608 Acc:1.0
step: 1800, Loss: 0.13045789301395416 Acc:1.0
step: 1900, Loss: 0.12478099018335342 Acc:1.0
step: 2000, Loss: 0.12132487446069717 Acc:1.0
step: 2100, Loss: 0.14285112917423248 Acc:1.0
step: 2200, Loss: 0.12169675529003143 Acc:1.0
step: 2300, Loss: 0.11850952357053757 Acc:1.0
step: 2400, Loss: 0.11665727198123932 Acc:1.0
step: 2500, Loss: 0.1169465109705925 Acc:1.0
step: 2600, Loss: 0.11728057265281677 Acc:1.0
step: 2700, Loss: 0.1182529479265213 Acc:1.0
step: 2800, Loss: 0.11564812064170837 Acc:1.0
step: 2900, Loss: 0.11568477004766464 Acc:1.0
step: 3000, Loss: 0.11529373377561569 Acc:1.0
step: 3100, Loss: 0.11499860137701035 Acc:1.0
step: 3200, Loss: 0.2291806936264038 Acc:1.0
step: 3300, Loss: 0.2870408892631531 Acc:1.0
step: 3400, Loss: 0.17952078580856323 Acc:1.0
step: 3500, Loss: 0.1466006338596344 Acc:1.0
step: 3600, Loss: 0.1320635825395584 Acc:1.0
step: 3700, Loss: 0.1547900140285492 Acc:1.0
step: 3800, Loss: 0.13368746638298035 Acc:1.0
step: 3900, Loss: 0.12386046350002289 Acc:1.0
step: 4000, Loss: 0.12390189617872238 Acc:1.0
step: 4100, Loss: 0.12087105959653854 Acc:1.0
step: 4200, Loss: 0.12087971717119217 Acc:1.0
step: 4300, Loss: 0.11967980861663818 Acc:1.0
step: 4400, Loss: 0.1878238022327423 Acc:1.0
step: 4500, Loss: 0.11751106381416321 Acc:1.0
step: 4600, Loss: 0.11865004897117615 Acc:1.0
step: 4700, Loss: 0.11900489032268524 Acc:1.0
step: 4800, Loss: 0.1165689155459404 Acc:1.0
step: 4900, Loss: 0.1172114685177803 Acc:1.0
training successfully ended.
validating...
validate data length:107
acc: 0.6862745098039216
precision: 0.7307692307692307
recall: 0.6785714285714286
F_score: 0.7037037037037038
******fold 2******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:957
step: 0, Loss: 13.925384521484375 Acc:0.8611111111111112
step: 100, Loss: 1.8325285911560059 Acc:0.9722222222222222
step: 200, Loss: 0.33127546310424805 Acc:1.0
step: 300, Loss: 0.27588698267936707 Acc:1.0
step: 400, Loss: 0.29608026146888733 Acc:1.0
step: 500, Loss: 0.38104379177093506 Acc:1.0
step: 600, Loss: 0.15082092583179474 Acc:1.0
step: 700, Loss: 0.14429324865341187 Acc:1.0
step: 800, Loss: 0.15498940646648407 Acc:1.0
step: 900, Loss: 0.1838974505662918 Acc:1.0
step: 1000, Loss: 0.1362200379371643 Acc:1.0
step: 1100, Loss: 0.12916064262390137 Acc:1.0
step: 1200, Loss: 0.14153550565242767 Acc:1.0
step: 1300, Loss: 0.12917090952396393 Acc:1.0
step: 1400, Loss: 0.12540148198604584 Acc:1.0
step: 1500, Loss: 0.1398819088935852 Acc:1.0
step: 1600, Loss: 0.12186352163553238 Acc:1.0
step: 1700, Loss: 0.20214399695396423 Acc:1.0
step: 1800, Loss: 0.12281106412410736 Acc:1.0
step: 1900, Loss: 0.1187237948179245 Acc:1.0
step: 2000, Loss: 0.12002021819353104 Acc:1.0
step: 2100, Loss: 0.11870063096284866 Acc:1.0
step: 2200, Loss: 0.11969002336263657 Acc:1.0
step: 2300, Loss: 0.11858892440795898 Acc:1.0
step: 2400, Loss: 0.11636243760585785 Acc:1.0
step: 2500, Loss: 0.11785292625427246 Acc:1.0
step: 2600, Loss: 0.11564493179321289 Acc:1.0
step: 2700, Loss: 0.12895691394805908 Acc:1.0
step: 2800, Loss: 0.11574333161115646 Acc:1.0
step: 2900, Loss: 0.11563369631767273 Acc:1.0
step: 3000, Loss: 0.11513842642307281 Acc:1.0
step: 3100, Loss: 0.11544998735189438 Acc:1.0
step: 3200, Loss: 0.1149652898311615 Acc:1.0
step: 3300, Loss: 0.11484064161777496 Acc:1.0
step: 3400, Loss: 0.11462216079235077 Acc:1.0
step: 3500, Loss: 0.1145576760172844 Acc:1.0
step: 3600, Loss: 0.11468564718961716 Acc:1.0
step: 3700, Loss: 0.11434558779001236 Acc:1.0
step: 3800, Loss: 0.11405662447214127 Acc:1.0
step: 3900, Loss: 2.73796010017395 Acc:0.9444444444444444
step: 4000, Loss: 0.16618448495864868 Acc:1.0
step: 4100, Loss: 0.1735648512840271 Acc:1.0
step: 4200, Loss: 0.15219134092330933 Acc:1.0
step: 4300, Loss: 0.12806622684001923 Acc:1.0
step: 4400, Loss: 0.20585298538208008 Acc:1.0
step: 4500, Loss: 0.1248617097735405 Acc:1.0
step: 4600, Loss: 0.12158477306365967 Acc:1.0
step: 4700, Loss: 0.11956896632909775 Acc:1.0
step: 4800, Loss: 0.11895404756069183 Acc:1.0
step: 4900, Loss: 0.1230158731341362 Acc:1.0
training successfully ended.
validating...
validate data length:107
acc: 0.7352941176470589
precision: 0.6226415094339622
recall: 0.825
F_score: 0.7096774193548386
******fold 3******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationArousal-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:957
step: 0, Loss: 8.900976181030273 Acc:0.9166666666666666
step: 100, Loss: 0.9514782428741455 Acc:1.0
step: 200, Loss: 2.476856231689453 Acc:0.9722222222222222
step: 300, Loss: 0.17948812246322632 Acc:1.0
step: 400, Loss: 0.22462382912635803 Acc:1.0
step: 500, Loss: 0.16289421916007996 Acc:1.0
step: 600, Loss: 0.14925779402256012 Acc:1.0
step: 700, Loss: 0.1431514471769333 Acc:1.0
step: 800, Loss: 0.13274578750133514 Acc:1.0
step: 900, Loss: 0.13090674579143524 Acc:1.0
step: 1000, Loss: 0.12783557176589966 Acc:1.0
step: 1100, Loss: 0.13106362521648407 Acc:1.0
step: 1200, Loss: 0.1232459768652916 Acc:1.0
step: 1300, Loss: 0.12426048517227173 Acc:1.0
step: 1400, Loss: 0.12218423187732697 Acc:1.0
step: 1500, Loss: 0.12369676679372787 Acc:1.0
step: 1600, Loss: 0.11980810761451721 Acc:1.0
step: 1700, Loss: 0.1874256730079651 Acc:1.0
step: 1800, Loss: 0.1176934689283371 Acc:1.0
step: 1900, Loss: 0.1208416074514389 Acc:1.0
step: 2000, Loss: 0.11810725182294846 Acc:1.0
step: 2100, Loss: 0.11754234880208969 Acc:1.0
step: 2200, Loss: 0.11607806384563446 Acc:1.0
step: 2300, Loss: 0.2627730071544647 Acc:1.0
step: 2400, Loss: 0.18797940015792847 Acc:1.0
step: 2500, Loss: 0.1416856050491333 Acc:1.0
step: 2600, Loss: 0.12800443172454834 Acc:1.0
step: 2700, Loss: 0.1299808919429779 Acc:1.0
step: 2800, Loss: 0.12753444910049438 Acc:1.0
step: 2900, Loss: 0.12896478176116943 Acc:1.0
step: 3000, Loss: 0.12172697484493256 Acc:1.0
step: 3100, Loss: 0.12353742122650146 Acc:1.0
step: 3200, Loss: 0.11930889636278152 Acc:1.0
step: 3300, Loss: 0.12226743251085281 Acc:1.0
step: 3400, Loss: 0.11985266208648682 Acc:1.0
step: 3500, Loss: 0.11802349984645844 Acc:1.0
step: 3600, Loss: 0.11594342440366745 Acc:1.0
step: 3700, Loss: 0.1161675900220871 Acc:1.0
step: 3800, Loss: 0.11682799458503723 Acc:1.0
step: 3900, Loss: 0.11483757197856903 Acc:1.0
step: 4000, Loss: 0.11612673103809357 Acc:1.0
step: 4100, Loss: 0.11522654443979263 Acc:1.0
step: 4200, Loss: 0.11502143740653992 Acc:1.0
step: 4300, Loss: 0.11532503366470337 Acc:1.0
step: 4400, Loss: 0.18071472644805908 Acc:1.0
step: 1400, Loss: 2.7685432163742723e+34 Acc:0.5
step: 1500, Loss: 3.1527131487657044e+34 Acc:0.5277777777777778
step: 1600, Loss: 3.1411049850173253e+34 Acc:0.4722222222222222
step: 1700, Loss: 3.5322162947970384e+34 Acc:0.4444444444444444
step: 1800, Loss: 2.752400478261991e+34 Acc:0.5
step: 1900, Loss: 3.3762046537580905e+34 Acc:0.4722222222222222
step: 2000, Loss: 3.2163194983362186e+34 Acc:0.5833333333333334
step: 2100, Loss: 3.609259493141964e+34 Acc:0.4444444444444444
step: 2200, Loss: 2.4836508757853627e+34 Acc:0.5277777777777778
step: 2300, Loss: 3.8795609772518404e+34 Acc:0.4166666666666667
step: 2400, Loss: 2.7303730683789626e+34 Acc:0.4444444444444444
step: 2500, Loss: 2.9095096769397693e+34 Acc:0.4444444444444444
step: 2600, Loss: 3.182961470861619e+34 Acc:0.4166666666666667
step: 2700, Loss: 3.0052034323285606e+34 Acc:0.5
step: 2800, Loss: 2.895459800257904e+34 Acc:0.3611111111111111
step: 2900, Loss: 2.5176023717147957e+34 Acc:0.5555555555555556
step: 3000, Loss: 2.577936102997455e+34 Acc:0.5277777777777778
step: 3100, Loss: 2.845852819415652e+34 Acc:0.4444444444444444
step: 3200, Loss: 1.9531121124969923e+34 Acc:0.5
step: 3300, Loss: 2.7685432163742723e+34 Acc:0.5
step: 3400, Loss: 3.1527131487657044e+34 Acc:0.5277777777777778
step: 3500, Loss: 3.1411049850173253e+34 Acc:0.4722222222222222
step: 3600, Loss: 3.5322162947970384e+34 Acc:0.4444444444444444
step: 3700, Loss: 2.752400478261991e+34 Acc:0.5
step: 3800, Loss: 3.3762046537580905e+34 Acc:0.4722222222222222
step: 3900, Loss: 3.2163194983362186e+34 Acc:0.5833333333333334
step: 4000, Loss: 3.609259493141964e+34 Acc:0.4444444444444444
step: 4100, Loss: 2.4836508757853627e+34 Acc:0.5277777777777778
step: 4200, Loss: 3.8795609772518404e+34 Acc:0.4166666666666667
step: 4300, Loss: 2.7303730683789626e+34 Acc:0.4444444444444444
step: 4400, Loss: 2.9095096769397693e+34 Acc:0.4444444444444444
step: 4500, Loss: 3.182961470861619e+34 Acc:0.4166666666666667
step: 4600, Loss: 3.0052034323285606e+34 Acc:0.5
step: 4700, Loss: 2.895459800257904e+34 Acc:0.3611111111111111
step: 4800, Loss: 2.5176023717147957e+34 Acc:0.5555555555555556
step: 4900, Loss: 2.577936102997455e+34 Acc:0.5277777777777778
training successfully ended.
validating...
validate data length:76
acc: 0.5416666666666666
precision: 0.5522388059701493
recall: 0.925
F_score: 0.6915887850467289
******fold 4******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 3.441833797352797e+34 Acc:0.5
step: 100, Loss: 3.1216455579517906e+34 Acc:0.5833333333333334
step: 200, Loss: 3.3159154883168453e+34 Acc:0.4722222222222222
step: 300, Loss: 2.608246322095334e+34 Acc:0.5555555555555556
step: 400, Loss: 3.457720529464954e+34 Acc:0.4444444444444444
step: 500, Loss: 2.307109237134906e+34 Acc:0.4722222222222222
step: 600, Loss: 3.617591819958386e+34 Acc:0.4166666666666667
step: 700, Loss: 2.856753129049568e+34 Acc:0.4722222222222222
step: 800, Loss: 3.6270499294465342e+34 Acc:0.4166666666666667
step: 900, Loss: 2.6326956378712204e+34 Acc:0.4166666666666667
step: 1000, Loss: 2.883797167147796e+34 Acc:0.5555555555555556
step: 1100, Loss: 2.807278360203512e+34 Acc:0.5
step: 1200, Loss: 2.6056424390167013e+34 Acc:0.5277777777777778
step: 1300, Loss: 2.494307558819547e+34 Acc:0.4444444444444444
step: 1400, Loss: 2.2807978119519268e+34 Acc:0.5555555555555556
step: 1500, Loss: 2.906478704547583e+34 Acc:0.5555555555555556
step: 1600, Loss: 3.145349386236019e+34 Acc:0.5
step: 1700, Loss: 3.0883823566842003e+34 Acc:0.5277777777777778
step: 1800, Loss: 2.8656024196263954e+34 Acc:0.5
step: 1900, Loss: 3.441833797352797e+34 Acc:0.5
step: 2000, Loss: 3.1216455579517906e+34 Acc:0.5833333333333334
step: 2100, Loss: 3.3159154883168453e+34 Acc:0.4722222222222222
step: 2200, Loss: 2.608246322095334e+34 Acc:0.5555555555555556
step: 2300, Loss: 3.457720529464954e+34 Acc:0.4444444444444444
step: 2400, Loss: 2.307109237134906e+34 Acc:0.4722222222222222
step: 2500, Loss: 3.617591819958386e+34 Acc:0.4166666666666667
step: 2600, Loss: 2.856753129049568e+34 Acc:0.4722222222222222
step: 2700, Loss: 3.6270499294465342e+34 Acc:0.4166666666666667
step: 2800, Loss: 2.6326956378712204e+34 Acc:0.4166666666666667
step: 2900, Loss: 2.883797167147796e+34 Acc:0.5555555555555556
step: 3000, Loss: 2.807278360203512e+34 Acc:0.5
step: 3100, Loss: 2.6056424390167013e+34 Acc:0.5277777777777778
step: 3200, Loss: 2.494307558819547e+34 Acc:0.4444444444444444
step: 3300, Loss: 2.2807978119519268e+34 Acc:0.5555555555555556
step: 3400, Loss: 2.906478704547583e+34 Acc:0.5555555555555556
step: 3500, Loss: 3.145349386236019e+34 Acc:0.5
step: 3600, Loss: 3.0883823566842003e+34 Acc:0.5277777777777778
step: 3700, Loss: 2.8656024196263954e+34 Acc:0.5
step: 3800, Loss: 3.441833797352797e+34 Acc:0.5
step: 3900, Loss: 3.1216455579517906e+34 Acc:0.5833333333333334
step: 4000, Loss: 3.3159154883168453e+34 Acc:0.4722222222222222
step: 4100, Loss: 2.608246322095334e+34 Acc:0.5555555555555556
step: 4200, Loss: 3.457720529464954e+34 Acc:0.4444444444444444
step: 4300, Loss: 2.307109237134906e+34 Acc:0.4722222222222222
step: 4400, Loss: 3.617591819958386e+34 Acc:0.4166666666666667
step: 4500, Loss: 2.856753129049568e+34 Acc:0.4722222222222222
step: 4600, Loss: 3.6270499294465342e+34 Acc:0.4166666666666667
step: 4700, Loss: 2.6326956378712204e+34 Acc:0.4166666666666667
step: 4800, Loss: 2.883797167147796e+34 Acc:0.5555555555555556
step: 4900, Loss: 2.807278360203512e+34 Acc:0.5
training successfully ended.
validating...
validate data length:76
acc: 0.3611111111111111
precision: 0.3870967741935484
recall: 0.75
F_score: 0.5106382978723403
******fold 5******

in cross val model path:/home/sjf/eegall/intermodel/modelsave9/AblationValence-norm-lr5e-05DEAP10_36_5000_scebaseseed74fold_best_model.pth
Training... train_data length:684
step: 0, Loss: 2.2257629592134245e+34 Acc:0.5555555555555556
step: 100, Loss: 2.8775136310963915e+34 Acc:0.5555555555555556
step: 200, Loss: 3.597308667590711e+34 Acc:0.4166666666666667
step: 300, Loss: 2.532291768220956e+34 Acc:0.5277777777777778
step: 400, Loss: 3.4821210704032926e+34 Acc:0.4722222222222222
step: 500, Loss: 2.313312802259773e+34 Acc:0.4444444444444444
step: 600, Loss: 2.9372120515508896e+34 Acc:0.5277777777777778
step: 700, Loss: 3.024994132148608e+34 Acc:0.4722222222222222
step: 800, Loss: 3.4322553600928304e+34 Acc:0.4444444444444444
step: 900, Loss: 2.7730245593164854e+34 Acc:0.3888888888888889
step: 1000, Loss: 3.043718470418815e+34 Acc:0.4444444444444444
step: 1100, Loss: 3.0167922842123267e+34 Acc:0.4722222222222222
step: 1200, Loss: 2.7662114325162744e+34 Acc:0.4444444444444444
step: 1300, Loss: 2.464893360722103e+34 Acc:0.4444444444444444
step: 1400, Loss: 2.639483758282638e+34 Acc:0.5277777777777778
step: 1500, Loss: 2.4970944094359863e+34 Acc:0.5833333333333334
step: 1600, Loss: 2.761811793616654e+34 Acc:0.5
step: 1700, Loss: 3.694568417129198e+34 Acc:0.5
step: 1800, Loss: 2.801218148535194e+34 Acc:0.5
step: 1900, Loss: 2.2257629592134245e+34 Acc:0.5555555555555556
step: 2000, Loss: 2.8775136310963915e+34 Acc:0.5555555555555556
step: 2100, Loss: 3.597308667590711e+34 Acc:0.4166666666666667
step: 2200, Loss: 2.532291768220956e+34 Acc:0.5277777777777778
step: 2300, Loss: 3.4821210704032926e+34 Acc:0.4722222222222222
step: 2400, Loss: 2.313312802259773e+34 Acc:0.4444444444444444
step: 2500, Loss: 2.9372120515508896e+34 Acc:0.5277777777777778
step: 2600, Loss: 3.024994132148608e+34 Acc:0.4722222222222222
step: 2700, Loss: 3.4322553600928304e+34 Acc:0.4444444444444444
step: 2800, Loss: 2.7730245593164854e+34 Acc:0.3888888888888889
step: 2900, Loss: 3.043718470418815e+34 Acc:0.4444444444444444
step: 3000, Loss: 3.0167922842123267e+34 Acc:0.4722222222222222
step: 3100, Loss: 2.7662114325162744e+34 Acc:0.4444444444444444
step: 3200, Loss: 2.464893360722103e+34 Acc:0.4444444444444444
step: 3300, Loss: 2.639483758282638e+34 Acc:0.5277777777777778
step: 3400, Loss: 2.4970944094359863e+34 Acc:0.5833333333333334
step: 3500, Loss: 2.761811793616654e+34 Acc:0.5
step: 3600, Loss: 3.694568417129198e+34 Acc:0.5
